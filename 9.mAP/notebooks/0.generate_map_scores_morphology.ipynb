{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import itertools\n",
                "import logging\n",
                "import pathlib\n",
                "import sys\n",
                "from typing import Optional\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import toml\n",
                "from copairs.map import run_pipeline\n",
                "from pycytominer import feature_select\n",
                "\n",
                "# imports src\n",
                "sys.path.append(\"../\")\n",
                "from src import utils\n",
                "\n",
                "# setting up logger\n",
                "logging.basicConfig(\n",
                "    filename=\"map_analysis_testing.log\",\n",
                "    level=logging.DEBUG,\n",
                "    format=\"%(levelname)s:%(asctime)s:%(name)s:%(message)s\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper functions\n",
                "Set of helper functions to help out throughout the notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Helper function\n",
                "\n",
                "\n",
                "def shuffle_meta_labels(\n",
                "    dataset: pd.DataFrame, target_col: str, seed: Optional[int] = 0\n",
                ") -> pd.DataFrame:\n",
                "    \"\"\"shuffles labels or values within a single selected column\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    dataset : pd.DataFrame\n",
                "        dataframe containing the dataset\n",
                "\n",
                "    target_col : str\n",
                "        Column to select in order to conduct the shuffling\n",
                "\n",
                "    seed : int\n",
                "        setting random seed\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    pd.DataFrame\n",
                "        shuffled dataset\n",
                "\n",
                "    Raises\n",
                "    ------\n",
                "    TypeError\n",
                "        raised if incorrect types are provided\n",
                "    \"\"\"\n",
                "    # setting seed\n",
                "    np.random.seed(seed)\n",
                "\n",
                "    # type checking\n",
                "    if not isinstance(target_col, str):\n",
                "        raise TypeError(\"'target_col' must be a string type\")\n",
                "    if not isinstance(dataset, pd.DataFrame):\n",
                "        raise TypeError(\"'dataset' must be a pandas dataframe\")\n",
                "\n",
                "    # selecting column, shuffle values within column, add to dataframe\n",
                "    dataset[target_col] = np.random.permutation(dataset[target_col].values)\n",
                "    return dataset\n",
                "\n",
                "\n",
                "def shuffle_features(feature_vals: np.array, seed: Optional[int] = 0) -> np.array:\n",
                "    \"\"\"suffles all values within feature space\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    feature_vals : np.array\n",
                "        shuffled\n",
                "\n",
                "    seed : Optional[int]\n",
                "        setting random seed\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.array\n",
                "        Returns shuffled values within the feature space\n",
                "\n",
                "    Raisespairs(sameby=pos_s\n",
                "    TypeError\n",
                "        Raised if a numpy array is not provided\n",
                "    \"\"\"\n",
                "    # setting seed\n",
                "    np.random.seed(seed)\n",
                "\n",
                "    # shuffle given array\n",
                "    if not isinstance(feature_vals, np.ndarray):\n",
                "        raise TypeError(\"'feature_vals' must be a numpy array\")\n",
                "    if feature_vals.ndim != 2:\n",
                "        raise TypeError(\"'feature_vals' must be a 2x2 matrix\")\n",
                "\n",
                "    # creating a copy for feature vales to prevent overwriting of global variables\n",
                "    feature_vals = np.copy(feature_vals)\n",
                "\n",
                "    # shuffling feature space\n",
                "    n_cols = feature_vals.shape[1]\n",
                "    for col_idx in range(0, n_cols):\n",
                "        # selecting column, shuffle, and update:\n",
                "        # feature_vals[:, col_idx] = np.random.permutation(feature_vals[:, col_idx])\n",
                "        np.random.shuffle(feature_vals[:, col_idx])\n",
                "    return feature_vals"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting up Paths and loading data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load in the treatment groups\n",
                "ground_truth = pathlib.Path(\n",
                "    \"../../4.sc_Morphology_Neural_Network_MLP_Model/MLP_utils/ground_truth.toml\"\n",
                ").resolve(strict=True)\n",
                "# load in the ground truth\n",
                "ground_truth = toml.load(ground_truth)\n",
                "apoptosis_ground_truth = ground_truth[\"Apoptosis\"][\"apoptosis_groups_list\"]\n",
                "pyroptosis_ground_truth = ground_truth[\"Pyroptosis\"][\"pyroptosis_groups_list\"]\n",
                "control_ground_truth = ground_truth[\"Healthy\"][\"healthy_groups_list\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_cell_data = pathlib.Path(\n",
                "    f\"../../data/PBMC_preprocessed_sc_norm_aggregated.parquet\"\n",
                ").resolve(strict=True)\n",
                "df = pd.read_parquet(single_cell_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# out paths\n",
                "map_out_dir = pathlib.Path(\"../data/processed/mAP_scores/morphology/\")\n",
                "map_out_dir.mkdir(exist_ok=True, parents=True)\n",
                "\n",
                "# regular data output\n",
                "# saving to csv\n",
                "regular_feat_map_path = pathlib.Path(map_out_dir / \"mAP_scores_regular_class.csv\")\n",
                "\n",
                "# shuffled data output\n",
                "shuffled_feat_map_path = pathlib.Path(map_out_dir / \"mAP_scores_shuffled_class.csv\")\n",
                "\n",
                "# shuffled feature space output\n",
                "shuffled_feat_space_map_path = pathlib.Path(\n",
                "    map_out_dir / \"mAP_scores_shuffled_feature_space_class.csv\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clean up data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add apoptosis, pyroptosis and healthy columns to dataframe\n",
                "df[\"Apoptosis\"] = df.apply(\n",
                "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "    in apoptosis_ground_truth,\n",
                "    axis=1,\n",
                ")\n",
                "df[\"Pyroptosis\"] = df.apply(\n",
                "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "    in pyroptosis_ground_truth,\n",
                "    axis=1,\n",
                ")\n",
                "df[\"Control\"] = df.apply(\n",
                "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "    in control_ground_truth,\n",
                "    axis=1,\n",
                ")\n",
                "\n",
                "# merge apoptosis, pyroptosis, and healthy columns into one column\n",
                "df[\"Metadata_labels\"] = df.apply(\n",
                "    lambda row: \"Apoptosis\"\n",
                "    if row[\"Apoptosis\"]\n",
                "    else \"Pyroptosis\"\n",
                "    if row[\"Pyroptosis\"]\n",
                "    else \"Control\",\n",
                "    axis=1,\n",
                ")\n",
                "# # drop apoptosis, pyroptosis, and healthy columns\n",
                "df.drop(columns=[\"Apoptosis\", \"Pyroptosis\", \"Control\"], inplace=True)\n",
                "df.drop(columns=[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"], inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# output directories\n",
                "map_out_dir = pathlib.Path(\"../data/processed/mAP_scores/\")\n",
                "map_out_dir.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### mAP Pipeline Parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The null size needs to be determined for the mAP pipeline. This is the size of the null class that is used to determine the mAP score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "65"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tmp = (\n",
                "    df.groupby([\"Metadata_labels\"])\n",
                "    .count()\n",
                "    .reset_index()[[\"Metadata_Well\", \"Metadata_labels\"]]\n",
                ")\n",
                "# get the Pyroptosis number of Metadata_Well\n",
                "Pyroptosis_count = tmp[tmp[\"Metadata_labels\"] == \"Pyroptosis\"][\"Metadata_Well\"].values[\n",
                "    0\n",
                "]\n",
                "Pyroptosis_count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_sameby = [\n",
                "    \"Metadata_labels\",\n",
                "]\n",
                "pos_diffby = [\"Metadata_Well\"]\n",
                "\n",
                "neg_sameby = []\n",
                "neg_diffby = [\"Metadata_labels\"]\n",
                "\n",
                "null_size = Pyroptosis_count\n",
                "batch_size = 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### mAP analysis for non-shuffled data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate the permutations of cell death labels via itertools\n",
                "pos_samby_permutations = list(itertools.combinations(df[\"Metadata_labels\"].unique(), 2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(\n",
                "    columns=[\n",
                "        \"Metadata_Well\",\n",
                "        \"Metadata_labels\",\n",
                "        \"average_precision\",\n",
                "        \"p_value\",\n",
                "        \"n_pos_pairs\",\n",
                "        \"n_total_pairs\",\n",
                "        \"shuffled\",\n",
                "        \"comparison\",\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "33f8d014225443ed95217b379e30f9dc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/5320 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9febcc8641af4c2dbe10b787d67e072a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/5265 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "dfd623767cb044c09281f77d03f4a8c2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_600064/1417819569.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
                        "  results_df = pd.concat([results_df, result], ignore_index=True)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c702306f816a4bc9b5fe0002db22e707",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2108 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7d0e2c372d60460bbe3baa5a3cf384eb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/520 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "61daf5f24a324fa78eb80697179acd44",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "97b1aa2ad7b94c2a876ff25fac3d97a9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/3268 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cf501a52171b42859458afa6e8c466ca",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/648 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2fda199643c74978b97f545fad43184b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "for i in pos_samby_permutations:\n",
                "    tmp = df.copy()\n",
                "    # get only the rows with the current permutation\n",
                "    tmp = tmp[tmp[\"Metadata_labels\"].isin(i)]\n",
                "    # This will generated 100 values [0..100] as seed values\n",
                "    # This will occur per phenotype\n",
                "\n",
                "    # spliting metadata and raw feature values\n",
                "    logging.info(\"splitting data set into metadata and raw feature values\")\n",
                "    df_meta, df_feats = utils.split_data(tmp)\n",
                "    df_feats = np.array(df_feats)\n",
                "\n",
                "    # execute pipeline on negative control with training dataset with cp features\n",
                "    try:\n",
                "        # execute pipeline on negative control with trianing dataset with cp features\n",
                "\n",
                "        logging.info(f\"Running pipeline on CP features using phenotype\")\n",
                "        result = run_pipeline(\n",
                "            meta=df_meta,\n",
                "            feats=df_feats,\n",
                "            pos_sameby=pos_sameby,\n",
                "            pos_diffby=pos_diffby,\n",
                "            neg_sameby=neg_sameby,\n",
                "            neg_diffby=neg_diffby,\n",
                "            batch_size=batch_size,\n",
                "            null_size=null_size,\n",
                "        )\n",
                "\n",
                "        # adding columns\n",
                "        result[\"shuffled\"] = \"non-shuffled\"\n",
                "        result[\"comparison\"] = \"_vs_\".join(i)\n",
                "\n",
                "    except ZeroDivisionError as e:\n",
                "        logging.warning(f\"{e} captured on phenotye:. Skipping\")\n",
                "\n",
                "    # concatenating all datasets\n",
                "    results_df = pd.concat([results_df, result], ignore_index=True)\n",
                "results_df.to_csv(regular_feat_map_path, index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### mAP analysis for shuffled data (Feature space)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(\n",
                "    columns=[\n",
                "        \"Metadata_Well\",\n",
                "        \"Metadata_labels\",\n",
                "        \"average_precision\",\n",
                "        \"p_value\",\n",
                "        \"n_pos_pairs\",\n",
                "        \"n_total_pairs\",\n",
                "        \"shuffled\",\n",
                "        \"comparison\",\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "eecadbde943145f7940c83c83867a66f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/5320 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4f5f98015aaf476e91a02f408a6834a8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/5265 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "889db09bf77c4d95b39b87651ab3d0ac",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_600064/1372217129.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
                        "  results_df = pd.concat([results_df, shuffled_feat_map], ignore_index=True)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9d9c31c50e7b4084a6775c85711780b7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2108 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ae46d1c4aada42ee98a0b506f2f5b796",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/520 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7c131de9f98346ecbaecc7563db8f13e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f7a52a36663e47d5893a33e5cff7fd61",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/3268 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e650379448104c44b9c6a8bb93cfc8cb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/648 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4e8c91e19c1e4ec3b5b75cab092e012a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "for i in pos_samby_permutations:\n",
                "    tmp = df.copy()\n",
                "    # get only the rows with the current permutation\n",
                "    tmp = tmp[tmp[\"Metadata_labels\"].isin(i)]\n",
                "    # This will generated 100 values [0..100] as seed values\n",
                "    seed = np.random.randint(0, 100)\n",
                "\n",
                "    # split the shuffled dataset\n",
                "    # spliting metadata and raw feature values\n",
                "    logging.info(\"splitting shuffled data set into metadata and raw feature values\")\n",
                "    (\n",
                "        df_meta,\n",
                "        df_feats,\n",
                "    ) = utils.split_data(tmp)\n",
                "\n",
                "    df_feats = np.array(df_feats)\n",
                "\n",
                "    # shuffling the features, this will overwrite the generated feature space from above with the shuffled one\n",
                "    df_feats = shuffle_features(feature_vals=df_feats, seed=seed)\n",
                "\n",
                "    try:\n",
                "        # execute pipeline on negative control with trianing dataset with cp features\n",
                "        logging.info(\n",
                "            f\"Running pipeline on CP features using phenotype, feature space is shuffled\"\n",
                "        )\n",
                "        shuffled_feat_map = run_pipeline(\n",
                "            meta=df_meta,\n",
                "            feats=df_feats,\n",
                "            pos_sameby=pos_sameby,\n",
                "            pos_diffby=pos_diffby,\n",
                "            neg_sameby=neg_sameby,\n",
                "            neg_diffby=neg_diffby,\n",
                "            batch_size=batch_size,\n",
                "            null_size=null_size,\n",
                "        )\n",
                "\n",
                "        # adding shuffle label column\n",
                "        shuffled_feat_map[\"shuffled\"] = \"shuffled\"\n",
                "        shuffled_feat_map[\"comparison\"] = \"_vs_\".join(i)\n",
                "\n",
                "    except ZeroDivisionError as e:\n",
                "        logging.warning(f\"{e} captured on phenotype:  Skipping\")\n",
                "\n",
                "    # concatenating all datasets\n",
                "    results_df = pd.concat([results_df, shuffled_feat_map], ignore_index=True)\n",
                "    # saving to csv\n",
                "results_df.to_csv(shuffled_feat_space_map_path, index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "map",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
