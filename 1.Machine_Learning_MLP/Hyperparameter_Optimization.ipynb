{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning via Optuna for Binary MLP model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Being a binary model this notebook will be limited to predicting one class 1 or 0, yes or no.\n",
    "### Here I will be predicting if a cell received a treatment or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import optuna\n",
    "import plotly\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.utils import df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Subset option yes or no? if no SUBSET_NUMBER won't be used\n",
    "SUBSET_OPTION = False\n",
    "\n",
    "# number of rows to subset main df for\n",
    "# in this casse each row is 1 cell\n",
    "SUBSET_NUMBER = 15000\n",
    "\n",
    "# Batch of data to load into data loader (1 is equivalent to 1 row or 1 cell in this case)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# number of epochs to use for model optimization\n",
    "OPTIM_EPOCHS = 100\n",
    "# number of trials to use for model optimization\n",
    "N_TRIALS = 500\n",
    "\n",
    "# number of epochs to use for optimized model\n",
    "TRAIN_EPOCHS = 100\n",
    "\n",
    "\n",
    "# device use\n",
    "# defined as global for use in the optimizer function and training function\n",
    "# global DEVICE\n",
    "DEVICE = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# set data file path under pathlib path for multi-system use\n",
    "file_path = Path(\n",
    "    \"../../Extracted_Features_(CSV_files)/interstellar_wave3_sc_norm_fs_cellprofiler.csv.gz\"\n",
    ")\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    low_memory=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine treatment with dosage to be able to discern treatments with different doses as a different condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine treatment and dose\n",
    "df[\"Metadata_treatment\"] = df[\"Metadata_treatment\"] + \"_\" + df[\"Metadata_dose\"]\n",
    "print(df[\"Metadata_treatment\"].unique())\n",
    "\n",
    "# Generate df speceific to analysis and model\n",
    "df = df.query(\n",
    "    \"Metadata_treatment == 'LPS_10Âµg/ml'| Metadata_treatment == 'Media only_0'\"\n",
    ")\n",
    "print(df[\"Metadata_treatment\"].unique())\n",
    "\n",
    "df_stats(df)\n",
    "# Drop na and reindex accordingly\n",
    "df = df.dropna()\n",
    "df.reindex()\n",
    "# Check for Nans again\n",
    "df_stats(df)\n",
    "# Understand categorical data such as treatment and dosing\n",
    "df[[\"Metadata_treatment\", \"Metadata_dose\"]].drop_duplicates()\n",
    "if SUBSET_OPTION:\n",
    "    df = df.sample(n=SUBSET_NUMBER)\n",
    "else:\n",
    "    pass\n",
    "# Code snipptet for metadata extraction by Jenna Tomkinson\n",
    "df_metadata = list(df.columns[df.columns.str.startswith(\"Metadata\")])\n",
    "\n",
    "# define which columns are data and which are descriptive\n",
    "df_descriptive = df[df_metadata]\n",
    "df_values = df.drop(columns=df_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting strings into numbers\n",
    "df_values[\"Metadata_treatment\"] = le.fit_transform(df_descriptive[\"Metadata_treatment\"])\n",
    "# split into X and Y where Y are the predictive column and x are the observable data\n",
    "df_values_X = df_values.drop(\"Metadata_treatment\", axis=1)\n",
    "df_values_Y = df_values[\"Metadata_treatment\"]\n",
    "\n",
    "# Random seed set for reproducibility\n",
    "seed = 1\n",
    "# split data into train-test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df_values_X, df_values_Y, test_size=0.15, random_state=seed, stratify=df_values_Y\n",
    ")\n",
    "# split train data into train-validate\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=seed, stratify=Y_train\n",
    ")\n",
    "\n",
    "# reset the index to avoid downstream errors\n",
    "X_train = X_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data class for x and y data\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    A class for formatting data for a data loader\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    X : Pandas DataFrame\n",
    "        the X dimension of data (features)\n",
    "    Y : Pandas DataFrame\n",
    "        the Y dimension of data (predictor)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __len__:\n",
    "        returns the length of the X dimension\n",
    "    -------\n",
    "    __getitem__:\n",
    "        returns a row of the X and Y dimension given an index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "    ):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce data objects for train, val and test datasets\n",
    "train_data = Dataset(\n",
    "    torch.FloatTensor(X_train.values), torch.FloatTensor(Y_train.values)\n",
    ")\n",
    "val_data = Dataset(torch.FloatTensor(X_val.values), torch.FloatTensor(Y_val.values))\n",
    "test_data = Dataset(torch.FloatTensor(X_test.values), torch.FloatTensor(Y_test.values))\n",
    "\n",
    "IN_FEATURES = X_train.shape[1]\n",
    "print(\"Number of in features: \", IN_FEATURES)\n",
    "out_features = len(df_values[\"Metadata_treatment\"].unique())\n",
    "print(\"Number of out features: \", out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data class into a dataloader to be compatible with pytorch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/code/ludovicocuoghi/pytorch-pytorch-lightning-w-optuna-opt\n",
    "def build_model_custom(trial, in_features):\n",
    "    \"\"\"\n",
    "    This function lays out the general arcitecture of a Nueral Network.\n",
    "    There are variables throughout to optimizet the hyperparameters of this model.\n",
    "    This function is meant to be used with optuna to optimize functions.\n",
    "\n",
    "    Parameters:\n",
    "        trial : optuna object\n",
    "            an optuna object foe which optimizatioon trial to input for\n",
    "            what parameters to use in the derfined search space\n",
    "        in_features : int\n",
    "            the number of input features to define the shape of the model\n",
    "\n",
    "    Return:\n",
    "        nn.Sequential(*layers) : dict\n",
    "            this returns in a dict the architecture of the model with optimized parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # number of hidden layers\n",
    "    # suugest.int takes into account the defined search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n",
    "\n",
    "    #  layers will be added to this list and called upon later\n",
    "    layers = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "\n",
    "        # the number of units within a hidden layer\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 2, 50)\n",
    "\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        # activation function\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # dropout rate\n",
    "        p = trial.suggest_float(\"dropout_{}\".format(i), 0.1, 0.5, step=0.05)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "\n",
    "    # final layer append\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "\n",
    "    # add layers to the model\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training and tracking model\n",
    "def objective(trial, return_info=False):\n",
    "    \"\"\"\n",
    "    This function trains the model and tests it on validation data.\n",
    "    The accuarcy and loss output is how the success of the model is tracked.\n",
    "\n",
    "    Parameters:\n",
    "        trial : optuna object\n",
    "\n",
    "        return_info : bool\n",
    "            If set to False only one metric will be returned to optimize the model\n",
    "            If set to True multiple metrics will be returned via printing\n",
    "            This is required as the optmization of the model is tacked by one output metric\n",
    "            and returning more than one metric will cause the optimization to fail but after optmization\n",
    "            it is nice to know about what the other output metrics are\n",
    "\n",
    "    Return:\n",
    "        metric(s) : str or float\n",
    "            if return_info == True:\n",
    "                Mean Validation Accuracy\n",
    "                Mean Validation Loss\n",
    "                Mean Training Accuracy\n",
    "                Mean Training Loss\n",
    "            if return_info == False:\n",
    "                return the mean validation accuracy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # calling model function\n",
    "    model = build_model_custom(trial, IN_FEATURES)\n",
    "\n",
    "    # param dictionary for optimization\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1),\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "        \"n_unit\": trial.suggest_int(\"n_unit\", 1, 50),\n",
    "    }\n",
    "\n",
    "    # param optimizer pick\n",
    "    optimizer = getattr(optim, params[\"optimizer\"])(\n",
    "        model.parameters(), lr=params[\"learning_rate\"]\n",
    "    )\n",
    "    # loss function\n",
    "\n",
    "    # for binary model use different for multi-class\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # send model to device(cuda)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = criterion.to(DEVICE)\n",
    "\n",
    "    # train set accuracy and loss\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "\n",
    "    # validation set accuracy and loss\n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "\n",
    "    # total number of data to pass through\n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(OPTIM_EPOCHS):\n",
    "\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(\n",
    "                DEVICE\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            # LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()  # sum all batch losses\n",
    "            # ACCURACY\n",
    "            correct += torch.sum(y_pred == y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(\n",
    "            running_loss / total_step\n",
    "        )  # get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(\n",
    "                    DEVICE\n",
    "                ), y_valid_batch.to(DEVICE)\n",
    "                # PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                # LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss += loss_v.item()\n",
    "                # ACCURACY\n",
    "                correct_v += torch.sum(y_pred == y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss / total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # I want information returned but only 1 metric required for the optimize function called by study.optimize\n",
    "    # with out this conditional statement the optimization will fail\n",
    "    if return_info == True:\n",
    "        print(f\"Validation Accuracy: {np.mean(valid_acc)}\")\n",
    "        print(f\"Validation Loss: {np.mean(valid_loss)}\")\n",
    "        print(f\"Training Accuracy: {np.mean(train_acc)}\")\n",
    "        print(f\"Training Loss: {np.mean(train_loss)}\")\n",
    "    else:\n",
    "        return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study is the object for model optimzation\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# Here I appply the optimize function of the study to the objective function \\\n",
    "# This optimizes each parameter specified to be optinmized from the defined search space\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "# Prints out the best trial's optimized parameters\n",
    "objective(study.best_trial, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_trial_params(best_params):\n",
    "    \"\"\"\n",
    "    This function extractions the best parameters from the best trial.\n",
    "\n",
    "    These extracted parameters will be used to create a new model.\n",
    "\n",
    "    Parameters:\n",
    "        best_params : obj\n",
    "            study.best_params\n",
    "            the best_params function of the study object\n",
    "    Return:\n",
    "        param_dict : dict\n",
    "            dictionary of all of the params for the best trial model\n",
    "    \"\"\"\n",
    "\n",
    "    params = best_params\n",
    "    units = []\n",
    "    dropout = []\n",
    "    n_layers = params[\"n_layers\"]\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    for i in range(params[\"n_layers\"]):\n",
    "        units.append(params[f\"n_units_l{i}\"])\n",
    "        dropout.append(params[f\"dropout_{i}\"])\n",
    "    param_dict = {\n",
    "        \"units\": units,\n",
    "        \"dropout\": dropout,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"learning_rate\": lr,\n",
    "    }\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function\n",
    "param_dict = extract_best_trial_params(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for new optimized model\n",
    "def optimized_model(in_features, paramater_dict):\n",
    "    \"\"\"\n",
    "    This function uses the extracted optimized functions to create a new model\n",
    "\n",
    "    Parameters:\n",
    "        in_features : int\n",
    "            this is the number of in features to used for the model\n",
    "        parameter_dict : dict\n",
    "            this is a dictionary returned fropm the extract_best_trial_params function\n",
    "\n",
    "    Return:\n",
    "        nn.Sequential(*layers) : dict\n",
    "            this returns in a dict the architecture of the model with optimized parameters\n",
    "    \"\"\"\n",
    "    n_layers = paramater_dict[\"n_layers\"]\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    in_features\n",
    "\n",
    "    for i in range(n_layers):\n",
    "\n",
    "        out_features = paramater_dict[\"units\"][i]\n",
    "\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = paramater_dict[\"dropout\"][i]\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = optimized_model(IN_FEATURES, param_dict).cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optim_method = param_dict[\"optimizer\"].strip(\"'\")\n",
    "optimizer = f'optim.{optim_method}(model.parameters(), lr=param_dict[\"learning_rate\"])'\n",
    "optimizer = eval(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train_optimized_model(EPOCHS):\n",
    "    \"\"\"\n",
    "    This function trains the optimized model on the training dataset\n",
    "\n",
    "    Parameters:\n",
    "        EPOCHS : int\n",
    "            the number of epochs to train the model for\n",
    "    Return:\n",
    "        training metrics : str\n",
    "\n",
    "    \"\"\"\n",
    "    early_stopping_patience = 15\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "\n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    valid_loss_min = np.inf\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(\n",
    "                DEVICE\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            # LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()  # sum loss for every batch\n",
    "            # ACCURACY\n",
    "            correct += torch.sum(y_pred == y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(\n",
    "            100 * correct / total\n",
    "        )  # calculate accuracy among all entries in the batches\n",
    "        train_loss.append(\n",
    "            running_loss / total_step\n",
    "        )  # get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(\n",
    "                    DEVICE\n",
    "                ), y_valid_batch.to(DEVICE)\n",
    "                # PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                # LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss += loss_v.item()\n",
    "                # ACCURACY\n",
    "                correct_v += torch.sum(y_pred == y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss / total_step_val)\n",
    "\n",
    "        if np.mean(valid_loss) <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), \"./state_dict.pt\")\n",
    "            print(\n",
    "                f\"Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...\"\n",
    "            )\n",
    "            valid_loss_min = np.mean(valid_loss)\n",
    "            early_stopping_counter = 0  # reset counter if validation loss decreases\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 0:01}: Validation loss did not decrease\")\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_patience:\n",
    "            print(\"Early stopped at epoch :\", epoch)\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the optimized trainig model\n",
    "train_optimized_model(TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimized_model():\n",
    "    \"\"\"\n",
    "    This function tests the trained optimized model on test dataset\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "    Return:\n",
    "        y_pred_list : list\n",
    "            lsit of predicted values\n",
    "        y_pred_prob_list : list\n",
    "            lsit of probabilities for predicted values\n",
    "    \"\"\"\n",
    "    y_pred_prob_list = []\n",
    "    y_pred_list = []\n",
    "\n",
    "    # Loading the best model\n",
    "    # model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            # PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())\n",
    "    y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    return y_pred_list, y_pred_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the testing function and outputing list values of tested model\n",
    "y_pred_list, y_pred_prob_list = test_optimized_model()\n",
    "\n",
    "# If output list is nested\n",
    "def un_nest(lst):\n",
    "    \"\"\"\n",
    "    returns an un-nested list from a nested list\n",
    "\n",
    "    Parameters:\n",
    "        lst : list\n",
    "            a list of lists\n",
    "    \"\"\"\n",
    "    new_lst = []\n",
    "    for i in lst:\n",
    "        for j in i:\n",
    "            new_lst.append(j)\n",
    "    return new_lst\n",
    "\n",
    "\n",
    "# un-nest list if nested i.e. length of input data does not match length of output data\n",
    "if len(y_pred_list) != len(Y_test):\n",
    "    y_pred_list = un_nest(y_pred_list)\n",
    "    y_pred_prob_list = un_nest(y_pred_prob_list)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_output(prediction_list, prediction_probability_list, test_data):\n",
    "    \"\"\"\n",
    "    Function outputs visulaization of testing the model\n",
    "\n",
    "    Parameters:\n",
    "        prediction_list : lsit of predicted values from model\n",
    "        prediction_probability_list : list of probabailities of predicted values from model\n",
    "        test_data : input data to model\n",
    "\n",
    "    Return:\n",
    "        classification report\n",
    "        confusion matrix\n",
    "        AUC graph of accuracy and false positive rates\n",
    "\n",
    "    \"\"\"\n",
    "    # Classification report\n",
    "    print(classification_report(test_data, prediction_list))\n",
    "\n",
    "    # confusion matrix\n",
    "    confusion_matrix(test_data, prediction_list)\n",
    "\n",
    "    # AUC graph of accuracy and false positive rates\n",
    "    plt.figure(figsize=(5.5, 4))\n",
    "    fpr, tpr, _ = roc_curve(test_data, prediction_probability_list)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, \"b\", label=\"AUC = %0.2f\" % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], \"r--\")\n",
    "    plt.title(\"ROC curve\", fontsize=25)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=18)\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=18)\n",
    "    plt.legend(\n",
    "        loc=\"lower right\",\n",
    "        fontsize=24,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        frameon=True,\n",
    "        handlelength=0,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call visulalization function\n",
    "results_output(y_pred_list, y_pred_prob_list, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interstellar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72ae02083a9ca7d143c492d1aec380c7bf553ec51bd66e90e72bba65228121b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
