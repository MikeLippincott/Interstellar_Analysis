{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35633c12",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning via Optuna for Binary MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171444ad",
   "metadata": {},
   "source": [
    "### Being a binary model this notebook will be limited to predicting one class 1 or 0, yes or no.\n",
    "### Here I will be predicting if a cell received a treatment or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa0306d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lippincm/miniconda3/envs/Interstellar/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from MLP_utils.parameters import Parameters\n",
    "\n",
    "\n",
    "from MLP_utils.utils import (\n",
    "    Dataset_formatter,\n",
    "    data_split,\n",
    "    extract_best_trial_params,\n",
    "    objective_model_optimizer,\n",
    "    optimized_model_create,\n",
    "    plot_metric_vs_epoch,\n",
    "    results_output,\n",
    "    test_optimized_model,\n",
    "    train_optimized_model,\n",
    "    un_nest,\n",
    ")\n",
    "\n",
    "from utils.utils import df_stats\n",
    "from MLP_utils.utils import parameter_set\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5549d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# set data file path under pathlib path for multi-system use\n",
    "file_path = Path(\n",
    "    \"../../Extracted_Features_(CSV_files)/interstellar_wave3_sc_norm_fs_cellprofiler.csv.gz\"\n",
    ")\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    low_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9230d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.optionxform = str\n",
    "config.read(\"MLP_utils/config.ini\")\n",
    "\n",
    "\n",
    "params = Parameters()\n",
    "\n",
    "params = parameter_set(params, config)\n",
    "# int(params.DATA_SUBSET_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6682c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LPS_10µg/ml' 'Disulfiram_2.5µM' 'LPS_1µg/ml' 'Disulfiram_0.1µM'\n",
      " 'H2O2_500µM' 'Thapsi_10µM' 'H2O2_50µM' 'Thapsi_1µM' 'ATP_1mM'\n",
      " 'LPS + Nigericin_1µg/ml + 10µM' 'ATP_0.1mM'\n",
      " 'LPS + Nigericin_1µg/ml + 1µM' 'Flagellin_1µg/ml' 'DMSO 0.1%_0'\n",
      " 'Flagellin_0.1µg/ml' 'Media only_0']\n",
      "['LPS_10µg/ml' 'Media only_0']\n",
      "The dimensions of the data are: (36718, 569)\n",
      "Number of total missing values across all columns: 470\n",
      "The dimensions of the data are: (36589, 569)\n",
      "Number of total missing values across all columns: 0\n",
      "False\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "# Combine treatment with dosage to be able to discern treatments with different doses as a different condition\n",
    "# Combine treatment and dose\n",
    "df = df.assign(\n",
    "    Metadata_Treatment_and_Dose=lambda x: df[\"Metadata_treatment\"]\n",
    "    + \"_\"\n",
    "    + df[\"Metadata_dose\"]\n",
    ")\n",
    "\n",
    "# df[\"Metadata_treatment\"] = df[\"Metadata_treatment\"] + \"_\" + df[\"Metadata_dose\"]\n",
    "print(df[\"Metadata_Treatment_and_Dose\"].unique())\n",
    "\n",
    "# Generate df speceific to analysis and model\n",
    "df = df.query(\n",
    "    \"Metadata_Treatment_and_Dose == 'LPS_10µg/ml'| Metadata_Treatment_and_Dose == 'Media only_0'\"\n",
    ")\n",
    "print(df[\"Metadata_Treatment_and_Dose\"].unique())\n",
    "\n",
    "df_stats(df)\n",
    "# Drop na and reindex accordingly\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Check for Nans again\n",
    "df_stats(df)\n",
    "# Understand categorical data such as treatment and dosing\n",
    "df[[\"Metadata_Treatment_and_Dose\"]].drop_duplicates()\n",
    "print(params.DATA_SUBSET_OPTION)\n",
    "print(params.DATA_SUBSET_NUMBER)\n",
    "if params.DATA_SUBSET_OPTION:\n",
    "    df = df.sample(n=params.DATA_SUBSET_NUMBER)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Code snippet for metadata extraction by Jenna Tomkinson\n",
    "df_metadata = list(df.columns[df.columns.str.startswith(\"Metadata\")])\n",
    "\n",
    "# define which columns are data and which are descriptive\n",
    "df_descriptive = df[df_metadata]\n",
    "df_values = df.drop(columns=df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31370d",
   "metadata": {},
   "source": [
    " ### Setting up data for network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4146d56",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Creating label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting strings into numbers\n",
    "df_values[\"Metadata_Treatment_and_Dose\"] = le.fit_transform(\n",
    "    df_descriptive[\"Metadata_Treatment_and_Dose\"]\n",
    ")\n",
    "# split into X and Y where Y are the predictive column and x are the observable data\n",
    "df_values_X = df_values.drop(\"Metadata_Treatment_and_Dose\", axis=1)\n",
    "df_values_Y = df_values[\"Metadata_Treatment_and_Dose\"]\n",
    "\n",
    "df_values_X.head()\n",
    "\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = data_split(\n",
    "    X_vals=df_values_X,\n",
    "    y_vals=df_values_Y,\n",
    "    train=0.8,\n",
    "    val=0.1,\n",
    "    test=0.1,\n",
    "    seed=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdcfd00e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of in features:  555\n",
      "Number of out features:  1\n"
     ]
    }
   ],
   "source": [
    "# produce data objects for train, val and test datasets\n",
    "train_data = Dataset_formatter(\n",
    "    torch.FloatTensor(X_train.values), torch.FloatTensor(Y_train.values)\n",
    ")\n",
    "val_data = Dataset_formatter(\n",
    "    torch.FloatTensor(X_val.values), torch.FloatTensor(Y_val.values)\n",
    ")\n",
    "test_data = Dataset_formatter(\n",
    "    torch.FloatTensor(X_test.values), torch.FloatTensor(Y_test.values)\n",
    ")\n",
    "\n",
    "IN_FEATURES = X_train.shape[1]\n",
    "print(\"Number of in features: \", IN_FEATURES)\n",
    "OUT_FEATURES = len(df_values[\"Metadata_Treatment_and_Dose\"].unique()) - 1\n",
    "print(\"Number of out features: \", OUT_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2cee70",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# convert data class into a dataloader to be compatible with pytorch\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, batch_size=params.BATCH_SIZE\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_data, batch_size=params.BATCH_SIZE\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, batch_size=params.BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ceef247",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-28 08:47:20,592]\u001b[0m A new study created in memory with name: no-name-a5a18efd-b6ca-4109-85c0-3de935c46e68\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:22,552]\u001b[0m Trial 0 finished with value: 0.7000388181209565 and parameters: {'n_layers': 7, 'n_units_l0': 9, 'dropout_0': 0.2172528688237627, 'n_units_l1': 3, 'dropout_1': 0.1684361821728212, 'n_units_l2': 9, 'dropout_2': 0.15475714634194, 'n_units_l3': 7, 'dropout_3': 0.29686719905233017, 'n_units_l4': 2, 'dropout_4': 0.3758293114719894, 'n_units_l5': 5, 'dropout_5': 0.2372372824160495, 'n_units_l6': 8, 'dropout_6': 0.32770227074454283, 'learning_rate': 0.42058970799158496, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.7000388181209565.\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:24,246]\u001b[0m Trial 1 finished with value: 0.6974609106779098 and parameters: {'n_layers': 3, 'n_units_l0': 3, 'dropout_0': 0.2558268182091479, 'n_units_l1': 8, 'dropout_1': 0.33699775495143147, 'n_units_l2': 6, 'dropout_2': 0.2731970317916646, 'learning_rate': 0.7911451798311284, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.7000388181209565.\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:25,890]\u001b[0m Trial 2 finished with value: 9.408899171948432 and parameters: {'n_layers': 5, 'n_units_l0': 5, 'dropout_0': 0.1099267866421184, 'n_units_l1': 5, 'dropout_1': 0.2988363587048406, 'n_units_l2': 2, 'dropout_2': 0.12870600586533334, 'n_units_l3': 9, 'dropout_3': 0.1481127016923427, 'n_units_l4': 8, 'dropout_4': 0.1011502104902593, 'learning_rate': 0.6299098070224987, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 9.408899171948432.\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:28,075]\u001b[0m Trial 3 finished with value: 27.735158182382584 and parameters: {'n_layers': 10, 'n_units_l0': 6, 'dropout_0': 0.34647970688206975, 'n_units_l1': 5, 'dropout_1': 0.12795833853266608, 'n_units_l2': 9, 'dropout_2': 0.21144224085623897, 'n_units_l3': 10, 'dropout_3': 0.3276377147178375, 'n_units_l4': 5, 'dropout_4': 0.3831144798998769, 'n_units_l5': 10, 'dropout_5': 0.35533941844919337, 'n_units_l6': 5, 'dropout_6': 0.17180168910244062, 'n_units_l7': 3, 'dropout_7': 0.28895453853268316, 'n_units_l8': 8, 'dropout_8': 0.3681882402119173, 'n_units_l9': 7, 'dropout_9': 0.25665849667283147, 'learning_rate': 0.5240472204218168, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 27.735158182382584.\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:29,838]\u001b[0m Trial 4 finished with value: 0.6356258457899093 and parameters: {'n_layers': 5, 'n_units_l0': 6, 'dropout_0': 0.3561593818169443, 'n_units_l1': 6, 'dropout_1': 0.29442573483178525, 'n_units_l2': 10, 'dropout_2': 0.13556134454429766, 'n_units_l3': 4, 'dropout_3': 0.1738181299002295, 'n_units_l4': 2, 'dropout_4': 0.24810502954324612, 'learning_rate': 0.6857456783009132, 'optimizer': 'SGD'}. Best is trial 3 with value: 27.735158182382584.\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:31,759]\u001b[0m Trial 5 finished with value: 590705.0739962017 and parameters: {'n_layers': 10, 'n_units_l0': 5, 'dropout_0': 0.3189441522866735, 'n_units_l1': 9, 'dropout_1': 0.22460699905699866, 'n_units_l2': 3, 'dropout_2': 0.2128601975312363, 'n_units_l3': 10, 'dropout_3': 0.37609121127045764, 'n_units_l4': 2, 'dropout_4': 0.3507772695738224, 'n_units_l5': 6, 'dropout_5': 0.24394756326099662, 'n_units_l6': 5, 'dropout_6': 0.30819027678121236, 'n_units_l7': 3, 'dropout_7': 0.2880060615317779, 'n_units_l8': 6, 'dropout_8': 0.3424099118328451, 'n_units_l9': 10, 'dropout_9': 0.1758433270579431, 'learning_rate': 0.5635586483391104, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 590705.0739962017.\u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:31,775]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:31,795]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:31,820]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-03-28 08:47:31,842]\u001b[0m Trial 9 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.0\n",
      "Validation Loss: 0.852381380200386\n",
      "Training Accuracy: 50.16500000000002\n",
      "Training Loss: 184582.15838991464\n"
     ]
    }
   ],
   "source": [
    "# wrap the objective function inside of a lambda function to pass args...\n",
    "objective_lambda_func = lambda trial: objective_model_optimizer(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    trial=trial,\n",
    "    in_features=IN_FEATURES,\n",
    "    out_features=OUT_FEATURES,\n",
    "    params=params,\n",
    "    metric=\"loss\",\n",
    "    return_info=False,\n",
    ")\n",
    "# Study is the object for model optimization\n",
    "study = optuna.create_study(direction=f\"{params.DIRECTION}\")\n",
    "# Here I apply the optimize function of the study to the objective function\n",
    "# This optimizes each parameter specified to be optimized from the defined search space\n",
    "study.optimize(objective_lambda_func, n_trials=params.N_TRIALS)\n",
    "# Prints out the best trial's optimized parameters\n",
    "objective_model_optimizer(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    trial=study.best_trial,\n",
    "    in_features=IN_FEATURES,\n",
    "    out_features=OUT_FEATURES,\n",
    "    params=params,\n",
    "    metric=params.METRIC,\n",
    "    return_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30756fe3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          0.7000388181209565,
          0.6974609106779098,
          9.408899171948432,
          27.735158182382584,
          0.6356258457899093,
          590705.0739962017
         ]
        },
        {
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5
         ],
         "y": [
          0.7000388181209565,
          0.7000388181209565,
          9.408899171948432,
          27.735158182382584,
          27.735158182382584,
          590705.0739962017
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7ddf13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial0",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          1.3636701107025146,
          1.0297062397003174,
          0.9187750816345215,
          0.8625089377164841,
          0.8287623167037964,
          0.8066050310929617,
          0.7906700117247445,
          0.778500571846962,
          0.7690491146511502,
          0.7615299761295319,
          0.7553284059871327,
          0.7501571476459503,
          0.7458291603968694,
          0.7421115679400307,
          0.7388530254364014,
          0.7359990775585175,
          0.7334845767301672,
          0.7312436567412482,
          0.7292463246144747,
          0.7274570345878602,
          0.7258305521238417,
          0.7243452045050535,
          0.7229891497155895,
          0.7217457418640455,
          0.7206042575836181,
          0.719553910768949,
          0.718579437997606,
          0.7176717264311654,
          0.7168260874419377,
          0.7160371124744416,
          0.7153005350020624,
          0.7146109864115715,
          0.7139621665983489,
          0.7133504292544197,
          0.712773483140128,
          0.7122290780146917,
          0.7117148831083968,
          0.7112277338379308,
          0.710764929270133,
          0.710324938595295,
          0.7099065562573875,
          0.7095085098629906,
          0.7091291394344595,
          0.7087667394768108,
          0.708420193195343,
          0.7080887374670609,
          0.7077716069018587,
          0.707467811803023,
          0.7071762972948502,
          0.7068962895870209,
          0.7066272637423348,
          0.706368706547297,
          0.7061199770783478,
          0.7058803946883591,
          0.7056494398550553,
          0.7054267387304988,
          0.7052119227877834,
          0.7050045461490236,
          0.7048041547759104,
          0.7046104003985723,
          0.7044230123035243,
          0.7042417103244413,
          0.7040661715325855,
          0.7038960894569755,
          0.7037312214191144,
          0.7035713656382128,
          0.7034163039122054,
          0.7032657975659651,
          0.7031196373096411,
          0.7029776496546609,
          0.7028396759234684,
          0.7027055414186584,
          0.7025750728502665,
          0.7024481232101852,
          0.7023245636622111,
          0.7022042627397337,
          0.7020870865165413,
          0.7019729086985955,
          0.7018616222128083,
          0.7017531216144561,
          0.7016473022508033,
          0.7015440594859239,
          0.7014433030622551,
          0.7013449470202128,
          0.7012489080429077,
          0.7011551025301911,
          0.7010634500404884,
          0.7009738819165663,
          0.7008863292383344,
          0.7008007221751743,
          0.7007169952759376,
          0.7006350891745609,
          0.7005549444947191,
          0.7004765056549235,
          0.7003997181591235,
          0.7003245304028193,
          0.7002508929095317,
          0.7001787588304403,
          0.7001080820054719,
          0.7000388181209565
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial1",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.8076270222663879,
          0.7932362258434296,
          0.7859774430592855,
          0.7651185840368271,
          0.7528732180595398,
          0.7492491801579794,
          0.744517947946276,
          0.7381956800818443,
          0.7341455221176147,
          0.7320310652256012,
          0.7294480421326377,
          0.7264392524957657,
          0.7241946092018714,
          0.7225016057491302,
          0.7206905563672383,
          0.7189909741282463,
          0.7177150389727425,
          0.7165735363960266,
          0.7153702848835995,
          0.7142820805311203,
          0.7133504947026571,
          0.7124545845118436,
          0.7116219790085502,
          0.7109066223104795,
          0.7102387237548828,
          0.7095844745635986,
          0.7089819179640876,
          0.708425692149571,
          0.7078988798733415,
          0.7074167569478352,
          0.7069726009522715,
          0.7065456137061119,
          0.7061396457932212,
          0.7057587802410126,
          0.7053984488759723,
          0.7050611939695146,
          0.7047451412355578,
          0.7044425857694525,
          0.7041530777246524,
          0.7038779512047768,
          0.7036164373886294,
          0.7033690370264507,
          0.7031339642613433,
          0.7029082043604418,
          0.7026914967430963,
          0.7024841269721156,
          0.702286053211131,
          0.7020970607797304,
          0.7019157458324822,
          0.7017409956455231,
          0.7015727629848555,
          0.7014111097042377,
          0.7012559715306984,
          0.7011068187378071,
          0.700962839343331,
          0.7008237040468625,
          0.700689413045582,
          0.700559945969746,
          0.7004350585452581,
          0.7003142784039179,
          0.7001972774990269,
          0.7000839854440382,
          0.6999743751117161,
          0.6998683093115687,
          0.6997655052405137,
          0.6996657216187679,
          0.6995688641249244,
          0.6994748939486111,
          0.6993837175161942,
          0.6992951512336731,
          0.6992090285663873,
          0.6991252650817236,
          0.6990438193491061,
          0.6989646135149775,
          0.6988875222206116,
          0.6988124314107393,
          0.6987392732075283,
          0.6986680053747617,
          0.6985985636711121,
          0.6985308580100537,
          0.6984648049613575,
          0.6984003571475424,
          0.6983374730650201,
          0.6982760968662444,
          0.6982161606059355,
          0.6981576098952182,
          0.6981004024374073,
          0.6980445019223473,
          0.6979898629563578,
          0.6979364342159695,
          0.6978841751486391,
          0.6978330534437428,
          0.6977830349758107,
          0.6977340819987845,
          0.6976861577284964,
          0.6976392287760973,
          0.6975932692744068,
          0.6975482495463624,
          0.6975041393077734,
          0.6974609106779098
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial2",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          855.9894409179688,
          434.5096535682678,
          290.8702353636424,
          218.41753751039505,
          174.8726662158966,
          145.84275687734285,
          125.10710783515658,
          109.55537120997906,
          97.45957618951797,
          87.78294030427932,
          79.86569288643923,
          73.26798682411511,
          67.68531258748128,
          62.90016335248947,
          58.753034134705864,
          55.1242961846292,
          51.922468693817365,
          49.07639993561639,
          46.52991748169849,
          44.2380833953619,
          42.164519344057354,
          40.27946124835448,
          38.55832137491392,
          36.98060996582111,
          35.529115614891055,
          34.18927482916759,
          32.948681665791405,
          31.796702476484434,
          30.724170308688592,
          29.72314048409462,
          28.786693434561453,
          27.90877455472946,
          27.08406312718536,
          26.307864406529596,
          25.576020201614924,
          24.884834342532688,
          24.23101025175404,
          23.611598380302127,
          23.02395169857221,
          22.465687902271746,
          21.934657124484456,
          21.42891428016481,
          20.946695248747982,
          20.48639633574269,
          20.046556460857392,
          19.62584167589312,
          19.22303168063468,
          18.837008035431307,
          18.466743845112468,
          18.111294726133348,
          17.769790893676234,
          17.441430267233116,
          17.125472505137605,
          16.82123390392021,
          16.528083087097514,
          16.245437269764288,
          15.972758456280356,
          15.70954820616492,
          15.455338946843552,
          15.209681728482247,
          14.972136489680556,
          14.742274584308747,
          14.51969296875454,
          14.304026022553444,
          14.094945893837856,
          13.892155305002674,
          13.695379749162873,
          13.504362126483636,
          13.318859721439471,
          13.13864261678287,
          12.963492778825088,
          12.793203422592747,
          12.627578455291383,
          12.466431935896745,
          12.309587496121724,
          12.156877748276058,
          12.00814364095787,
          11.863233804702759,
          11.722003869618042,
          11.58431584686041,
          11.45003761332712,
          11.319042568526617,
          11.191209506557648,
          11.066422638438997,
          10.944571680181166,
          10.825551882039669,
          10.709263934486215,
          10.595613736320626,
          10.484512053848652,
          10.375874130593406,
          10.269619308330201,
          10.165670670245005,
          10.063954733392244,
          9.964401182342083,
          9.86694264035476,
          9.771514468515912,
          9.67805459020064,
          9.58650333297496,
          9.496803285497608,
          9.408899171948432
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial3",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          2704.08837890625,
          1352.3909019231796,
          901.8249845107397,
          676.5420342832804,
          541.3722777247428,
          451.25911647081375,
          386.89257969175065,
          338.6176826134324,
          301.0705453356107,
          271.03284031748774,
          246.45654125647113,
          225.97629761199155,
          208.64686710100906,
          193.79307710272926,
          180.91980165640513,
          169.6556971296668,
          159.71679601248573,
          150.88223629527621,
          142.97765552056464,
          135.86356688439847,
          129.4270567723683,
          123.57574661753394,
          118.23332909138307,
          113.33621761699517,
          108.83099491357804,
          104.67244485020638,
          100.82202272503464,
          97.2466660887003,
          93.91786716724265,
          90.81093290845553,
          87.90437690481063,
          85.1794144436717,
          82.61954599257672,
          80.21021569827023,
          77.93853089809417,
          75.79303009808064,
          73.7634894541792,
          71.84076013376838,
          70.01663079322913,
          68.2837106153369,
          66.63532941515852,
          65.06545189164933,
          63.568603760974355,
          62.13980783522129,
          60.774528564347165,
          59.46862375995387,
          58.218302553004406,
          57.02008875956138,
          55.870788970772104,
          54.76746469736099,
          53.70740788824418,
          52.68811926818811,
          51.70728896361477,
          50.76277909234718,
          49.85260808467865,
          48.974936588534284,
          48.12805481228912,
          47.31037118517119,
          46.52040219407971,
          45.75676328241825,
          45.0181606568274,
          44.30338391469371,
          43.611299383261844,
          42.94084409251809,
          42.29102031542705,
          41.660890593673244,
          41.04957323465774,
          40.456238217213574,
          39.88010345334592,
          39.320431402751375,
          38.77652597343418,
          38.24772969964478,
          37.73342114033764,
          37.233012495814144,
          36.745947398344676,
          36.27169886623558,
          35.80976740183768,
          35.35967923051272,
          34.9209846567504,
          34.493256540596484,
          34.0760888884097,
          33.66909554237273,
          33.27190896976425,
          32.88417913090615,
          32.50557244314867,
          32.13577081020488,
          31.774470725278746,
          31.421382436698135,
          31.076229174485366,
          30.73874643445015,
          30.408681302280215,
          30.08579183337481,
          29.7698464726889,
          29.460623506535875,
          29.15791055779708,
          28.861504103367526,
          28.571209024522723,
          28.28683818238122,
          28.008212020902924,
          27.735158182382584
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial4",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.7043095827102661,
          0.7005163729190826,
          0.6983443895975748,
          0.6970864832401276,
          0.6962705492973328,
          0.6957078576087952,
          0.6952938437461853,
          0.6949666440486908,
          0.6947036120626662,
          0.6944844543933868,
          0.6942811066454108,
          0.6940947969754537,
          0.6939080036603488,
          0.6937202385493687,
          0.6935372352600098,
          0.6933454014360905,
          0.6931447491926306,
          0.6929472386837006,
          0.692741688929106,
          0.6925256043672562,
          0.6922917507943653,
          0.6920489750125192,
          0.6918138680250748,
          0.6915783435106277,
          0.6913358569145203,
          0.6910723814597497,
          0.6907905053209376,
          0.6904844003064292,
          0.6901532000508802,
          0.6898038466771443,
          0.6894294869515204,
          0.6890291646122932,
          0.6885869268215064,
          0.6881343091235441,
          0.6876343335424151,
          0.6871182438400056,
          0.6865641442505089,
          0.686000008332102,
          0.6854475079438626,
          0.6848478958010673,
          0.6842352442625093,
          0.6835761453424182,
          0.6828653230223545,
          0.682204316962849,
          0.6815626700719197,
          0.6808740807616193,
          0.6801306377065942,
          0.6793660894036293,
          0.6786430611902353,
          0.6779564654827118,
          0.6772495952307009,
          0.676515288077868,
          0.6757629086386483,
          0.6750296630241253,
          0.6742502765222029,
          0.673475342137473,
          0.6726148369019491,
          0.6715510836962996,
          0.6705572039394055,
          0.6696737408638,
          0.6688237659266738,
          0.6679824350341674,
          0.6672048587647695,
          0.6662445859983563,
          0.6651262054076561,
          0.6642837190266812,
          0.663444090245375,
          0.6627248131177005,
          0.6618704994519552,
          0.6610524109431676,
          0.6603044800355401,
          0.6595943404568566,
          0.6588130772930302,
          0.6579715504839614,
          0.6568320480982462,
          0.6557691120787671,
          0.6550313992933794,
          0.6540153179413233,
          0.6529365581802175,
          0.6519951045513153,
          0.6507953405380249,
          0.6497943437680965,
          0.6486093545534525,
          0.6477494488159815,
          0.6468757061397328,
          0.6457670095355011,
          0.6449199161310305,
          0.6441225605932149,
          0.6435241846556075,
          0.6426716181966994,
          0.6418252292570177,
          0.6412079327780268,
          0.6402040957122721,
          0.639294999711057,
          0.638455647543857,
          0.6377472194532553,
          0.6372246729958918,
          0.6365385639424227,
          0.6359233741808419,
          0.6356258457899093
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial5",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          59070408,
          29535219.21249962,
          19690146.39387049,
          14767609.97466506,
          11814088.120423114,
          9845073.550059825,
          8438634.570916193,
          7383805.336433969,
          6563382.598450105,
          5907044.408039957,
          5370040.434059366,
          4922537.122408162,
          4543880.474092438,
          4219317.632687734,
          3938029.8368166885,
          3691903.0154474676,
          3474732.2907341192,
          3281691.646577007,
          3108971.0702697258,
          2953522.5516521214,
          2812878.6539336415,
          2685020.565199803,
          2568280.5712612374,
          2461268.910284966,
          2362818.182309973,
          2271940.5873405016,
          2187794.6660997933,
          2109659.167778062,
          2036912.3244517734,
          1969015.2706074237,
          1905498.6717805383,
          1845951.8603224643,
          1790013.9464838686,
          1737366.4981327862,
          1687727.4753796987,
          1640846.1760992508,
          1596499.0010974116,
          1554485.887936553,
          1514627.2934027787,
          1476761.628603077,
          1440743.0694140282,
          1406439.6797234174,
          1373731.796545037,
          1342510.6353450255,
          1312677.0813246926,
          1284140.6383622147,
          1256818.5121321715,
          1230634.807835343,
          1205519.826165889,
          1181409.44376253,
          1158244.5665477677,
          1135970.6461431796,
          1114537.2510299908,
          1093897.6853583024,
          1074008.6493406838,
          1054829.9360321388,
          1036324.1600279703,
          1018456.5142274129,
          1001194.5513331901,
          984507.9872011472,
          968368.5235325625,
          952749.6877252719,
          937626.6879770831,
          922976.2819732549,
          908776.6576951257,
          895007.3250643556,
          881649.0172906606,
          868683.6009241877,
          856093.9937294247,
          843864.0895985152,
          831978.6898099226,
          820423.4400155403,
          809184.772406939,
          798249.8525708285,
          787606.5305960584,
          777243.2960406024,
          767149.2364075695,
          757313.9988153898,
          747727.7545790921,
          738381.1664480068,
          729265.3582702882,
          720371.8868771218,
          711692.7159993871,
          703220.1920474236,
          694947.0216004716,
          686866.2504666025,
          678971.2441868145,
          671255.6698683791,
          663713.4792429276,
          656338.8928539852,
          649126.3852870943,
          642070.6713631924,
          635166.6932227394,
          628409.6082342596,
          621794.7776665575,
          615317.7560689164,
          608974.2813081914,
          602760.2652158719,
          596671.7848019925,
          590705.0739962017
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial6",
         "type": "scatter",
         "x": [
          0
         ],
         "y": [
          0.6799896359443665
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial7",
         "type": "scatter",
         "x": [
          0
         ],
         "y": [
          0.7020909190177917
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial8",
         "type": "scatter",
         "x": [
          0
         ],
         "y": [
          0.7869514226913452
         ]
        },
        {
         "marker": {
          "maxdisplayed": 10
         },
         "mode": "lines+markers",
         "name": "Trial9",
         "type": "scatter",
         "x": [
          0
         ],
         "y": [
          64.31001281738281
         ]
        }
       ],
       "layout": {
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Intermediate Values Plot"
        },
        "xaxis": {
         "title": {
          "text": "Step"
         }
        },
        "yaxis": {
         "title": {
          "text": "Intermediate Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36975487",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# call function for best trial parameter extraction\n",
    "param_dict = extract_best_trial_params(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "343e3219",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSprop\n",
      "RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.5635586483391104\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0: Validation loss decreased (inf --> 16.152294).  Saving model ...\n",
      "\t Train_Loss: 206285184.3574 Train_Acc: 50.167 Val_Loss: 16.1523  BEST VAL Loss: 16.1523  Val_Acc: 50.000\n",
      "\n",
      "Epoch 1: Validation loss decreased (16.152294 --> 8.422843).  Saving model ...\n",
      "\t Train_Loss: 103142596.5691 Train_Acc: 50.167 Val_Loss: 8.4228  BEST VAL Loss: 8.4228  Val_Acc: 50.000\n",
      "\n",
      "Epoch 2: Validation loss decreased (8.422843 --> 5.846282).  Saving model ...\n",
      "\t Train_Loss: 68761731.2776 Train_Acc: 49.833 Val_Loss: 5.8463  BEST VAL Loss: 5.8463  Val_Acc: 50.000\n",
      "\n",
      "Epoch 3: Validation loss decreased (5.846282 --> 4.558025).  Saving model ...\n",
      "\t Train_Loss: 51571298.6313 Train_Acc: 50.583 Val_Loss: 4.5580  BEST VAL Loss: 4.5580  Val_Acc: 50.000\n",
      "\n",
      "Epoch 4: Validation loss decreased (4.558025 --> 3.785103).  Saving model ...\n",
      "\t Train_Loss: 41257039.0438 Train_Acc: 50.417 Val_Loss: 3.7851  BEST VAL Loss: 3.7851  Val_Acc: 50.000\n",
      "\n",
      "Epoch 5: Validation loss decreased (3.785103 --> 3.269859).  Saving model ...\n",
      "\t Train_Loss: 34380865.9853 Train_Acc: 51.417 Val_Loss: 3.2699  BEST VAL Loss: 3.2699  Val_Acc: 50.000\n",
      "\n",
      "Epoch 6: Validation loss decreased (3.269859 --> 2.901843).  Saving model ...\n",
      "\t Train_Loss: 29469313.8007 Train_Acc: 50.500 Val_Loss: 2.9018  BEST VAL Loss: 2.9018  Val_Acc: 50.000\n",
      "\n",
      "Epoch 7: Validation loss decreased (2.901843 --> 2.625842).  Saving model ...\n",
      "\t Train_Loss: 25785649.6624 Train_Acc: 49.833 Val_Loss: 2.6258  BEST VAL Loss: 2.6258  Val_Acc: 50.000\n",
      "\n",
      "Epoch 8: Validation loss decreased (2.625842 --> 2.411207).  Saving model ...\n",
      "\t Train_Loss: 22920577.5548 Train_Acc: 49.500 Val_Loss: 2.4112  BEST VAL Loss: 2.4112  Val_Acc: 50.000\n",
      "\n",
      "Epoch 9: Validation loss decreased (2.411207 --> 2.239499).  Saving model ...\n",
      "\t Train_Loss: 20628519.8686 Train_Acc: 50.167 Val_Loss: 2.2395  BEST VAL Loss: 2.2395  Val_Acc: 50.000\n",
      "\n",
      "Epoch 10: Validation loss decreased (2.239499 --> 2.099005).  Saving model ...\n",
      "\t Train_Loss: 18753199.9436 Train_Acc: 50.167 Val_Loss: 2.0990  BEST VAL Loss: 2.0990  Val_Acc: 50.000\n",
      "\n",
      "Epoch 11: Validation loss decreased (2.099005 --> 1.981950).  Saving model ...\n",
      "\t Train_Loss: 17190433.3394 Train_Acc: 50.167 Val_Loss: 1.9820  BEST VAL Loss: 1.9820  Val_Acc: 50.000\n",
      "\n",
      "Epoch 12: Validation loss decreased (1.981950 --> 1.882919).  Saving model ...\n",
      "\t Train_Loss: 15868092.3666 Train_Acc: 50.167 Val_Loss: 1.8829  BEST VAL Loss: 1.8829  Val_Acc: 50.000\n",
      "\n",
      "Epoch 13: Validation loss decreased (1.882919 --> 1.798052).  Saving model ...\n",
      "\t Train_Loss: 14734657.2471 Train_Acc: 50.167 Val_Loss: 1.7981  BEST VAL Loss: 1.7981  Val_Acc: 50.000\n",
      "\n",
      "Epoch 14: Validation loss decreased (1.798052 --> 1.724538).  Saving model ...\n",
      "\t Train_Loss: 13752346.8102 Train_Acc: 50.167 Val_Loss: 1.7245  BEST VAL Loss: 1.7245  Val_Acc: 50.000\n",
      "\n",
      "Epoch 15: Validation loss decreased (1.724538 --> 1.660248).  Saving model ...\n",
      "\t Train_Loss: 12892825.1780 Train_Acc: 50.167 Val_Loss: 1.6602  BEST VAL Loss: 1.6602  Val_Acc: 50.000\n",
      "\n",
      "Epoch 16: Validation loss decreased (1.660248 --> 1.603597).  Saving model ...\n",
      "\t Train_Loss: 12134423.7378 Train_Acc: 50.167 Val_Loss: 1.6036  BEST VAL Loss: 1.6036  Val_Acc: 50.000\n",
      "\n",
      "Epoch 17: Validation loss decreased (1.603597 --> 1.553335).  Saving model ...\n",
      "\t Train_Loss: 11460289.1244 Train_Acc: 50.167 Val_Loss: 1.5533  BEST VAL Loss: 1.5533  Val_Acc: 50.000\n",
      "\n",
      "Epoch 18: Validation loss decreased (1.553335 --> 1.508483).  Saving model ...\n",
      "\t Train_Loss: 10857116.0494 Train_Acc: 50.167 Val_Loss: 1.5085  BEST VAL Loss: 1.5085  Val_Acc: 50.000\n",
      "\n",
      "Epoch 19: Validation loss decreased (1.508483 --> 1.468277).  Saving model ...\n",
      "\t Train_Loss: 10314260.2819 Train_Acc: 50.167 Val_Loss: 1.4683  BEST VAL Loss: 1.4683  Val_Acc: 50.000\n",
      "\n",
      "Epoch 20: Validation loss decreased (1.468277 --> 1.432100).  Saving model ...\n",
      "\t Train_Loss: 9823105.0639 Train_Acc: 50.167 Val_Loss: 1.4321  BEST VAL Loss: 1.4321  Val_Acc: 50.000\n",
      "\n",
      "Epoch 21: Validation loss decreased (1.432100 --> 1.399421).  Saving model ...\n",
      "\t Train_Loss: 9376600.3204 Train_Acc: 50.167 Val_Loss: 1.3994  BEST VAL Loss: 1.3994  Val_Acc: 50.000\n",
      "\n",
      "Epoch 22: Validation loss decreased (1.399421 --> 1.369727).  Saving model ...\n",
      "\t Train_Loss: 8968922.0766 Train_Acc: 50.167 Val_Loss: 1.3697  BEST VAL Loss: 1.3697  Val_Acc: 50.000\n",
      "\n",
      "Epoch 23: Validation loss decreased (1.369727 --> 1.342523).  Saving model ...\n",
      "\t Train_Loss: 8595217.0197 Train_Acc: 50.167 Val_Loss: 1.3425  BEST VAL Loss: 1.3425  Val_Acc: 50.000\n",
      "\n",
      "Epoch 24: Validation loss decreased (1.342523 --> 1.317451).  Saving model ...\n",
      "\t Train_Loss: 8251408.3675 Train_Acc: 50.167 Val_Loss: 1.3175  BEST VAL Loss: 1.3175  Val_Acc: 50.000\n",
      "\n",
      "Epoch 25: Validation loss decreased (1.317451 --> 1.294199).  Saving model ...\n",
      "\t Train_Loss: 7934046.5346 Train_Acc: 50.167 Val_Loss: 1.2942  BEST VAL Loss: 1.2942  Val_Acc: 50.000\n",
      "\n",
      "Epoch 26: Validation loss decreased (1.294199 --> 1.272580).  Saving model ...\n",
      "\t Train_Loss: 7640192.9855 Train_Acc: 50.167 Val_Loss: 1.2726  BEST VAL Loss: 1.2726  Val_Acc: 50.000\n",
      "\n",
      "Epoch 27: Validation loss decreased (1.272580 --> 1.252405).  Saving model ...\n",
      "\t Train_Loss: 7367328.9755 Train_Acc: 50.167 Val_Loss: 1.2524  BEST VAL Loss: 1.2524  Val_Acc: 50.000\n",
      "\n",
      "Epoch 28: Validation loss decreased (1.252405 --> 1.233560).  Saving model ...\n",
      "\t Train_Loss: 7113283.1731 Train_Acc: 50.167 Val_Loss: 1.2336  BEST VAL Loss: 1.2336  Val_Acc: 50.000\n",
      "\n",
      "Epoch 29: Validation loss decreased (1.233560 --> 1.215944).  Saving model ...\n",
      "\t Train_Loss: 6876173.7574 Train_Acc: 50.167 Val_Loss: 1.2159  BEST VAL Loss: 1.2159  Val_Acc: 50.000\n",
      "\n",
      "Epoch 30: Validation loss decreased (1.215944 --> 1.199416).  Saving model ...\n",
      "\t Train_Loss: 6654361.7234 Train_Acc: 50.167 Val_Loss: 1.1994  BEST VAL Loss: 1.1994  Val_Acc: 50.000\n",
      "\n",
      "Epoch 31: Validation loss decreased (1.199416 --> 1.183887).  Saving model ...\n",
      "\t Train_Loss: 6446412.9415 Train_Acc: 50.167 Val_Loss: 1.1839  BEST VAL Loss: 1.1839  Val_Acc: 50.000\n",
      "\n",
      "Epoch 32: Validation loss decreased (1.183887 --> 1.169292).  Saving model ...\n",
      "\t Train_Loss: 6251067.1160 Train_Acc: 50.167 Val_Loss: 1.1693  BEST VAL Loss: 1.1693  Val_Acc: 50.000\n",
      "\n",
      "Epoch 33: Validation loss decreased (1.169292 --> 1.155549).  Saving model ...\n",
      "\t Train_Loss: 6067212.2214 Train_Acc: 50.167 Val_Loss: 1.1555  BEST VAL Loss: 1.1555  Val_Acc: 50.000\n",
      "\n",
      "Epoch 34: Validation loss decreased (1.155549 --> 1.142608).  Saving model ...\n",
      "\t Train_Loss: 5893863.3208 Train_Acc: 50.167 Val_Loss: 1.1426  BEST VAL Loss: 1.1426  Val_Acc: 50.000\n",
      "\n",
      "Epoch 35: Validation loss decreased (1.142608 --> 1.130382).  Saving model ...\n",
      "\t Train_Loss: 5730144.9147 Train_Acc: 50.167 Val_Loss: 1.1304  BEST VAL Loss: 1.1304  Val_Acc: 50.000\n",
      "\n",
      "Epoch 36: Validation loss decreased (1.130382 --> 1.118826).  Saving model ...\n",
      "\t Train_Loss: 5575276.1522 Train_Acc: 50.167 Val_Loss: 1.1188  BEST VAL Loss: 1.1188  Val_Acc: 50.000\n",
      "\n",
      "Epoch 37: Validation loss decreased (1.118826 --> 1.107903).  Saving model ...\n",
      "\t Train_Loss: 5428558.3771 Train_Acc: 50.167 Val_Loss: 1.1079  BEST VAL Loss: 1.1079  Val_Acc: 50.000\n",
      "\n",
      "Epoch 38: Validation loss decreased (1.107903 --> 1.097553).  Saving model ...\n",
      "\t Train_Loss: 5289364.5906 Train_Acc: 50.167 Val_Loss: 1.0976  BEST VAL Loss: 1.0976  Val_Acc: 50.000\n",
      "\n",
      "Epoch 39: Validation loss decreased (1.097553 --> 1.087732).  Saving model ...\n",
      "\t Train_Loss: 5157130.4934 Train_Acc: 50.167 Val_Loss: 1.0877  BEST VAL Loss: 1.0877  Val_Acc: 50.000\n",
      "\n",
      "Epoch 40: Validation loss decreased (1.087732 --> 1.078403).  Saving model ...\n",
      "\t Train_Loss: 5031346.8400 Train_Acc: 50.167 Val_Loss: 1.0784  BEST VAL Loss: 1.0784  Val_Acc: 50.000\n",
      "\n",
      "Epoch 41: Validation loss decreased (1.078403 --> 1.069524).  Saving model ...\n",
      "\t Train_Loss: 4911552.8843 Train_Acc: 50.167 Val_Loss: 1.0695  BEST VAL Loss: 1.0695  Val_Acc: 50.000\n",
      "\n",
      "Epoch 42: Validation loss decreased (1.069524 --> 1.061071).  Saving model ...\n",
      "\t Train_Loss: 4797330.7406 Train_Acc: 50.167 Val_Loss: 1.0611  BEST VAL Loss: 1.0611  Val_Acc: 50.000\n",
      "\n",
      "Epoch 43: Validation loss decreased (1.061071 --> 1.053000).  Saving model ...\n",
      "\t Train_Loss: 4688300.5125 Train_Acc: 50.167 Val_Loss: 1.0530  BEST VAL Loss: 1.0530  Val_Acc: 50.000\n",
      "\n",
      "Epoch 44: Validation loss decreased (1.053000 --> 1.045280).  Saving model ...\n",
      "\t Train_Loss: 4584116.0723 Train_Acc: 50.167 Val_Loss: 1.0453  BEST VAL Loss: 1.0453  Val_Acc: 50.000\n",
      "\n",
      "Epoch 45: Validation loss decreased (1.045280 --> 1.037898).  Saving model ...\n",
      "\t Train_Loss: 4484461.3904 Train_Acc: 50.167 Val_Loss: 1.0379  BEST VAL Loss: 1.0379  Val_Acc: 50.000\n",
      "\n",
      "Epoch 46: Validation loss decreased (1.037898 --> 1.030830).  Saving model ...\n",
      "\t Train_Loss: 4389047.3332 Train_Acc: 50.167 Val_Loss: 1.0308  BEST VAL Loss: 1.0308  Val_Acc: 50.000\n",
      "\n",
      "Epoch 47: Validation loss decreased (1.030830 --> 1.024049).  Saving model ...\n",
      "\t Train_Loss: 4297608.8618 Train_Acc: 50.167 Val_Loss: 1.0240  BEST VAL Loss: 1.0240  Val_Acc: 50.000\n",
      "\n",
      "Epoch 48: Validation loss decreased (1.024049 --> 1.017544).  Saving model ...\n",
      "\t Train_Loss: 4209902.5728 Train_Acc: 50.167 Val_Loss: 1.0175  BEST VAL Loss: 1.0175  Val_Acc: 50.000\n",
      "\n",
      "Epoch 49: Validation loss decreased (1.017544 --> 1.011294).  Saving model ...\n",
      "\t Train_Loss: 4125704.5354 Train_Acc: 50.167 Val_Loss: 1.0113  BEST VAL Loss: 1.0113  Val_Acc: 50.000\n",
      "\n",
      "Epoch 50: Validation loss decreased (1.011294 --> 1.005288).  Saving model ...\n",
      "\t Train_Loss: 4044808.3819 Train_Acc: 50.167 Val_Loss: 1.0053  BEST VAL Loss: 1.0053  Val_Acc: 50.000\n",
      "\n",
      "Epoch 51: Validation loss decreased (1.005288 --> 0.999513).  Saving model ...\n",
      "\t Train_Loss: 3967023.6188 Train_Acc: 50.167 Val_Loss: 0.9995  BEST VAL Loss: 0.9995  Val_Acc: 50.000\n",
      "\n",
      "Epoch 52: Validation loss decreased (0.999513 --> 0.993956).  Saving model ...\n",
      "\t Train_Loss: 3892174.1299 Train_Acc: 50.167 Val_Loss: 0.9940  BEST VAL Loss: 0.9940  Val_Acc: 50.000\n",
      "\n",
      "Epoch 53: Validation loss decreased (0.993956 --> 0.988602).  Saving model ...\n",
      "\t Train_Loss: 3820096.8442 Train_Acc: 50.167 Val_Loss: 0.9886  BEST VAL Loss: 0.9886  Val_Acc: 50.000\n",
      "\n",
      "Epoch 54: Validation loss decreased (0.988602 --> 0.983442).  Saving model ...\n",
      "\t Train_Loss: 3750640.5507 Train_Acc: 50.167 Val_Loss: 0.9834  BEST VAL Loss: 0.9834  Val_Acc: 50.000\n",
      "\n",
      "Epoch 55: Validation loss decreased (0.983442 --> 0.978462).  Saving model ...\n",
      "\t Train_Loss: 3683664.8392 Train_Acc: 50.167 Val_Loss: 0.9785  BEST VAL Loss: 0.9785  Val_Acc: 50.000\n",
      "\n",
      "Epoch 56: Validation loss decreased (0.978462 --> 0.973651).  Saving model ...\n",
      "\t Train_Loss: 3619039.1526 Train_Acc: 50.167 Val_Loss: 0.9737  BEST VAL Loss: 0.9737  Val_Acc: 50.000\n",
      "\n",
      "Epoch 57: Validation loss decreased (0.973651 --> 0.969007).  Saving model ...\n",
      "\t Train_Loss: 3556641.9379 Train_Acc: 50.167 Val_Loss: 0.9690  BEST VAL Loss: 0.9690  Val_Acc: 50.000\n",
      "\n",
      "Epoch 58: Validation loss decreased (0.969007 --> 0.964524).  Saving model ...\n",
      "\t Train_Loss: 3496359.8831 Train_Acc: 50.167 Val_Loss: 0.9645  BEST VAL Loss: 0.9645  Val_Acc: 50.000\n",
      "\n",
      "Epoch 59: Validation loss decreased (0.964524 --> 0.960190).  Saving model ...\n",
      "\t Train_Loss: 3438087.2301 Train_Acc: 50.167 Val_Loss: 0.9602  BEST VAL Loss: 0.9602  Val_Acc: 50.000\n",
      "\n",
      "Epoch 60: Validation loss decreased (0.960190 --> 0.955995).  Saving model ...\n",
      "\t Train_Loss: 3381725.1559 Train_Acc: 50.167 Val_Loss: 0.9560  BEST VAL Loss: 0.9560  Val_Acc: 50.000\n",
      "\n",
      "Epoch 61: Validation loss decreased (0.955995 --> 0.951937).  Saving model ...\n",
      "\t Train_Loss: 3327181.2131 Train_Acc: 50.167 Val_Loss: 0.9519  BEST VAL Loss: 0.9519  Val_Acc: 50.000\n",
      "\n",
      "Epoch 62: Validation loss decreased (0.951937 --> 0.948009).  Saving model ...\n",
      "\t Train_Loss: 3274368.8240 Train_Acc: 50.167 Val_Loss: 0.9480  BEST VAL Loss: 0.9480  Val_Acc: 50.000\n",
      "\n",
      "Epoch 63: Validation loss decreased (0.948009 --> 0.944201).  Saving model ...\n",
      "\t Train_Loss: 3223206.8222 Train_Acc: 50.167 Val_Loss: 0.9442  BEST VAL Loss: 0.9442  Val_Acc: 50.000\n",
      "\n",
      "Epoch 64: Validation loss decreased (0.944201 --> 0.940512).  Saving model ...\n",
      "\t Train_Loss: 3173619.0357 Train_Acc: 50.167 Val_Loss: 0.9405  BEST VAL Loss: 0.9405  Val_Acc: 50.000\n",
      "\n",
      "Epoch 65: Validation loss decreased (0.940512 --> 0.936932).  Saving model ...\n",
      "\t Train_Loss: 3125533.9095 Train_Acc: 50.167 Val_Loss: 0.9369  BEST VAL Loss: 0.9369  Val_Acc: 50.000\n",
      "\n",
      "Epoch 66: Validation loss decreased (0.936932 --> 0.933455).  Saving model ...\n",
      "\t Train_Loss: 3078884.1601 Train_Acc: 50.167 Val_Loss: 0.9335  BEST VAL Loss: 0.9335  Val_Acc: 50.000\n",
      "\n",
      "Epoch 67: Validation loss decreased (0.933455 --> 0.930079).  Saving model ...\n",
      "\t Train_Loss: 3033606.4622 Train_Acc: 50.167 Val_Loss: 0.9301  BEST VAL Loss: 0.9301  Val_Acc: 50.000\n",
      "\n",
      "Epoch 68: Validation loss decreased (0.930079 --> 0.926799).  Saving model ...\n",
      "\t Train_Loss: 2989641.1614 Train_Acc: 50.167 Val_Loss: 0.9268  BEST VAL Loss: 0.9268  Val_Acc: 50.000\n",
      "\n",
      "Epoch 69: Validation loss decreased (0.926799 --> 0.923612).  Saving model ...\n",
      "\t Train_Loss: 2946932.0120 Train_Acc: 50.167 Val_Loss: 0.9236  BEST VAL Loss: 0.9236  Val_Acc: 50.000\n",
      "\n",
      "Epoch 70: Validation loss decreased (0.923612 --> 0.920514).  Saving model ...\n",
      "\t Train_Loss: 2905425.9372 Train_Acc: 50.167 Val_Loss: 0.9205  BEST VAL Loss: 0.9205  Val_Acc: 50.000\n",
      "\n",
      "Epoch 71: Validation loss decreased (0.920514 --> 0.917503).  Saving model ...\n",
      "\t Train_Loss: 2865072.8089 Train_Acc: 50.167 Val_Loss: 0.9175  BEST VAL Loss: 0.9175  Val_Acc: 50.000\n",
      "\n",
      "Epoch 72: Validation loss decreased (0.917503 --> 0.914576).  Saving model ...\n",
      "\t Train_Loss: 2825825.2458 Train_Acc: 50.167 Val_Loss: 0.9146  BEST VAL Loss: 0.9146  Val_Acc: 50.000\n",
      "\n",
      "Epoch 73: Validation loss decreased (0.914576 --> 0.911728).  Saving model ...\n",
      "\t Train_Loss: 2787638.4277 Train_Acc: 50.167 Val_Loss: 0.9117  BEST VAL Loss: 0.9117  Val_Acc: 50.000\n",
      "\n",
      "Epoch 74: Validation loss decreased (0.911728 --> 0.908958).  Saving model ...\n",
      "\t Train_Loss: 2750469.9247 Train_Acc: 50.167 Val_Loss: 0.9090  BEST VAL Loss: 0.9090  Val_Acc: 50.000\n",
      "\n",
      "Epoch 75: Validation loss decreased (0.908958 --> 0.906262).  Saving model ...\n",
      "\t Train_Loss: 2714279.5402 Train_Acc: 50.167 Val_Loss: 0.9063  BEST VAL Loss: 0.9063  Val_Acc: 50.000\n",
      "\n",
      "Epoch 76: Validation loss decreased (0.906262 --> 0.903638).  Saving model ...\n",
      "\t Train_Loss: 2679029.1657 Train_Acc: 50.167 Val_Loss: 0.9036  BEST VAL Loss: 0.9036  Val_Acc: 50.000\n",
      "\n",
      "Epoch 77: Validation loss decreased (0.903638 --> 0.901082).  Saving model ...\n",
      "\t Train_Loss: 2644682.6469 Train_Acc: 50.167 Val_Loss: 0.9011  BEST VAL Loss: 0.9011  Val_Acc: 50.000\n",
      "\n",
      "Epoch 78: Validation loss decreased (0.901082 --> 0.898592).  Saving model ...\n",
      "\t Train_Loss: 2611205.6603 Train_Acc: 50.167 Val_Loss: 0.8986  BEST VAL Loss: 0.8986  Val_Acc: 50.000\n",
      "\n",
      "Epoch 79: Validation loss decreased (0.898592 --> 0.896165).  Saving model ...\n",
      "\t Train_Loss: 2578565.5983 Train_Acc: 50.167 Val_Loss: 0.8962  BEST VAL Loss: 0.8962  Val_Acc: 50.000\n",
      "\n",
      "Epoch 80: Validation loss decreased (0.896165 --> 0.893798).  Saving model ...\n",
      "\t Train_Loss: 2546731.4638 Train_Acc: 50.167 Val_Loss: 0.8938  BEST VAL Loss: 0.8938  Val_Acc: 50.000\n",
      "\n",
      "Epoch 81: Validation loss decreased (0.893798 --> 0.891489).  Saving model ...\n",
      "\t Train_Loss: 2515673.7716 Train_Acc: 50.167 Val_Loss: 0.8915  BEST VAL Loss: 0.8915  Val_Acc: 50.000\n",
      "\n",
      "Epoch 82: Validation loss decreased (0.891489 --> 0.889236).  Saving model ...\n",
      "\t Train_Loss: 2485364.4576 Train_Acc: 50.167 Val_Loss: 0.8892  BEST VAL Loss: 0.8892  Val_Acc: 50.000\n",
      "\n",
      "Epoch 83: Validation loss decreased (0.889236 --> 0.887036).  Saving model ...\n",
      "\t Train_Loss: 2455776.7938 Train_Acc: 50.167 Val_Loss: 0.8870  BEST VAL Loss: 0.8870  Val_Acc: 50.000\n",
      "\n",
      "Epoch 84: Validation loss decreased (0.887036 --> 0.884887).  Saving model ...\n",
      "\t Train_Loss: 2426885.3104 Train_Acc: 50.167 Val_Loss: 0.8849  BEST VAL Loss: 0.8849  Val_Acc: 50.000\n",
      "\n",
      "Epoch 85: Validation loss decreased (0.884887 --> 0.882788).  Saving model ...\n",
      "\t Train_Loss: 2398665.7220 Train_Acc: 50.167 Val_Loss: 0.8828  BEST VAL Loss: 0.8828  Val_Acc: 50.000\n",
      "\n",
      "Epoch 86: Validation loss decreased (0.882788 --> 0.880736).  Saving model ...\n",
      "\t Train_Loss: 2371094.8597 Train_Acc: 50.167 Val_Loss: 0.8807  BEST VAL Loss: 0.8807  Val_Acc: 50.000\n",
      "\n",
      "Epoch 87: Validation loss decreased (0.880736 --> 0.878731).  Saving model ...\n",
      "\t Train_Loss: 2344150.6079 Train_Acc: 50.167 Val_Loss: 0.8787  BEST VAL Loss: 0.8787  Val_Acc: 50.000\n",
      "\n",
      "Epoch 88: Validation loss decreased (0.878731 --> 0.876769).  Saving model ...\n",
      "\t Train_Loss: 2317811.8449 Train_Acc: 50.167 Val_Loss: 0.8768  BEST VAL Loss: 0.8768  Val_Acc: 50.000\n",
      "\n",
      "Epoch 89: Validation loss decreased (0.876769 --> 0.874851).  Saving model ...\n",
      "\t Train_Loss: 2292058.3878 Train_Acc: 50.167 Val_Loss: 0.8749  BEST VAL Loss: 0.8749  Val_Acc: 50.000\n",
      "\n",
      "Epoch 90: Validation loss decreased (0.874851 --> 0.872975).  Saving model ...\n",
      "\t Train_Loss: 2266870.9407 Train_Acc: 50.167 Val_Loss: 0.8730  BEST VAL Loss: 0.8730  Val_Acc: 50.000\n",
      "\n",
      "Epoch 91: Validation loss decreased (0.872975 --> 0.871140).  Saving model ...\n",
      "\t Train_Loss: 2242231.0468 Train_Acc: 50.167 Val_Loss: 0.8711  BEST VAL Loss: 0.8711  Val_Acc: 50.000\n",
      "\n",
      "Epoch 92: Validation loss decreased (0.871140 --> 0.869343).  Saving model ...\n",
      "\t Train_Loss: 2218121.0431 Train_Acc: 50.167 Val_Loss: 0.8693  BEST VAL Loss: 0.8693  Val_Acc: 50.000\n",
      "\n",
      "Epoch 93: Validation loss decreased (0.869343 --> 0.867585).  Saving model ...\n",
      "\t Train_Loss: 2194524.0182 Train_Acc: 50.167 Val_Loss: 0.8676  BEST VAL Loss: 0.8676  Val_Acc: 50.000\n",
      "\n",
      "Epoch 94: Validation loss decreased (0.867585 --> 0.865865).  Saving model ...\n",
      "\t Train_Loss: 2171423.7728 Train_Acc: 50.167 Val_Loss: 0.8659  BEST VAL Loss: 0.8659  Val_Acc: 50.000\n",
      "\n",
      "Epoch 95: Validation loss decreased (0.865865 --> 0.864180).  Saving model ...\n",
      "\t Train_Loss: 2148804.7825 Train_Acc: 50.167 Val_Loss: 0.8642  BEST VAL Loss: 0.8642  Val_Acc: 50.000\n",
      "\n",
      "Epoch 96: Validation loss decreased (0.864180 --> 0.862530).  Saving model ...\n",
      "\t Train_Loss: 2126652.1631 Train_Acc: 50.167 Val_Loss: 0.8625  BEST VAL Loss: 0.8625  Val_Acc: 50.000\n",
      "\n",
      "Epoch 97: Validation loss decreased (0.862530 --> 0.860914).  Saving model ...\n",
      "\t Train_Loss: 2104951.6380 Train_Acc: 50.167 Val_Loss: 0.8609  BEST VAL Loss: 0.8609  Val_Acc: 50.000\n",
      "\n",
      "Epoch 98: Validation loss decreased (0.860914 --> 0.859331).  Saving model ...\n",
      "\t Train_Loss: 2083689.5074 Train_Acc: 50.167 Val_Loss: 0.8593  BEST VAL Loss: 0.8593  Val_Acc: 50.000\n",
      "\n",
      "Epoch 99: Validation loss decreased (0.859331 --> 0.857780).  Saving model ...\n",
      "\t Train_Loss: 2062852.6193 Train_Acc: 50.167 Val_Loss: 0.8578  BEST VAL Loss: 0.8578  Val_Acc: 50.000\n",
      "\n",
      "Epoch 100: Validation loss decreased (0.857780 --> 0.856260).  Saving model ...\n",
      "\t Train_Loss: 2042428.3429 Train_Acc: 50.167 Val_Loss: 0.8563  BEST VAL Loss: 0.8563  Val_Acc: 50.000\n",
      "\n",
      "Epoch 101: Validation loss decreased (0.856260 --> 0.854770).  Saving model ...\n",
      "\t Train_Loss: 2022404.5426 Train_Acc: 50.167 Val_Loss: 0.8548  BEST VAL Loss: 0.8548  Val_Acc: 50.000\n",
      "\n",
      "Epoch 102: Validation loss decreased (0.854770 --> 0.853308).  Saving model ...\n",
      "\t Train_Loss: 2002769.5538 Train_Acc: 50.167 Val_Loss: 0.8533  BEST VAL Loss: 0.8533  Val_Acc: 50.000\n",
      "\n",
      "Epoch 103: Validation loss decreased (0.853308 --> 0.851875).  Saving model ...\n",
      "\t Train_Loss: 1983512.1610 Train_Acc: 50.167 Val_Loss: 0.8519  BEST VAL Loss: 0.8519  Val_Acc: 50.000\n",
      "\n",
      "Epoch 104: Validation loss decreased (0.851875 --> 0.850470).  Saving model ...\n",
      "\t Train_Loss: 1964621.5757 Train_Acc: 50.167 Val_Loss: 0.8505  BEST VAL Loss: 0.8505  Val_Acc: 50.000\n",
      "\n",
      "Epoch 105: Validation loss decreased (0.850470 --> 0.849090).  Saving model ...\n",
      "\t Train_Loss: 1946087.4165 Train_Acc: 50.167 Val_Loss: 0.8491  BEST VAL Loss: 0.8491  Val_Acc: 50.000\n",
      "\n",
      "Epoch 106: Validation loss decreased (0.849090 --> 0.847737).  Saving model ...\n",
      "\t Train_Loss: 1927899.6903 Train_Acc: 50.167 Val_Loss: 0.8477  BEST VAL Loss: 0.8477  Val_Acc: 50.000\n",
      "\n",
      "Epoch 107: Validation loss decreased (0.847737 --> 0.846408).  Saving model ...\n",
      "\t Train_Loss: 1910048.7737 Train_Acc: 50.167 Val_Loss: 0.8464  BEST VAL Loss: 0.8464  Val_Acc: 50.000\n",
      "\n",
      "Epoch 108: Validation loss decreased (0.846408 --> 0.845104).  Saving model ...\n",
      "\t Train_Loss: 1892525.3969 Train_Acc: 50.167 Val_Loss: 0.8451  BEST VAL Loss: 0.8451  Val_Acc: 50.000\n",
      "\n",
      "Epoch 109: Validation loss decreased (0.845104 --> 0.843823).  Saving model ...\n",
      "\t Train_Loss: 1875320.6270 Train_Acc: 50.167 Val_Loss: 0.8438  BEST VAL Loss: 0.8438  Val_Acc: 50.000\n",
      "\n",
      "Epoch 110: Validation loss decreased (0.843823 --> 0.842565).  Saving model ...\n",
      "\t Train_Loss: 1858425.8529 Train_Acc: 50.167 Val_Loss: 0.8426  BEST VAL Loss: 0.8426  Val_Acc: 50.000\n",
      "\n",
      "Epoch 111: Validation loss decreased (0.842565 --> 0.841330).  Saving model ...\n",
      "\t Train_Loss: 1841832.7712 Train_Acc: 50.167 Val_Loss: 0.8413  BEST VAL Loss: 0.8413  Val_Acc: 50.000\n",
      "\n",
      "Epoch 112: Validation loss decreased (0.841330 --> 0.840116).  Saving model ...\n",
      "\t Train_Loss: 1825533.3724 Train_Acc: 50.167 Val_Loss: 0.8401  BEST VAL Loss: 0.8401  Val_Acc: 50.000\n",
      "\n",
      "Epoch 113: Validation loss decreased (0.840116 --> 0.838924).  Saving model ...\n",
      "\t Train_Loss: 1809519.9279 Train_Acc: 50.167 Val_Loss: 0.8389  BEST VAL Loss: 0.8389  Val_Acc: 50.000\n",
      "\n",
      "Epoch 114: Validation loss decreased (0.838924 --> 0.837752).  Saving model ...\n",
      "\t Train_Loss: 1793784.9781 Train_Acc: 50.167 Val_Loss: 0.8378  BEST VAL Loss: 0.8378  Val_Acc: 50.000\n",
      "\n",
      "Epoch 115: Validation loss decreased (0.837752 --> 0.836601).  Saving model ...\n",
      "\t Train_Loss: 1778321.3206 Train_Acc: 50.167 Val_Loss: 0.8366  BEST VAL Loss: 0.8366  Val_Acc: 50.000\n",
      "\n",
      "Epoch 116: Validation loss decreased (0.836601 --> 0.835469).  Saving model ...\n",
      "\t Train_Loss: 1763121.9991 Train_Acc: 50.167 Val_Loss: 0.8355  BEST VAL Loss: 0.8355  Val_Acc: 50.000\n",
      "\n",
      "Epoch 117: Validation loss decreased (0.835469 --> 0.834357).  Saving model ...\n",
      "\t Train_Loss: 1748180.2932 Train_Acc: 50.167 Val_Loss: 0.8344  BEST VAL Loss: 0.8344  Val_Acc: 50.000\n",
      "\n",
      "Epoch 118: Validation loss decreased (0.834357 --> 0.833263).  Saving model ...\n",
      "\t Train_Loss: 1733489.7084 Train_Acc: 50.167 Val_Loss: 0.8333  BEST VAL Loss: 0.8333  Val_Acc: 50.000\n",
      "\n",
      "Epoch 119: Validation loss decreased (0.833263 --> 0.832187).  Saving model ...\n",
      "\t Train_Loss: 1719043.9667 Train_Acc: 50.167 Val_Loss: 0.8322  BEST VAL Loss: 0.8322  Val_Acc: 50.000\n",
      "\n",
      "Epoch 120: Validation loss decreased (0.832187 --> 0.831130).  Saving model ...\n",
      "\t Train_Loss: 1704836.9975 Train_Acc: 50.167 Val_Loss: 0.8311  BEST VAL Loss: 0.8311  Val_Acc: 50.000\n",
      "\n",
      "Epoch 121: Validation loss decreased (0.831130 --> 0.830089).  Saving model ...\n",
      "\t Train_Loss: 1690862.9296 Train_Acc: 50.167 Val_Loss: 0.8301  BEST VAL Loss: 0.8301  Val_Acc: 50.000\n",
      "\n",
      "Epoch 122: Validation loss decreased (0.830089 --> 0.829066).  Saving model ...\n",
      "\t Train_Loss: 1677116.0822 Train_Acc: 50.167 Val_Loss: 0.8291  BEST VAL Loss: 0.8291  Val_Acc: 50.000\n",
      "\n",
      "Epoch 123: Validation loss decreased (0.829066 --> 0.828059).  Saving model ...\n",
      "\t Train_Loss: 1663590.9582 Train_Acc: 50.167 Val_Loss: 0.8281  BEST VAL Loss: 0.8281  Val_Acc: 50.000\n",
      "\n",
      "Epoch 124: Validation loss decreased (0.828059 --> 0.827069).  Saving model ...\n",
      "\t Train_Loss: 1650282.2361 Train_Acc: 50.167 Val_Loss: 0.8271  BEST VAL Loss: 0.8271  Val_Acc: 50.000\n",
      "\n",
      "Epoch 125: Validation loss decreased (0.827069 --> 0.826094).  Saving model ...\n",
      "\t Train_Loss: 1637184.7637 Train_Acc: 50.167 Val_Loss: 0.8261  BEST VAL Loss: 0.8261  Val_Acc: 50.000\n",
      "\n",
      "Epoch 126: Validation loss decreased (0.826094 --> 0.825134).  Saving model ...\n",
      "\t Train_Loss: 1624293.5506 Train_Acc: 50.167 Val_Loss: 0.8251  BEST VAL Loss: 0.8251  Val_Acc: 50.000\n",
      "\n",
      "Epoch 127: Validation loss decreased (0.825134 --> 0.824190).  Saving model ...\n",
      "\t Train_Loss: 1611603.7627 Train_Acc: 50.167 Val_Loss: 0.8242  BEST VAL Loss: 0.8242  Val_Acc: 50.000\n",
      "\n",
      "Epoch 128: Validation loss decreased (0.824190 --> 0.823260).  Saving model ...\n",
      "\t Train_Loss: 1599110.7157 Train_Acc: 50.167 Val_Loss: 0.8233  BEST VAL Loss: 0.8233  Val_Acc: 50.000\n",
      "\n",
      "Epoch 129: Validation loss decreased (0.823260 --> 0.822344).  Saving model ...\n",
      "\t Train_Loss: 1586809.8695 Train_Acc: 50.167 Val_Loss: 0.8223  BEST VAL Loss: 0.8223  Val_Acc: 50.000\n",
      "\n",
      "Epoch 130: Validation loss decreased (0.822344 --> 0.821442).  Saving model ...\n",
      "\t Train_Loss: 1574696.8224 Train_Acc: 50.167 Val_Loss: 0.8214  BEST VAL Loss: 0.8214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 131: Validation loss decreased (0.821442 --> 0.820554).  Saving model ...\n",
      "\t Train_Loss: 1562767.3064 Train_Acc: 50.167 Val_Loss: 0.8206  BEST VAL Loss: 0.8206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 132: Validation loss decreased (0.820554 --> 0.819679).  Saving model ...\n",
      "\t Train_Loss: 1551017.1815 Train_Acc: 50.167 Val_Loss: 0.8197  BEST VAL Loss: 0.8197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 133: Validation loss decreased (0.819679 --> 0.818818).  Saving model ...\n",
      "\t Train_Loss: 1539442.4317 Train_Acc: 50.167 Val_Loss: 0.8188  BEST VAL Loss: 0.8188  Val_Acc: 50.000\n",
      "\n",
      "Epoch 134: Validation loss decreased (0.818818 --> 0.817969).  Saving model ...\n",
      "\t Train_Loss: 1528039.1596 Train_Acc: 50.167 Val_Loss: 0.8180  BEST VAL Loss: 0.8180  Val_Acc: 50.000\n",
      "\n",
      "Epoch 135: Validation loss decreased (0.817969 --> 0.817132).  Saving model ...\n",
      "\t Train_Loss: 1516803.5827 Train_Acc: 50.167 Val_Loss: 0.8171  BEST VAL Loss: 0.8171  Val_Acc: 50.000\n",
      "\n",
      "Epoch 136: Validation loss decreased (0.817132 --> 0.816308).  Saving model ...\n",
      "\t Train_Loss: 1505732.0289 Train_Acc: 50.167 Val_Loss: 0.8163  BEST VAL Loss: 0.8163  Val_Acc: 50.000\n",
      "\n",
      "Epoch 137: Validation loss decreased (0.816308 --> 0.815496).  Saving model ...\n",
      "\t Train_Loss: 1494820.9323 Train_Acc: 50.167 Val_Loss: 0.8155  BEST VAL Loss: 0.8155  Val_Acc: 50.000\n",
      "\n",
      "Epoch 138: Validation loss decreased (0.815496 --> 0.814695).  Saving model ...\n",
      "\t Train_Loss: 1484066.8299 Train_Acc: 50.167 Val_Loss: 0.8147  BEST VAL Loss: 0.8147  Val_Acc: 50.000\n",
      "\n",
      "Epoch 139: Validation loss decreased (0.814695 --> 0.813906).  Saving model ...\n",
      "\t Train_Loss: 1473466.3576 Train_Acc: 50.167 Val_Loss: 0.8139  BEST VAL Loss: 0.8139  Val_Acc: 50.000\n",
      "\n",
      "Epoch 140: Validation loss decreased (0.813906 --> 0.813128).  Saving model ...\n",
      "\t Train_Loss: 1463016.2466 Train_Acc: 50.167 Val_Loss: 0.8131  BEST VAL Loss: 0.8131  Val_Acc: 50.000\n",
      "\n",
      "Epoch 141: Validation loss decreased (0.813128 --> 0.812361).  Saving model ...\n",
      "\t Train_Loss: 1452713.3202 Train_Acc: 50.167 Val_Loss: 0.8124  BEST VAL Loss: 0.8124  Val_Acc: 50.000\n",
      "\n",
      "Epoch 142: Validation loss decreased (0.812361 --> 0.811605).  Saving model ...\n",
      "\t Train_Loss: 1442554.4907 Train_Acc: 50.167 Val_Loss: 0.8116  BEST VAL Loss: 0.8116  Val_Acc: 50.000\n",
      "\n",
      "Epoch 143: Validation loss decreased (0.811605 --> 0.810859).  Saving model ...\n",
      "\t Train_Loss: 1432536.7561 Train_Acc: 50.167 Val_Loss: 0.8109  BEST VAL Loss: 0.8109  Val_Acc: 50.000\n",
      "\n",
      "Epoch 144: Validation loss decreased (0.810859 --> 0.810123).  Saving model ...\n",
      "\t Train_Loss: 1422657.1971 Train_Acc: 50.167 Val_Loss: 0.8101  BEST VAL Loss: 0.8101  Val_Acc: 50.000\n",
      "\n",
      "Epoch 145: Validation loss decreased (0.810123 --> 0.809398).  Saving model ...\n",
      "\t Train_Loss: 1412912.9746 Train_Acc: 50.167 Val_Loss: 0.8094  BEST VAL Loss: 0.8094  Val_Acc: 50.000\n",
      "\n",
      "Epoch 146: Validation loss decreased (0.809398 --> 0.808682).  Saving model ...\n",
      "\t Train_Loss: 1403301.3265 Train_Acc: 50.167 Val_Loss: 0.8087  BEST VAL Loss: 0.8087  Val_Acc: 50.000\n",
      "\n",
      "Epoch 147: Validation loss decreased (0.808682 --> 0.807977).  Saving model ...\n",
      "\t Train_Loss: 1393819.5655 Train_Acc: 50.167 Val_Loss: 0.8080  BEST VAL Loss: 0.8080  Val_Acc: 50.000\n",
      "\n",
      "Epoch 148: Validation loss decreased (0.807977 --> 0.807280).  Saving model ...\n",
      "\t Train_Loss: 1384465.0765 Train_Acc: 50.167 Val_Loss: 0.8073  BEST VAL Loss: 0.8073  Val_Acc: 50.000\n",
      "\n",
      "Epoch 149: Validation loss decreased (0.807280 --> 0.806593).  Saving model ...\n",
      "\t Train_Loss: 1375235.3140 Train_Acc: 50.167 Val_Loss: 0.8066  BEST VAL Loss: 0.8066  Val_Acc: 50.000\n",
      "\n",
      "Epoch 150: Validation loss decreased (0.806593 --> 0.805915).  Saving model ...\n",
      "\t Train_Loss: 1366127.8000 Train_Acc: 50.167 Val_Loss: 0.8059  BEST VAL Loss: 0.8059  Val_Acc: 50.000\n",
      "\n",
      "Epoch 151: Validation loss decreased (0.805915 --> 0.805246).  Saving model ...\n",
      "\t Train_Loss: 1357140.1217 Train_Acc: 50.167 Val_Loss: 0.8052  BEST VAL Loss: 0.8052  Val_Acc: 50.000\n",
      "\n",
      "Epoch 152: Validation loss decreased (0.805246 --> 0.804586).  Saving model ...\n",
      "\t Train_Loss: 1348269.9295 Train_Acc: 50.167 Val_Loss: 0.8046  BEST VAL Loss: 0.8046  Val_Acc: 50.000\n",
      "\n",
      "Epoch 153: Validation loss decreased (0.804586 --> 0.803934).  Saving model ...\n",
      "\t Train_Loss: 1339514.9345 Train_Acc: 50.167 Val_Loss: 0.8039  BEST VAL Loss: 0.8039  Val_Acc: 50.000\n",
      "\n",
      "Epoch 154: Validation loss decreased (0.803934 --> 0.803291).  Saving model ...\n",
      "\t Train_Loss: 1330872.9072 Train_Acc: 50.167 Val_Loss: 0.8033  BEST VAL Loss: 0.8033  Val_Acc: 50.000\n",
      "\n",
      "Epoch 155: Validation loss decreased (0.803291 --> 0.802656).  Saving model ...\n",
      "\t Train_Loss: 1322341.6751 Train_Acc: 50.167 Val_Loss: 0.8027  BEST VAL Loss: 0.8027  Val_Acc: 50.000\n",
      "\n",
      "Epoch 156: Validation loss decreased (0.802656 --> 0.802029).  Saving model ...\n",
      "\t Train_Loss: 1313919.1212 Train_Acc: 50.167 Val_Loss: 0.8020  BEST VAL Loss: 0.8020  Val_Acc: 50.000\n",
      "\n",
      "Epoch 157: Validation loss decreased (0.802029 --> 0.801409).  Saving model ...\n",
      "\t Train_Loss: 1305603.1818 Train_Acc: 50.167 Val_Loss: 0.8014  BEST VAL Loss: 0.8014  Val_Acc: 50.000\n",
      "\n",
      "Epoch 158: Validation loss decreased (0.801409 --> 0.800798).  Saving model ...\n",
      "\t Train_Loss: 1297391.8455 Train_Acc: 50.167 Val_Loss: 0.8008  BEST VAL Loss: 0.8008  Val_Acc: 50.000\n",
      "\n",
      "Epoch 159: Validation loss decreased (0.800798 --> 0.800195).  Saving model ...\n",
      "\t Train_Loss: 1289283.1508 Train_Acc: 50.167 Val_Loss: 0.8002  BEST VAL Loss: 0.8002  Val_Acc: 50.000\n",
      "\n",
      "Epoch 160: Validation loss decreased (0.800195 --> 0.799598).  Saving model ...\n",
      "\t Train_Loss: 1281275.1853 Train_Acc: 50.167 Val_Loss: 0.7996  BEST VAL Loss: 0.7996  Val_Acc: 50.000\n",
      "\n",
      "Epoch 161: Validation loss decreased (0.799598 --> 0.799010).  Saving model ...\n",
      "\t Train_Loss: 1273366.0836 Train_Acc: 50.167 Val_Loss: 0.7990  BEST VAL Loss: 0.7990  Val_Acc: 50.000\n",
      "\n",
      "Epoch 162: Validation loss decreased (0.799010 --> 0.798428).  Saving model ...\n",
      "\t Train_Loss: 1265554.0260 Train_Acc: 50.167 Val_Loss: 0.7984  BEST VAL Loss: 0.7984  Val_Acc: 50.000\n",
      "\n",
      "Epoch 163: Validation loss decreased (0.798428 --> 0.797854).  Saving model ...\n",
      "\t Train_Loss: 1257837.2375 Train_Acc: 50.167 Val_Loss: 0.7979  BEST VAL Loss: 0.7979  Val_Acc: 50.000\n",
      "\n",
      "Epoch 164: Validation loss decreased (0.797854 --> 0.797286).  Saving model ...\n",
      "\t Train_Loss: 1250213.9857 Train_Acc: 50.167 Val_Loss: 0.7973  BEST VAL Loss: 0.7973  Val_Acc: 50.000\n",
      "\n",
      "Epoch 165: Validation loss decreased (0.797286 --> 0.796725).  Saving model ...\n",
      "\t Train_Loss: 1242682.5804 Train_Acc: 50.167 Val_Loss: 0.7967  BEST VAL Loss: 0.7967  Val_Acc: 50.000\n",
      "\n",
      "Epoch 166: Validation loss decreased (0.796725 --> 0.796171).  Saving model ...\n",
      "\t Train_Loss: 1235241.3716 Train_Acc: 50.167 Val_Loss: 0.7962  BEST VAL Loss: 0.7962  Val_Acc: 50.000\n",
      "\n",
      "Epoch 167: Validation loss decreased (0.796171 --> 0.795624).  Saving model ...\n",
      "\t Train_Loss: 1227888.7486 Train_Acc: 50.167 Val_Loss: 0.7956  BEST VAL Loss: 0.7956  Val_Acc: 50.000\n",
      "\n",
      "Epoch 168: Validation loss decreased (0.795624 --> 0.795083).  Saving model ...\n",
      "\t Train_Loss: 1220623.1388 Train_Acc: 50.167 Val_Loss: 0.7951  BEST VAL Loss: 0.7951  Val_Acc: 50.000\n",
      "\n",
      "Epoch 169: Validation loss decreased (0.795083 --> 0.794549).  Saving model ...\n",
      "\t Train_Loss: 1213443.0069 Train_Acc: 50.167 Val_Loss: 0.7945  BEST VAL Loss: 0.7945  Val_Acc: 50.000\n",
      "\n",
      "Epoch 170: Validation loss decreased (0.794549 --> 0.794020).  Saving model ...\n",
      "\t Train_Loss: 1206346.8530 Train_Acc: 50.167 Val_Loss: 0.7940  BEST VAL Loss: 0.7940  Val_Acc: 50.000\n",
      "\n",
      "Epoch 171: Validation loss decreased (0.794020 --> 0.793498).  Saving model ...\n",
      "\t Train_Loss: 1199333.2126 Train_Acc: 50.167 Val_Loss: 0.7935  BEST VAL Loss: 0.7935  Val_Acc: 50.000\n",
      "\n",
      "Epoch 172: Validation loss decreased (0.793498 --> 0.792982).  Saving model ...\n",
      "\t Train_Loss: 1192400.6548 Train_Acc: 50.167 Val_Loss: 0.7930  BEST VAL Loss: 0.7930  Val_Acc: 50.000\n",
      "\n",
      "Epoch 173: Validation loss decreased (0.792982 --> 0.792472).  Saving model ...\n",
      "\t Train_Loss: 1185547.7815 Train_Acc: 50.167 Val_Loss: 0.7925  BEST VAL Loss: 0.7925  Val_Acc: 50.000\n",
      "\n",
      "Epoch 174: Validation loss decreased (0.792472 --> 0.791968).  Saving model ...\n",
      "\t Train_Loss: 1178773.2267 Train_Acc: 50.167 Val_Loss: 0.7920  BEST VAL Loss: 0.7920  Val_Acc: 50.000\n",
      "\n",
      "Epoch 175: Validation loss decreased (0.791968 --> 0.791469).  Saving model ...\n",
      "\t Train_Loss: 1172075.6556 Train_Acc: 50.167 Val_Loss: 0.7915  BEST VAL Loss: 0.7915  Val_Acc: 50.000\n",
      "\n",
      "Epoch 176: Validation loss decreased (0.791469 --> 0.790976).  Saving model ...\n",
      "\t Train_Loss: 1165453.7632 Train_Acc: 50.167 Val_Loss: 0.7910  BEST VAL Loss: 0.7910  Val_Acc: 50.000\n",
      "\n",
      "Epoch 177: Validation loss decreased (0.790976 --> 0.790489).  Saving model ...\n",
      "\t Train_Loss: 1158906.2741 Train_Acc: 50.167 Val_Loss: 0.7905  BEST VAL Loss: 0.7905  Val_Acc: 50.000\n",
      "\n",
      "Epoch 178: Validation loss decreased (0.790489 --> 0.790007).  Saving model ...\n",
      "\t Train_Loss: 1152431.9413 Train_Acc: 50.167 Val_Loss: 0.7900  BEST VAL Loss: 0.7900  Val_Acc: 50.000\n",
      "\n",
      "Epoch 179: Validation loss decreased (0.790007 --> 0.789530).  Saving model ...\n",
      "\t Train_Loss: 1146029.5455 Train_Acc: 50.167 Val_Loss: 0.7895  BEST VAL Loss: 0.7895  Val_Acc: 50.000\n",
      "\n",
      "Epoch 180: Validation loss decreased (0.789530 --> 0.789059).  Saving model ...\n",
      "\t Train_Loss: 1139697.8945 Train_Acc: 50.167 Val_Loss: 0.7891  BEST VAL Loss: 0.7891  Val_Acc: 50.000\n",
      "\n",
      "Epoch 181: Validation loss decreased (0.789059 --> 0.788592).  Saving model ...\n",
      "\t Train_Loss: 1133435.8220 Train_Acc: 50.167 Val_Loss: 0.7886  BEST VAL Loss: 0.7886  Val_Acc: 50.000\n",
      "\n",
      "Epoch 182: Validation loss decreased (0.788592 --> 0.788131).  Saving model ...\n",
      "\t Train_Loss: 1127242.1875 Train_Acc: 50.167 Val_Loss: 0.7881  BEST VAL Loss: 0.7881  Val_Acc: 50.000\n",
      "\n",
      "Epoch 183: Validation loss decreased (0.788131 --> 0.787675).  Saving model ...\n",
      "\t Train_Loss: 1121115.8751 Train_Acc: 50.167 Val_Loss: 0.7877  BEST VAL Loss: 0.7877  Val_Acc: 50.000\n",
      "\n",
      "Epoch 184: Validation loss decreased (0.787675 --> 0.787224).  Saving model ...\n",
      "\t Train_Loss: 1115055.7930 Train_Acc: 50.167 Val_Loss: 0.7872  BEST VAL Loss: 0.7872  Val_Acc: 50.000\n",
      "\n",
      "Epoch 185: Validation loss decreased (0.787224 --> 0.786778).  Saving model ...\n",
      "\t Train_Loss: 1109060.8732 Train_Acc: 50.167 Val_Loss: 0.7868  BEST VAL Loss: 0.7868  Val_Acc: 50.000\n",
      "\n",
      "Epoch 186: Validation loss decreased (0.786778 --> 0.786336).  Saving model ...\n",
      "\t Train_Loss: 1103130.0702 Train_Acc: 50.167 Val_Loss: 0.7863  BEST VAL Loss: 0.7863  Val_Acc: 50.000\n",
      "\n",
      "Epoch 187: Validation loss decreased (0.786336 --> 0.785900).  Saving model ...\n",
      "\t Train_Loss: 1097262.3608 Train_Acc: 50.167 Val_Loss: 0.7859  BEST VAL Loss: 0.7859  Val_Acc: 50.000\n",
      "\n",
      "Epoch 188: Validation loss decreased (0.785900 --> 0.785467).  Saving model ...\n",
      "\t Train_Loss: 1091456.7435 Train_Acc: 50.167 Val_Loss: 0.7855  BEST VAL Loss: 0.7855  Val_Acc: 50.000\n",
      "\n",
      "Epoch 189: Validation loss decreased (0.785467 --> 0.785040).  Saving model ...\n",
      "\t Train_Loss: 1085712.2381 Train_Acc: 50.167 Val_Loss: 0.7850  BEST VAL Loss: 0.7850  Val_Acc: 50.000\n",
      "\n",
      "Epoch 190: Validation loss decreased (0.785040 --> 0.784617).  Saving model ...\n",
      "\t Train_Loss: 1080027.8845 Train_Acc: 50.167 Val_Loss: 0.7846  BEST VAL Loss: 0.7846  Val_Acc: 50.000\n",
      "\n",
      "Epoch 191: Validation loss decreased (0.784617 --> 0.784198).  Saving model ...\n",
      "\t Train_Loss: 1074402.7429 Train_Acc: 50.167 Val_Loss: 0.7842  BEST VAL Loss: 0.7842  Val_Acc: 50.000\n",
      "\n",
      "Epoch 192: Validation loss decreased (0.784198 --> 0.783783).  Saving model ...\n",
      "\t Train_Loss: 1068835.8930 Train_Acc: 50.167 Val_Loss: 0.7838  BEST VAL Loss: 0.7838  Val_Acc: 50.000\n",
      "\n",
      "Epoch 193: Validation loss decreased (0.783783 --> 0.783373).  Saving model ...\n",
      "\t Train_Loss: 1063326.4332 Train_Acc: 50.167 Val_Loss: 0.7834  BEST VAL Loss: 0.7834  Val_Acc: 50.000\n",
      "\n",
      "Epoch 194: Validation loss decreased (0.783373 --> 0.782967).  Saving model ...\n",
      "\t Train_Loss: 1057873.4808 Train_Acc: 50.167 Val_Loss: 0.7830  BEST VAL Loss: 0.7830  Val_Acc: 50.000\n",
      "\n",
      "Epoch 195: Validation loss decreased (0.782967 --> 0.782565).  Saving model ...\n",
      "\t Train_Loss: 1052476.1707 Train_Acc: 50.167 Val_Loss: 0.7826  BEST VAL Loss: 0.7826  Val_Acc: 50.000\n",
      "\n",
      "Epoch 196: Validation loss decreased (0.782565 --> 0.782168).  Saving model ...\n",
      "\t Train_Loss: 1047133.6556 Train_Acc: 50.167 Val_Loss: 0.7822  BEST VAL Loss: 0.7822  Val_Acc: 50.000\n",
      "\n",
      "Epoch 197: Validation loss decreased (0.782168 --> 0.781774).  Saving model ...\n",
      "\t Train_Loss: 1041845.1053 Train_Acc: 50.167 Val_Loss: 0.7818  BEST VAL Loss: 0.7818  Val_Acc: 50.000\n",
      "\n",
      "Epoch 198: Validation loss decreased (0.781774 --> 0.781384).  Saving model ...\n",
      "\t Train_Loss: 1036609.7063 Train_Acc: 50.167 Val_Loss: 0.7814  BEST VAL Loss: 0.7814  Val_Acc: 50.000\n",
      "\n",
      "Epoch 199: Validation loss decreased (0.781384 --> 0.780998).  Saving model ...\n",
      "\t Train_Loss: 1031426.6613 Train_Acc: 50.167 Val_Loss: 0.7810  BEST VAL Loss: 0.7810  Val_Acc: 50.000\n",
      "\n",
      "Epoch 200: Validation loss decreased (0.780998 --> 0.780616).  Saving model ...\n",
      "\t Train_Loss: 1026295.1889 Train_Acc: 50.167 Val_Loss: 0.7806  BEST VAL Loss: 0.7806  Val_Acc: 50.000\n",
      "\n",
      "Epoch 201: Validation loss decreased (0.780616 --> 0.780238).  Saving model ...\n",
      "\t Train_Loss: 1021214.5231 Train_Acc: 50.167 Val_Loss: 0.7802  BEST VAL Loss: 0.7802  Val_Acc: 50.000\n",
      "\n",
      "Epoch 202: Validation loss decreased (0.780238 --> 0.779864).  Saving model ...\n",
      "\t Train_Loss: 1016183.9132 Train_Acc: 50.167 Val_Loss: 0.7799  BEST VAL Loss: 0.7799  Val_Acc: 50.000\n",
      "\n",
      "Epoch 203: Validation loss decreased (0.779864 --> 0.779493).  Saving model ...\n",
      "\t Train_Loss: 1011202.6229 Train_Acc: 50.167 Val_Loss: 0.7795  BEST VAL Loss: 0.7795  Val_Acc: 50.000\n",
      "\n",
      "Epoch 204: Validation loss decreased (0.779493 --> 0.779126).  Saving model ...\n",
      "\t Train_Loss: 1006269.9306 Train_Acc: 50.167 Val_Loss: 0.7791  BEST VAL Loss: 0.7791  Val_Acc: 50.000\n",
      "\n",
      "Epoch 205: Validation loss decreased (0.779126 --> 0.778762).  Saving model ...\n",
      "\t Train_Loss: 1001385.1286 Train_Acc: 50.167 Val_Loss: 0.7788  BEST VAL Loss: 0.7788  Val_Acc: 50.000\n",
      "\n",
      "Epoch 206: Validation loss decreased (0.778762 --> 0.778402).  Saving model ...\n",
      "\t Train_Loss: 996547.5226 Train_Acc: 50.167 Val_Loss: 0.7784  BEST VAL Loss: 0.7784  Val_Acc: 50.000\n",
      "\n",
      "Epoch 207: Validation loss decreased (0.778402 --> 0.778045).  Saving model ...\n",
      "\t Train_Loss: 991756.4322 Train_Acc: 50.167 Val_Loss: 0.7780  BEST VAL Loss: 0.7780  Val_Acc: 50.000\n",
      "\n",
      "Epoch 208: Validation loss decreased (0.778045 --> 0.777692).  Saving model ...\n",
      "\t Train_Loss: 987011.1894 Train_Acc: 50.167 Val_Loss: 0.7777  BEST VAL Loss: 0.7777  Val_Acc: 50.000\n",
      "\n",
      "Epoch 209: Validation loss decreased (0.777692 --> 0.777342).  Saving model ...\n",
      "\t Train_Loss: 982311.1395 Train_Acc: 50.167 Val_Loss: 0.7773  BEST VAL Loss: 0.7773  Val_Acc: 50.000\n",
      "\n",
      "Epoch 210: Validation loss decreased (0.777342 --> 0.776995).  Saving model ...\n",
      "\t Train_Loss: 977655.6398 Train_Acc: 50.167 Val_Loss: 0.7770  BEST VAL Loss: 0.7770  Val_Acc: 50.000\n",
      "\n",
      "Epoch 211: Validation loss decreased (0.776995 --> 0.776652).  Saving model ...\n",
      "\t Train_Loss: 973044.0599 Train_Acc: 50.167 Val_Loss: 0.7767  BEST VAL Loss: 0.7767  Val_Acc: 50.000\n",
      "\n",
      "Epoch 212: Validation loss decreased (0.776652 --> 0.776312).  Saving model ...\n",
      "\t Train_Loss: 968475.7812 Train_Acc: 50.167 Val_Loss: 0.7763  BEST VAL Loss: 0.7763  Val_Acc: 50.000\n",
      "\n",
      "Epoch 213: Validation loss decreased (0.776312 --> 0.775975).  Saving model ...\n",
      "\t Train_Loss: 963950.1968 Train_Acc: 50.167 Val_Loss: 0.7760  BEST VAL Loss: 0.7760  Val_Acc: 50.000\n",
      "\n",
      "Epoch 214: Validation loss decreased (0.775975 --> 0.775641).  Saving model ...\n",
      "\t Train_Loss: 959466.7108 Train_Acc: 50.167 Val_Loss: 0.7756  BEST VAL Loss: 0.7756  Val_Acc: 50.000\n",
      "\n",
      "Epoch 215: Validation loss decreased (0.775641 --> 0.775311).  Saving model ...\n",
      "\t Train_Loss: 955024.7385 Train_Acc: 50.167 Val_Loss: 0.7753  BEST VAL Loss: 0.7753  Val_Acc: 50.000\n",
      "\n",
      "Epoch 216: Validation loss decreased (0.775311 --> 0.774983).  Saving model ...\n",
      "\t Train_Loss: 950623.7061 Train_Acc: 50.167 Val_Loss: 0.7750  BEST VAL Loss: 0.7750  Val_Acc: 50.000\n",
      "\n",
      "Epoch 217: Validation loss decreased (0.774983 --> 0.774658).  Saving model ...\n",
      "\t Train_Loss: 946263.0501 Train_Acc: 50.167 Val_Loss: 0.7747  BEST VAL Loss: 0.7747  Val_Acc: 50.000\n",
      "\n",
      "Epoch 218: Validation loss decreased (0.774658 --> 0.774337).  Saving model ...\n",
      "\t Train_Loss: 941942.2175 Train_Acc: 50.167 Val_Loss: 0.7743  BEST VAL Loss: 0.7743  Val_Acc: 50.000\n",
      "\n",
      "Epoch 219: Validation loss decreased (0.774337 --> 0.774018).  Saving model ...\n",
      "\t Train_Loss: 937660.6651 Train_Acc: 50.167 Val_Loss: 0.7740  BEST VAL Loss: 0.7740  Val_Acc: 50.000\n",
      "\n",
      "Epoch 220: Validation loss decreased (0.774018 --> 0.773702).  Saving model ...\n",
      "\t Train_Loss: 933417.8599 Train_Acc: 50.167 Val_Loss: 0.7737  BEST VAL Loss: 0.7737  Val_Acc: 50.000\n",
      "\n",
      "Epoch 221: Validation loss decreased (0.773702 --> 0.773389).  Saving model ...\n",
      "\t Train_Loss: 929213.2781 Train_Acc: 50.167 Val_Loss: 0.7734  BEST VAL Loss: 0.7734  Val_Acc: 50.000\n",
      "\n",
      "Epoch 222: Validation loss decreased (0.773389 --> 0.773079).  Saving model ...\n",
      "\t Train_Loss: 925046.4056 Train_Acc: 50.167 Val_Loss: 0.7731  BEST VAL Loss: 0.7731  Val_Acc: 50.000\n",
      "\n",
      "Epoch 223: Validation loss decreased (0.773079 --> 0.772772).  Saving model ...\n",
      "\t Train_Loss: 920916.7372 Train_Acc: 50.167 Val_Loss: 0.7728  BEST VAL Loss: 0.7728  Val_Acc: 50.000\n",
      "\n",
      "Epoch 224: Validation loss decreased (0.772772 --> 0.772467).  Saving model ...\n",
      "\t Train_Loss: 916823.7771 Train_Acc: 50.167 Val_Loss: 0.7725  BEST VAL Loss: 0.7725  Val_Acc: 50.000\n",
      "\n",
      "Epoch 225: Validation loss decreased (0.772467 --> 0.772165).  Saving model ...\n",
      "\t Train_Loss: 912767.0378 Train_Acc: 50.167 Val_Loss: 0.7722  BEST VAL Loss: 0.7722  Val_Acc: 50.000\n",
      "\n",
      "Epoch 226: Validation loss decreased (0.772165 --> 0.771866).  Saving model ...\n",
      "\t Train_Loss: 908746.0408 Train_Acc: 50.167 Val_Loss: 0.7719  BEST VAL Loss: 0.7719  Val_Acc: 50.000\n",
      "\n",
      "Epoch 227: Validation loss decreased (0.771866 --> 0.771569).  Saving model ...\n",
      "\t Train_Loss: 904760.3156 Train_Acc: 50.167 Val_Loss: 0.7716  BEST VAL Loss: 0.7716  Val_Acc: 50.000\n",
      "\n",
      "Epoch 228: Validation loss decreased (0.771569 --> 0.771275).  Saving model ...\n",
      "\t Train_Loss: 900809.4003 Train_Acc: 50.167 Val_Loss: 0.7713  BEST VAL Loss: 0.7713  Val_Acc: 50.000\n",
      "\n",
      "Epoch 229: Validation loss decreased (0.771275 --> 0.770983).  Saving model ...\n",
      "\t Train_Loss: 896892.8407 Train_Acc: 50.167 Val_Loss: 0.7710  BEST VAL Loss: 0.7710  Val_Acc: 50.000\n",
      "\n",
      "Epoch 230: Validation loss decreased (0.770983 --> 0.770694).  Saving model ...\n",
      "\t Train_Loss: 893010.1908 Train_Acc: 50.167 Val_Loss: 0.7707  BEST VAL Loss: 0.7707  Val_Acc: 50.000\n",
      "\n",
      "Epoch 231: Validation loss decreased (0.770694 --> 0.770408).  Saving model ...\n",
      "\t Train_Loss: 889161.0119 Train_Acc: 50.167 Val_Loss: 0.7704  BEST VAL Loss: 0.7704  Val_Acc: 50.000\n",
      "\n",
      "Epoch 232: Validation loss decreased (0.770408 --> 0.770123).  Saving model ...\n",
      "\t Train_Loss: 885344.8733 Train_Acc: 50.167 Val_Loss: 0.7701  BEST VAL Loss: 0.7701  Val_Acc: 50.000\n",
      "\n",
      "Epoch 233: Validation loss decreased (0.770123 --> 0.769842).  Saving model ...\n",
      "\t Train_Loss: 881561.3512 Train_Acc: 50.167 Val_Loss: 0.7698  BEST VAL Loss: 0.7698  Val_Acc: 50.000\n",
      "\n",
      "Epoch 234: Validation loss decreased (0.769842 --> 0.769562).  Saving model ...\n",
      "\t Train_Loss: 877810.0293 Train_Acc: 50.167 Val_Loss: 0.7696  BEST VAL Loss: 0.7696  Val_Acc: 50.000\n",
      "\n",
      "Epoch 235: Validation loss decreased (0.769562 --> 0.769286).  Saving model ...\n",
      "\t Train_Loss: 874090.4982 Train_Acc: 50.167 Val_Loss: 0.7693  BEST VAL Loss: 0.7693  Val_Acc: 50.000\n",
      "\n",
      "Epoch 236: Validation loss decreased (0.769286 --> 0.769011).  Saving model ...\n",
      "\t Train_Loss: 870402.3556 Train_Acc: 50.167 Val_Loss: 0.7690  BEST VAL Loss: 0.7690  Val_Acc: 50.000\n",
      "\n",
      "Epoch 237: Validation loss decreased (0.769011 --> 0.768739).  Saving model ...\n",
      "\t Train_Loss: 866745.2058 Train_Acc: 50.167 Val_Loss: 0.7687  BEST VAL Loss: 0.7687  Val_Acc: 50.000\n",
      "\n",
      "Epoch 238: Validation loss decreased (0.768739 --> 0.768469).  Saving model ...\n",
      "\t Train_Loss: 863118.6598 Train_Acc: 50.167 Val_Loss: 0.7685  BEST VAL Loss: 0.7685  Val_Acc: 50.000\n",
      "\n",
      "Epoch 239: Validation loss decreased (0.768469 --> 0.768201).  Saving model ...\n",
      "\t Train_Loss: 859522.3350 Train_Acc: 50.167 Val_Loss: 0.7682  BEST VAL Loss: 0.7682  Val_Acc: 50.000\n",
      "\n",
      "Epoch 240: Validation loss decreased (0.768201 --> 0.767936).  Saving model ...\n",
      "\t Train_Loss: 855955.8552 Train_Acc: 50.167 Val_Loss: 0.7679  BEST VAL Loss: 0.7679  Val_Acc: 50.000\n",
      "\n",
      "Epoch 241: Validation loss decreased (0.767936 --> 0.767672).  Saving model ...\n",
      "\t Train_Loss: 852418.8504 Train_Acc: 50.167 Val_Loss: 0.7677  BEST VAL Loss: 0.7677  Val_Acc: 50.000\n",
      "\n",
      "Epoch 242: Validation loss decreased (0.767672 --> 0.767411).  Saving model ...\n",
      "\t Train_Loss: 848910.9568 Train_Acc: 50.167 Val_Loss: 0.7674  BEST VAL Loss: 0.7674  Val_Acc: 50.000\n",
      "\n",
      "Epoch 243: Validation loss decreased (0.767411 --> 0.767152).  Saving model ...\n",
      "\t Train_Loss: 845431.8164 Train_Acc: 50.167 Val_Loss: 0.7672  BEST VAL Loss: 0.7672  Val_Acc: 50.000\n",
      "\n",
      "Epoch 244: Validation loss decreased (0.767152 --> 0.766895).  Saving model ...\n",
      "\t Train_Loss: 841981.0772 Train_Acc: 50.167 Val_Loss: 0.7669  BEST VAL Loss: 0.7669  Val_Acc: 50.000\n",
      "\n",
      "Epoch 245: Validation loss decreased (0.766895 --> 0.766640).  Saving model ...\n",
      "\t Train_Loss: 838558.3927 Train_Acc: 50.167 Val_Loss: 0.7666  BEST VAL Loss: 0.7666  Val_Acc: 50.000\n",
      "\n",
      "Epoch 246: Validation loss decreased (0.766640 --> 0.766388).  Saving model ...\n",
      "\t Train_Loss: 835163.4223 Train_Acc: 50.167 Val_Loss: 0.7664  BEST VAL Loss: 0.7664  Val_Acc: 50.000\n",
      "\n",
      "Epoch 247: Validation loss decreased (0.766388 --> 0.766137).  Saving model ...\n",
      "\t Train_Loss: 831795.8307 Train_Acc: 50.167 Val_Loss: 0.7661  BEST VAL Loss: 0.7661  Val_Acc: 50.000\n",
      "\n",
      "Epoch 248: Validation loss decreased (0.766137 --> 0.765888).  Saving model ...\n",
      "\t Train_Loss: 828455.2880 Train_Acc: 50.167 Val_Loss: 0.7659  BEST VAL Loss: 0.7659  Val_Acc: 50.000\n",
      "\n",
      "Epoch 249: Validation loss decreased (0.765888 --> 0.765642).  Saving model ...\n",
      "\t Train_Loss: 825141.4697 Train_Acc: 50.167 Val_Loss: 0.7656  BEST VAL Loss: 0.7656  Val_Acc: 50.000\n",
      "\n",
      "Epoch 250: Validation loss decreased (0.765642 --> 0.765397).  Saving model ...\n",
      "\t Train_Loss: 821854.0563 Train_Acc: 50.167 Val_Loss: 0.7654  BEST VAL Loss: 0.7654  Val_Acc: 50.000\n",
      "\n",
      "Epoch 251: Validation loss decreased (0.765397 --> 0.765154).  Saving model ...\n",
      "\t Train_Loss: 818592.7335 Train_Acc: 50.167 Val_Loss: 0.7652  BEST VAL Loss: 0.7652  Val_Acc: 50.000\n",
      "\n",
      "Epoch 252: Validation loss decreased (0.765154 --> 0.764913).  Saving model ...\n",
      "\t Train_Loss: 815357.1918 Train_Acc: 50.167 Val_Loss: 0.7649  BEST VAL Loss: 0.7649  Val_Acc: 50.000\n",
      "\n",
      "Epoch 253: Validation loss decreased (0.764913 --> 0.764674).  Saving model ...\n",
      "\t Train_Loss: 812147.1269 Train_Acc: 50.167 Val_Loss: 0.7647  BEST VAL Loss: 0.7647  Val_Acc: 50.000\n",
      "\n",
      "Epoch 254: Validation loss decreased (0.764674 --> 0.764437).  Saving model ...\n",
      "\t Train_Loss: 808962.2390 Train_Acc: 50.167 Val_Loss: 0.7644  BEST VAL Loss: 0.7644  Val_Acc: 50.000\n",
      "\n",
      "Epoch 255: Validation loss decreased (0.764437 --> 0.764202).  Saving model ...\n",
      "\t Train_Loss: 805802.2330 Train_Acc: 50.167 Val_Loss: 0.7642  BEST VAL Loss: 0.7642  Val_Acc: 50.000\n",
      "\n",
      "Epoch 256: Validation loss decreased (0.764202 --> 0.763968).  Saving model ...\n",
      "\t Train_Loss: 802666.8185 Train_Acc: 50.167 Val_Loss: 0.7640  BEST VAL Loss: 0.7640  Val_Acc: 50.000\n",
      "\n",
      "Epoch 257: Validation loss decreased (0.763968 --> 0.763737).  Saving model ...\n",
      "\t Train_Loss: 799555.7095 Train_Acc: 50.167 Val_Loss: 0.7637  BEST VAL Loss: 0.7637  Val_Acc: 50.000\n",
      "\n",
      "Epoch 258: Validation loss decreased (0.763737 --> 0.763507).  Saving model ...\n",
      "\t Train_Loss: 796468.6245 Train_Acc: 50.167 Val_Loss: 0.7635  BEST VAL Loss: 0.7635  Val_Acc: 50.000\n",
      "\n",
      "Epoch 259: Validation loss decreased (0.763507 --> 0.763279).  Saving model ...\n",
      "\t Train_Loss: 793405.2864 Train_Acc: 50.167 Val_Loss: 0.7633  BEST VAL Loss: 0.7633  Val_Acc: 50.000\n",
      "\n",
      "Epoch 260: Validation loss decreased (0.763279 --> 0.763053).  Saving model ...\n",
      "\t Train_Loss: 790365.4221 Train_Acc: 50.167 Val_Loss: 0.7631  BEST VAL Loss: 0.7631  Val_Acc: 50.000\n",
      "\n",
      "Epoch 261: Validation loss decreased (0.763053 --> 0.762828).  Saving model ...\n",
      "\t Train_Loss: 787348.7629 Train_Acc: 50.167 Val_Loss: 0.7628  BEST VAL Loss: 0.7628  Val_Acc: 50.000\n",
      "\n",
      "Epoch 262: Validation loss decreased (0.762828 --> 0.762605).  Saving model ...\n",
      "\t Train_Loss: 784355.0440 Train_Acc: 50.167 Val_Loss: 0.7626  BEST VAL Loss: 0.7626  Val_Acc: 50.000\n",
      "\n",
      "Epoch 263: Validation loss decreased (0.762605 --> 0.762384).  Saving model ...\n",
      "\t Train_Loss: 781384.0048 Train_Acc: 50.167 Val_Loss: 0.7624  BEST VAL Loss: 0.7624  Val_Acc: 50.000\n",
      "\n",
      "Epoch 264: Validation loss decreased (0.762384 --> 0.762165).  Saving model ...\n",
      "\t Train_Loss: 778435.3886 Train_Acc: 50.167 Val_Loss: 0.7622  BEST VAL Loss: 0.7622  Val_Acc: 50.000\n",
      "\n",
      "Epoch 265: Validation loss decreased (0.762165 --> 0.761947).  Saving model ...\n",
      "\t Train_Loss: 775508.9424 Train_Acc: 50.167 Val_Loss: 0.7619  BEST VAL Loss: 0.7619  Val_Acc: 50.000\n",
      "\n",
      "Epoch 266: Validation loss decreased (0.761947 --> 0.761730).  Saving model ...\n",
      "\t Train_Loss: 772604.4172 Train_Acc: 50.167 Val_Loss: 0.7617  BEST VAL Loss: 0.7617  Val_Acc: 50.000\n",
      "\n",
      "Epoch 267: Validation loss decreased (0.761730 --> 0.761516).  Saving model ...\n",
      "\t Train_Loss: 769721.5675 Train_Acc: 50.167 Val_Loss: 0.7615  BEST VAL Loss: 0.7615  Val_Acc: 50.000\n",
      "\n",
      "Epoch 268: Validation loss decreased (0.761516 --> 0.761303).  Saving model ...\n",
      "\t Train_Loss: 766860.1516 Train_Acc: 50.167 Val_Loss: 0.7613  BEST VAL Loss: 0.7613  Val_Acc: 50.000\n",
      "\n",
      "Epoch 269: Validation loss decreased (0.761303 --> 0.761091).  Saving model ...\n",
      "\t Train_Loss: 764019.9315 Train_Acc: 50.167 Val_Loss: 0.7611  BEST VAL Loss: 0.7611  Val_Acc: 50.000\n",
      "\n",
      "Epoch 270: Validation loss decreased (0.761091 --> 0.760882).  Saving model ...\n",
      "\t Train_Loss: 761200.6723 Train_Acc: 50.167 Val_Loss: 0.7609  BEST VAL Loss: 0.7609  Val_Acc: 50.000\n",
      "\n",
      "Epoch 271: Validation loss decreased (0.760882 --> 0.760673).  Saving model ...\n",
      "\t Train_Loss: 758402.1430 Train_Acc: 50.167 Val_Loss: 0.7607  BEST VAL Loss: 0.7607  Val_Acc: 50.000\n",
      "\n",
      "Epoch 272: Validation loss decreased (0.760673 --> 0.760466).  Saving model ...\n",
      "\t Train_Loss: 755624.1158 Train_Acc: 50.167 Val_Loss: 0.7605  BEST VAL Loss: 0.7605  Val_Acc: 50.000\n",
      "\n",
      "Epoch 273: Validation loss decreased (0.760466 --> 0.760261).  Saving model ...\n",
      "\t Train_Loss: 752866.3661 Train_Acc: 50.167 Val_Loss: 0.7603  BEST VAL Loss: 0.7603  Val_Acc: 50.000\n",
      "\n",
      "Epoch 274: Validation loss decreased (0.760261 --> 0.760057).  Saving model ...\n",
      "\t Train_Loss: 750128.6728 Train_Acc: 50.167 Val_Loss: 0.7601  BEST VAL Loss: 0.7601  Val_Acc: 50.000\n",
      "\n",
      "Epoch 275: Validation loss decreased (0.760057 --> 0.759855).  Saving model ...\n",
      "\t Train_Loss: 747410.8178 Train_Acc: 50.167 Val_Loss: 0.7599  BEST VAL Loss: 0.7599  Val_Acc: 50.000\n",
      "\n",
      "Epoch 276: Validation loss decreased (0.759855 --> 0.759654).  Saving model ...\n",
      "\t Train_Loss: 744712.5863 Train_Acc: 50.167 Val_Loss: 0.7597  BEST VAL Loss: 0.7597  Val_Acc: 50.000\n",
      "\n",
      "Epoch 277: Validation loss decreased (0.759654 --> 0.759455).  Saving model ...\n",
      "\t Train_Loss: 742033.7666 Train_Acc: 50.167 Val_Loss: 0.7595  BEST VAL Loss: 0.7595  Val_Acc: 50.000\n",
      "\n",
      "Epoch 278: Validation loss decreased (0.759455 --> 0.759257).  Saving model ...\n",
      "\t Train_Loss: 739374.1499 Train_Acc: 50.167 Val_Loss: 0.7593  BEST VAL Loss: 0.7593  Val_Acc: 50.000\n",
      "\n",
      "Epoch 279: Validation loss decreased (0.759257 --> 0.759060).  Saving model ...\n",
      "\t Train_Loss: 736733.5305 Train_Acc: 50.167 Val_Loss: 0.7591  BEST VAL Loss: 0.7591  Val_Acc: 50.000\n",
      "\n",
      "Epoch 280: Validation loss decreased (0.759060 --> 0.758865).  Saving model ...\n",
      "\t Train_Loss: 734111.7054 Train_Acc: 50.167 Val_Loss: 0.7589  BEST VAL Loss: 0.7589  Val_Acc: 50.000\n",
      "\n",
      "Epoch 281: Validation loss decreased (0.758865 --> 0.758671).  Saving model ...\n",
      "\t Train_Loss: 731508.4749 Train_Acc: 50.167 Val_Loss: 0.7587  BEST VAL Loss: 0.7587  Val_Acc: 50.000\n",
      "\n",
      "Epoch 282: Validation loss decreased (0.758671 --> 0.758479).  Saving model ...\n",
      "\t Train_Loss: 728923.6418 Train_Acc: 50.167 Val_Loss: 0.7585  BEST VAL Loss: 0.7585  Val_Acc: 50.000\n",
      "\n",
      "Epoch 283: Validation loss decreased (0.758479 --> 0.758288).  Saving model ...\n",
      "\t Train_Loss: 726357.0118 Train_Acc: 50.167 Val_Loss: 0.7583  BEST VAL Loss: 0.7583  Val_Acc: 50.000\n",
      "\n",
      "Epoch 284: Validation loss decreased (0.758288 --> 0.758098).  Saving model ...\n",
      "\t Train_Loss: 723808.3931 Train_Acc: 50.167 Val_Loss: 0.7581  BEST VAL Loss: 0.7581  Val_Acc: 50.000\n",
      "\n",
      "Epoch 285: Validation loss decreased (0.758098 --> 0.757909).  Saving model ...\n",
      "\t Train_Loss: 721277.5970 Train_Acc: 50.167 Val_Loss: 0.7579  BEST VAL Loss: 0.7579  Val_Acc: 50.000\n",
      "\n",
      "Epoch 286: Validation loss decreased (0.757909 --> 0.757722).  Saving model ...\n",
      "\t Train_Loss: 718764.4371 Train_Acc: 50.167 Val_Loss: 0.7577  BEST VAL Loss: 0.7577  Val_Acc: 50.000\n",
      "\n",
      "Epoch 287: Validation loss decreased (0.757722 --> 0.757537).  Saving model ...\n",
      "\t Train_Loss: 716268.7297 Train_Acc: 50.167 Val_Loss: 0.7575  BEST VAL Loss: 0.7575  Val_Acc: 50.000\n",
      "\n",
      "Epoch 288: Validation loss decreased (0.757537 --> 0.757352).  Saving model ...\n",
      "\t Train_Loss: 713790.2936 Train_Acc: 50.167 Val_Loss: 0.7574  BEST VAL Loss: 0.7574  Val_Acc: 50.000\n",
      "\n",
      "Epoch 289: Validation loss decreased (0.757352 --> 0.757169).  Saving model ...\n",
      "\t Train_Loss: 711328.9502 Train_Acc: 50.167 Val_Loss: 0.7572  BEST VAL Loss: 0.7572  Val_Acc: 50.000\n",
      "\n",
      "Epoch 290: Validation loss decreased (0.757169 --> 0.756987).  Saving model ...\n",
      "\t Train_Loss: 708884.5232 Train_Acc: 50.167 Val_Loss: 0.7570  BEST VAL Loss: 0.7570  Val_Acc: 50.000\n",
      "\n",
      "Epoch 291: Validation loss decreased (0.756987 --> 0.756806).  Saving model ...\n",
      "\t Train_Loss: 706456.8389 Train_Acc: 50.167 Val_Loss: 0.7568  BEST VAL Loss: 0.7568  Val_Acc: 50.000\n",
      "\n",
      "Epoch 292: Validation loss decreased (0.756806 --> 0.756627).  Saving model ...\n",
      "\t Train_Loss: 704045.7258 Train_Acc: 50.167 Val_Loss: 0.7566  BEST VAL Loss: 0.7566  Val_Acc: 50.000\n",
      "\n",
      "Epoch 293: Validation loss decreased (0.756627 --> 0.756448).  Saving model ...\n",
      "\t Train_Loss: 701651.0149 Train_Acc: 50.167 Val_Loss: 0.7564  BEST VAL Loss: 0.7564  Val_Acc: 50.000\n",
      "\n",
      "Epoch 294: Validation loss decreased (0.756448 --> 0.756271).  Saving model ...\n",
      "\t Train_Loss: 699272.5392 Train_Acc: 50.167 Val_Loss: 0.7563  BEST VAL Loss: 0.7563  Val_Acc: 50.000\n",
      "\n",
      "Epoch 295: Validation loss decreased (0.756271 --> 0.756095).  Saving model ...\n",
      "\t Train_Loss: 696910.1344 Train_Acc: 50.167 Val_Loss: 0.7561  BEST VAL Loss: 0.7561  Val_Acc: 50.000\n",
      "\n",
      "Epoch 296: Validation loss decreased (0.756095 --> 0.755921).  Saving model ...\n",
      "\t Train_Loss: 694563.6380 Train_Acc: 50.167 Val_Loss: 0.7559  BEST VAL Loss: 0.7559  Val_Acc: 50.000\n",
      "\n",
      "Epoch 297: Validation loss decreased (0.755921 --> 0.755747).  Saving model ...\n",
      "\t Train_Loss: 692232.8899 Train_Acc: 50.167 Val_Loss: 0.7557  BEST VAL Loss: 0.7557  Val_Acc: 50.000\n",
      "\n",
      "Epoch 298: Validation loss decreased (0.755747 --> 0.755575).  Saving model ...\n",
      "\t Train_Loss: 689917.7321 Train_Acc: 50.167 Val_Loss: 0.7556  BEST VAL Loss: 0.7556  Val_Acc: 50.000\n",
      "\n",
      "Epoch 299: Validation loss decreased (0.755575 --> 0.755404).  Saving model ...\n",
      "\t Train_Loss: 687618.0086 Train_Acc: 50.167 Val_Loss: 0.7554  BEST VAL Loss: 0.7554  Val_Acc: 50.000\n",
      "\n",
      "Epoch 300: Validation loss decreased (0.755404 --> 0.755234).  Saving model ...\n",
      "\t Train_Loss: 685333.5658 Train_Acc: 50.167 Val_Loss: 0.7552  BEST VAL Loss: 0.7552  Val_Acc: 50.000\n",
      "\n",
      "Epoch 301: Validation loss decreased (0.755234 --> 0.755065).  Saving model ...\n",
      "\t Train_Loss: 683064.2517 Train_Acc: 50.167 Val_Loss: 0.7551  BEST VAL Loss: 0.7551  Val_Acc: 50.000\n",
      "\n",
      "Epoch 302: Validation loss decreased (0.755065 --> 0.754897).  Saving model ...\n",
      "\t Train_Loss: 680809.9165 Train_Acc: 50.167 Val_Loss: 0.7549  BEST VAL Loss: 0.7549  Val_Acc: 50.000\n",
      "\n",
      "Epoch 303: Validation loss decreased (0.754897 --> 0.754730).  Saving model ...\n",
      "\t Train_Loss: 678570.4125 Train_Acc: 50.167 Val_Loss: 0.7547  BEST VAL Loss: 0.7547  Val_Acc: 50.000\n",
      "\n",
      "Epoch 304: Validation loss decreased (0.754730 --> 0.754565).  Saving model ...\n",
      "\t Train_Loss: 676345.5938 Train_Acc: 50.167 Val_Loss: 0.7546  BEST VAL Loss: 0.7546  Val_Acc: 50.000\n",
      "\n",
      "Epoch 305: Validation loss decreased (0.754565 --> 0.754400).  Saving model ...\n",
      "\t Train_Loss: 674135.3164 Train_Acc: 50.167 Val_Loss: 0.7544  BEST VAL Loss: 0.7544  Val_Acc: 50.000\n",
      "\n",
      "Epoch 306: Validation loss decreased (0.754400 --> 0.754236).  Saving model ...\n",
      "\t Train_Loss: 671939.4382 Train_Acc: 50.167 Val_Loss: 0.7542  BEST VAL Loss: 0.7542  Val_Acc: 50.000\n",
      "\n",
      "Epoch 307: Validation loss decreased (0.754236 --> 0.754074).  Saving model ...\n",
      "\t Train_Loss: 669757.8189 Train_Acc: 50.167 Val_Loss: 0.7541  BEST VAL Loss: 0.7541  Val_Acc: 50.000\n",
      "\n",
      "Epoch 308: Validation loss decreased (0.754074 --> 0.753913).  Saving model ...\n",
      "\t Train_Loss: 667590.3201 Train_Acc: 50.167 Val_Loss: 0.7539  BEST VAL Loss: 0.7539  Val_Acc: 50.000\n",
      "\n",
      "Epoch 309: Validation loss decreased (0.753913 --> 0.753752).  Saving model ...\n",
      "\t Train_Loss: 665436.8052 Train_Acc: 50.167 Val_Loss: 0.7538  BEST VAL Loss: 0.7538  Val_Acc: 50.000\n",
      "\n",
      "Epoch 310: Validation loss decreased (0.753752 --> 0.753593).  Saving model ...\n",
      "\t Train_Loss: 663297.1393 Train_Acc: 50.167 Val_Loss: 0.7536  BEST VAL Loss: 0.7536  Val_Acc: 50.000\n",
      "\n",
      "Epoch 311: Validation loss decreased (0.753593 --> 0.753435).  Saving model ...\n",
      "\t Train_Loss: 661171.1892 Train_Acc: 50.167 Val_Loss: 0.7534  BEST VAL Loss: 0.7534  Val_Acc: 50.000\n",
      "\n",
      "Epoch 312: Validation loss decreased (0.753435 --> 0.753278).  Saving model ...\n",
      "\t Train_Loss: 659058.8234 Train_Acc: 50.167 Val_Loss: 0.7533  BEST VAL Loss: 0.7533  Val_Acc: 50.000\n",
      "\n",
      "Epoch 313: Validation loss decreased (0.753278 --> 0.753121).  Saving model ...\n",
      "\t Train_Loss: 656959.9122 Train_Acc: 50.167 Val_Loss: 0.7531  BEST VAL Loss: 0.7531  Val_Acc: 50.000\n",
      "\n",
      "Epoch 314: Validation loss decreased (0.753121 --> 0.752966).  Saving model ...\n",
      "\t Train_Loss: 654874.3274 Train_Acc: 50.167 Val_Loss: 0.7530  BEST VAL Loss: 0.7530  Val_Acc: 50.000\n",
      "\n",
      "Epoch 315: Validation loss decreased (0.752966 --> 0.752812).  Saving model ...\n",
      "\t Train_Loss: 652801.9425 Train_Acc: 50.167 Val_Loss: 0.7528  BEST VAL Loss: 0.7528  Val_Acc: 50.000\n",
      "\n",
      "Epoch 316: Validation loss decreased (0.752812 --> 0.752659).  Saving model ...\n",
      "\t Train_Loss: 650742.6326 Train_Acc: 50.167 Val_Loss: 0.7527  BEST VAL Loss: 0.7527  Val_Acc: 50.000\n",
      "\n",
      "Epoch 317: Validation loss decreased (0.752659 --> 0.752506).  Saving model ...\n",
      "\t Train_Loss: 648696.2744 Train_Acc: 50.167 Val_Loss: 0.7525  BEST VAL Loss: 0.7525  Val_Acc: 50.000\n",
      "\n",
      "Epoch 318: Validation loss decreased (0.752506 --> 0.752355).  Saving model ...\n",
      "\t Train_Loss: 646662.7459 Train_Acc: 50.167 Val_Loss: 0.7524  BEST VAL Loss: 0.7524  Val_Acc: 50.000\n",
      "\n",
      "Epoch 319: Validation loss decreased (0.752355 --> 0.752204).  Saving model ...\n",
      "\t Train_Loss: 644641.9271 Train_Acc: 50.167 Val_Loss: 0.7522  BEST VAL Loss: 0.7522  Val_Acc: 50.000\n",
      "\n",
      "Epoch 320: Validation loss decreased (0.752204 --> 0.752055).  Saving model ...\n",
      "\t Train_Loss: 642633.6989 Train_Acc: 50.167 Val_Loss: 0.7521  BEST VAL Loss: 0.7521  Val_Acc: 50.000\n",
      "\n",
      "Epoch 321: Validation loss decreased (0.752055 --> 0.751906).  Saving model ...\n",
      "\t Train_Loss: 640637.9443 Train_Acc: 50.167 Val_Loss: 0.7519  BEST VAL Loss: 0.7519  Val_Acc: 50.000\n",
      "\n",
      "Epoch 322: Validation loss decreased (0.751906 --> 0.751759).  Saving model ...\n",
      "\t Train_Loss: 638654.5473 Train_Acc: 50.167 Val_Loss: 0.7518  BEST VAL Loss: 0.7518  Val_Acc: 50.000\n",
      "\n",
      "Epoch 323: Validation loss decreased (0.751759 --> 0.751612).  Saving model ...\n",
      "\t Train_Loss: 636683.3934 Train_Acc: 50.167 Val_Loss: 0.7516  BEST VAL Loss: 0.7516  Val_Acc: 50.000\n",
      "\n",
      "Epoch 324: Validation loss decreased (0.751612 --> 0.751466).  Saving model ...\n",
      "\t Train_Loss: 634724.3698 Train_Acc: 50.167 Val_Loss: 0.7515  BEST VAL Loss: 0.7515  Val_Acc: 50.000\n",
      "\n",
      "Epoch 325: Validation loss decreased (0.751466 --> 0.751321).  Saving model ...\n",
      "\t Train_Loss: 632777.3647 Train_Acc: 50.167 Val_Loss: 0.7513  BEST VAL Loss: 0.7513  Val_Acc: 50.000\n",
      "\n",
      "Epoch 326: Validation loss decreased (0.751321 --> 0.751177).  Saving model ...\n",
      "\t Train_Loss: 630842.2678 Train_Acc: 50.167 Val_Loss: 0.7512  BEST VAL Loss: 0.7512  Val_Acc: 50.000\n",
      "\n",
      "Epoch 327: Validation loss decreased (0.751177 --> 0.751034).  Saving model ...\n",
      "\t Train_Loss: 628918.9704 Train_Acc: 50.167 Val_Loss: 0.7510  BEST VAL Loss: 0.7510  Val_Acc: 50.000\n",
      "\n",
      "Epoch 328: Validation loss decreased (0.751034 --> 0.750892).  Saving model ...\n",
      "\t Train_Loss: 627007.3647 Train_Acc: 50.167 Val_Loss: 0.7509  BEST VAL Loss: 0.7509  Val_Acc: 50.000\n",
      "\n",
      "Epoch 329: Validation loss decreased (0.750892 --> 0.750750).  Saving model ...\n",
      "\t Train_Loss: 625107.3445 Train_Acc: 50.167 Val_Loss: 0.7508  BEST VAL Loss: 0.7508  Val_Acc: 50.000\n",
      "\n",
      "Epoch 330: Validation loss decreased (0.750750 --> 0.750610).  Saving model ...\n",
      "\t Train_Loss: 623218.8048 Train_Acc: 50.167 Val_Loss: 0.7506  BEST VAL Loss: 0.7506  Val_Acc: 50.000\n",
      "\n",
      "Epoch 331: Validation loss decreased (0.750610 --> 0.750470).  Saving model ...\n",
      "\t Train_Loss: 621341.6419 Train_Acc: 50.167 Val_Loss: 0.7505  BEST VAL Loss: 0.7505  Val_Acc: 50.000\n",
      "\n",
      "Epoch 332: Validation loss decreased (0.750470 --> 0.750331).  Saving model ...\n",
      "\t Train_Loss: 619475.7532 Train_Acc: 50.167 Val_Loss: 0.7503  BEST VAL Loss: 0.7503  Val_Acc: 50.000\n",
      "\n",
      "Epoch 333: Validation loss decreased (0.750331 --> 0.750193).  Saving model ...\n",
      "\t Train_Loss: 617621.0374 Train_Acc: 50.167 Val_Loss: 0.7502  BEST VAL Loss: 0.7502  Val_Acc: 50.000\n",
      "\n",
      "Epoch 334: Validation loss decreased (0.750193 --> 0.750056).  Saving model ...\n",
      "\t Train_Loss: 615777.3946 Train_Acc: 50.167 Val_Loss: 0.7501  BEST VAL Loss: 0.7501  Val_Acc: 50.000\n",
      "\n",
      "Epoch 335: Validation loss decreased (0.750056 --> 0.749919).  Saving model ...\n",
      "\t Train_Loss: 613944.7259 Train_Acc: 50.167 Val_Loss: 0.7499  BEST VAL Loss: 0.7499  Val_Acc: 50.000\n",
      "\n",
      "Epoch 336: Validation loss decreased (0.749919 --> 0.749783).  Saving model ...\n",
      "\t Train_Loss: 612122.9336 Train_Acc: 50.167 Val_Loss: 0.7498  BEST VAL Loss: 0.7498  Val_Acc: 50.000\n",
      "\n",
      "Epoch 337: Validation loss decreased (0.749783 --> 0.749649).  Saving model ...\n",
      "\t Train_Loss: 610311.9211 Train_Acc: 50.167 Val_Loss: 0.7496  BEST VAL Loss: 0.7496  Val_Acc: 50.000\n",
      "\n",
      "Epoch 338: Validation loss decreased (0.749649 --> 0.749515).  Saving model ...\n",
      "\t Train_Loss: 608511.5930 Train_Acc: 50.167 Val_Loss: 0.7495  BEST VAL Loss: 0.7495  Val_Acc: 50.000\n",
      "\n",
      "Epoch 339: Validation loss decreased (0.749515 --> 0.749381).  Saving model ...\n",
      "\t Train_Loss: 606721.8551 Train_Acc: 50.167 Val_Loss: 0.7494  BEST VAL Loss: 0.7494  Val_Acc: 50.000\n",
      "\n",
      "Epoch 340: Validation loss decreased (0.749381 --> 0.749249).  Saving model ...\n",
      "\t Train_Loss: 604942.6142 Train_Acc: 50.167 Val_Loss: 0.7492  BEST VAL Loss: 0.7492  Val_Acc: 50.000\n",
      "\n",
      "Epoch 341: Validation loss decreased (0.749249 --> 0.749117).  Saving model ...\n",
      "\t Train_Loss: 603173.7782 Train_Acc: 50.167 Val_Loss: 0.7491  BEST VAL Loss: 0.7491  Val_Acc: 50.000\n",
      "\n",
      "Epoch 342: Validation loss decreased (0.749117 --> 0.748986).  Saving model ...\n",
      "\t Train_Loss: 601415.2561 Train_Acc: 50.167 Val_Loss: 0.7490  BEST VAL Loss: 0.7490  Val_Acc: 50.000\n",
      "\n",
      "Epoch 343: Validation loss decreased (0.748986 --> 0.748856).  Saving model ...\n",
      "\t Train_Loss: 599666.9580 Train_Acc: 50.167 Val_Loss: 0.7489  BEST VAL Loss: 0.7489  Val_Acc: 50.000\n",
      "\n",
      "Epoch 344: Validation loss decreased (0.748856 --> 0.748727).  Saving model ...\n",
      "\t Train_Loss: 597928.7949 Train_Acc: 50.167 Val_Loss: 0.7487  BEST VAL Loss: 0.7487  Val_Acc: 50.000\n",
      "\n",
      "Epoch 345: Validation loss decreased (0.748727 --> 0.748598).  Saving model ...\n",
      "\t Train_Loss: 596200.6790 Train_Acc: 50.167 Val_Loss: 0.7486  BEST VAL Loss: 0.7486  Val_Acc: 50.000\n",
      "\n",
      "Epoch 346: Validation loss decreased (0.748598 --> 0.748470).  Saving model ...\n",
      "\t Train_Loss: 594482.5235 Train_Acc: 50.167 Val_Loss: 0.7485  BEST VAL Loss: 0.7485  Val_Acc: 50.000\n",
      "\n",
      "Epoch 347: Validation loss decreased (0.748470 --> 0.748343).  Saving model ...\n",
      "\t Train_Loss: 592774.2424 Train_Acc: 50.167 Val_Loss: 0.7483  BEST VAL Loss: 0.7483  Val_Acc: 50.000\n",
      "\n",
      "Epoch 348: Validation loss decreased (0.748343 --> 0.748217).  Saving model ...\n",
      "\t Train_Loss: 591075.7509 Train_Acc: 50.167 Val_Loss: 0.7482  BEST VAL Loss: 0.7482  Val_Acc: 50.000\n",
      "\n",
      "Epoch 349: Validation loss decreased (0.748217 --> 0.748091).  Saving model ...\n",
      "\t Train_Loss: 589386.9650 Train_Acc: 50.167 Val_Loss: 0.7481  BEST VAL Loss: 0.7481  Val_Acc: 50.000\n",
      "\n",
      "Epoch 350: Validation loss decreased (0.748091 --> 0.747966).  Saving model ...\n",
      "\t Train_Loss: 587707.8019 Train_Acc: 50.167 Val_Loss: 0.7480  BEST VAL Loss: 0.7480  Val_Acc: 50.000\n",
      "\n",
      "Epoch 351: Validation loss decreased (0.747966 --> 0.747842).  Saving model ...\n",
      "\t Train_Loss: 586038.1794 Train_Acc: 50.167 Val_Loss: 0.7478  BEST VAL Loss: 0.7478  Val_Acc: 50.000\n",
      "\n",
      "Epoch 352: Validation loss decreased (0.747842 --> 0.747718).  Saving model ...\n",
      "\t Train_Loss: 584378.0166 Train_Acc: 50.167 Val_Loss: 0.7477  BEST VAL Loss: 0.7477  Val_Acc: 50.000\n",
      "\n",
      "Epoch 353: Validation loss decreased (0.747718 --> 0.747595).  Saving model ...\n",
      "\t Train_Loss: 582727.2332 Train_Acc: 50.167 Val_Loss: 0.7476  BEST VAL Loss: 0.7476  Val_Acc: 50.000\n",
      "\n",
      "Epoch 354: Validation loss decreased (0.747595 --> 0.747473).  Saving model ...\n",
      "\t Train_Loss: 581085.7501 Train_Acc: 50.167 Val_Loss: 0.7475  BEST VAL Loss: 0.7475  Val_Acc: 50.000\n",
      "\n",
      "Epoch 355: Validation loss decreased (0.747473 --> 0.747351).  Saving model ...\n",
      "\t Train_Loss: 579453.4887 Train_Acc: 50.167 Val_Loss: 0.7474  BEST VAL Loss: 0.7474  Val_Acc: 50.000\n",
      "\n",
      "Epoch 356: Validation loss decreased (0.747351 --> 0.747231).  Saving model ...\n",
      "\t Train_Loss: 577830.3717 Train_Acc: 50.167 Val_Loss: 0.7472  BEST VAL Loss: 0.7472  Val_Acc: 50.000\n",
      "\n",
      "Epoch 357: Validation loss decreased (0.747231 --> 0.747110).  Saving model ...\n",
      "\t Train_Loss: 576216.3223 Train_Acc: 50.167 Val_Loss: 0.7471  BEST VAL Loss: 0.7471  Val_Acc: 50.000\n",
      "\n",
      "Epoch 358: Validation loss decreased (0.747110 --> 0.746991).  Saving model ...\n",
      "\t Train_Loss: 574611.2649 Train_Acc: 50.167 Val_Loss: 0.7470  BEST VAL Loss: 0.7470  Val_Acc: 50.000\n",
      "\n",
      "Epoch 359: Validation loss decreased (0.746991 --> 0.746872).  Saving model ...\n",
      "\t Train_Loss: 573015.1244 Train_Acc: 50.167 Val_Loss: 0.7469  BEST VAL Loss: 0.7469  Val_Acc: 50.000\n",
      "\n",
      "Epoch 360: Validation loss decreased (0.746872 --> 0.746754).  Saving model ...\n",
      "\t Train_Loss: 571427.8268 Train_Acc: 50.167 Val_Loss: 0.7468  BEST VAL Loss: 0.7468  Val_Acc: 50.000\n",
      "\n",
      "Epoch 361: Validation loss decreased (0.746754 --> 0.746636).  Saving model ...\n",
      "\t Train_Loss: 569849.2989 Train_Acc: 50.167 Val_Loss: 0.7466  BEST VAL Loss: 0.7466  Val_Acc: 50.000\n",
      "\n",
      "Epoch 362: Validation loss decreased (0.746636 --> 0.746520).  Saving model ...\n",
      "\t Train_Loss: 568279.4680 Train_Acc: 50.167 Val_Loss: 0.7465  BEST VAL Loss: 0.7465  Val_Acc: 50.000\n",
      "\n",
      "Epoch 363: Validation loss decreased (0.746520 --> 0.746403).  Saving model ...\n",
      "\t Train_Loss: 566718.2626 Train_Acc: 50.167 Val_Loss: 0.7464  BEST VAL Loss: 0.7464  Val_Acc: 50.000\n",
      "\n",
      "Epoch 364: Validation loss decreased (0.746403 --> 0.746288).  Saving model ...\n",
      "\t Train_Loss: 565165.6118 Train_Acc: 50.167 Val_Loss: 0.7463  BEST VAL Loss: 0.7463  Val_Acc: 50.000\n",
      "\n",
      "Epoch 365: Validation loss decreased (0.746288 --> 0.746173).  Saving model ...\n",
      "\t Train_Loss: 563621.4454 Train_Acc: 50.167 Val_Loss: 0.7462  BEST VAL Loss: 0.7462  Val_Acc: 50.000\n",
      "\n",
      "Epoch 366: Validation loss decreased (0.746173 --> 0.746058).  Saving model ...\n",
      "\t Train_Loss: 562085.6940 Train_Acc: 50.167 Val_Loss: 0.7461  BEST VAL Loss: 0.7461  Val_Acc: 50.000\n",
      "\n",
      "Epoch 367: Validation loss decreased (0.746058 --> 0.745945).  Saving model ...\n",
      "\t Train_Loss: 560558.2892 Train_Acc: 50.167 Val_Loss: 0.7459  BEST VAL Loss: 0.7459  Val_Acc: 50.000\n",
      "\n",
      "Epoch 368: Validation loss decreased (0.745945 --> 0.745832).  Saving model ...\n",
      "\t Train_Loss: 559039.1629 Train_Acc: 50.167 Val_Loss: 0.7458  BEST VAL Loss: 0.7458  Val_Acc: 50.000\n",
      "\n",
      "Epoch 369: Validation loss decreased (0.745832 --> 0.745719).  Saving model ...\n",
      "\t Train_Loss: 557528.2482 Train_Acc: 50.167 Val_Loss: 0.7457  BEST VAL Loss: 0.7457  Val_Acc: 50.000\n",
      "\n",
      "Epoch 370: Validation loss decreased (0.745719 --> 0.745607).  Saving model ...\n",
      "\t Train_Loss: 556025.4785 Train_Acc: 50.167 Val_Loss: 0.7456  BEST VAL Loss: 0.7456  Val_Acc: 50.000\n",
      "\n",
      "Epoch 371: Validation loss decreased (0.745607 --> 0.745496).  Saving model ...\n",
      "\t Train_Loss: 554530.7882 Train_Acc: 50.167 Val_Loss: 0.7455  BEST VAL Loss: 0.7455  Val_Acc: 50.000\n",
      "\n",
      "Epoch 372: Validation loss decreased (0.745496 --> 0.745385).  Saving model ...\n",
      "\t Train_Loss: 553044.1124 Train_Acc: 50.167 Val_Loss: 0.7454  BEST VAL Loss: 0.7454  Val_Acc: 50.000\n",
      "\n",
      "Epoch 373: Validation loss decreased (0.745385 --> 0.745275).  Saving model ...\n",
      "\t Train_Loss: 551565.3867 Train_Acc: 50.167 Val_Loss: 0.7453  BEST VAL Loss: 0.7453  Val_Acc: 50.000\n",
      "\n",
      "Epoch 374: Validation loss decreased (0.745275 --> 0.745166).  Saving model ...\n",
      "\t Train_Loss: 550094.5476 Train_Acc: 50.167 Val_Loss: 0.7452  BEST VAL Loss: 0.7452  Val_Acc: 50.000\n",
      "\n",
      "Epoch 375: Validation loss decreased (0.745166 --> 0.745057).  Saving model ...\n",
      "\t Train_Loss: 548631.5320 Train_Acc: 50.167 Val_Loss: 0.7451  BEST VAL Loss: 0.7451  Val_Acc: 50.000\n",
      "\n",
      "Epoch 376: Validation loss decreased (0.745057 --> 0.744949).  Saving model ...\n",
      "\t Train_Loss: 547176.2778 Train_Acc: 50.167 Val_Loss: 0.7449  BEST VAL Loss: 0.7449  Val_Acc: 50.000\n",
      "\n",
      "Epoch 377: Validation loss decreased (0.744949 --> 0.744841).  Saving model ...\n",
      "\t Train_Loss: 545728.7234 Train_Acc: 50.167 Val_Loss: 0.7448  BEST VAL Loss: 0.7448  Val_Acc: 50.000\n",
      "\n",
      "Epoch 378: Validation loss decreased (0.744841 --> 0.744734).  Saving model ...\n",
      "\t Train_Loss: 544288.8078 Train_Acc: 50.167 Val_Loss: 0.7447  BEST VAL Loss: 0.7447  Val_Acc: 50.000\n",
      "\n",
      "Epoch 379: Validation loss decreased (0.744734 --> 0.744627).  Saving model ...\n",
      "\t Train_Loss: 542856.4707 Train_Acc: 50.167 Val_Loss: 0.7446  BEST VAL Loss: 0.7446  Val_Acc: 50.000\n",
      "\n",
      "Epoch 380: Validation loss decreased (0.744627 --> 0.744521).  Saving model ...\n",
      "\t Train_Loss: 541431.6524 Train_Acc: 50.167 Val_Loss: 0.7445  BEST VAL Loss: 0.7445  Val_Acc: 50.000\n",
      "\n",
      "Epoch 381: Validation loss decreased (0.744521 --> 0.744415).  Saving model ...\n",
      "\t Train_Loss: 540014.2939 Train_Acc: 50.167 Val_Loss: 0.7444  BEST VAL Loss: 0.7444  Val_Acc: 50.000\n",
      "\n",
      "Epoch 382: Validation loss decreased (0.744415 --> 0.744310).  Saving model ...\n",
      "\t Train_Loss: 538604.3367 Train_Acc: 50.167 Val_Loss: 0.7443  BEST VAL Loss: 0.7443  Val_Acc: 50.000\n",
      "\n",
      "Epoch 383: Validation loss decreased (0.744310 --> 0.744206).  Saving model ...\n",
      "\t Train_Loss: 537201.7231 Train_Acc: 50.167 Val_Loss: 0.7442  BEST VAL Loss: 0.7442  Val_Acc: 50.000\n",
      "\n",
      "Epoch 384: Validation loss decreased (0.744206 --> 0.744102).  Saving model ...\n",
      "\t Train_Loss: 535806.3958 Train_Acc: 50.167 Val_Loss: 0.7441  BEST VAL Loss: 0.7441  Val_Acc: 50.000\n",
      "\n",
      "Epoch 385: Validation loss decreased (0.744102 --> 0.743999).  Saving model ...\n",
      "\t Train_Loss: 534418.2981 Train_Acc: 50.167 Val_Loss: 0.7440  BEST VAL Loss: 0.7440  Val_Acc: 50.000\n",
      "\n",
      "Epoch 386: Validation loss decreased (0.743999 --> 0.743896).  Saving model ...\n",
      "\t Train_Loss: 533037.3741 Train_Acc: 50.167 Val_Loss: 0.7439  BEST VAL Loss: 0.7439  Val_Acc: 50.000\n",
      "\n",
      "Epoch 387: Validation loss decreased (0.743896 --> 0.743794).  Saving model ...\n",
      "\t Train_Loss: 531663.5683 Train_Acc: 50.167 Val_Loss: 0.7438  BEST VAL Loss: 0.7438  Val_Acc: 50.000\n",
      "\n",
      "Epoch 388: Validation loss decreased (0.743794 --> 0.743692).  Saving model ...\n",
      "\t Train_Loss: 530296.8257 Train_Acc: 50.167 Val_Loss: 0.7437  BEST VAL Loss: 0.7437  Val_Acc: 50.000\n",
      "\n",
      "Epoch 389: Validation loss decreased (0.743692 --> 0.743591).  Saving model ...\n",
      "\t Train_Loss: 528937.0920 Train_Acc: 50.167 Val_Loss: 0.7436  BEST VAL Loss: 0.7436  Val_Acc: 50.000\n",
      "\n",
      "Epoch 390: Validation loss decreased (0.743591 --> 0.743490).  Saving model ...\n",
      "\t Train_Loss: 527584.3135 Train_Acc: 50.167 Val_Loss: 0.7435  BEST VAL Loss: 0.7435  Val_Acc: 50.000\n",
      "\n",
      "Epoch 391: Validation loss decreased (0.743490 --> 0.743390).  Saving model ...\n",
      "\t Train_Loss: 526238.4370 Train_Acc: 50.167 Val_Loss: 0.7434  BEST VAL Loss: 0.7434  Val_Acc: 50.000\n",
      "\n",
      "Epoch 392: Validation loss decreased (0.743390 --> 0.743290).  Saving model ...\n",
      "\t Train_Loss: 524899.4097 Train_Acc: 50.167 Val_Loss: 0.7433  BEST VAL Loss: 0.7433  Val_Acc: 50.000\n",
      "\n",
      "Epoch 393: Validation loss decreased (0.743290 --> 0.743191).  Saving model ...\n",
      "\t Train_Loss: 523567.1794 Train_Acc: 50.167 Val_Loss: 0.7432  BEST VAL Loss: 0.7432  Val_Acc: 50.000\n",
      "\n",
      "Epoch 394: Validation loss decreased (0.743191 --> 0.743092).  Saving model ...\n",
      "\t Train_Loss: 522241.6947 Train_Acc: 50.167 Val_Loss: 0.7431  BEST VAL Loss: 0.7431  Val_Acc: 50.000\n",
      "\n",
      "Epoch 395: Validation loss decreased (0.743092 --> 0.742994).  Saving model ...\n",
      "\t Train_Loss: 520922.9043 Train_Acc: 50.167 Val_Loss: 0.7430  BEST VAL Loss: 0.7430  Val_Acc: 50.000\n",
      "\n",
      "Epoch 396: Validation loss decreased (0.742994 --> 0.742896).  Saving model ...\n",
      "\t Train_Loss: 519610.7577 Train_Acc: 50.167 Val_Loss: 0.7429  BEST VAL Loss: 0.7429  Val_Acc: 50.000\n",
      "\n",
      "Epoch 397: Validation loss decreased (0.742896 --> 0.742799).  Saving model ...\n",
      "\t Train_Loss: 518305.2048 Train_Acc: 50.167 Val_Loss: 0.7428  BEST VAL Loss: 0.7428  Val_Acc: 50.000\n",
      "\n",
      "Epoch 398: Validation loss decreased (0.742799 --> 0.742703).  Saving model ...\n",
      "\t Train_Loss: 517006.1960 Train_Acc: 50.167 Val_Loss: 0.7427  BEST VAL Loss: 0.7427  Val_Acc: 50.000\n",
      "\n",
      "Epoch 399: Validation loss decreased (0.742703 --> 0.742606).  Saving model ...\n",
      "\t Train_Loss: 515713.6823 Train_Acc: 50.167 Val_Loss: 0.7426  BEST VAL Loss: 0.7426  Val_Acc: 50.000\n",
      "\n",
      "Epoch 400: Validation loss decreased (0.742606 --> 0.742511).  Saving model ...\n",
      "\t Train_Loss: 514427.6150 Train_Acc: 50.167 Val_Loss: 0.7425  BEST VAL Loss: 0.7425  Val_Acc: 50.000\n",
      "\n",
      "Epoch 401: Validation loss decreased (0.742511 --> 0.742415).  Saving model ...\n",
      "\t Train_Loss: 513147.9461 Train_Acc: 50.167 Val_Loss: 0.7424  BEST VAL Loss: 0.7424  Val_Acc: 50.000\n",
      "\n",
      "Epoch 402: Validation loss decreased (0.742415 --> 0.742321).  Saving model ...\n",
      "\t Train_Loss: 511874.6279 Train_Acc: 50.167 Val_Loss: 0.7423  BEST VAL Loss: 0.7423  Val_Acc: 50.000\n",
      "\n",
      "Epoch 403: Validation loss decreased (0.742321 --> 0.742226).  Saving model ...\n",
      "\t Train_Loss: 510607.6132 Train_Acc: 50.167 Val_Loss: 0.7422  BEST VAL Loss: 0.7422  Val_Acc: 50.000\n",
      "\n",
      "Epoch 404: Validation loss decreased (0.742226 --> 0.742132).  Saving model ...\n",
      "\t Train_Loss: 509346.8554 Train_Acc: 50.167 Val_Loss: 0.7421  BEST VAL Loss: 0.7421  Val_Acc: 50.000\n",
      "\n",
      "Epoch 405: Validation loss decreased (0.742132 --> 0.742039).  Saving model ...\n",
      "\t Train_Loss: 508092.3082 Train_Acc: 50.167 Val_Loss: 0.7420  BEST VAL Loss: 0.7420  Val_Acc: 50.000\n",
      "\n",
      "Epoch 406: Validation loss decreased (0.742039 --> 0.741946).  Saving model ...\n",
      "\t Train_Loss: 506843.9259 Train_Acc: 50.167 Val_Loss: 0.7419  BEST VAL Loss: 0.7419  Val_Acc: 50.000\n",
      "\n",
      "Epoch 407: Validation loss decreased (0.741946 --> 0.741854).  Saving model ...\n",
      "\t Train_Loss: 505601.6631 Train_Acc: 50.167 Val_Loss: 0.7419  BEST VAL Loss: 0.7419  Val_Acc: 50.000\n",
      "\n",
      "Epoch 408: Validation loss decreased (0.741854 --> 0.741761).  Saving model ...\n",
      "\t Train_Loss: 504365.4749 Train_Acc: 50.167 Val_Loss: 0.7418  BEST VAL Loss: 0.7418  Val_Acc: 50.000\n",
      "\n",
      "Epoch 409: Validation loss decreased (0.741761 --> 0.741670).  Saving model ...\n",
      "\t Train_Loss: 503135.3170 Train_Acc: 50.167 Val_Loss: 0.7417  BEST VAL Loss: 0.7417  Val_Acc: 50.000\n",
      "\n",
      "Epoch 410: Validation loss decreased (0.741670 --> 0.741579).  Saving model ...\n",
      "\t Train_Loss: 501911.1452 Train_Acc: 50.167 Val_Loss: 0.7416  BEST VAL Loss: 0.7416  Val_Acc: 50.000\n",
      "\n",
      "Epoch 411: Validation loss decreased (0.741579 --> 0.741488).  Saving model ...\n",
      "\t Train_Loss: 500692.9159 Train_Acc: 50.167 Val_Loss: 0.7415  BEST VAL Loss: 0.7415  Val_Acc: 50.000\n",
      "\n",
      "Epoch 412: Validation loss decreased (0.741488 --> 0.741398).  Saving model ...\n",
      "\t Train_Loss: 499480.5861 Train_Acc: 50.167 Val_Loss: 0.7414  BEST VAL Loss: 0.7414  Val_Acc: 50.000\n",
      "\n",
      "Epoch 413: Validation loss decreased (0.741398 --> 0.741308).  Saving model ...\n",
      "\t Train_Loss: 498274.1130 Train_Acc: 50.167 Val_Loss: 0.7413  BEST VAL Loss: 0.7413  Val_Acc: 50.000\n",
      "\n",
      "Epoch 414: Validation loss decreased (0.741308 --> 0.741219).  Saving model ...\n",
      "\t Train_Loss: 497073.4541 Train_Acc: 50.167 Val_Loss: 0.7412  BEST VAL Loss: 0.7412  Val_Acc: 50.000\n",
      "\n",
      "Epoch 415: Validation loss decreased (0.741219 --> 0.741130).  Saving model ...\n",
      "\t Train_Loss: 495878.5677 Train_Acc: 50.167 Val_Loss: 0.7411  BEST VAL Loss: 0.7411  Val_Acc: 50.000\n",
      "\n",
      "Epoch 416: Validation loss decreased (0.741130 --> 0.741041).  Saving model ...\n",
      "\t Train_Loss: 494689.4122 Train_Acc: 50.167 Val_Loss: 0.7410  BEST VAL Loss: 0.7410  Val_Acc: 50.000\n",
      "\n",
      "Epoch 417: Validation loss decreased (0.741041 --> 0.740953).  Saving model ...\n",
      "\t Train_Loss: 493505.9464 Train_Acc: 50.167 Val_Loss: 0.7410  BEST VAL Loss: 0.7410  Val_Acc: 50.000\n",
      "\n",
      "Epoch 418: Validation loss decreased (0.740953 --> 0.740865).  Saving model ...\n",
      "\t Train_Loss: 492328.1296 Train_Acc: 50.167 Val_Loss: 0.7409  BEST VAL Loss: 0.7409  Val_Acc: 50.000\n",
      "\n",
      "Epoch 419: Validation loss decreased (0.740865 --> 0.740778).  Saving model ...\n",
      "\t Train_Loss: 491155.9214 Train_Acc: 50.167 Val_Loss: 0.7408  BEST VAL Loss: 0.7408  Val_Acc: 50.000\n",
      "\n",
      "Epoch 420: Validation loss decreased (0.740778 --> 0.740691).  Saving model ...\n",
      "\t Train_Loss: 489989.2819 Train_Acc: 50.167 Val_Loss: 0.7407  BEST VAL Loss: 0.7407  Val_Acc: 50.000\n",
      "\n",
      "Epoch 421: Validation loss decreased (0.740691 --> 0.740605).  Saving model ...\n",
      "\t Train_Loss: 488828.1715 Train_Acc: 50.167 Val_Loss: 0.7406  BEST VAL Loss: 0.7406  Val_Acc: 50.000\n",
      "\n",
      "Epoch 422: Validation loss decreased (0.740605 --> 0.740519).  Saving model ...\n",
      "\t Train_Loss: 487672.5511 Train_Acc: 50.167 Val_Loss: 0.7405  BEST VAL Loss: 0.7405  Val_Acc: 50.000\n",
      "\n",
      "Epoch 423: Validation loss decreased (0.740519 --> 0.740433).  Saving model ...\n",
      "\t Train_Loss: 486522.3816 Train_Acc: 50.167 Val_Loss: 0.7404  BEST VAL Loss: 0.7404  Val_Acc: 50.000\n",
      "\n",
      "Epoch 424: Validation loss decreased (0.740433 --> 0.740348).  Saving model ...\n",
      "\t Train_Loss: 485377.6247 Train_Acc: 50.167 Val_Loss: 0.7403  BEST VAL Loss: 0.7403  Val_Acc: 50.000\n",
      "\n",
      "Epoch 425: Validation loss decreased (0.740348 --> 0.740263).  Saving model ...\n",
      "\t Train_Loss: 484238.2423 Train_Acc: 50.167 Val_Loss: 0.7403  BEST VAL Loss: 0.7403  Val_Acc: 50.000\n",
      "\n",
      "Epoch 426: Validation loss decreased (0.740263 --> 0.740179).  Saving model ...\n",
      "\t Train_Loss: 483104.1965 Train_Acc: 50.167 Val_Loss: 0.7402  BEST VAL Loss: 0.7402  Val_Acc: 50.000\n",
      "\n",
      "Epoch 427: Validation loss decreased (0.740179 --> 0.740095).  Saving model ...\n",
      "\t Train_Loss: 481975.4500 Train_Acc: 50.167 Val_Loss: 0.7401  BEST VAL Loss: 0.7401  Val_Acc: 50.000\n",
      "\n",
      "Epoch 428: Validation loss decreased (0.740095 --> 0.740011).  Saving model ...\n",
      "\t Train_Loss: 480851.9658 Train_Acc: 50.167 Val_Loss: 0.7400  BEST VAL Loss: 0.7400  Val_Acc: 50.000\n",
      "\n",
      "Epoch 429: Validation loss decreased (0.740011 --> 0.739928).  Saving model ...\n",
      "\t Train_Loss: 479733.7070 Train_Acc: 50.167 Val_Loss: 0.7399  BEST VAL Loss: 0.7399  Val_Acc: 50.000\n",
      "\n",
      "Epoch 430: Validation loss decreased (0.739928 --> 0.739845).  Saving model ...\n",
      "\t Train_Loss: 478620.6374 Train_Acc: 50.167 Val_Loss: 0.7398  BEST VAL Loss: 0.7398  Val_Acc: 50.000\n",
      "\n",
      "Epoch 431: Validation loss decreased (0.739845 --> 0.739762).  Saving model ...\n",
      "\t Train_Loss: 477512.7209 Train_Acc: 50.167 Val_Loss: 0.7398  BEST VAL Loss: 0.7398  Val_Acc: 50.000\n",
      "\n",
      "Epoch 432: Validation loss decreased (0.739762 --> 0.739680).  Saving model ...\n",
      "\t Train_Loss: 476409.9218 Train_Acc: 50.167 Val_Loss: 0.7397  BEST VAL Loss: 0.7397  Val_Acc: 50.000\n",
      "\n",
      "Epoch 433: Validation loss decreased (0.739680 --> 0.739599).  Saving model ...\n",
      "\t Train_Loss: 475312.2047 Train_Acc: 50.167 Val_Loss: 0.7396  BEST VAL Loss: 0.7396  Val_Acc: 50.000\n",
      "\n",
      "Epoch 434: Validation loss decreased (0.739599 --> 0.739517).  Saving model ...\n",
      "\t Train_Loss: 474219.5346 Train_Acc: 50.167 Val_Loss: 0.7395  BEST VAL Loss: 0.7395  Val_Acc: 50.000\n",
      "\n",
      "Epoch 435: Validation loss decreased (0.739517 --> 0.739436).  Saving model ...\n",
      "\t Train_Loss: 473131.8767 Train_Acc: 50.167 Val_Loss: 0.7394  BEST VAL Loss: 0.7394  Val_Acc: 50.000\n",
      "\n",
      "Epoch 436: Validation loss decreased (0.739436 --> 0.739356).  Saving model ...\n",
      "\t Train_Loss: 472049.1967 Train_Acc: 50.167 Val_Loss: 0.7394  BEST VAL Loss: 0.7394  Val_Acc: 50.000\n",
      "\n",
      "Epoch 437: Validation loss decreased (0.739356 --> 0.739275).  Saving model ...\n",
      "\t Train_Loss: 470971.4604 Train_Acc: 50.167 Val_Loss: 0.7393  BEST VAL Loss: 0.7393  Val_Acc: 50.000\n",
      "\n",
      "Epoch 438: Validation loss decreased (0.739275 --> 0.739196).  Saving model ...\n",
      "\t Train_Loss: 469898.6341 Train_Acc: 50.167 Val_Loss: 0.7392  BEST VAL Loss: 0.7392  Val_Acc: 50.000\n",
      "\n",
      "Epoch 439: Validation loss decreased (0.739196 --> 0.739116).  Saving model ...\n",
      "\t Train_Loss: 468830.6842 Train_Acc: 50.167 Val_Loss: 0.7391  BEST VAL Loss: 0.7391  Val_Acc: 50.000\n",
      "\n",
      "Epoch 440: Validation loss decreased (0.739116 --> 0.739037).  Saving model ...\n",
      "\t Train_Loss: 467767.5777 Train_Acc: 50.167 Val_Loss: 0.7390  BEST VAL Loss: 0.7390  Val_Acc: 50.000\n",
      "\n",
      "Epoch 441: Validation loss decreased (0.739037 --> 0.738958).  Saving model ...\n",
      "\t Train_Loss: 466709.2816 Train_Acc: 50.167 Val_Loss: 0.7390  BEST VAL Loss: 0.7390  Val_Acc: 50.000\n",
      "\n",
      "Epoch 442: Validation loss decreased (0.738958 --> 0.738880).  Saving model ...\n",
      "\t Train_Loss: 465655.7633 Train_Acc: 50.167 Val_Loss: 0.7389  BEST VAL Loss: 0.7389  Val_Acc: 50.000\n",
      "\n",
      "Epoch 443: Validation loss decreased (0.738880 --> 0.738802).  Saving model ...\n",
      "\t Train_Loss: 464606.9907 Train_Acc: 50.167 Val_Loss: 0.7388  BEST VAL Loss: 0.7388  Val_Acc: 50.000\n",
      "\n",
      "Epoch 444: Validation loss decreased (0.738802 --> 0.738724).  Saving model ...\n",
      "\t Train_Loss: 463562.9316 Train_Acc: 50.167 Val_Loss: 0.7387  BEST VAL Loss: 0.7387  Val_Acc: 50.000\n",
      "\n",
      "Epoch 445: Validation loss decreased (0.738724 --> 0.738647).  Saving model ...\n",
      "\t Train_Loss: 462523.5544 Train_Acc: 50.167 Val_Loss: 0.7386  BEST VAL Loss: 0.7386  Val_Acc: 50.000\n",
      "\n",
      "Epoch 446: Validation loss decreased (0.738647 --> 0.738570).  Saving model ...\n",
      "\t Train_Loss: 461488.8277 Train_Acc: 50.167 Val_Loss: 0.7386  BEST VAL Loss: 0.7386  Val_Acc: 50.000\n",
      "\n",
      "Epoch 447: Validation loss decreased (0.738570 --> 0.738493).  Saving model ...\n",
      "\t Train_Loss: 460458.7203 Train_Acc: 50.167 Val_Loss: 0.7385  BEST VAL Loss: 0.7385  Val_Acc: 50.000\n",
      "\n",
      "Epoch 448: Validation loss decreased (0.738493 --> 0.738417).  Saving model ...\n",
      "\t Train_Loss: 459433.2013 Train_Acc: 50.167 Val_Loss: 0.7384  BEST VAL Loss: 0.7384  Val_Acc: 50.000\n",
      "\n",
      "Epoch 449: Validation loss decreased (0.738417 --> 0.738341).  Saving model ...\n",
      "\t Train_Loss: 458412.2402 Train_Acc: 50.167 Val_Loss: 0.7383  BEST VAL Loss: 0.7383  Val_Acc: 50.000\n",
      "\n",
      "Epoch 450: Validation loss decreased (0.738341 --> 0.738265).  Saving model ...\n",
      "\t Train_Loss: 457395.8066 Train_Acc: 50.167 Val_Loss: 0.7383  BEST VAL Loss: 0.7383  Val_Acc: 50.000\n",
      "\n",
      "Epoch 451: Validation loss decreased (0.738265 --> 0.738190).  Saving model ...\n",
      "\t Train_Loss: 456383.8706 Train_Acc: 50.167 Val_Loss: 0.7382  BEST VAL Loss: 0.7382  Val_Acc: 50.000\n",
      "\n",
      "Epoch 452: Validation loss decreased (0.738190 --> 0.738115).  Saving model ...\n",
      "\t Train_Loss: 455376.4022 Train_Acc: 50.167 Val_Loss: 0.7381  BEST VAL Loss: 0.7381  Val_Acc: 50.000\n",
      "\n",
      "Epoch 453: Validation loss decreased (0.738115 --> 0.738040).  Saving model ...\n",
      "\t Train_Loss: 454373.3720 Train_Acc: 50.167 Val_Loss: 0.7380  BEST VAL Loss: 0.7380  Val_Acc: 50.000\n",
      "\n",
      "Epoch 454: Validation loss decreased (0.738040 --> 0.737965).  Saving model ...\n",
      "\t Train_Loss: 453374.7508 Train_Acc: 50.167 Val_Loss: 0.7380  BEST VAL Loss: 0.7380  Val_Acc: 50.000\n",
      "\n",
      "Epoch 455: Validation loss decreased (0.737965 --> 0.737891).  Saving model ...\n",
      "\t Train_Loss: 452380.5094 Train_Acc: 50.167 Val_Loss: 0.7379  BEST VAL Loss: 0.7379  Val_Acc: 50.000\n",
      "\n",
      "Epoch 456: Validation loss decreased (0.737891 --> 0.737818).  Saving model ...\n",
      "\t Train_Loss: 451390.6193 Train_Acc: 50.167 Val_Loss: 0.7378  BEST VAL Loss: 0.7378  Val_Acc: 50.000\n",
      "\n",
      "Epoch 457: Validation loss decreased (0.737818 --> 0.737744).  Saving model ...\n",
      "\t Train_Loss: 450405.0518 Train_Acc: 50.167 Val_Loss: 0.7377  BEST VAL Loss: 0.7377  Val_Acc: 50.000\n",
      "\n",
      "Epoch 458: Validation loss decreased (0.737744 --> 0.737671).  Saving model ...\n",
      "\t Train_Loss: 449423.7787 Train_Acc: 50.167 Val_Loss: 0.7377  BEST VAL Loss: 0.7377  Val_Acc: 50.000\n",
      "\n",
      "Epoch 459: Validation loss decreased (0.737671 --> 0.737599).  Saving model ...\n",
      "\t Train_Loss: 448446.7720 Train_Acc: 50.167 Val_Loss: 0.7376  BEST VAL Loss: 0.7376  Val_Acc: 50.000\n",
      "\n",
      "Epoch 460: Validation loss decreased (0.737599 --> 0.737526).  Saving model ...\n",
      "\t Train_Loss: 447474.0040 Train_Acc: 50.167 Val_Loss: 0.7375  BEST VAL Loss: 0.7375  Val_Acc: 50.000\n",
      "\n",
      "Epoch 461: Validation loss decreased (0.737526 --> 0.737454).  Saving model ...\n",
      "\t Train_Loss: 446505.4470 Train_Acc: 50.167 Val_Loss: 0.7375  BEST VAL Loss: 0.7375  Val_Acc: 50.000\n",
      "\n",
      "Epoch 462: Validation loss decreased (0.737454 --> 0.737382).  Saving model ...\n",
      "\t Train_Loss: 445541.0739 Train_Acc: 50.167 Val_Loss: 0.7374  BEST VAL Loss: 0.7374  Val_Acc: 50.000\n",
      "\n",
      "Epoch 463: Validation loss decreased (0.737382 --> 0.737311).  Saving model ...\n",
      "\t Train_Loss: 444580.8576 Train_Acc: 50.167 Val_Loss: 0.7373  BEST VAL Loss: 0.7373  Val_Acc: 50.000\n",
      "\n",
      "Epoch 464: Validation loss decreased (0.737311 --> 0.737240).  Saving model ...\n",
      "\t Train_Loss: 443624.7713 Train_Acc: 50.167 Val_Loss: 0.7372  BEST VAL Loss: 0.7372  Val_Acc: 50.000\n",
      "\n",
      "Epoch 465: Validation loss decreased (0.737240 --> 0.737169).  Saving model ...\n",
      "\t Train_Loss: 442672.7883 Train_Acc: 50.167 Val_Loss: 0.7372  BEST VAL Loss: 0.7372  Val_Acc: 50.000\n",
      "\n",
      "Epoch 466: Validation loss decreased (0.737169 --> 0.737098).  Saving model ...\n",
      "\t Train_Loss: 441724.8823 Train_Acc: 50.167 Val_Loss: 0.7371  BEST VAL Loss: 0.7371  Val_Acc: 50.000\n",
      "\n",
      "Epoch 467: Validation loss decreased (0.737098 --> 0.737028).  Saving model ...\n",
      "\t Train_Loss: 440781.0272 Train_Acc: 50.167 Val_Loss: 0.7370  BEST VAL Loss: 0.7370  Val_Acc: 50.000\n",
      "\n",
      "Epoch 468: Validation loss decreased (0.737028 --> 0.736958).  Saving model ...\n",
      "\t Train_Loss: 439841.1971 Train_Acc: 50.167 Val_Loss: 0.7370  BEST VAL Loss: 0.7370  Val_Acc: 50.000\n",
      "\n",
      "Epoch 469: Validation loss decreased (0.736958 --> 0.736888).  Saving model ...\n",
      "\t Train_Loss: 438905.3663 Train_Acc: 50.167 Val_Loss: 0.7369  BEST VAL Loss: 0.7369  Val_Acc: 50.000\n",
      "\n",
      "Epoch 470: Validation loss decreased (0.736888 --> 0.736819).  Saving model ...\n",
      "\t Train_Loss: 437973.5092 Train_Acc: 50.167 Val_Loss: 0.7368  BEST VAL Loss: 0.7368  Val_Acc: 50.000\n",
      "\n",
      "Epoch 471: Validation loss decreased (0.736819 --> 0.736750).  Saving model ...\n",
      "\t Train_Loss: 437045.6008 Train_Acc: 50.167 Val_Loss: 0.7367  BEST VAL Loss: 0.7367  Val_Acc: 50.000\n",
      "\n",
      "Epoch 472: Validation loss decreased (0.736750 --> 0.736681).  Saving model ...\n",
      "\t Train_Loss: 436121.6158 Train_Acc: 50.167 Val_Loss: 0.7367  BEST VAL Loss: 0.7367  Val_Acc: 50.000\n",
      "\n",
      "Epoch 473: Validation loss decreased (0.736681 --> 0.736613).  Saving model ...\n",
      "\t Train_Loss: 435201.5295 Train_Acc: 50.167 Val_Loss: 0.7366  BEST VAL Loss: 0.7366  Val_Acc: 50.000\n",
      "\n",
      "Epoch 474: Validation loss decreased (0.736613 --> 0.736544).  Saving model ...\n",
      "\t Train_Loss: 434285.3172 Train_Acc: 50.167 Val_Loss: 0.7365  BEST VAL Loss: 0.7365  Val_Acc: 50.000\n",
      "\n",
      "Epoch 475: Validation loss decreased (0.736544 --> 0.736476).  Saving model ...\n",
      "\t Train_Loss: 433372.9546 Train_Acc: 50.167 Val_Loss: 0.7365  BEST VAL Loss: 0.7365  Val_Acc: 50.000\n",
      "\n",
      "Epoch 476: Validation loss decreased (0.736476 --> 0.736409).  Saving model ...\n",
      "\t Train_Loss: 432464.4173 Train_Acc: 50.167 Val_Loss: 0.7364  BEST VAL Loss: 0.7364  Val_Acc: 50.000\n",
      "\n",
      "Epoch 477: Validation loss decreased (0.736409 --> 0.736341).  Saving model ...\n",
      "\t Train_Loss: 431559.6815 Train_Acc: 50.167 Val_Loss: 0.7363  BEST VAL Loss: 0.7363  Val_Acc: 50.000\n",
      "\n",
      "Epoch 478: Validation loss decreased (0.736341 --> 0.736274).  Saving model ...\n",
      "\t Train_Loss: 430658.7233 Train_Acc: 50.167 Val_Loss: 0.7363  BEST VAL Loss: 0.7363  Val_Acc: 50.000\n",
      "\n",
      "Epoch 479: Validation loss decreased (0.736274 --> 0.736208).  Saving model ...\n",
      "\t Train_Loss: 429761.5191 Train_Acc: 50.167 Val_Loss: 0.7362  BEST VAL Loss: 0.7362  Val_Acc: 50.000\n",
      "\n",
      "Epoch 480: Validation loss decreased (0.736208 --> 0.736141).  Saving model ...\n",
      "\t Train_Loss: 428868.0455 Train_Acc: 50.167 Val_Loss: 0.7361  BEST VAL Loss: 0.7361  Val_Acc: 50.000\n",
      "\n",
      "Epoch 481: Validation loss decreased (0.736141 --> 0.736075).  Saving model ...\n",
      "\t Train_Loss: 427978.2792 Train_Acc: 50.167 Val_Loss: 0.7361  BEST VAL Loss: 0.7361  Val_Acc: 50.000\n",
      "\n",
      "Epoch 482: Validation loss decreased (0.736075 --> 0.736009).  Saving model ...\n",
      "\t Train_Loss: 427092.1973 Train_Acc: 50.167 Val_Loss: 0.7360  BEST VAL Loss: 0.7360  Val_Acc: 50.000\n",
      "\n",
      "Epoch 483: Validation loss decreased (0.736009 --> 0.735943).  Saving model ...\n",
      "\t Train_Loss: 426209.7769 Train_Acc: 50.167 Val_Loss: 0.7359  BEST VAL Loss: 0.7359  Val_Acc: 50.000\n",
      "\n",
      "Epoch 484: Validation loss decreased (0.735943 --> 0.735878).  Saving model ...\n",
      "\t Train_Loss: 425330.9953 Train_Acc: 50.167 Val_Loss: 0.7359  BEST VAL Loss: 0.7359  Val_Acc: 50.000\n",
      "\n",
      "Epoch 485: Validation loss decreased (0.735878 --> 0.735813).  Saving model ...\n",
      "\t Train_Loss: 424455.8300 Train_Acc: 50.167 Val_Loss: 0.7358  BEST VAL Loss: 0.7358  Val_Acc: 50.000\n",
      "\n",
      "Epoch 486: Validation loss decreased (0.735813 --> 0.735748).  Saving model ...\n",
      "\t Train_Loss: 423584.2589 Train_Acc: 50.167 Val_Loss: 0.7357  BEST VAL Loss: 0.7357  Val_Acc: 50.000\n",
      "\n",
      "Epoch 487: Validation loss decreased (0.735748 --> 0.735683).  Saving model ...\n",
      "\t Train_Loss: 422716.2599 Train_Acc: 50.167 Val_Loss: 0.7357  BEST VAL Loss: 0.7357  Val_Acc: 50.000\n",
      "\n",
      "Epoch 488: Validation loss decreased (0.735683 --> 0.735619).  Saving model ...\n",
      "\t Train_Loss: 421851.8109 Train_Acc: 50.167 Val_Loss: 0.7356  BEST VAL Loss: 0.7356  Val_Acc: 50.000\n",
      "\n",
      "Epoch 489: Validation loss decreased (0.735619 --> 0.735555).  Saving model ...\n",
      "\t Train_Loss: 420990.8902 Train_Acc: 50.167 Val_Loss: 0.7356  BEST VAL Loss: 0.7356  Val_Acc: 50.000\n",
      "\n",
      "Epoch 490: Validation loss decreased (0.735555 --> 0.735491).  Saving model ...\n",
      "\t Train_Loss: 420133.4764 Train_Acc: 50.167 Val_Loss: 0.7355  BEST VAL Loss: 0.7355  Val_Acc: 50.000\n",
      "\n",
      "Epoch 491: Validation loss decreased (0.735491 --> 0.735427).  Saving model ...\n",
      "\t Train_Loss: 419279.5480 Train_Acc: 50.167 Val_Loss: 0.7354  BEST VAL Loss: 0.7354  Val_Acc: 50.000\n",
      "\n",
      "Epoch 492: Validation loss decreased (0.735427 --> 0.735364).  Saving model ...\n",
      "\t Train_Loss: 418429.0838 Train_Acc: 50.167 Val_Loss: 0.7354  BEST VAL Loss: 0.7354  Val_Acc: 50.000\n",
      "\n",
      "Epoch 493: Validation loss decreased (0.735364 --> 0.735301).  Saving model ...\n",
      "\t Train_Loss: 417582.0628 Train_Acc: 50.167 Val_Loss: 0.7353  BEST VAL Loss: 0.7353  Val_Acc: 50.000\n",
      "\n",
      "Epoch 494: Validation loss decreased (0.735301 --> 0.735238).  Saving model ...\n",
      "\t Train_Loss: 416738.4641 Train_Acc: 50.167 Val_Loss: 0.7352  BEST VAL Loss: 0.7352  Val_Acc: 50.000\n",
      "\n",
      "Epoch 495: Validation loss decreased (0.735238 --> 0.735176).  Saving model ...\n",
      "\t Train_Loss: 415898.2670 Train_Acc: 50.167 Val_Loss: 0.7352  BEST VAL Loss: 0.7352  Val_Acc: 50.000\n",
      "\n",
      "Epoch 496: Validation loss decreased (0.735176 --> 0.735113).  Saving model ...\n",
      "\t Train_Loss: 415061.4510 Train_Acc: 50.167 Val_Loss: 0.7351  BEST VAL Loss: 0.7351  Val_Acc: 50.000\n",
      "\n",
      "Epoch 497: Validation loss decreased (0.735113 --> 0.735051).  Saving model ...\n",
      "\t Train_Loss: 414227.9957 Train_Acc: 50.167 Val_Loss: 0.7351  BEST VAL Loss: 0.7351  Val_Acc: 50.000\n",
      "\n",
      "Epoch 498: Validation loss decreased (0.735051 --> 0.734989).  Saving model ...\n",
      "\t Train_Loss: 413397.8809 Train_Acc: 50.167 Val_Loss: 0.7350  BEST VAL Loss: 0.7350  Val_Acc: 50.000\n",
      "\n",
      "Epoch 499: Validation loss decreased (0.734989 --> 0.734928).  Saving model ...\n",
      "\t Train_Loss: 412571.0865 Train_Acc: 50.167 Val_Loss: 0.7349  BEST VAL Loss: 0.7349  Val_Acc: 50.000\n",
      "\n",
      "Epoch 500: Validation loss decreased (0.734928 --> 0.734867).  Saving model ...\n",
      "\t Train_Loss: 411747.5927 Train_Acc: 50.167 Val_Loss: 0.7349  BEST VAL Loss: 0.7349  Val_Acc: 50.000\n",
      "\n",
      "Epoch 501: Validation loss decreased (0.734867 --> 0.734806).  Saving model ...\n",
      "\t Train_Loss: 410927.3798 Train_Acc: 50.167 Val_Loss: 0.7348  BEST VAL Loss: 0.7348  Val_Acc: 50.000\n",
      "\n",
      "Epoch 502: Validation loss decreased (0.734806 --> 0.734745).  Saving model ...\n",
      "\t Train_Loss: 410110.4282 Train_Acc: 50.167 Val_Loss: 0.7347  BEST VAL Loss: 0.7347  Val_Acc: 50.000\n",
      "\n",
      "Epoch 503: Validation loss decreased (0.734745 --> 0.734684).  Saving model ...\n",
      "\t Train_Loss: 409296.7184 Train_Acc: 50.167 Val_Loss: 0.7347  BEST VAL Loss: 0.7347  Val_Acc: 50.000\n",
      "\n",
      "Epoch 504: Validation loss decreased (0.734684 --> 0.734624).  Saving model ...\n",
      "\t Train_Loss: 408486.2312 Train_Acc: 50.167 Val_Loss: 0.7346  BEST VAL Loss: 0.7346  Val_Acc: 50.000\n",
      "\n",
      "Epoch 505: Validation loss decreased (0.734624 --> 0.734564).  Saving model ...\n",
      "\t Train_Loss: 407678.9476 Train_Acc: 50.167 Val_Loss: 0.7346  BEST VAL Loss: 0.7346  Val_Acc: 50.000\n",
      "\n",
      "Epoch 506: Validation loss decreased (0.734564 --> 0.734504).  Saving model ...\n",
      "\t Train_Loss: 406874.8485 Train_Acc: 50.167 Val_Loss: 0.7345  BEST VAL Loss: 0.7345  Val_Acc: 50.000\n",
      "\n",
      "Epoch 507: Validation loss decreased (0.734504 --> 0.734444).  Saving model ...\n",
      "\t Train_Loss: 406073.9151 Train_Acc: 50.167 Val_Loss: 0.7344  BEST VAL Loss: 0.7344  Val_Acc: 50.000\n",
      "\n",
      "Epoch 508: Validation loss decreased (0.734444 --> 0.734385).  Saving model ...\n",
      "\t Train_Loss: 405276.1288 Train_Acc: 50.167 Val_Loss: 0.7344  BEST VAL Loss: 0.7344  Val_Acc: 50.000\n",
      "\n",
      "Epoch 509: Validation loss decreased (0.734385 --> 0.734326).  Saving model ...\n",
      "\t Train_Loss: 404481.4711 Train_Acc: 50.167 Val_Loss: 0.7343  BEST VAL Loss: 0.7343  Val_Acc: 50.000\n",
      "\n",
      "Epoch 510: Validation loss decreased (0.734326 --> 0.734267).  Saving model ...\n",
      "\t Train_Loss: 403689.9237 Train_Acc: 50.167 Val_Loss: 0.7343  BEST VAL Loss: 0.7343  Val_Acc: 50.000\n",
      "\n",
      "Epoch 511: Validation loss decreased (0.734267 --> 0.734208).  Saving model ...\n",
      "\t Train_Loss: 402901.4681 Train_Acc: 50.167 Val_Loss: 0.7342  BEST VAL Loss: 0.7342  Val_Acc: 50.000\n",
      "\n",
      "Epoch 512: Validation loss decreased (0.734208 --> 0.734150).  Saving model ...\n",
      "\t Train_Loss: 402116.0865 Train_Acc: 50.167 Val_Loss: 0.7341  BEST VAL Loss: 0.7341  Val_Acc: 50.000\n",
      "\n",
      "Epoch 513: Validation loss decreased (0.734150 --> 0.734091).  Saving model ...\n",
      "\t Train_Loss: 401333.7609 Train_Acc: 50.167 Val_Loss: 0.7341  BEST VAL Loss: 0.7341  Val_Acc: 50.000\n",
      "\n",
      "Epoch 514: Validation loss decreased (0.734091 --> 0.734033).  Saving model ...\n",
      "\t Train_Loss: 400554.4734 Train_Acc: 50.167 Val_Loss: 0.7340  BEST VAL Loss: 0.7340  Val_Acc: 50.000\n",
      "\n",
      "Epoch 515: Validation loss decreased (0.734033 --> 0.733976).  Saving model ...\n",
      "\t Train_Loss: 399778.2064 Train_Acc: 50.167 Val_Loss: 0.7340  BEST VAL Loss: 0.7340  Val_Acc: 50.000\n",
      "\n",
      "Epoch 516: Validation loss decreased (0.733976 --> 0.733918).  Saving model ...\n",
      "\t Train_Loss: 399004.9424 Train_Acc: 50.167 Val_Loss: 0.7339  BEST VAL Loss: 0.7339  Val_Acc: 50.000\n",
      "\n",
      "Epoch 517: Validation loss decreased (0.733918 --> 0.733861).  Saving model ...\n",
      "\t Train_Loss: 398234.6639 Train_Acc: 50.167 Val_Loss: 0.7339  BEST VAL Loss: 0.7339  Val_Acc: 50.000\n",
      "\n",
      "Epoch 518: Validation loss decreased (0.733861 --> 0.733803).  Saving model ...\n",
      "\t Train_Loss: 397467.3538 Train_Acc: 50.167 Val_Loss: 0.7338  BEST VAL Loss: 0.7338  Val_Acc: 50.000\n",
      "\n",
      "Epoch 519: Validation loss decreased (0.733803 --> 0.733747).  Saving model ...\n",
      "\t Train_Loss: 396702.9948 Train_Acc: 50.167 Val_Loss: 0.7337  BEST VAL Loss: 0.7337  Val_Acc: 50.000\n",
      "\n",
      "Epoch 520: Validation loss decreased (0.733747 --> 0.733690).  Saving model ...\n",
      "\t Train_Loss: 395941.5701 Train_Acc: 50.167 Val_Loss: 0.7337  BEST VAL Loss: 0.7337  Val_Acc: 50.000\n",
      "\n",
      "Epoch 521: Validation loss decreased (0.733690 --> 0.733633).  Saving model ...\n",
      "\t Train_Loss: 395183.0627 Train_Acc: 50.167 Val_Loss: 0.7336  BEST VAL Loss: 0.7336  Val_Acc: 50.000\n",
      "\n",
      "Epoch 522: Validation loss decreased (0.733633 --> 0.733577).  Saving model ...\n",
      "\t Train_Loss: 394427.4559 Train_Acc: 50.167 Val_Loss: 0.7336  BEST VAL Loss: 0.7336  Val_Acc: 50.000\n",
      "\n",
      "Epoch 523: Validation loss decreased (0.733577 --> 0.733521).  Saving model ...\n",
      "\t Train_Loss: 393674.7331 Train_Acc: 50.167 Val_Loss: 0.7335  BEST VAL Loss: 0.7335  Val_Acc: 50.000\n",
      "\n",
      "Epoch 524: Validation loss decreased (0.733521 --> 0.733465).  Saving model ...\n",
      "\t Train_Loss: 392924.8778 Train_Acc: 50.167 Val_Loss: 0.7335  BEST VAL Loss: 0.7335  Val_Acc: 50.000\n",
      "\n",
      "Epoch 525: Validation loss decreased (0.733465 --> 0.733410).  Saving model ...\n",
      "\t Train_Loss: 392177.8736 Train_Acc: 50.167 Val_Loss: 0.7334  BEST VAL Loss: 0.7334  Val_Acc: 50.000\n",
      "\n",
      "Epoch 526: Validation loss decreased (0.733410 --> 0.733354).  Saving model ...\n",
      "\t Train_Loss: 391433.7044 Train_Acc: 50.167 Val_Loss: 0.7334  BEST VAL Loss: 0.7334  Val_Acc: 50.000\n",
      "\n",
      "Epoch 527: Validation loss decreased (0.733354 --> 0.733299).  Saving model ...\n",
      "\t Train_Loss: 390692.3541 Train_Acc: 50.167 Val_Loss: 0.7333  BEST VAL Loss: 0.7333  Val_Acc: 50.000\n",
      "\n",
      "Epoch 528: Validation loss decreased (0.733299 --> 0.733244).  Saving model ...\n",
      "\t Train_Loss: 389953.8065 Train_Acc: 50.167 Val_Loss: 0.7332  BEST VAL Loss: 0.7332  Val_Acc: 50.000\n",
      "\n",
      "Epoch 529: Validation loss decreased (0.733244 --> 0.733189).  Saving model ...\n",
      "\t Train_Loss: 389218.0459 Train_Acc: 50.167 Val_Loss: 0.7332  BEST VAL Loss: 0.7332  Val_Acc: 50.000\n",
      "\n",
      "Epoch 530: Validation loss decreased (0.733189 --> 0.733135).  Saving model ...\n",
      "\t Train_Loss: 388485.0566 Train_Acc: 50.167 Val_Loss: 0.7331  BEST VAL Loss: 0.7331  Val_Acc: 50.000\n",
      "\n",
      "Epoch 531: Validation loss decreased (0.733135 --> 0.733080).  Saving model ...\n",
      "\t Train_Loss: 387754.8228 Train_Acc: 50.167 Val_Loss: 0.7331  BEST VAL Loss: 0.7331  Val_Acc: 50.000\n",
      "\n",
      "Epoch 532: Validation loss decreased (0.733080 --> 0.733026).  Saving model ...\n",
      "\t Train_Loss: 387027.3292 Train_Acc: 50.167 Val_Loss: 0.7330  BEST VAL Loss: 0.7330  Val_Acc: 50.000\n",
      "\n",
      "Epoch 533: Validation loss decreased (0.733026 --> 0.732972).  Saving model ...\n",
      "\t Train_Loss: 386302.5602 Train_Acc: 50.167 Val_Loss: 0.7330  BEST VAL Loss: 0.7330  Val_Acc: 50.000\n",
      "\n",
      "Epoch 534: Validation loss decreased (0.732972 --> 0.732919).  Saving model ...\n",
      "\t Train_Loss: 385580.5007 Train_Acc: 50.167 Val_Loss: 0.7329  BEST VAL Loss: 0.7329  Val_Acc: 50.000\n",
      "\n",
      "Epoch 535: Validation loss decreased (0.732919 --> 0.732865).  Saving model ...\n",
      "\t Train_Loss: 384861.1354 Train_Acc: 50.167 Val_Loss: 0.7329  BEST VAL Loss: 0.7329  Val_Acc: 50.000\n",
      "\n",
      "Epoch 536: Validation loss decreased (0.732865 --> 0.732812).  Saving model ...\n",
      "\t Train_Loss: 384144.4493 Train_Acc: 50.167 Val_Loss: 0.7328  BEST VAL Loss: 0.7328  Val_Acc: 50.000\n",
      "\n",
      "Epoch 537: Validation loss decreased (0.732812 --> 0.732759).  Saving model ...\n",
      "\t Train_Loss: 383430.4275 Train_Acc: 50.167 Val_Loss: 0.7328  BEST VAL Loss: 0.7328  Val_Acc: 50.000\n",
      "\n",
      "Epoch 538: Validation loss decreased (0.732759 --> 0.732706).  Saving model ...\n",
      "\t Train_Loss: 382719.0551 Train_Acc: 50.167 Val_Loss: 0.7327  BEST VAL Loss: 0.7327  Val_Acc: 50.000\n",
      "\n",
      "Epoch 539: Validation loss decreased (0.732706 --> 0.732653).  Saving model ...\n",
      "\t Train_Loss: 382010.3174 Train_Acc: 50.167 Val_Loss: 0.7327  BEST VAL Loss: 0.7327  Val_Acc: 50.000\n",
      "\n",
      "Epoch 540: Validation loss decreased (0.732653 --> 0.732600).  Saving model ...\n",
      "\t Train_Loss: 381304.1998 Train_Acc: 50.167 Val_Loss: 0.7326  BEST VAL Loss: 0.7326  Val_Acc: 50.000\n",
      "\n",
      "Epoch 541: Validation loss decreased (0.732600 --> 0.732548).  Saving model ...\n",
      "\t Train_Loss: 380600.6878 Train_Acc: 50.167 Val_Loss: 0.7325  BEST VAL Loss: 0.7325  Val_Acc: 50.000\n",
      "\n",
      "Epoch 542: Validation loss decreased (0.732548 --> 0.732496).  Saving model ...\n",
      "\t Train_Loss: 379899.7670 Train_Acc: 50.167 Val_Loss: 0.7325  BEST VAL Loss: 0.7325  Val_Acc: 50.000\n",
      "\n",
      "Epoch 543: Validation loss decreased (0.732496 --> 0.732444).  Saving model ...\n",
      "\t Train_Loss: 379201.4232 Train_Acc: 50.167 Val_Loss: 0.7324  BEST VAL Loss: 0.7324  Val_Acc: 50.000\n",
      "\n",
      "Epoch 544: Validation loss decreased (0.732444 --> 0.732392).  Saving model ...\n",
      "\t Train_Loss: 378505.6420 Train_Acc: 50.167 Val_Loss: 0.7324  BEST VAL Loss: 0.7324  Val_Acc: 50.000\n",
      "\n",
      "Epoch 545: Validation loss decreased (0.732392 --> 0.732340).  Saving model ...\n",
      "\t Train_Loss: 377812.4095 Train_Acc: 50.167 Val_Loss: 0.7323  BEST VAL Loss: 0.7323  Val_Acc: 50.000\n",
      "\n",
      "Epoch 546: Validation loss decreased (0.732340 --> 0.732289).  Saving model ...\n",
      "\t Train_Loss: 377121.7117 Train_Acc: 50.167 Val_Loss: 0.7323  BEST VAL Loss: 0.7323  Val_Acc: 50.000\n",
      "\n",
      "Epoch 547: Validation loss decreased (0.732289 --> 0.732238).  Saving model ...\n",
      "\t Train_Loss: 376433.5347 Train_Acc: 50.167 Val_Loss: 0.7322  BEST VAL Loss: 0.7322  Val_Acc: 50.000\n",
      "\n",
      "Epoch 548: Validation loss decreased (0.732238 --> 0.732187).  Saving model ...\n",
      "\t Train_Loss: 375747.8647 Train_Acc: 50.167 Val_Loss: 0.7322  BEST VAL Loss: 0.7322  Val_Acc: 50.000\n",
      "\n",
      "Epoch 549: Validation loss decreased (0.732187 --> 0.732136).  Saving model ...\n",
      "\t Train_Loss: 375064.6880 Train_Acc: 50.167 Val_Loss: 0.7321  BEST VAL Loss: 0.7321  Val_Acc: 50.000\n",
      "\n",
      "Epoch 550: Validation loss decreased (0.732136 --> 0.732085).  Saving model ...\n",
      "\t Train_Loss: 374383.9911 Train_Acc: 50.167 Val_Loss: 0.7321  BEST VAL Loss: 0.7321  Val_Acc: 50.000\n",
      "\n",
      "Epoch 551: Validation loss decreased (0.732085 --> 0.732035).  Saving model ...\n",
      "\t Train_Loss: 373705.7605 Train_Acc: 50.167 Val_Loss: 0.7320  BEST VAL Loss: 0.7320  Val_Acc: 50.000\n",
      "\n",
      "Epoch 552: Validation loss decreased (0.732035 --> 0.731984).  Saving model ...\n",
      "\t Train_Loss: 373029.9829 Train_Acc: 50.167 Val_Loss: 0.7320  BEST VAL Loss: 0.7320  Val_Acc: 50.000\n",
      "\n",
      "Epoch 553: Validation loss decreased (0.731984 --> 0.731934).  Saving model ...\n",
      "\t Train_Loss: 372356.6448 Train_Acc: 50.167 Val_Loss: 0.7319  BEST VAL Loss: 0.7319  Val_Acc: 50.000\n",
      "\n",
      "Epoch 554: Validation loss decreased (0.731934 --> 0.731884).  Saving model ...\n",
      "\t Train_Loss: 371685.7332 Train_Acc: 50.167 Val_Loss: 0.7319  BEST VAL Loss: 0.7319  Val_Acc: 50.000\n",
      "\n",
      "Epoch 555: Validation loss decreased (0.731884 --> 0.731834).  Saving model ...\n",
      "\t Train_Loss: 371017.2350 Train_Acc: 50.167 Val_Loss: 0.7318  BEST VAL Loss: 0.7318  Val_Acc: 50.000\n",
      "\n",
      "Epoch 556: Validation loss decreased (0.731834 --> 0.731785).  Saving model ...\n",
      "\t Train_Loss: 370351.1371 Train_Acc: 50.167 Val_Loss: 0.7318  BEST VAL Loss: 0.7318  Val_Acc: 50.000\n",
      "\n",
      "Epoch 557: Validation loss decreased (0.731785 --> 0.731735).  Saving model ...\n",
      "\t Train_Loss: 369687.4266 Train_Acc: 50.167 Val_Loss: 0.7317  BEST VAL Loss: 0.7317  Val_Acc: 50.000\n",
      "\n",
      "Epoch 558: Validation loss decreased (0.731735 --> 0.731686).  Saving model ...\n",
      "\t Train_Loss: 369026.0908 Train_Acc: 50.167 Val_Loss: 0.7317  BEST VAL Loss: 0.7317  Val_Acc: 50.000\n",
      "\n",
      "Epoch 559: Validation loss decreased (0.731686 --> 0.731637).  Saving model ...\n",
      "\t Train_Loss: 368367.1169 Train_Acc: 50.167 Val_Loss: 0.7316  BEST VAL Loss: 0.7316  Val_Acc: 50.000\n",
      "\n",
      "Epoch 560: Validation loss decreased (0.731637 --> 0.731588).  Saving model ...\n",
      "\t Train_Loss: 367710.4922 Train_Acc: 50.167 Val_Loss: 0.7316  BEST VAL Loss: 0.7316  Val_Acc: 50.000\n",
      "\n",
      "Epoch 561: Validation loss decreased (0.731588 --> 0.731540).  Saving model ...\n",
      "\t Train_Loss: 367056.2044 Train_Acc: 50.167 Val_Loss: 0.7315  BEST VAL Loss: 0.7315  Val_Acc: 50.000\n",
      "\n",
      "Epoch 562: Validation loss decreased (0.731540 --> 0.731491).  Saving model ...\n",
      "\t Train_Loss: 366404.2408 Train_Acc: 50.167 Val_Loss: 0.7315  BEST VAL Loss: 0.7315  Val_Acc: 50.000\n",
      "\n",
      "Epoch 563: Validation loss decreased (0.731491 --> 0.731443).  Saving model ...\n",
      "\t Train_Loss: 365754.5891 Train_Acc: 50.167 Val_Loss: 0.7314  BEST VAL Loss: 0.7314  Val_Acc: 50.000\n",
      "\n",
      "Epoch 564: Validation loss decreased (0.731443 --> 0.731394).  Saving model ...\n",
      "\t Train_Loss: 365107.2371 Train_Acc: 50.167 Val_Loss: 0.7314  BEST VAL Loss: 0.7314  Val_Acc: 50.000\n",
      "\n",
      "Epoch 565: Validation loss decreased (0.731394 --> 0.731346).  Saving model ...\n",
      "\t Train_Loss: 364462.1726 Train_Acc: 50.167 Val_Loss: 0.7313  BEST VAL Loss: 0.7313  Val_Acc: 50.000\n",
      "\n",
      "Epoch 566: Validation loss decreased (0.731346 --> 0.731299).  Saving model ...\n",
      "\t Train_Loss: 363819.3834 Train_Acc: 50.167 Val_Loss: 0.7313  BEST VAL Loss: 0.7313  Val_Acc: 50.000\n",
      "\n",
      "Epoch 567: Validation loss decreased (0.731299 --> 0.731251).  Saving model ...\n",
      "\t Train_Loss: 363178.8575 Train_Acc: 50.167 Val_Loss: 0.7313  BEST VAL Loss: 0.7313  Val_Acc: 50.000\n",
      "\n",
      "Epoch 568: Validation loss decreased (0.731251 --> 0.731203).  Saving model ...\n",
      "\t Train_Loss: 362540.5831 Train_Acc: 50.167 Val_Loss: 0.7312  BEST VAL Loss: 0.7312  Val_Acc: 50.000\n",
      "\n",
      "Epoch 569: Validation loss decreased (0.731203 --> 0.731156).  Saving model ...\n",
      "\t Train_Loss: 361904.5482 Train_Acc: 50.167 Val_Loss: 0.7312  BEST VAL Loss: 0.7312  Val_Acc: 50.000\n",
      "\n",
      "Epoch 570: Validation loss decreased (0.731156 --> 0.731109).  Saving model ...\n",
      "\t Train_Loss: 361270.7411 Train_Acc: 50.167 Val_Loss: 0.7311  BEST VAL Loss: 0.7311  Val_Acc: 50.000\n",
      "\n",
      "Epoch 571: Validation loss decreased (0.731109 --> 0.731062).  Saving model ...\n",
      "\t Train_Loss: 360639.1502 Train_Acc: 50.167 Val_Loss: 0.7311  BEST VAL Loss: 0.7311  Val_Acc: 50.000\n",
      "\n",
      "Epoch 572: Validation loss decreased (0.731062 --> 0.731015).  Saving model ...\n",
      "\t Train_Loss: 360009.7637 Train_Acc: 50.167 Val_Loss: 0.7310  BEST VAL Loss: 0.7310  Val_Acc: 50.000\n",
      "\n",
      "Epoch 573: Validation loss decreased (0.731015 --> 0.730968).  Saving model ...\n",
      "\t Train_Loss: 359382.5702 Train_Acc: 50.167 Val_Loss: 0.7310  BEST VAL Loss: 0.7310  Val_Acc: 50.000\n",
      "\n",
      "Epoch 574: Validation loss decreased (0.730968 --> 0.730922).  Saving model ...\n",
      "\t Train_Loss: 358757.5583 Train_Acc: 50.167 Val_Loss: 0.7309  BEST VAL Loss: 0.7309  Val_Acc: 50.000\n",
      "\n",
      "Epoch 575: Validation loss decreased (0.730922 --> 0.730875).  Saving model ...\n",
      "\t Train_Loss: 358134.7165 Train_Acc: 50.167 Val_Loss: 0.7309  BEST VAL Loss: 0.7309  Val_Acc: 50.000\n",
      "\n",
      "Epoch 576: Validation loss decreased (0.730875 --> 0.730829).  Saving model ...\n",
      "\t Train_Loss: 357514.0336 Train_Acc: 50.167 Val_Loss: 0.7308  BEST VAL Loss: 0.7308  Val_Acc: 50.000\n",
      "\n",
      "Epoch 577: Validation loss decreased (0.730829 --> 0.730783).  Saving model ...\n",
      "\t Train_Loss: 356895.4985 Train_Acc: 50.167 Val_Loss: 0.7308  BEST VAL Loss: 0.7308  Val_Acc: 50.000\n",
      "\n",
      "Epoch 578: Validation loss decreased (0.730783 --> 0.730737).  Saving model ...\n",
      "\t Train_Loss: 356279.0998 Train_Acc: 50.167 Val_Loss: 0.7307  BEST VAL Loss: 0.7307  Val_Acc: 50.000\n",
      "\n",
      "Epoch 579: Validation loss decreased (0.730737 --> 0.730691).  Saving model ...\n",
      "\t Train_Loss: 355664.8267 Train_Acc: 50.167 Val_Loss: 0.7307  BEST VAL Loss: 0.7307  Val_Acc: 50.000\n",
      "\n",
      "Epoch 580: Validation loss decreased (0.730691 --> 0.730646).  Saving model ...\n",
      "\t Train_Loss: 355052.6682 Train_Acc: 50.167 Val_Loss: 0.7306  BEST VAL Loss: 0.7306  Val_Acc: 50.000\n",
      "\n",
      "Epoch 581: Validation loss decreased (0.730646 --> 0.730601).  Saving model ...\n",
      "\t Train_Loss: 354442.6133 Train_Acc: 50.167 Val_Loss: 0.7306  BEST VAL Loss: 0.7306  Val_Acc: 50.000\n",
      "\n",
      "Epoch 582: Validation loss decreased (0.730601 --> 0.730555).  Saving model ...\n",
      "\t Train_Loss: 353834.6512 Train_Acc: 50.167 Val_Loss: 0.7306  BEST VAL Loss: 0.7306  Val_Acc: 50.000\n",
      "\n",
      "Epoch 583: Validation loss decreased (0.730555 --> 0.730510).  Saving model ...\n",
      "\t Train_Loss: 353228.7711 Train_Acc: 50.167 Val_Loss: 0.7305  BEST VAL Loss: 0.7305  Val_Acc: 50.000\n",
      "\n",
      "Epoch 584: Validation loss decreased (0.730510 --> 0.730465).  Saving model ...\n",
      "\t Train_Loss: 352624.9624 Train_Acc: 50.167 Val_Loss: 0.7305  BEST VAL Loss: 0.7305  Val_Acc: 50.000\n",
      "\n",
      "Epoch 585: Validation loss decreased (0.730465 --> 0.730420).  Saving model ...\n",
      "\t Train_Loss: 352023.2146 Train_Acc: 50.167 Val_Loss: 0.7304  BEST VAL Loss: 0.7304  Val_Acc: 50.000\n",
      "\n",
      "Epoch 586: Validation loss decreased (0.730420 --> 0.730376).  Saving model ...\n",
      "\t Train_Loss: 351423.5169 Train_Acc: 50.167 Val_Loss: 0.7304  BEST VAL Loss: 0.7304  Val_Acc: 50.000\n",
      "\n",
      "Epoch 587: Validation loss decreased (0.730376 --> 0.730331).  Saving model ...\n",
      "\t Train_Loss: 350825.8591 Train_Acc: 50.167 Val_Loss: 0.7303  BEST VAL Loss: 0.7303  Val_Acc: 50.000\n",
      "\n",
      "Epoch 588: Validation loss decreased (0.730331 --> 0.730287).  Saving model ...\n",
      "\t Train_Loss: 350230.2306 Train_Acc: 50.167 Val_Loss: 0.7303  BEST VAL Loss: 0.7303  Val_Acc: 50.000\n",
      "\n",
      "Epoch 589: Validation loss decreased (0.730287 --> 0.730243).  Saving model ...\n",
      "\t Train_Loss: 349636.6213 Train_Acc: 50.167 Val_Loss: 0.7302  BEST VAL Loss: 0.7302  Val_Acc: 50.000\n",
      "\n",
      "Epoch 590: Validation loss decreased (0.730243 --> 0.730199).  Saving model ...\n",
      "\t Train_Loss: 349045.0207 Train_Acc: 50.167 Val_Loss: 0.7302  BEST VAL Loss: 0.7302  Val_Acc: 50.000\n",
      "\n",
      "Epoch 591: Validation loss decreased (0.730199 --> 0.730155).  Saving model ...\n",
      "\t Train_Loss: 348455.4188 Train_Acc: 50.167 Val_Loss: 0.7302  BEST VAL Loss: 0.7302  Val_Acc: 50.000\n",
      "\n",
      "Epoch 592: Validation loss decreased (0.730155 --> 0.730111).  Saving model ...\n",
      "\t Train_Loss: 347867.8055 Train_Acc: 50.167 Val_Loss: 0.7301  BEST VAL Loss: 0.7301  Val_Acc: 50.000\n",
      "\n",
      "Epoch 593: Validation loss decreased (0.730111 --> 0.730067).  Saving model ...\n",
      "\t Train_Loss: 347282.1706 Train_Acc: 50.167 Val_Loss: 0.7301  BEST VAL Loss: 0.7301  Val_Acc: 50.000\n",
      "\n",
      "Epoch 594: Validation loss decreased (0.730067 --> 0.730024).  Saving model ...\n",
      "\t Train_Loss: 346698.5043 Train_Acc: 50.167 Val_Loss: 0.7300  BEST VAL Loss: 0.7300  Val_Acc: 50.000\n",
      "\n",
      "Epoch 595: Validation loss decreased (0.730024 --> 0.729981).  Saving model ...\n",
      "\t Train_Loss: 346116.7966 Train_Acc: 50.167 Val_Loss: 0.7300  BEST VAL Loss: 0.7300  Val_Acc: 50.000\n",
      "\n",
      "Epoch 596: Validation loss decreased (0.729981 --> 0.729938).  Saving model ...\n",
      "\t Train_Loss: 345537.0376 Train_Acc: 50.167 Val_Loss: 0.7299  BEST VAL Loss: 0.7299  Val_Acc: 50.000\n",
      "\n",
      "Epoch 597: Validation loss decreased (0.729938 --> 0.729895).  Saving model ...\n",
      "\t Train_Loss: 344959.2177 Train_Acc: 50.167 Val_Loss: 0.7299  BEST VAL Loss: 0.7299  Val_Acc: 50.000\n",
      "\n",
      "Epoch 598: Validation loss decreased (0.729895 --> 0.729852).  Saving model ...\n",
      "\t Train_Loss: 344383.3270 Train_Acc: 50.167 Val_Loss: 0.7299  BEST VAL Loss: 0.7299  Val_Acc: 50.000\n",
      "\n",
      "Epoch 599: Validation loss decreased (0.729852 --> 0.729809).  Saving model ...\n",
      "\t Train_Loss: 343809.3560 Train_Acc: 50.167 Val_Loss: 0.7298  BEST VAL Loss: 0.7298  Val_Acc: 50.000\n",
      "\n",
      "Epoch 600: Validation loss decreased (0.729809 --> 0.729766).  Saving model ...\n",
      "\t Train_Loss: 343237.2950 Train_Acc: 50.167 Val_Loss: 0.7298  BEST VAL Loss: 0.7298  Val_Acc: 50.000\n",
      "\n",
      "Epoch 601: Validation loss decreased (0.729766 --> 0.729724).  Saving model ...\n",
      "\t Train_Loss: 342667.1345 Train_Acc: 50.167 Val_Loss: 0.7297  BEST VAL Loss: 0.7297  Val_Acc: 50.000\n",
      "\n",
      "Epoch 602: Validation loss decreased (0.729724 --> 0.729682).  Saving model ...\n",
      "\t Train_Loss: 342098.8652 Train_Acc: 50.167 Val_Loss: 0.7297  BEST VAL Loss: 0.7297  Val_Acc: 50.000\n",
      "\n",
      "Epoch 603: Validation loss decreased (0.729682 --> 0.729639).  Saving model ...\n",
      "\t Train_Loss: 341532.4775 Train_Acc: 50.167 Val_Loss: 0.7296  BEST VAL Loss: 0.7296  Val_Acc: 50.000\n",
      "\n",
      "Epoch 604: Validation loss decreased (0.729639 --> 0.729597).  Saving model ...\n",
      "\t Train_Loss: 340967.9621 Train_Acc: 50.167 Val_Loss: 0.7296  BEST VAL Loss: 0.7296  Val_Acc: 50.000\n",
      "\n",
      "Epoch 605: Validation loss decreased (0.729597 --> 0.729556).  Saving model ...\n",
      "\t Train_Loss: 340405.3099 Train_Acc: 50.167 Val_Loss: 0.7296  BEST VAL Loss: 0.7296  Val_Acc: 50.000\n",
      "\n",
      "Epoch 606: Validation loss decreased (0.729556 --> 0.729514).  Saving model ...\n",
      "\t Train_Loss: 339844.5115 Train_Acc: 50.167 Val_Loss: 0.7295  BEST VAL Loss: 0.7295  Val_Acc: 50.000\n",
      "\n",
      "Epoch 607: Validation loss decreased (0.729514 --> 0.729472).  Saving model ...\n",
      "\t Train_Loss: 339285.5579 Train_Acc: 50.167 Val_Loss: 0.7295  BEST VAL Loss: 0.7295  Val_Acc: 50.000\n",
      "\n",
      "Epoch 608: Validation loss decreased (0.729472 --> 0.729431).  Saving model ...\n",
      "\t Train_Loss: 338728.4399 Train_Acc: 50.167 Val_Loss: 0.7294  BEST VAL Loss: 0.7294  Val_Acc: 50.000\n",
      "\n",
      "Epoch 609: Validation loss decreased (0.729431 --> 0.729389).  Saving model ...\n",
      "\t Train_Loss: 338173.1485 Train_Acc: 50.167 Val_Loss: 0.7294  BEST VAL Loss: 0.7294  Val_Acc: 50.000\n",
      "\n",
      "Epoch 610: Validation loss decreased (0.729389 --> 0.729348).  Saving model ...\n",
      "\t Train_Loss: 337619.6748 Train_Acc: 50.167 Val_Loss: 0.7293  BEST VAL Loss: 0.7293  Val_Acc: 50.000\n",
      "\n",
      "Epoch 611: Validation loss decreased (0.729348 --> 0.729307).  Saving model ...\n",
      "\t Train_Loss: 337068.0098 Train_Acc: 50.167 Val_Loss: 0.7293  BEST VAL Loss: 0.7293  Val_Acc: 50.000\n",
      "\n",
      "Epoch 612: Validation loss decreased (0.729307 --> 0.729266).  Saving model ...\n",
      "\t Train_Loss: 336518.1447 Train_Acc: 50.167 Val_Loss: 0.7293  BEST VAL Loss: 0.7293  Val_Acc: 50.000\n",
      "\n",
      "Epoch 613: Validation loss decreased (0.729266 --> 0.729225).  Saving model ...\n",
      "\t Train_Loss: 335970.0707 Train_Acc: 50.167 Val_Loss: 0.7292  BEST VAL Loss: 0.7292  Val_Acc: 50.000\n",
      "\n",
      "Epoch 614: Validation loss decreased (0.729225 --> 0.729185).  Saving model ...\n",
      "\t Train_Loss: 335423.7791 Train_Acc: 50.167 Val_Loss: 0.7292  BEST VAL Loss: 0.7292  Val_Acc: 50.000\n",
      "\n",
      "Epoch 615: Validation loss decreased (0.729185 --> 0.729144).  Saving model ...\n",
      "\t Train_Loss: 334879.2611 Train_Acc: 50.167 Val_Loss: 0.7291  BEST VAL Loss: 0.7291  Val_Acc: 50.000\n",
      "\n",
      "Epoch 616: Validation loss decreased (0.729144 --> 0.729104).  Saving model ...\n",
      "\t Train_Loss: 334336.5082 Train_Acc: 50.167 Val_Loss: 0.7291  BEST VAL Loss: 0.7291  Val_Acc: 50.000\n",
      "\n",
      "Epoch 617: Validation loss decreased (0.729104 --> 0.729063).  Saving model ...\n",
      "\t Train_Loss: 333795.5117 Train_Acc: 50.167 Val_Loss: 0.7291  BEST VAL Loss: 0.7291  Val_Acc: 50.000\n",
      "\n",
      "Epoch 618: Validation loss decreased (0.729063 --> 0.729023).  Saving model ...\n",
      "\t Train_Loss: 333256.2632 Train_Acc: 50.167 Val_Loss: 0.7290  BEST VAL Loss: 0.7290  Val_Acc: 50.000\n",
      "\n",
      "Epoch 619: Validation loss decreased (0.729023 --> 0.728983).  Saving model ...\n",
      "\t Train_Loss: 332718.7543 Train_Acc: 50.167 Val_Loss: 0.7290  BEST VAL Loss: 0.7290  Val_Acc: 50.000\n",
      "\n",
      "Epoch 620: Validation loss decreased (0.728983 --> 0.728943).  Saving model ...\n",
      "\t Train_Loss: 332182.9764 Train_Acc: 50.167 Val_Loss: 0.7289  BEST VAL Loss: 0.7289  Val_Acc: 50.000\n",
      "\n",
      "Epoch 621: Validation loss decreased (0.728943 --> 0.728904).  Saving model ...\n",
      "\t Train_Loss: 331648.9213 Train_Acc: 50.167 Val_Loss: 0.7289  BEST VAL Loss: 0.7289  Val_Acc: 50.000\n",
      "\n",
      "Epoch 622: Validation loss decreased (0.728904 --> 0.728864).  Saving model ...\n",
      "\t Train_Loss: 331116.5807 Train_Acc: 50.167 Val_Loss: 0.7289  BEST VAL Loss: 0.7289  Val_Acc: 50.000\n",
      "\n",
      "Epoch 623: Validation loss decreased (0.728864 --> 0.728825).  Saving model ...\n",
      "\t Train_Loss: 330585.9462 Train_Acc: 50.167 Val_Loss: 0.7288  BEST VAL Loss: 0.7288  Val_Acc: 50.000\n",
      "\n",
      "Epoch 624: Validation loss decreased (0.728825 --> 0.728785).  Saving model ...\n",
      "\t Train_Loss: 330057.0099 Train_Acc: 50.167 Val_Loss: 0.7288  BEST VAL Loss: 0.7288  Val_Acc: 50.000\n",
      "\n",
      "Epoch 625: Validation loss decreased (0.728785 --> 0.728746).  Saving model ...\n",
      "\t Train_Loss: 329529.7634 Train_Acc: 50.167 Val_Loss: 0.7287  BEST VAL Loss: 0.7287  Val_Acc: 50.000\n",
      "\n",
      "Epoch 626: Validation loss decreased (0.728746 --> 0.728707).  Saving model ...\n",
      "\t Train_Loss: 329004.1987 Train_Acc: 50.167 Val_Loss: 0.7287  BEST VAL Loss: 0.7287  Val_Acc: 50.000\n",
      "\n",
      "Epoch 627: Validation loss decreased (0.728707 --> 0.728668).  Saving model ...\n",
      "\t Train_Loss: 328480.3078 Train_Acc: 50.167 Val_Loss: 0.7287  BEST VAL Loss: 0.7287  Val_Acc: 50.000\n",
      "\n",
      "Epoch 628: Validation loss decreased (0.728668 --> 0.728629).  Saving model ...\n",
      "\t Train_Loss: 327958.0826 Train_Acc: 50.167 Val_Loss: 0.7286  BEST VAL Loss: 0.7286  Val_Acc: 50.000\n",
      "\n",
      "Epoch 629: Validation loss decreased (0.728629 --> 0.728590).  Saving model ...\n",
      "\t Train_Loss: 327437.5154 Train_Acc: 50.167 Val_Loss: 0.7286  BEST VAL Loss: 0.7286  Val_Acc: 50.000\n",
      "\n",
      "Epoch 630: Validation loss decreased (0.728590 --> 0.728551).  Saving model ...\n",
      "\t Train_Loss: 326918.5981 Train_Acc: 50.167 Val_Loss: 0.7286  BEST VAL Loss: 0.7286  Val_Acc: 50.000\n",
      "\n",
      "Epoch 631: Validation loss decreased (0.728551 --> 0.728513).  Saving model ...\n",
      "\t Train_Loss: 326401.3229 Train_Acc: 50.167 Val_Loss: 0.7285  BEST VAL Loss: 0.7285  Val_Acc: 50.000\n",
      "\n",
      "Epoch 632: Validation loss decreased (0.728513 --> 0.728475).  Saving model ...\n",
      "\t Train_Loss: 325885.6821 Train_Acc: 50.167 Val_Loss: 0.7285  BEST VAL Loss: 0.7285  Val_Acc: 50.000\n",
      "\n",
      "Epoch 633: Validation loss decreased (0.728475 --> 0.728436).  Saving model ...\n",
      "\t Train_Loss: 325371.6680 Train_Acc: 50.167 Val_Loss: 0.7284  BEST VAL Loss: 0.7284  Val_Acc: 50.000\n",
      "\n",
      "Epoch 634: Validation loss decreased (0.728436 --> 0.728398).  Saving model ...\n",
      "\t Train_Loss: 324859.2727 Train_Acc: 50.167 Val_Loss: 0.7284  BEST VAL Loss: 0.7284  Val_Acc: 50.000\n",
      "\n",
      "Epoch 635: Validation loss decreased (0.728398 --> 0.728360).  Saving model ...\n",
      "\t Train_Loss: 324348.4888 Train_Acc: 50.167 Val_Loss: 0.7284  BEST VAL Loss: 0.7284  Val_Acc: 50.000\n",
      "\n",
      "Epoch 636: Validation loss decreased (0.728360 --> 0.728322).  Saving model ...\n",
      "\t Train_Loss: 323839.3086 Train_Acc: 50.167 Val_Loss: 0.7283  BEST VAL Loss: 0.7283  Val_Acc: 50.000\n",
      "\n",
      "Epoch 637: Validation loss decreased (0.728322 --> 0.728284).  Saving model ...\n",
      "\t Train_Loss: 323331.7246 Train_Acc: 50.167 Val_Loss: 0.7283  BEST VAL Loss: 0.7283  Val_Acc: 50.000\n",
      "\n",
      "Epoch 638: Validation loss decreased (0.728284 --> 0.728247).  Saving model ...\n",
      "\t Train_Loss: 322825.7293 Train_Acc: 50.167 Val_Loss: 0.7282  BEST VAL Loss: 0.7282  Val_Acc: 50.000\n",
      "\n",
      "Epoch 639: Validation loss decreased (0.728247 --> 0.728209).  Saving model ...\n",
      "\t Train_Loss: 322321.3152 Train_Acc: 50.167 Val_Loss: 0.7282  BEST VAL Loss: 0.7282  Val_Acc: 50.000\n",
      "\n",
      "Epoch 640: Validation loss decreased (0.728209 --> 0.728172).  Saving model ...\n",
      "\t Train_Loss: 321818.4749 Train_Acc: 50.167 Val_Loss: 0.7282  BEST VAL Loss: 0.7282  Val_Acc: 50.000\n",
      "\n",
      "Epoch 641: Validation loss decreased (0.728172 --> 0.728134).  Saving model ...\n",
      "\t Train_Loss: 321317.2011 Train_Acc: 50.167 Val_Loss: 0.7281  BEST VAL Loss: 0.7281  Val_Acc: 50.000\n",
      "\n",
      "Epoch 642: Validation loss decreased (0.728134 --> 0.728097).  Saving model ...\n",
      "\t Train_Loss: 320817.4865 Train_Acc: 50.167 Val_Loss: 0.7281  BEST VAL Loss: 0.7281  Val_Acc: 50.000\n",
      "\n",
      "Epoch 643: Validation loss decreased (0.728097 --> 0.728060).  Saving model ...\n",
      "\t Train_Loss: 320319.3238 Train_Acc: 50.167 Val_Loss: 0.7281  BEST VAL Loss: 0.7281  Val_Acc: 50.000\n",
      "\n",
      "Epoch 644: Validation loss decreased (0.728060 --> 0.728023).  Saving model ...\n",
      "\t Train_Loss: 319822.7058 Train_Acc: 50.167 Val_Loss: 0.7280  BEST VAL Loss: 0.7280  Val_Acc: 50.000\n",
      "\n",
      "Epoch 645: Validation loss decreased (0.728023 --> 0.727986).  Saving model ...\n",
      "\t Train_Loss: 319327.6253 Train_Acc: 50.167 Val_Loss: 0.7280  BEST VAL Loss: 0.7280  Val_Acc: 50.000\n",
      "\n",
      "Epoch 646: Validation loss decreased (0.727986 --> 0.727950).  Saving model ...\n",
      "\t Train_Loss: 318834.0752 Train_Acc: 50.167 Val_Loss: 0.7279  BEST VAL Loss: 0.7279  Val_Acc: 50.000\n",
      "\n",
      "Epoch 647: Validation loss decreased (0.727950 --> 0.727913).  Saving model ...\n",
      "\t Train_Loss: 318342.0484 Train_Acc: 50.167 Val_Loss: 0.7279  BEST VAL Loss: 0.7279  Val_Acc: 50.000\n",
      "\n",
      "Epoch 648: Validation loss decreased (0.727913 --> 0.727876).  Saving model ...\n",
      "\t Train_Loss: 317851.5378 Train_Acc: 50.167 Val_Loss: 0.7279  BEST VAL Loss: 0.7279  Val_Acc: 50.000\n",
      "\n",
      "Epoch 649: Validation loss decreased (0.727876 --> 0.727840).  Saving model ...\n",
      "\t Train_Loss: 317362.5365 Train_Acc: 50.167 Val_Loss: 0.7278  BEST VAL Loss: 0.7278  Val_Acc: 50.000\n",
      "\n",
      "Epoch 650: Validation loss decreased (0.727840 --> 0.727804).  Saving model ...\n",
      "\t Train_Loss: 316875.0376 Train_Acc: 50.167 Val_Loss: 0.7278  BEST VAL Loss: 0.7278  Val_Acc: 50.000\n",
      "\n",
      "Epoch 651: Validation loss decreased (0.727804 --> 0.727768).  Saving model ...\n",
      "\t Train_Loss: 316389.0340 Train_Acc: 50.167 Val_Loss: 0.7278  BEST VAL Loss: 0.7278  Val_Acc: 50.000\n",
      "\n",
      "Epoch 652: Validation loss decreased (0.727768 --> 0.727732).  Saving model ...\n",
      "\t Train_Loss: 315904.5189 Train_Acc: 50.167 Val_Loss: 0.7277  BEST VAL Loss: 0.7277  Val_Acc: 50.000\n",
      "\n",
      "Epoch 653: Validation loss decreased (0.727732 --> 0.727696).  Saving model ...\n",
      "\t Train_Loss: 315421.4856 Train_Acc: 50.167 Val_Loss: 0.7277  BEST VAL Loss: 0.7277  Val_Acc: 50.000\n",
      "\n",
      "Epoch 654: Validation loss decreased (0.727696 --> 0.727660).  Saving model ...\n",
      "\t Train_Loss: 314939.9271 Train_Acc: 50.167 Val_Loss: 0.7277  BEST VAL Loss: 0.7277  Val_Acc: 50.000\n",
      "\n",
      "Epoch 655: Validation loss decreased (0.727660 --> 0.727624).  Saving model ...\n",
      "\t Train_Loss: 314459.8368 Train_Acc: 50.167 Val_Loss: 0.7276  BEST VAL Loss: 0.7276  Val_Acc: 50.000\n",
      "\n",
      "Epoch 656: Validation loss decreased (0.727624 --> 0.727588).  Saving model ...\n",
      "\t Train_Loss: 313981.2080 Train_Acc: 50.167 Val_Loss: 0.7276  BEST VAL Loss: 0.7276  Val_Acc: 50.000\n",
      "\n",
      "Epoch 657: Validation loss decreased (0.727588 --> 0.727553).  Saving model ...\n",
      "\t Train_Loss: 313504.0340 Train_Acc: 50.167 Val_Loss: 0.7276  BEST VAL Loss: 0.7276  Val_Acc: 50.000\n",
      "\n",
      "Epoch 658: Validation loss decreased (0.727553 --> 0.727517).  Saving model ...\n",
      "\t Train_Loss: 313028.3082 Train_Acc: 50.167 Val_Loss: 0.7275  BEST VAL Loss: 0.7275  Val_Acc: 50.000\n",
      "\n",
      "Epoch 659: Validation loss decreased (0.727517 --> 0.727482).  Saving model ...\n",
      "\t Train_Loss: 312554.0239 Train_Acc: 50.167 Val_Loss: 0.7275  BEST VAL Loss: 0.7275  Val_Acc: 50.000\n",
      "\n",
      "Epoch 660: Validation loss decreased (0.727482 --> 0.727447).  Saving model ...\n",
      "\t Train_Loss: 312081.1747 Train_Acc: 50.167 Val_Loss: 0.7274  BEST VAL Loss: 0.7274  Val_Acc: 50.000\n",
      "\n",
      "Epoch 661: Validation loss decreased (0.727447 --> 0.727412).  Saving model ...\n",
      "\t Train_Loss: 311609.7541 Train_Acc: 50.167 Val_Loss: 0.7274  BEST VAL Loss: 0.7274  Val_Acc: 50.000\n",
      "\n",
      "Epoch 662: Validation loss decreased (0.727412 --> 0.727377).  Saving model ...\n",
      "\t Train_Loss: 311139.7555 Train_Acc: 50.167 Val_Loss: 0.7274  BEST VAL Loss: 0.7274  Val_Acc: 50.000\n",
      "\n",
      "Epoch 663: Validation loss decreased (0.727377 --> 0.727342).  Saving model ...\n",
      "\t Train_Loss: 310671.1726 Train_Acc: 50.167 Val_Loss: 0.7273  BEST VAL Loss: 0.7273  Val_Acc: 50.000\n",
      "\n",
      "Epoch 664: Validation loss decreased (0.727342 --> 0.727307).  Saving model ...\n",
      "\t Train_Loss: 310203.9989 Train_Acc: 50.167 Val_Loss: 0.7273  BEST VAL Loss: 0.7273  Val_Acc: 50.000\n",
      "\n",
      "Epoch 665: Validation loss decreased (0.727307 --> 0.727273).  Saving model ...\n",
      "\t Train_Loss: 309738.2282 Train_Acc: 50.167 Val_Loss: 0.7273  BEST VAL Loss: 0.7273  Val_Acc: 50.000\n",
      "\n",
      "Epoch 666: Validation loss decreased (0.727273 --> 0.727238).  Saving model ...\n",
      "\t Train_Loss: 309273.8541 Train_Acc: 50.167 Val_Loss: 0.7272  BEST VAL Loss: 0.7272  Val_Acc: 50.000\n",
      "\n",
      "Epoch 667: Validation loss decreased (0.727238 --> 0.727203).  Saving model ...\n",
      "\t Train_Loss: 308810.8704 Train_Acc: 50.167 Val_Loss: 0.7272  BEST VAL Loss: 0.7272  Val_Acc: 50.000\n",
      "\n",
      "Epoch 668: Validation loss decreased (0.727203 --> 0.727169).  Saving model ...\n",
      "\t Train_Loss: 308349.2707 Train_Acc: 50.167 Val_Loss: 0.7272  BEST VAL Loss: 0.7272  Val_Acc: 50.000\n",
      "\n",
      "Epoch 669: Validation loss decreased (0.727169 --> 0.727135).  Saving model ...\n",
      "\t Train_Loss: 307889.0490 Train_Acc: 50.167 Val_Loss: 0.7271  BEST VAL Loss: 0.7271  Val_Acc: 50.000\n",
      "\n",
      "Epoch 670: Validation loss decreased (0.727135 --> 0.727101).  Saving model ...\n",
      "\t Train_Loss: 307430.1990 Train_Acc: 50.167 Val_Loss: 0.7271  BEST VAL Loss: 0.7271  Val_Acc: 50.000\n",
      "\n",
      "Epoch 671: Validation loss decreased (0.727101 --> 0.727067).  Saving model ...\n",
      "\t Train_Loss: 306972.7146 Train_Acc: 50.167 Val_Loss: 0.7271  BEST VAL Loss: 0.7271  Val_Acc: 50.000\n",
      "\n",
      "Epoch 672: Validation loss decreased (0.727067 --> 0.727033).  Saving model ...\n",
      "\t Train_Loss: 306516.5898 Train_Acc: 50.167 Val_Loss: 0.7270  BEST VAL Loss: 0.7270  Val_Acc: 50.000\n",
      "\n",
      "Epoch 673: Validation loss decreased (0.727033 --> 0.726999).  Saving model ...\n",
      "\t Train_Loss: 306061.8184 Train_Acc: 50.167 Val_Loss: 0.7270  BEST VAL Loss: 0.7270  Val_Acc: 50.000\n",
      "\n",
      "Epoch 674: Validation loss decreased (0.726999 --> 0.726965).  Saving model ...\n",
      "\t Train_Loss: 305608.3946 Train_Acc: 50.167 Val_Loss: 0.7270  BEST VAL Loss: 0.7270  Val_Acc: 50.000\n",
      "\n",
      "Epoch 675: Validation loss decreased (0.726965 --> 0.726931).  Saving model ...\n",
      "\t Train_Loss: 305156.3122 Train_Acc: 50.167 Val_Loss: 0.7269  BEST VAL Loss: 0.7269  Val_Acc: 50.000\n",
      "\n",
      "Epoch 676: Validation loss decreased (0.726931 --> 0.726898).  Saving model ...\n",
      "\t Train_Loss: 304705.5653 Train_Acc: 50.167 Val_Loss: 0.7269  BEST VAL Loss: 0.7269  Val_Acc: 50.000\n",
      "\n",
      "Epoch 677: Validation loss decreased (0.726898 --> 0.726864).  Saving model ...\n",
      "\t Train_Loss: 304256.1481 Train_Acc: 50.167 Val_Loss: 0.7269  BEST VAL Loss: 0.7269  Val_Acc: 50.000\n",
      "\n",
      "Epoch 678: Validation loss decreased (0.726864 --> 0.726831).  Saving model ...\n",
      "\t Train_Loss: 303808.0547 Train_Acc: 50.167 Val_Loss: 0.7268  BEST VAL Loss: 0.7268  Val_Acc: 50.000\n",
      "\n",
      "Epoch 679: Validation loss decreased (0.726831 --> 0.726798).  Saving model ...\n",
      "\t Train_Loss: 303361.2792 Train_Acc: 50.167 Val_Loss: 0.7268  BEST VAL Loss: 0.7268  Val_Acc: 50.000\n",
      "\n",
      "Epoch 680: Validation loss decreased (0.726798 --> 0.726765).  Saving model ...\n",
      "\t Train_Loss: 302915.8158 Train_Acc: 50.167 Val_Loss: 0.7268  BEST VAL Loss: 0.7268  Val_Acc: 50.000\n",
      "\n",
      "Epoch 681: Validation loss decreased (0.726765 --> 0.726732).  Saving model ...\n",
      "\t Train_Loss: 302471.6587 Train_Acc: 50.167 Val_Loss: 0.7267  BEST VAL Loss: 0.7267  Val_Acc: 50.000\n",
      "\n",
      "Epoch 682: Validation loss decreased (0.726732 --> 0.726699).  Saving model ...\n",
      "\t Train_Loss: 302028.8023 Train_Acc: 50.167 Val_Loss: 0.7267  BEST VAL Loss: 0.7267  Val_Acc: 50.000\n",
      "\n",
      "Epoch 683: Validation loss decreased (0.726699 --> 0.726666).  Saving model ...\n",
      "\t Train_Loss: 301587.2407 Train_Acc: 50.167 Val_Loss: 0.7267  BEST VAL Loss: 0.7267  Val_Acc: 50.000\n",
      "\n",
      "Epoch 684: Validation loss decreased (0.726666 --> 0.726633).  Saving model ...\n",
      "\t Train_Loss: 301146.9684 Train_Acc: 50.167 Val_Loss: 0.7266  BEST VAL Loss: 0.7266  Val_Acc: 50.000\n",
      "\n",
      "Epoch 685: Validation loss decreased (0.726633 --> 0.726600).  Saving model ...\n",
      "\t Train_Loss: 300707.9797 Train_Acc: 50.167 Val_Loss: 0.7266  BEST VAL Loss: 0.7266  Val_Acc: 50.000\n",
      "\n",
      "Epoch 686: Validation loss decreased (0.726600 --> 0.726568).  Saving model ...\n",
      "\t Train_Loss: 300270.2689 Train_Acc: 50.167 Val_Loss: 0.7266  BEST VAL Loss: 0.7266  Val_Acc: 50.000\n",
      "\n",
      "Epoch 687: Validation loss decreased (0.726568 --> 0.726535).  Saving model ...\n",
      "\t Train_Loss: 299833.8306 Train_Acc: 50.167 Val_Loss: 0.7265  BEST VAL Loss: 0.7265  Val_Acc: 50.000\n",
      "\n",
      "Epoch 688: Validation loss decreased (0.726535 --> 0.726503).  Saving model ...\n",
      "\t Train_Loss: 299398.6592 Train_Acc: 50.167 Val_Loss: 0.7265  BEST VAL Loss: 0.7265  Val_Acc: 50.000\n",
      "\n",
      "Epoch 689: Validation loss decreased (0.726503 --> 0.726470).  Saving model ...\n",
      "\t Train_Loss: 298964.7491 Train_Acc: 50.167 Val_Loss: 0.7265  BEST VAL Loss: 0.7265  Val_Acc: 50.000\n",
      "\n",
      "Epoch 690: Validation loss decreased (0.726470 --> 0.726438).  Saving model ...\n",
      "\t Train_Loss: 298532.0949 Train_Acc: 50.167 Val_Loss: 0.7264  BEST VAL Loss: 0.7264  Val_Acc: 50.000\n",
      "\n",
      "Epoch 691: Validation loss decreased (0.726438 --> 0.726406).  Saving model ...\n",
      "\t Train_Loss: 298100.6912 Train_Acc: 50.167 Val_Loss: 0.7264  BEST VAL Loss: 0.7264  Val_Acc: 50.000\n",
      "\n",
      "Epoch 692: Validation loss decreased (0.726406 --> 0.726374).  Saving model ...\n",
      "\t Train_Loss: 297670.5324 Train_Acc: 50.167 Val_Loss: 0.7264  BEST VAL Loss: 0.7264  Val_Acc: 50.000\n",
      "\n",
      "Epoch 693: Validation loss decreased (0.726374 --> 0.726342).  Saving model ...\n",
      "\t Train_Loss: 297241.6134 Train_Acc: 50.167 Val_Loss: 0.7263  BEST VAL Loss: 0.7263  Val_Acc: 50.000\n",
      "\n",
      "Epoch 694: Validation loss decreased (0.726342 --> 0.726310).  Saving model ...\n",
      "\t Train_Loss: 296813.9286 Train_Acc: 50.167 Val_Loss: 0.7263  BEST VAL Loss: 0.7263  Val_Acc: 50.000\n",
      "\n",
      "Epoch 695: Validation loss decreased (0.726310 --> 0.726279).  Saving model ...\n",
      "\t Train_Loss: 296387.4728 Train_Acc: 50.167 Val_Loss: 0.7263  BEST VAL Loss: 0.7263  Val_Acc: 50.000\n",
      "\n",
      "Epoch 696: Validation loss decreased (0.726279 --> 0.726247).  Saving model ...\n",
      "\t Train_Loss: 295962.2407 Train_Acc: 50.167 Val_Loss: 0.7262  BEST VAL Loss: 0.7262  Val_Acc: 50.000\n",
      "\n",
      "Epoch 697: Validation loss decreased (0.726247 --> 0.726215).  Saving model ...\n",
      "\t Train_Loss: 295538.2271 Train_Acc: 50.167 Val_Loss: 0.7262  BEST VAL Loss: 0.7262  Val_Acc: 50.000\n",
      "\n",
      "Epoch 698: Validation loss decreased (0.726215 --> 0.726184).  Saving model ...\n",
      "\t Train_Loss: 295115.4266 Train_Acc: 50.167 Val_Loss: 0.7262  BEST VAL Loss: 0.7262  Val_Acc: 50.000\n",
      "\n",
      "Epoch 699: Validation loss decreased (0.726184 --> 0.726153).  Saving model ...\n",
      "\t Train_Loss: 294693.8342 Train_Acc: 50.167 Val_Loss: 0.7262  BEST VAL Loss: 0.7262  Val_Acc: 50.000\n",
      "\n",
      "Epoch 700: Validation loss decreased (0.726153 --> 0.726121).  Saving model ...\n",
      "\t Train_Loss: 294273.4445 Train_Acc: 50.167 Val_Loss: 0.7261  BEST VAL Loss: 0.7261  Val_Acc: 50.000\n",
      "\n",
      "Epoch 701: Validation loss decreased (0.726121 --> 0.726090).  Saving model ...\n",
      "\t Train_Loss: 293854.2526 Train_Acc: 50.167 Val_Loss: 0.7261  BEST VAL Loss: 0.7261  Val_Acc: 50.000\n",
      "\n",
      "Epoch 702: Validation loss decreased (0.726090 --> 0.726059).  Saving model ...\n",
      "\t Train_Loss: 293436.2532 Train_Acc: 50.167 Val_Loss: 0.7261  BEST VAL Loss: 0.7261  Val_Acc: 50.000\n",
      "\n",
      "Epoch 703: Validation loss decreased (0.726059 --> 0.726028).  Saving model ...\n",
      "\t Train_Loss: 293019.4414 Train_Acc: 50.167 Val_Loss: 0.7260  BEST VAL Loss: 0.7260  Val_Acc: 50.000\n",
      "\n",
      "Epoch 704: Validation loss decreased (0.726028 --> 0.725997).  Saving model ...\n",
      "\t Train_Loss: 292603.8119 Train_Acc: 50.167 Val_Loss: 0.7260  BEST VAL Loss: 0.7260  Val_Acc: 50.000\n",
      "\n",
      "Epoch 705: Validation loss decreased (0.725997 --> 0.725966).  Saving model ...\n",
      "\t Train_Loss: 292189.3600 Train_Acc: 50.167 Val_Loss: 0.7260  BEST VAL Loss: 0.7260  Val_Acc: 50.000\n",
      "\n",
      "Epoch 706: Validation loss decreased (0.725966 --> 0.725935).  Saving model ...\n",
      "\t Train_Loss: 291776.0804 Train_Acc: 50.167 Val_Loss: 0.7259  BEST VAL Loss: 0.7259  Val_Acc: 50.000\n",
      "\n",
      "Epoch 707: Validation loss decreased (0.725935 --> 0.725905).  Saving model ...\n",
      "\t Train_Loss: 291363.9683 Train_Acc: 50.167 Val_Loss: 0.7259  BEST VAL Loss: 0.7259  Val_Acc: 50.000\n",
      "\n",
      "Epoch 708: Validation loss decreased (0.725905 --> 0.725874).  Saving model ...\n",
      "\t Train_Loss: 290953.0187 Train_Acc: 50.167 Val_Loss: 0.7259  BEST VAL Loss: 0.7259  Val_Acc: 50.000\n",
      "\n",
      "Epoch 709: Validation loss decreased (0.725874 --> 0.725844).  Saving model ...\n",
      "\t Train_Loss: 290543.2267 Train_Acc: 50.167 Val_Loss: 0.7258  BEST VAL Loss: 0.7258  Val_Acc: 50.000\n",
      "\n",
      "Epoch 710: Validation loss decreased (0.725844 --> 0.725813).  Saving model ...\n",
      "\t Train_Loss: 290134.5874 Train_Acc: 50.167 Val_Loss: 0.7258  BEST VAL Loss: 0.7258  Val_Acc: 50.000\n",
      "\n",
      "Epoch 711: Validation loss decreased (0.725813 --> 0.725783).  Saving model ...\n",
      "\t Train_Loss: 289727.0960 Train_Acc: 50.167 Val_Loss: 0.7258  BEST VAL Loss: 0.7258  Val_Acc: 50.000\n",
      "\n",
      "Epoch 712: Validation loss decreased (0.725783 --> 0.725753).  Saving model ...\n",
      "\t Train_Loss: 289320.7476 Train_Acc: 50.167 Val_Loss: 0.7258  BEST VAL Loss: 0.7258  Val_Acc: 50.000\n",
      "\n",
      "Epoch 713: Validation loss decreased (0.725753 --> 0.725722).  Saving model ...\n",
      "\t Train_Loss: 288915.5375 Train_Acc: 50.167 Val_Loss: 0.7257  BEST VAL Loss: 0.7257  Val_Acc: 50.000\n",
      "\n",
      "Epoch 714: Validation loss decreased (0.725722 --> 0.725692).  Saving model ...\n",
      "\t Train_Loss: 288511.4608 Train_Acc: 50.167 Val_Loss: 0.7257  BEST VAL Loss: 0.7257  Val_Acc: 50.000\n",
      "\n",
      "Epoch 715: Validation loss decreased (0.725692 --> 0.725662).  Saving model ...\n",
      "\t Train_Loss: 288108.5128 Train_Acc: 50.167 Val_Loss: 0.7257  BEST VAL Loss: 0.7257  Val_Acc: 50.000\n",
      "\n",
      "Epoch 716: Validation loss decreased (0.725662 --> 0.725632).  Saving model ...\n",
      "\t Train_Loss: 287706.6888 Train_Acc: 50.167 Val_Loss: 0.7256  BEST VAL Loss: 0.7256  Val_Acc: 50.000\n",
      "\n",
      "Epoch 717: Validation loss decreased (0.725632 --> 0.725603).  Saving model ...\n",
      "\t Train_Loss: 287305.9841 Train_Acc: 50.167 Val_Loss: 0.7256  BEST VAL Loss: 0.7256  Val_Acc: 50.000\n",
      "\n",
      "Epoch 718: Validation loss decreased (0.725603 --> 0.725573).  Saving model ...\n",
      "\t Train_Loss: 286906.3940 Train_Acc: 50.167 Val_Loss: 0.7256  BEST VAL Loss: 0.7256  Val_Acc: 50.000\n",
      "\n",
      "Epoch 719: Validation loss decreased (0.725573 --> 0.725543).  Saving model ...\n",
      "\t Train_Loss: 286507.9139 Train_Acc: 50.167 Val_Loss: 0.7255  BEST VAL Loss: 0.7255  Val_Acc: 50.000\n",
      "\n",
      "Epoch 720: Validation loss decreased (0.725543 --> 0.725514).  Saving model ...\n",
      "\t Train_Loss: 286110.5391 Train_Acc: 50.167 Val_Loss: 0.7255  BEST VAL Loss: 0.7255  Val_Acc: 50.000\n",
      "\n",
      "Epoch 721: Validation loss decreased (0.725514 --> 0.725484).  Saving model ...\n",
      "\t Train_Loss: 285714.2651 Train_Acc: 50.167 Val_Loss: 0.7255  BEST VAL Loss: 0.7255  Val_Acc: 50.000\n",
      "\n",
      "Epoch 722: Validation loss decreased (0.725484 --> 0.725455).  Saving model ...\n",
      "\t Train_Loss: 285319.0873 Train_Acc: 50.167 Val_Loss: 0.7255  BEST VAL Loss: 0.7255  Val_Acc: 50.000\n",
      "\n",
      "Epoch 723: Validation loss decreased (0.725455 --> 0.725425).  Saving model ...\n",
      "\t Train_Loss: 284925.0011 Train_Acc: 50.167 Val_Loss: 0.7254  BEST VAL Loss: 0.7254  Val_Acc: 50.000\n",
      "\n",
      "Epoch 724: Validation loss decreased (0.725425 --> 0.725396).  Saving model ...\n",
      "\t Train_Loss: 284532.0021 Train_Acc: 50.167 Val_Loss: 0.7254  BEST VAL Loss: 0.7254  Val_Acc: 50.000\n",
      "\n",
      "Epoch 725: Validation loss decreased (0.725396 --> 0.725367).  Saving model ...\n",
      "\t Train_Loss: 284140.0857 Train_Acc: 50.167 Val_Loss: 0.7254  BEST VAL Loss: 0.7254  Val_Acc: 50.000\n",
      "\n",
      "Epoch 726: Validation loss decreased (0.725367 --> 0.725338).  Saving model ...\n",
      "\t Train_Loss: 283749.2475 Train_Acc: 50.167 Val_Loss: 0.7253  BEST VAL Loss: 0.7253  Val_Acc: 50.000\n",
      "\n",
      "Epoch 727: Validation loss decreased (0.725338 --> 0.725309).  Saving model ...\n",
      "\t Train_Loss: 283359.4830 Train_Acc: 50.167 Val_Loss: 0.7253  BEST VAL Loss: 0.7253  Val_Acc: 50.000\n",
      "\n",
      "Epoch 728: Validation loss decreased (0.725309 --> 0.725280).  Saving model ...\n",
      "\t Train_Loss: 282970.7878 Train_Acc: 50.167 Val_Loss: 0.7253  BEST VAL Loss: 0.7253  Val_Acc: 50.000\n",
      "\n",
      "Epoch 729: Validation loss decreased (0.725280 --> 0.725251).  Saving model ...\n",
      "\t Train_Loss: 282583.1575 Train_Acc: 50.167 Val_Loss: 0.7253  BEST VAL Loss: 0.7253  Val_Acc: 50.000\n",
      "\n",
      "Epoch 730: Validation loss decreased (0.725251 --> 0.725222).  Saving model ...\n",
      "\t Train_Loss: 282196.5878 Train_Acc: 50.167 Val_Loss: 0.7252  BEST VAL Loss: 0.7252  Val_Acc: 50.000\n",
      "\n",
      "Epoch 731: Validation loss decreased (0.725222 --> 0.725193).  Saving model ...\n",
      "\t Train_Loss: 281811.0743 Train_Acc: 50.167 Val_Loss: 0.7252  BEST VAL Loss: 0.7252  Val_Acc: 50.000\n",
      "\n",
      "Epoch 732: Validation loss decreased (0.725193 --> 0.725165).  Saving model ...\n",
      "\t Train_Loss: 281426.6127 Train_Acc: 50.167 Val_Loss: 0.7252  BEST VAL Loss: 0.7252  Val_Acc: 50.000\n",
      "\n",
      "Epoch 733: Validation loss decreased (0.725165 --> 0.725136).  Saving model ...\n",
      "\t Train_Loss: 281043.1987 Train_Acc: 50.167 Val_Loss: 0.7251  BEST VAL Loss: 0.7251  Val_Acc: 50.000\n",
      "\n",
      "Epoch 734: Validation loss decreased (0.725136 --> 0.725108).  Saving model ...\n",
      "\t Train_Loss: 280660.8279 Train_Acc: 50.167 Val_Loss: 0.7251  BEST VAL Loss: 0.7251  Val_Acc: 50.000\n",
      "\n",
      "Epoch 735: Validation loss decreased (0.725108 --> 0.725079).  Saving model ...\n",
      "\t Train_Loss: 280279.4962 Train_Acc: 50.167 Val_Loss: 0.7251  BEST VAL Loss: 0.7251  Val_Acc: 50.000\n",
      "\n",
      "Epoch 736: Validation loss decreased (0.725079 --> 0.725051).  Saving model ...\n",
      "\t Train_Loss: 279899.1994 Train_Acc: 50.167 Val_Loss: 0.7251  BEST VAL Loss: 0.7251  Val_Acc: 50.000\n",
      "\n",
      "Epoch 737: Validation loss decreased (0.725051 --> 0.725023).  Saving model ...\n",
      "\t Train_Loss: 279519.9331 Train_Acc: 50.167 Val_Loss: 0.7250  BEST VAL Loss: 0.7250  Val_Acc: 50.000\n",
      "\n",
      "Epoch 738: Validation loss decreased (0.725023 --> 0.724995).  Saving model ...\n",
      "\t Train_Loss: 279141.6933 Train_Acc: 50.167 Val_Loss: 0.7250  BEST VAL Loss: 0.7250  Val_Acc: 50.000\n",
      "\n",
      "Epoch 739: Validation loss decreased (0.724995 --> 0.724967).  Saving model ...\n",
      "\t Train_Loss: 278764.4757 Train_Acc: 50.167 Val_Loss: 0.7250  BEST VAL Loss: 0.7250  Val_Acc: 50.000\n",
      "\n",
      "Epoch 740: Validation loss decreased (0.724967 --> 0.724939).  Saving model ...\n",
      "\t Train_Loss: 278388.2763 Train_Acc: 50.167 Val_Loss: 0.7249  BEST VAL Loss: 0.7249  Val_Acc: 50.000\n",
      "\n",
      "Epoch 741: Validation loss decreased (0.724939 --> 0.724911).  Saving model ...\n",
      "\t Train_Loss: 278013.0909 Train_Acc: 50.167 Val_Loss: 0.7249  BEST VAL Loss: 0.7249  Val_Acc: 50.000\n",
      "\n",
      "Epoch 742: Validation loss decreased (0.724911 --> 0.724883).  Saving model ...\n",
      "\t Train_Loss: 277638.9154 Train_Acc: 50.167 Val_Loss: 0.7249  BEST VAL Loss: 0.7249  Val_Acc: 50.000\n",
      "\n",
      "Epoch 743: Validation loss decreased (0.724883 --> 0.724855).  Saving model ...\n",
      "\t Train_Loss: 277265.7458 Train_Acc: 50.167 Val_Loss: 0.7249  BEST VAL Loss: 0.7249  Val_Acc: 50.000\n",
      "\n",
      "Epoch 744: Validation loss decreased (0.724855 --> 0.724827).  Saving model ...\n",
      "\t Train_Loss: 276893.5779 Train_Acc: 50.167 Val_Loss: 0.7248  BEST VAL Loss: 0.7248  Val_Acc: 50.000\n",
      "\n",
      "Epoch 745: Validation loss decreased (0.724827 --> 0.724800).  Saving model ...\n",
      "\t Train_Loss: 276522.4079 Train_Acc: 50.167 Val_Loss: 0.7248  BEST VAL Loss: 0.7248  Val_Acc: 50.000\n",
      "\n",
      "Epoch 746: Validation loss decreased (0.724800 --> 0.724772).  Saving model ...\n",
      "\t Train_Loss: 276152.2315 Train_Acc: 50.167 Val_Loss: 0.7248  BEST VAL Loss: 0.7248  Val_Acc: 50.000\n",
      "\n",
      "Epoch 747: Validation loss decreased (0.724772 --> 0.724745).  Saving model ...\n",
      "\t Train_Loss: 275783.0450 Train_Acc: 50.167 Val_Loss: 0.7247  BEST VAL Loss: 0.7247  Val_Acc: 50.000\n",
      "\n",
      "Epoch 748: Validation loss decreased (0.724745 --> 0.724717).  Saving model ...\n",
      "\t Train_Loss: 275414.8443 Train_Acc: 50.167 Val_Loss: 0.7247  BEST VAL Loss: 0.7247  Val_Acc: 50.000\n",
      "\n",
      "Epoch 749: Validation loss decreased (0.724717 --> 0.724690).  Saving model ...\n",
      "\t Train_Loss: 275047.6254 Train_Acc: 50.167 Val_Loss: 0.7247  BEST VAL Loss: 0.7247  Val_Acc: 50.000\n",
      "\n",
      "Epoch 750: Validation loss decreased (0.724690 --> 0.724663).  Saving model ...\n",
      "\t Train_Loss: 274681.3845 Train_Acc: 50.167 Val_Loss: 0.7247  BEST VAL Loss: 0.7247  Val_Acc: 50.000\n",
      "\n",
      "Epoch 751: Validation loss decreased (0.724663 --> 0.724636).  Saving model ...\n",
      "\t Train_Loss: 274316.1177 Train_Acc: 50.167 Val_Loss: 0.7246  BEST VAL Loss: 0.7246  Val_Acc: 50.000\n",
      "\n",
      "Epoch 752: Validation loss decreased (0.724636 --> 0.724608).  Saving model ...\n",
      "\t Train_Loss: 273951.8210 Train_Acc: 50.167 Val_Loss: 0.7246  BEST VAL Loss: 0.7246  Val_Acc: 50.000\n",
      "\n",
      "Epoch 753: Validation loss decreased (0.724608 --> 0.724581).  Saving model ...\n",
      "\t Train_Loss: 273588.4906 Train_Acc: 50.167 Val_Loss: 0.7246  BEST VAL Loss: 0.7246  Val_Acc: 50.000\n",
      "\n",
      "Epoch 754: Validation loss decreased (0.724581 --> 0.724554).  Saving model ...\n",
      "\t Train_Loss: 273226.1226 Train_Acc: 50.167 Val_Loss: 0.7246  BEST VAL Loss: 0.7246  Val_Acc: 50.000\n",
      "\n",
      "Epoch 755: Validation loss decreased (0.724554 --> 0.724527).  Saving model ...\n",
      "\t Train_Loss: 272864.7133 Train_Acc: 50.167 Val_Loss: 0.7245  BEST VAL Loss: 0.7245  Val_Acc: 50.000\n",
      "\n",
      "Epoch 756: Validation loss decreased (0.724527 --> 0.724501).  Saving model ...\n",
      "\t Train_Loss: 272504.2589 Train_Acc: 50.167 Val_Loss: 0.7245  BEST VAL Loss: 0.7245  Val_Acc: 50.000\n",
      "\n",
      "Epoch 757: Validation loss decreased (0.724501 --> 0.724474).  Saving model ...\n",
      "\t Train_Loss: 272144.7555 Train_Acc: 50.167 Val_Loss: 0.7245  BEST VAL Loss: 0.7245  Val_Acc: 50.000\n",
      "\n",
      "Epoch 758: Validation loss decreased (0.724474 --> 0.724447).  Saving model ...\n",
      "\t Train_Loss: 271786.1995 Train_Acc: 50.167 Val_Loss: 0.7244  BEST VAL Loss: 0.7244  Val_Acc: 50.000\n",
      "\n",
      "Epoch 759: Validation loss decreased (0.724447 --> 0.724421).  Saving model ...\n",
      "\t Train_Loss: 271428.5870 Train_Acc: 50.167 Val_Loss: 0.7244  BEST VAL Loss: 0.7244  Val_Acc: 50.000\n",
      "\n",
      "Epoch 760: Validation loss decreased (0.724421 --> 0.724394).  Saving model ...\n",
      "\t Train_Loss: 271071.9143 Train_Acc: 50.167 Val_Loss: 0.7244  BEST VAL Loss: 0.7244  Val_Acc: 50.000\n",
      "\n",
      "Epoch 761: Validation loss decreased (0.724394 --> 0.724368).  Saving model ...\n",
      "\t Train_Loss: 270716.1778 Train_Acc: 50.167 Val_Loss: 0.7244  BEST VAL Loss: 0.7244  Val_Acc: 50.000\n",
      "\n",
      "Epoch 762: Validation loss decreased (0.724368 --> 0.724341).  Saving model ...\n",
      "\t Train_Loss: 270361.3738 Train_Acc: 50.167 Val_Loss: 0.7243  BEST VAL Loss: 0.7243  Val_Acc: 50.000\n",
      "\n",
      "Epoch 763: Validation loss decreased (0.724341 --> 0.724315).  Saving model ...\n",
      "\t Train_Loss: 270007.4986 Train_Acc: 50.167 Val_Loss: 0.7243  BEST VAL Loss: 0.7243  Val_Acc: 50.000\n",
      "\n",
      "Epoch 764: Validation loss decreased (0.724315 --> 0.724288).  Saving model ...\n",
      "\t Train_Loss: 269654.5485 Train_Acc: 50.167 Val_Loss: 0.7243  BEST VAL Loss: 0.7243  Val_Acc: 50.000\n",
      "\n",
      "Epoch 765: Validation loss decreased (0.724288 --> 0.724262).  Saving model ...\n",
      "\t Train_Loss: 269302.5200 Train_Acc: 50.167 Val_Loss: 0.7243  BEST VAL Loss: 0.7243  Val_Acc: 50.000\n",
      "\n",
      "Epoch 766: Validation loss decreased (0.724262 --> 0.724236).  Saving model ...\n",
      "\t Train_Loss: 268951.4094 Train_Acc: 50.167 Val_Loss: 0.7242  BEST VAL Loss: 0.7242  Val_Acc: 50.000\n",
      "\n",
      "Epoch 767: Validation loss decreased (0.724236 --> 0.724210).  Saving model ...\n",
      "\t Train_Loss: 268601.2132 Train_Acc: 50.167 Val_Loss: 0.7242  BEST VAL Loss: 0.7242  Val_Acc: 50.000\n",
      "\n",
      "Epoch 768: Validation loss decreased (0.724210 --> 0.724184).  Saving model ...\n",
      "\t Train_Loss: 268251.9277 Train_Acc: 50.167 Val_Loss: 0.7242  BEST VAL Loss: 0.7242  Val_Acc: 50.000\n",
      "\n",
      "Epoch 769: Validation loss decreased (0.724184 --> 0.724158).  Saving model ...\n",
      "\t Train_Loss: 267903.5495 Train_Acc: 50.167 Val_Loss: 0.7242  BEST VAL Loss: 0.7242  Val_Acc: 50.000\n",
      "\n",
      "Epoch 770: Validation loss decreased (0.724158 --> 0.724132).  Saving model ...\n",
      "\t Train_Loss: 267556.0750 Train_Acc: 50.167 Val_Loss: 0.7241  BEST VAL Loss: 0.7241  Val_Acc: 50.000\n",
      "\n",
      "Epoch 771: Validation loss decreased (0.724132 --> 0.724106).  Saving model ...\n",
      "\t Train_Loss: 267209.5007 Train_Acc: 50.167 Val_Loss: 0.7241  BEST VAL Loss: 0.7241  Val_Acc: 50.000\n",
      "\n",
      "Epoch 772: Validation loss decreased (0.724106 --> 0.724081).  Saving model ...\n",
      "\t Train_Loss: 266863.8231 Train_Acc: 50.167 Val_Loss: 0.7241  BEST VAL Loss: 0.7241  Val_Acc: 50.000\n",
      "\n",
      "Epoch 773: Validation loss decreased (0.724081 --> 0.724055).  Saving model ...\n",
      "\t Train_Loss: 266519.0387 Train_Acc: 50.167 Val_Loss: 0.7241  BEST VAL Loss: 0.7241  Val_Acc: 50.000\n",
      "\n",
      "Epoch 774: Validation loss decreased (0.724055 --> 0.724029).  Saving model ...\n",
      "\t Train_Loss: 266175.1441 Train_Acc: 50.167 Val_Loss: 0.7240  BEST VAL Loss: 0.7240  Val_Acc: 50.000\n",
      "\n",
      "Epoch 775: Validation loss decreased (0.724029 --> 0.724004).  Saving model ...\n",
      "\t Train_Loss: 265832.1358 Train_Acc: 50.167 Val_Loss: 0.7240  BEST VAL Loss: 0.7240  Val_Acc: 50.000\n",
      "\n",
      "Epoch 776: Validation loss decreased (0.724004 --> 0.723978).  Saving model ...\n",
      "\t Train_Loss: 265490.0104 Train_Acc: 50.167 Val_Loss: 0.7240  BEST VAL Loss: 0.7240  Val_Acc: 50.000\n",
      "\n",
      "Epoch 777: Validation loss decreased (0.723978 --> 0.723953).  Saving model ...\n",
      "\t Train_Loss: 265148.7645 Train_Acc: 50.167 Val_Loss: 0.7240  BEST VAL Loss: 0.7240  Val_Acc: 50.000\n",
      "\n",
      "Epoch 778: Validation loss decreased (0.723953 --> 0.723928).  Saving model ...\n",
      "\t Train_Loss: 264808.3947 Train_Acc: 50.167 Val_Loss: 0.7239  BEST VAL Loss: 0.7239  Val_Acc: 50.000\n",
      "\n",
      "Epoch 779: Validation loss decreased (0.723928 --> 0.723902).  Saving model ...\n",
      "\t Train_Loss: 264468.8977 Train_Acc: 50.167 Val_Loss: 0.7239  BEST VAL Loss: 0.7239  Val_Acc: 50.000\n",
      "\n",
      "Epoch 780: Validation loss decreased (0.723902 --> 0.723877).  Saving model ...\n",
      "\t Train_Loss: 264130.2700 Train_Acc: 50.167 Val_Loss: 0.7239  BEST VAL Loss: 0.7239  Val_Acc: 50.000\n",
      "\n",
      "Epoch 781: Validation loss decreased (0.723877 --> 0.723852).  Saving model ...\n",
      "\t Train_Loss: 263792.5084 Train_Acc: 50.167 Val_Loss: 0.7239  BEST VAL Loss: 0.7239  Val_Acc: 50.000\n",
      "\n",
      "Epoch 782: Validation loss decreased (0.723852 --> 0.723827).  Saving model ...\n",
      "\t Train_Loss: 263455.6096 Train_Acc: 50.167 Val_Loss: 0.7238  BEST VAL Loss: 0.7238  Val_Acc: 50.000\n",
      "\n",
      "Epoch 783: Validation loss decreased (0.723827 --> 0.723802).  Saving model ...\n",
      "\t Train_Loss: 263119.5701 Train_Acc: 50.167 Val_Loss: 0.7238  BEST VAL Loss: 0.7238  Val_Acc: 50.000\n",
      "\n",
      "Epoch 784: Validation loss decreased (0.723802 --> 0.723777).  Saving model ...\n",
      "\t Train_Loss: 262784.3869 Train_Acc: 50.167 Val_Loss: 0.7238  BEST VAL Loss: 0.7238  Val_Acc: 50.000\n",
      "\n",
      "Epoch 785: Validation loss decreased (0.723777 --> 0.723752).  Saving model ...\n",
      "\t Train_Loss: 262450.0565 Train_Acc: 50.167 Val_Loss: 0.7238  BEST VAL Loss: 0.7238  Val_Acc: 50.000\n",
      "\n",
      "Epoch 786: Validation loss decreased (0.723752 --> 0.723727).  Saving model ...\n",
      "\t Train_Loss: 262116.5757 Train_Acc: 50.167 Val_Loss: 0.7237  BEST VAL Loss: 0.7237  Val_Acc: 50.000\n",
      "\n",
      "Epoch 787: Validation loss decreased (0.723727 --> 0.723703).  Saving model ...\n",
      "\t Train_Loss: 261783.9414 Train_Acc: 50.167 Val_Loss: 0.7237  BEST VAL Loss: 0.7237  Val_Acc: 50.000\n",
      "\n",
      "Epoch 788: Validation loss decreased (0.723703 --> 0.723678).  Saving model ...\n",
      "\t Train_Loss: 261452.1502 Train_Acc: 50.167 Val_Loss: 0.7237  BEST VAL Loss: 0.7237  Val_Acc: 50.000\n",
      "\n",
      "Epoch 789: Validation loss decreased (0.723678 --> 0.723653).  Saving model ...\n",
      "\t Train_Loss: 261121.1990 Train_Acc: 50.167 Val_Loss: 0.7237  BEST VAL Loss: 0.7237  Val_Acc: 50.000\n",
      "\n",
      "Epoch 790: Validation loss decreased (0.723653 --> 0.723629).  Saving model ...\n",
      "\t Train_Loss: 260791.0846 Train_Acc: 50.167 Val_Loss: 0.7236  BEST VAL Loss: 0.7236  Val_Acc: 50.000\n",
      "\n",
      "Epoch 791: Validation loss decreased (0.723629 --> 0.723604).  Saving model ...\n",
      "\t Train_Loss: 260461.8038 Train_Acc: 50.167 Val_Loss: 0.7236  BEST VAL Loss: 0.7236  Val_Acc: 50.000\n",
      "\n",
      "Epoch 792: Validation loss decreased (0.723604 --> 0.723580).  Saving model ...\n",
      "\t Train_Loss: 260133.3535 Train_Acc: 50.167 Val_Loss: 0.7236  BEST VAL Loss: 0.7236  Val_Acc: 50.000\n",
      "\n",
      "Epoch 793: Validation loss decreased (0.723580 --> 0.723555).  Saving model ...\n",
      "\t Train_Loss: 259805.7305 Train_Acc: 50.167 Val_Loss: 0.7236  BEST VAL Loss: 0.7236  Val_Acc: 50.000\n",
      "\n",
      "Epoch 794: Validation loss decreased (0.723555 --> 0.723531).  Saving model ...\n",
      "\t Train_Loss: 259478.9317 Train_Acc: 50.167 Val_Loss: 0.7235  BEST VAL Loss: 0.7235  Val_Acc: 50.000\n",
      "\n",
      "Epoch 795: Validation loss decreased (0.723531 --> 0.723507).  Saving model ...\n",
      "\t Train_Loss: 259152.9540 Train_Acc: 50.167 Val_Loss: 0.7235  BEST VAL Loss: 0.7235  Val_Acc: 50.000\n",
      "\n",
      "Epoch 796: Validation loss decreased (0.723507 --> 0.723482).  Saving model ...\n",
      "\t Train_Loss: 258827.7944 Train_Acc: 50.167 Val_Loss: 0.7235  BEST VAL Loss: 0.7235  Val_Acc: 50.000\n",
      "\n",
      "Epoch 797: Validation loss decreased (0.723482 --> 0.723458).  Saving model ...\n",
      "\t Train_Loss: 258503.4497 Train_Acc: 50.167 Val_Loss: 0.7235  BEST VAL Loss: 0.7235  Val_Acc: 50.000\n",
      "\n",
      "Epoch 798: Validation loss decreased (0.723458 --> 0.723434).  Saving model ...\n",
      "\t Train_Loss: 258179.9168 Train_Acc: 50.167 Val_Loss: 0.7234  BEST VAL Loss: 0.7234  Val_Acc: 50.000\n",
      "\n",
      "Epoch 799: Validation loss decreased (0.723434 --> 0.723410).  Saving model ...\n",
      "\t Train_Loss: 257857.1928 Train_Acc: 50.167 Val_Loss: 0.7234  BEST VAL Loss: 0.7234  Val_Acc: 50.000\n",
      "\n",
      "Epoch 800: Validation loss decreased (0.723410 --> 0.723386).  Saving model ...\n",
      "\t Train_Loss: 257535.2746 Train_Acc: 50.167 Val_Loss: 0.7234  BEST VAL Loss: 0.7234  Val_Acc: 50.000\n",
      "\n",
      "Epoch 801: Validation loss decreased (0.723386 --> 0.723362).  Saving model ...\n",
      "\t Train_Loss: 257214.1592 Train_Acc: 50.167 Val_Loss: 0.7234  BEST VAL Loss: 0.7234  Val_Acc: 50.000\n",
      "\n",
      "Epoch 802: Validation loss decreased (0.723362 --> 0.723339).  Saving model ...\n",
      "\t Train_Loss: 256893.8435 Train_Acc: 50.167 Val_Loss: 0.7233  BEST VAL Loss: 0.7233  Val_Acc: 50.000\n",
      "\n",
      "Epoch 803: Validation loss decreased (0.723339 --> 0.723315).  Saving model ...\n",
      "\t Train_Loss: 256574.3247 Train_Acc: 50.167 Val_Loss: 0.7233  BEST VAL Loss: 0.7233  Val_Acc: 50.000\n",
      "\n",
      "Epoch 804: Validation loss decreased (0.723315 --> 0.723291).  Saving model ...\n",
      "\t Train_Loss: 256255.5997 Train_Acc: 50.167 Val_Loss: 0.7233  BEST VAL Loss: 0.7233  Val_Acc: 50.000\n",
      "\n",
      "Epoch 805: Validation loss decreased (0.723291 --> 0.723267).  Saving model ...\n",
      "\t Train_Loss: 255937.6656 Train_Acc: 50.167 Val_Loss: 0.7233  BEST VAL Loss: 0.7233  Val_Acc: 50.000\n",
      "\n",
      "Epoch 806: Validation loss decreased (0.723267 --> 0.723244).  Saving model ...\n",
      "\t Train_Loss: 255620.5194 Train_Acc: 50.167 Val_Loss: 0.7232  BEST VAL Loss: 0.7232  Val_Acc: 50.000\n",
      "\n",
      "Epoch 807: Validation loss decreased (0.723244 --> 0.723220).  Saving model ...\n",
      "\t Train_Loss: 255304.1582 Train_Acc: 50.167 Val_Loss: 0.7232  BEST VAL Loss: 0.7232  Val_Acc: 50.000\n",
      "\n",
      "Epoch 808: Validation loss decreased (0.723220 --> 0.723197).  Saving model ...\n",
      "\t Train_Loss: 254988.5792 Train_Acc: 50.167 Val_Loss: 0.7232  BEST VAL Loss: 0.7232  Val_Acc: 50.000\n",
      "\n",
      "Epoch 809: Validation loss decreased (0.723197 --> 0.723173).  Saving model ...\n",
      "\t Train_Loss: 254673.7793 Train_Acc: 50.167 Val_Loss: 0.7232  BEST VAL Loss: 0.7232  Val_Acc: 50.000\n",
      "\n",
      "Epoch 810: Validation loss decreased (0.723173 --> 0.723150).  Saving model ...\n",
      "\t Train_Loss: 254359.7558 Train_Acc: 50.167 Val_Loss: 0.7231  BEST VAL Loss: 0.7231  Val_Acc: 50.000\n",
      "\n",
      "Epoch 811: Validation loss decreased (0.723150 --> 0.723127).  Saving model ...\n",
      "\t Train_Loss: 254046.5058 Train_Acc: 50.167 Val_Loss: 0.7231  BEST VAL Loss: 0.7231  Val_Acc: 50.000\n",
      "\n",
      "Epoch 812: Validation loss decreased (0.723127 --> 0.723103).  Saving model ...\n",
      "\t Train_Loss: 253734.0263 Train_Acc: 50.167 Val_Loss: 0.7231  BEST VAL Loss: 0.7231  Val_Acc: 50.000\n",
      "\n",
      "Epoch 813: Validation loss decreased (0.723103 --> 0.723080).  Saving model ...\n",
      "\t Train_Loss: 253422.3146 Train_Acc: 50.167 Val_Loss: 0.7231  BEST VAL Loss: 0.7231  Val_Acc: 50.000\n",
      "\n",
      "Epoch 814: Validation loss decreased (0.723080 --> 0.723057).  Saving model ...\n",
      "\t Train_Loss: 253111.3678 Train_Acc: 50.167 Val_Loss: 0.7231  BEST VAL Loss: 0.7231  Val_Acc: 50.000\n",
      "\n",
      "Epoch 815: Validation loss decreased (0.723057 --> 0.723034).  Saving model ...\n",
      "\t Train_Loss: 252801.1832 Train_Acc: 50.167 Val_Loss: 0.7230  BEST VAL Loss: 0.7230  Val_Acc: 50.000\n",
      "\n",
      "Epoch 816: Validation loss decreased (0.723034 --> 0.723011).  Saving model ...\n",
      "\t Train_Loss: 252491.7579 Train_Acc: 50.167 Val_Loss: 0.7230  BEST VAL Loss: 0.7230  Val_Acc: 50.000\n",
      "\n",
      "Epoch 817: Validation loss decreased (0.723011 --> 0.722988).  Saving model ...\n",
      "\t Train_Loss: 252183.0891 Train_Acc: 50.167 Val_Loss: 0.7230  BEST VAL Loss: 0.7230  Val_Acc: 50.000\n",
      "\n",
      "Epoch 818: Validation loss decreased (0.722988 --> 0.722965).  Saving model ...\n",
      "\t Train_Loss: 251875.1741 Train_Acc: 50.167 Val_Loss: 0.7230  BEST VAL Loss: 0.7230  Val_Acc: 50.000\n",
      "\n",
      "Epoch 819: Validation loss decreased (0.722965 --> 0.722942).  Saving model ...\n",
      "\t Train_Loss: 251568.0101 Train_Acc: 50.167 Val_Loss: 0.7229  BEST VAL Loss: 0.7229  Val_Acc: 50.000\n",
      "\n",
      "Epoch 820: Validation loss decreased (0.722942 --> 0.722919).  Saving model ...\n",
      "\t Train_Loss: 251261.5944 Train_Acc: 50.167 Val_Loss: 0.7229  BEST VAL Loss: 0.7229  Val_Acc: 50.000\n",
      "\n",
      "Epoch 821: Validation loss decreased (0.722919 --> 0.722896).  Saving model ...\n",
      "\t Train_Loss: 250955.9242 Train_Acc: 50.167 Val_Loss: 0.7229  BEST VAL Loss: 0.7229  Val_Acc: 50.000\n",
      "\n",
      "Epoch 822: Validation loss decreased (0.722896 --> 0.722874).  Saving model ...\n",
      "\t Train_Loss: 250650.9969 Train_Acc: 50.167 Val_Loss: 0.7229  BEST VAL Loss: 0.7229  Val_Acc: 50.000\n",
      "\n",
      "Epoch 823: Validation loss decreased (0.722874 --> 0.722851).  Saving model ...\n",
      "\t Train_Loss: 250346.8096 Train_Acc: 50.167 Val_Loss: 0.7229  BEST VAL Loss: 0.7229  Val_Acc: 50.000\n",
      "\n",
      "Epoch 824: Validation loss decreased (0.722851 --> 0.722829).  Saving model ...\n",
      "\t Train_Loss: 250043.3598 Train_Acc: 50.167 Val_Loss: 0.7228  BEST VAL Loss: 0.7228  Val_Acc: 50.000\n",
      "\n",
      "Epoch 825: Validation loss decreased (0.722829 --> 0.722806).  Saving model ...\n",
      "\t Train_Loss: 249740.6447 Train_Acc: 50.167 Val_Loss: 0.7228  BEST VAL Loss: 0.7228  Val_Acc: 50.000\n",
      "\n",
      "Epoch 826: Validation loss decreased (0.722806 --> 0.722784).  Saving model ...\n",
      "\t Train_Loss: 249438.6617 Train_Acc: 50.167 Val_Loss: 0.7228  BEST VAL Loss: 0.7228  Val_Acc: 50.000\n",
      "\n",
      "Epoch 827: Validation loss decreased (0.722784 --> 0.722761).  Saving model ...\n",
      "\t Train_Loss: 249137.4081 Train_Acc: 50.167 Val_Loss: 0.7228  BEST VAL Loss: 0.7228  Val_Acc: 50.000\n",
      "\n",
      "Epoch 828: Validation loss decreased (0.722761 --> 0.722739).  Saving model ...\n",
      "\t Train_Loss: 248836.8813 Train_Acc: 50.167 Val_Loss: 0.7227  BEST VAL Loss: 0.7227  Val_Acc: 50.000\n",
      "\n",
      "Epoch 829: Validation loss decreased (0.722739 --> 0.722716).  Saving model ...\n",
      "\t Train_Loss: 248537.0787 Train_Acc: 50.167 Val_Loss: 0.7227  BEST VAL Loss: 0.7227  Val_Acc: 50.000\n",
      "\n",
      "Epoch 830: Validation loss decreased (0.722716 --> 0.722694).  Saving model ...\n",
      "\t Train_Loss: 248237.9976 Train_Acc: 50.167 Val_Loss: 0.7227  BEST VAL Loss: 0.7227  Val_Acc: 50.000\n",
      "\n",
      "Epoch 831: Validation loss decreased (0.722694 --> 0.722672).  Saving model ...\n",
      "\t Train_Loss: 247939.6355 Train_Acc: 50.167 Val_Loss: 0.7227  BEST VAL Loss: 0.7227  Val_Acc: 50.000\n",
      "\n",
      "Epoch 832: Validation loss decreased (0.722672 --> 0.722650).  Saving model ...\n",
      "\t Train_Loss: 247641.9897 Train_Acc: 50.167 Val_Loss: 0.7226  BEST VAL Loss: 0.7226  Val_Acc: 50.000\n",
      "\n",
      "Epoch 833: Validation loss decreased (0.722650 --> 0.722628).  Saving model ...\n",
      "\t Train_Loss: 247345.0577 Train_Acc: 50.167 Val_Loss: 0.7226  BEST VAL Loss: 0.7226  Val_Acc: 50.000\n",
      "\n",
      "Epoch 834: Validation loss decreased (0.722628 --> 0.722606).  Saving model ...\n",
      "\t Train_Loss: 247048.8369 Train_Acc: 50.167 Val_Loss: 0.7226  BEST VAL Loss: 0.7226  Val_Acc: 50.000\n",
      "\n",
      "Epoch 835: Validation loss decreased (0.722606 --> 0.722584).  Saving model ...\n",
      "\t Train_Loss: 246753.3248 Train_Acc: 50.167 Val_Loss: 0.7226  BEST VAL Loss: 0.7226  Val_Acc: 50.000\n",
      "\n",
      "Epoch 836: Validation loss decreased (0.722584 --> 0.722562).  Saving model ...\n",
      "\t Train_Loss: 246458.5188 Train_Acc: 50.167 Val_Loss: 0.7226  BEST VAL Loss: 0.7226  Val_Acc: 50.000\n",
      "\n",
      "Epoch 837: Validation loss decreased (0.722562 --> 0.722540).  Saving model ...\n",
      "\t Train_Loss: 246164.4164 Train_Acc: 50.167 Val_Loss: 0.7225  BEST VAL Loss: 0.7225  Val_Acc: 50.000\n",
      "\n",
      "Epoch 838: Validation loss decreased (0.722540 --> 0.722518).  Saving model ...\n",
      "\t Train_Loss: 245871.0151 Train_Acc: 50.167 Val_Loss: 0.7225  BEST VAL Loss: 0.7225  Val_Acc: 50.000\n",
      "\n",
      "Epoch 839: Validation loss decreased (0.722518 --> 0.722496).  Saving model ...\n",
      "\t Train_Loss: 245578.3123 Train_Acc: 50.167 Val_Loss: 0.7225  BEST VAL Loss: 0.7225  Val_Acc: 50.000\n",
      "\n",
      "Epoch 840: Validation loss decreased (0.722496 --> 0.722474).  Saving model ...\n",
      "\t Train_Loss: 245286.3057 Train_Acc: 50.167 Val_Loss: 0.7225  BEST VAL Loss: 0.7225  Val_Acc: 50.000\n",
      "\n",
      "Epoch 841: Validation loss decreased (0.722474 --> 0.722453).  Saving model ...\n",
      "\t Train_Loss: 244994.9926 Train_Acc: 50.167 Val_Loss: 0.7225  BEST VAL Loss: 0.7225  Val_Acc: 50.000\n",
      "\n",
      "Epoch 842: Validation loss decreased (0.722453 --> 0.722431).  Saving model ...\n",
      "\t Train_Loss: 244704.3707 Train_Acc: 50.167 Val_Loss: 0.7224  BEST VAL Loss: 0.7224  Val_Acc: 50.000\n",
      "\n",
      "Epoch 843: Validation loss decreased (0.722431 --> 0.722409).  Saving model ...\n",
      "\t Train_Loss: 244414.4374 Train_Acc: 50.167 Val_Loss: 0.7224  BEST VAL Loss: 0.7224  Val_Acc: 50.000\n",
      "\n",
      "Epoch 844: Validation loss decreased (0.722409 --> 0.722388).  Saving model ...\n",
      "\t Train_Loss: 244125.1904 Train_Acc: 50.167 Val_Loss: 0.7224  BEST VAL Loss: 0.7224  Val_Acc: 50.000\n",
      "\n",
      "Epoch 845: Validation loss decreased (0.722388 --> 0.722366).  Saving model ...\n",
      "\t Train_Loss: 243836.6272 Train_Acc: 50.167 Val_Loss: 0.7224  BEST VAL Loss: 0.7224  Val_Acc: 50.000\n",
      "\n",
      "Epoch 846: Validation loss decreased (0.722366 --> 0.722345).  Saving model ...\n",
      "\t Train_Loss: 243548.7453 Train_Acc: 50.167 Val_Loss: 0.7223  BEST VAL Loss: 0.7223  Val_Acc: 50.000\n",
      "\n",
      "Epoch 847: Validation loss decreased (0.722345 --> 0.722324).  Saving model ...\n",
      "\t Train_Loss: 243261.5424 Train_Acc: 50.167 Val_Loss: 0.7223  BEST VAL Loss: 0.7223  Val_Acc: 50.000\n",
      "\n",
      "Epoch 848: Validation loss decreased (0.722324 --> 0.722302).  Saving model ...\n",
      "\t Train_Loss: 242975.0161 Train_Acc: 50.167 Val_Loss: 0.7223  BEST VAL Loss: 0.7223  Val_Acc: 50.000\n",
      "\n",
      "Epoch 849: Validation loss decreased (0.722302 --> 0.722281).  Saving model ...\n",
      "\t Train_Loss: 242689.1640 Train_Acc: 50.167 Val_Loss: 0.7223  BEST VAL Loss: 0.7223  Val_Acc: 50.000\n",
      "\n",
      "Epoch 850: Validation loss decreased (0.722281 --> 0.722260).  Saving model ...\n",
      "\t Train_Loss: 242403.9837 Train_Acc: 50.167 Val_Loss: 0.7223  BEST VAL Loss: 0.7223  Val_Acc: 50.000\n",
      "\n",
      "Epoch 851: Validation loss decreased (0.722260 --> 0.722239).  Saving model ...\n",
      "\t Train_Loss: 242119.4728 Train_Acc: 50.167 Val_Loss: 0.7222  BEST VAL Loss: 0.7222  Val_Acc: 50.000\n",
      "\n",
      "Epoch 852: Validation loss decreased (0.722239 --> 0.722218).  Saving model ...\n",
      "\t Train_Loss: 241835.6290 Train_Acc: 50.167 Val_Loss: 0.7222  BEST VAL Loss: 0.7222  Val_Acc: 50.000\n",
      "\n",
      "Epoch 853: Validation loss decreased (0.722218 --> 0.722196).  Saving model ...\n",
      "\t Train_Loss: 241552.4499 Train_Acc: 50.167 Val_Loss: 0.7222  BEST VAL Loss: 0.7222  Val_Acc: 50.000\n",
      "\n",
      "Epoch 854: Validation loss decreased (0.722196 --> 0.722175).  Saving model ...\n",
      "\t Train_Loss: 241269.9332 Train_Acc: 50.167 Val_Loss: 0.7222  BEST VAL Loss: 0.7222  Val_Acc: 50.000\n",
      "\n",
      "Epoch 855: Validation loss decreased (0.722175 --> 0.722154).  Saving model ...\n",
      "\t Train_Loss: 240988.0767 Train_Acc: 50.167 Val_Loss: 0.7222  BEST VAL Loss: 0.7222  Val_Acc: 50.000\n",
      "\n",
      "Epoch 856: Validation loss decreased (0.722154 --> 0.722133).  Saving model ...\n",
      "\t Train_Loss: 240706.8779 Train_Acc: 50.167 Val_Loss: 0.7221  BEST VAL Loss: 0.7221  Val_Acc: 50.000\n",
      "\n",
      "Epoch 857: Validation loss decreased (0.722133 --> 0.722113).  Saving model ...\n",
      "\t Train_Loss: 240426.3345 Train_Acc: 50.167 Val_Loss: 0.7221  BEST VAL Loss: 0.7221  Val_Acc: 50.000\n",
      "\n",
      "Epoch 858: Validation loss decreased (0.722113 --> 0.722092).  Saving model ...\n",
      "\t Train_Loss: 240146.4444 Train_Acc: 50.167 Val_Loss: 0.7221  BEST VAL Loss: 0.7221  Val_Acc: 50.000\n",
      "\n",
      "Epoch 859: Validation loss decreased (0.722092 --> 0.722071).  Saving model ...\n",
      "\t Train_Loss: 239867.2052 Train_Acc: 50.167 Val_Loss: 0.7221  BEST VAL Loss: 0.7221  Val_Acc: 50.000\n",
      "\n",
      "Epoch 860: Validation loss decreased (0.722071 --> 0.722050).  Saving model ...\n",
      "\t Train_Loss: 239588.6146 Train_Acc: 50.167 Val_Loss: 0.7221  BEST VAL Loss: 0.7221  Val_Acc: 50.000\n",
      "\n",
      "Epoch 861: Validation loss decreased (0.722050 --> 0.722030).  Saving model ...\n",
      "\t Train_Loss: 239310.6703 Train_Acc: 50.167 Val_Loss: 0.7220  BEST VAL Loss: 0.7220  Val_Acc: 50.000\n",
      "\n",
      "Epoch 862: Validation loss decreased (0.722030 --> 0.722009).  Saving model ...\n",
      "\t Train_Loss: 239033.3703 Train_Acc: 50.167 Val_Loss: 0.7220  BEST VAL Loss: 0.7220  Val_Acc: 50.000\n",
      "\n",
      "Epoch 863: Validation loss decreased (0.722009 --> 0.721988).  Saving model ...\n",
      "\t Train_Loss: 238756.7121 Train_Acc: 50.167 Val_Loss: 0.7220  BEST VAL Loss: 0.7220  Val_Acc: 50.000\n",
      "\n",
      "Epoch 864: Validation loss decreased (0.721988 --> 0.721968).  Saving model ...\n",
      "\t Train_Loss: 238480.6936 Train_Acc: 50.167 Val_Loss: 0.7220  BEST VAL Loss: 0.7220  Val_Acc: 50.000\n",
      "\n",
      "Epoch 865: Validation loss decreased (0.721968 --> 0.721947).  Saving model ...\n",
      "\t Train_Loss: 238205.3125 Train_Acc: 50.167 Val_Loss: 0.7219  BEST VAL Loss: 0.7219  Val_Acc: 50.000\n",
      "\n",
      "Epoch 866: Validation loss decreased (0.721947 --> 0.721927).  Saving model ...\n",
      "\t Train_Loss: 237930.5667 Train_Acc: 50.167 Val_Loss: 0.7219  BEST VAL Loss: 0.7219  Val_Acc: 50.000\n",
      "\n",
      "Epoch 867: Validation loss decreased (0.721927 --> 0.721906).  Saving model ...\n",
      "\t Train_Loss: 237656.4540 Train_Acc: 50.167 Val_Loss: 0.7219  BEST VAL Loss: 0.7219  Val_Acc: 50.000\n",
      "\n",
      "Epoch 868: Validation loss decreased (0.721906 --> 0.721886).  Saving model ...\n",
      "\t Train_Loss: 237382.9721 Train_Acc: 50.167 Val_Loss: 0.7219  BEST VAL Loss: 0.7219  Val_Acc: 50.000\n",
      "\n",
      "Epoch 869: Validation loss decreased (0.721886 --> 0.721866).  Saving model ...\n",
      "\t Train_Loss: 237110.1189 Train_Acc: 50.167 Val_Loss: 0.7219  BEST VAL Loss: 0.7219  Val_Acc: 50.000\n",
      "\n",
      "Epoch 870: Validation loss decreased (0.721866 --> 0.721845).  Saving model ...\n",
      "\t Train_Loss: 236837.8923 Train_Acc: 50.167 Val_Loss: 0.7218  BEST VAL Loss: 0.7218  Val_Acc: 50.000\n",
      "\n",
      "Epoch 871: Validation loss decreased (0.721845 --> 0.721825).  Saving model ...\n",
      "\t Train_Loss: 236566.2900 Train_Acc: 50.167 Val_Loss: 0.7218  BEST VAL Loss: 0.7218  Val_Acc: 50.000\n",
      "\n",
      "Epoch 872: Validation loss decreased (0.721825 --> 0.721805).  Saving model ...\n",
      "\t Train_Loss: 236295.3099 Train_Acc: 50.167 Val_Loss: 0.7218  BEST VAL Loss: 0.7218  Val_Acc: 50.000\n",
      "\n",
      "Epoch 873: Validation loss decreased (0.721805 --> 0.721785).  Saving model ...\n",
      "\t Train_Loss: 236024.9500 Train_Acc: 50.167 Val_Loss: 0.7218  BEST VAL Loss: 0.7218  Val_Acc: 50.000\n",
      "\n",
      "Epoch 874: Validation loss decreased (0.721785 --> 0.721765).  Saving model ...\n",
      "\t Train_Loss: 235755.2080 Train_Acc: 50.167 Val_Loss: 0.7218  BEST VAL Loss: 0.7218  Val_Acc: 50.000\n",
      "\n",
      "Epoch 875: Validation loss decreased (0.721765 --> 0.721745).  Saving model ...\n",
      "\t Train_Loss: 235486.0818 Train_Acc: 50.167 Val_Loss: 0.7217  BEST VAL Loss: 0.7217  Val_Acc: 50.000\n",
      "\n",
      "Epoch 876: Validation loss decreased (0.721745 --> 0.721725).  Saving model ...\n",
      "\t Train_Loss: 235217.5694 Train_Acc: 50.167 Val_Loss: 0.7217  BEST VAL Loss: 0.7217  Val_Acc: 50.000\n",
      "\n",
      "Epoch 877: Validation loss decreased (0.721725 --> 0.721705).  Saving model ...\n",
      "\t Train_Loss: 234949.6687 Train_Acc: 50.167 Val_Loss: 0.7217  BEST VAL Loss: 0.7217  Val_Acc: 50.000\n",
      "\n",
      "Epoch 878: Validation loss decreased (0.721705 --> 0.721685).  Saving model ...\n",
      "\t Train_Loss: 234682.3775 Train_Acc: 50.167 Val_Loss: 0.7217  BEST VAL Loss: 0.7217  Val_Acc: 50.000\n",
      "\n",
      "Epoch 879: Validation loss decreased (0.721685 --> 0.721665).  Saving model ...\n",
      "\t Train_Loss: 234415.6937 Train_Acc: 50.167 Val_Loss: 0.7217  BEST VAL Loss: 0.7217  Val_Acc: 50.000\n",
      "\n",
      "Epoch 880: Validation loss decreased (0.721665 --> 0.721645).  Saving model ...\n",
      "\t Train_Loss: 234149.6154 Train_Acc: 50.167 Val_Loss: 0.7216  BEST VAL Loss: 0.7216  Val_Acc: 50.000\n",
      "\n",
      "Epoch 881: Validation loss decreased (0.721645 --> 0.721626).  Saving model ...\n",
      "\t Train_Loss: 233884.1405 Train_Acc: 50.167 Val_Loss: 0.7216  BEST VAL Loss: 0.7216  Val_Acc: 50.000\n",
      "\n",
      "Epoch 882: Validation loss decreased (0.721626 --> 0.721606).  Saving model ...\n",
      "\t Train_Loss: 233619.2668 Train_Acc: 50.167 Val_Loss: 0.7216  BEST VAL Loss: 0.7216  Val_Acc: 50.000\n",
      "\n",
      "Epoch 883: Validation loss decreased (0.721606 --> 0.721586).  Saving model ...\n",
      "\t Train_Loss: 233354.9924 Train_Acc: 50.167 Val_Loss: 0.7216  BEST VAL Loss: 0.7216  Val_Acc: 50.000\n",
      "\n",
      "Epoch 884: Validation loss decreased (0.721586 --> 0.721567).  Saving model ...\n",
      "\t Train_Loss: 233091.3153 Train_Acc: 50.167 Val_Loss: 0.7216  BEST VAL Loss: 0.7216  Val_Acc: 50.000\n",
      "\n",
      "Epoch 885: Validation loss decreased (0.721567 --> 0.721547).  Saving model ...\n",
      "\t Train_Loss: 232828.2333 Train_Acc: 50.167 Val_Loss: 0.7215  BEST VAL Loss: 0.7215  Val_Acc: 50.000\n",
      "\n",
      "Epoch 886: Validation loss decreased (0.721547 --> 0.721527).  Saving model ...\n",
      "\t Train_Loss: 232565.7446 Train_Acc: 50.167 Val_Loss: 0.7215  BEST VAL Loss: 0.7215  Val_Acc: 50.000\n",
      "\n",
      "Epoch 887: Validation loss decreased (0.721527 --> 0.721508).  Saving model ...\n",
      "\t Train_Loss: 232303.8470 Train_Acc: 50.167 Val_Loss: 0.7215  BEST VAL Loss: 0.7215  Val_Acc: 50.000\n",
      "\n",
      "Epoch 888: Validation loss decreased (0.721508 --> 0.721488).  Saving model ...\n",
      "\t Train_Loss: 232042.5386 Train_Acc: 50.167 Val_Loss: 0.7215  BEST VAL Loss: 0.7215  Val_Acc: 50.000\n",
      "\n",
      "Epoch 889: Validation loss decreased (0.721488 --> 0.721469).  Saving model ...\n",
      "\t Train_Loss: 231781.8175 Train_Acc: 50.167 Val_Loss: 0.7215  BEST VAL Loss: 0.7215  Val_Acc: 50.000\n",
      "\n",
      "Epoch 890: Validation loss decreased (0.721469 --> 0.721450).  Saving model ...\n",
      "\t Train_Loss: 231521.6815 Train_Acc: 50.167 Val_Loss: 0.7214  BEST VAL Loss: 0.7214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 891: Validation loss decreased (0.721450 --> 0.721430).  Saving model ...\n",
      "\t Train_Loss: 231262.1289 Train_Acc: 50.167 Val_Loss: 0.7214  BEST VAL Loss: 0.7214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 892: Validation loss decreased (0.721430 --> 0.721411).  Saving model ...\n",
      "\t Train_Loss: 231003.1575 Train_Acc: 50.167 Val_Loss: 0.7214  BEST VAL Loss: 0.7214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 893: Validation loss decreased (0.721411 --> 0.721392).  Saving model ...\n",
      "\t Train_Loss: 230744.7655 Train_Acc: 50.167 Val_Loss: 0.7214  BEST VAL Loss: 0.7214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 894: Validation loss decreased (0.721392 --> 0.721373).  Saving model ...\n",
      "\t Train_Loss: 230486.9509 Train_Acc: 50.167 Val_Loss: 0.7214  BEST VAL Loss: 0.7214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 895: Validation loss decreased (0.721373 --> 0.721354).  Saving model ...\n",
      "\t Train_Loss: 230229.7118 Train_Acc: 50.167 Val_Loss: 0.7214  BEST VAL Loss: 0.7214  Val_Acc: 50.000\n",
      "\n",
      "Epoch 896: Validation loss decreased (0.721354 --> 0.721334).  Saving model ...\n",
      "\t Train_Loss: 229973.0462 Train_Acc: 50.167 Val_Loss: 0.7213  BEST VAL Loss: 0.7213  Val_Acc: 50.000\n",
      "\n",
      "Epoch 897: Validation loss decreased (0.721334 --> 0.721315).  Saving model ...\n",
      "\t Train_Loss: 229716.9523 Train_Acc: 50.167 Val_Loss: 0.7213  BEST VAL Loss: 0.7213  Val_Acc: 50.000\n",
      "\n",
      "Epoch 898: Validation loss decreased (0.721315 --> 0.721296).  Saving model ...\n",
      "\t Train_Loss: 229461.4281 Train_Acc: 50.167 Val_Loss: 0.7213  BEST VAL Loss: 0.7213  Val_Acc: 50.000\n",
      "\n",
      "Epoch 899: Validation loss decreased (0.721296 --> 0.721277).  Saving model ...\n",
      "\t Train_Loss: 229206.4717 Train_Acc: 50.167 Val_Loss: 0.7213  BEST VAL Loss: 0.7213  Val_Acc: 50.000\n",
      "\n",
      "Epoch 900: Validation loss decreased (0.721277 --> 0.721258).  Saving model ...\n",
      "\t Train_Loss: 228952.0813 Train_Acc: 50.167 Val_Loss: 0.7213  BEST VAL Loss: 0.7213  Val_Acc: 50.000\n",
      "\n",
      "Epoch 901: Validation loss decreased (0.721258 --> 0.721239).  Saving model ...\n",
      "\t Train_Loss: 228698.2550 Train_Acc: 50.167 Val_Loss: 0.7212  BEST VAL Loss: 0.7212  Val_Acc: 50.000\n",
      "\n",
      "Epoch 902: Validation loss decreased (0.721239 --> 0.721221).  Saving model ...\n",
      "\t Train_Loss: 228444.9908 Train_Acc: 50.167 Val_Loss: 0.7212  BEST VAL Loss: 0.7212  Val_Acc: 50.000\n",
      "\n",
      "Epoch 903: Validation loss decreased (0.721221 --> 0.721202).  Saving model ...\n",
      "\t Train_Loss: 228192.2869 Train_Acc: 50.167 Val_Loss: 0.7212  BEST VAL Loss: 0.7212  Val_Acc: 50.000\n",
      "\n",
      "Epoch 904: Validation loss decreased (0.721202 --> 0.721183).  Saving model ...\n",
      "\t Train_Loss: 227940.1415 Train_Acc: 50.167 Val_Loss: 0.7212  BEST VAL Loss: 0.7212  Val_Acc: 50.000\n",
      "\n",
      "Epoch 905: Validation loss decreased (0.721183 --> 0.721164).  Saving model ...\n",
      "\t Train_Loss: 227688.5527 Train_Acc: 50.167 Val_Loss: 0.7212  BEST VAL Loss: 0.7212  Val_Acc: 50.000\n",
      "\n",
      "Epoch 906: Validation loss decreased (0.721164 --> 0.721146).  Saving model ...\n",
      "\t Train_Loss: 227437.5187 Train_Acc: 50.167 Val_Loss: 0.7211  BEST VAL Loss: 0.7211  Val_Acc: 50.000\n",
      "\n",
      "Epoch 907: Validation loss decreased (0.721146 --> 0.721127).  Saving model ...\n",
      "\t Train_Loss: 227187.0377 Train_Acc: 50.167 Val_Loss: 0.7211  BEST VAL Loss: 0.7211  Val_Acc: 50.000\n",
      "\n",
      "Epoch 908: Validation loss decreased (0.721127 --> 0.721108).  Saving model ...\n",
      "\t Train_Loss: 226937.1077 Train_Acc: 50.167 Val_Loss: 0.7211  BEST VAL Loss: 0.7211  Val_Acc: 50.000\n",
      "\n",
      "Epoch 909: Validation loss decreased (0.721108 --> 0.721090).  Saving model ...\n",
      "\t Train_Loss: 226687.7270 Train_Acc: 50.167 Val_Loss: 0.7211  BEST VAL Loss: 0.7211  Val_Acc: 50.000\n",
      "\n",
      "Epoch 910: Validation loss decreased (0.721090 --> 0.721071).  Saving model ...\n",
      "\t Train_Loss: 226438.8939 Train_Acc: 50.167 Val_Loss: 0.7211  BEST VAL Loss: 0.7211  Val_Acc: 50.000\n",
      "\n",
      "Epoch 911: Validation loss decreased (0.721071 --> 0.721053).  Saving model ...\n",
      "\t Train_Loss: 226190.6064 Train_Acc: 50.167 Val_Loss: 0.7211  BEST VAL Loss: 0.7211  Val_Acc: 50.000\n",
      "\n",
      "Epoch 912: Validation loss decreased (0.721053 --> 0.721034).  Saving model ...\n",
      "\t Train_Loss: 225942.8628 Train_Acc: 50.167 Val_Loss: 0.7210  BEST VAL Loss: 0.7210  Val_Acc: 50.000\n",
      "\n",
      "Epoch 913: Validation loss decreased (0.721034 --> 0.721016).  Saving model ...\n",
      "\t Train_Loss: 225695.6613 Train_Acc: 50.167 Val_Loss: 0.7210  BEST VAL Loss: 0.7210  Val_Acc: 50.000\n",
      "\n",
      "Epoch 914: Validation loss decreased (0.721016 --> 0.720998).  Saving model ...\n",
      "\t Train_Loss: 225449.0001 Train_Acc: 50.167 Val_Loss: 0.7210  BEST VAL Loss: 0.7210  Val_Acc: 50.000\n",
      "\n",
      "Epoch 915: Validation loss decreased (0.720998 --> 0.720979).  Saving model ...\n",
      "\t Train_Loss: 225202.8775 Train_Acc: 50.167 Val_Loss: 0.7210  BEST VAL Loss: 0.7210  Val_Acc: 50.000\n",
      "\n",
      "Epoch 916: Validation loss decreased (0.720979 --> 0.720961).  Saving model ...\n",
      "\t Train_Loss: 224957.2917 Train_Acc: 50.167 Val_Loss: 0.7210  BEST VAL Loss: 0.7210  Val_Acc: 50.000\n",
      "\n",
      "Epoch 917: Validation loss decreased (0.720961 --> 0.720943).  Saving model ...\n",
      "\t Train_Loss: 224712.2410 Train_Acc: 50.167 Val_Loss: 0.7209  BEST VAL Loss: 0.7209  Val_Acc: 50.000\n",
      "\n",
      "Epoch 918: Validation loss decreased (0.720943 --> 0.720925).  Saving model ...\n",
      "\t Train_Loss: 224467.7235 Train_Acc: 50.167 Val_Loss: 0.7209  BEST VAL Loss: 0.7209  Val_Acc: 50.000\n",
      "\n",
      "Epoch 919: Validation loss decreased (0.720925 --> 0.720906).  Saving model ...\n",
      "\t Train_Loss: 224223.7376 Train_Acc: 50.167 Val_Loss: 0.7209  BEST VAL Loss: 0.7209  Val_Acc: 50.000\n",
      "\n",
      "Epoch 920: Validation loss decreased (0.720906 --> 0.720888).  Saving model ...\n",
      "\t Train_Loss: 223980.2816 Train_Acc: 50.167 Val_Loss: 0.7209  BEST VAL Loss: 0.7209  Val_Acc: 50.000\n",
      "\n",
      "Epoch 921: Validation loss decreased (0.720888 --> 0.720870).  Saving model ...\n",
      "\t Train_Loss: 223737.3536 Train_Acc: 50.167 Val_Loss: 0.7209  BEST VAL Loss: 0.7209  Val_Acc: 50.000\n",
      "\n",
      "Epoch 922: Validation loss decreased (0.720870 --> 0.720852).  Saving model ...\n",
      "\t Train_Loss: 223494.9520 Train_Acc: 50.167 Val_Loss: 0.7209  BEST VAL Loss: 0.7209  Val_Acc: 50.000\n",
      "\n",
      "Epoch 923: Validation loss decreased (0.720852 --> 0.720834).  Saving model ...\n",
      "\t Train_Loss: 223253.0752 Train_Acc: 50.167 Val_Loss: 0.7208  BEST VAL Loss: 0.7208  Val_Acc: 50.000\n",
      "\n",
      "Epoch 924: Validation loss decreased (0.720834 --> 0.720816).  Saving model ...\n",
      "\t Train_Loss: 223011.7212 Train_Acc: 50.167 Val_Loss: 0.7208  BEST VAL Loss: 0.7208  Val_Acc: 50.000\n",
      "\n",
      "Epoch 925: Validation loss decreased (0.720816 --> 0.720798).  Saving model ...\n",
      "\t Train_Loss: 222770.8886 Train_Acc: 50.167 Val_Loss: 0.7208  BEST VAL Loss: 0.7208  Val_Acc: 50.000\n",
      "\n",
      "Epoch 926: Validation loss decreased (0.720798 --> 0.720780).  Saving model ...\n",
      "\t Train_Loss: 222530.5756 Train_Acc: 50.167 Val_Loss: 0.7208  BEST VAL Loss: 0.7208  Val_Acc: 50.000\n",
      "\n",
      "Epoch 927: Validation loss decreased (0.720780 --> 0.720762).  Saving model ...\n",
      "\t Train_Loss: 222290.7804 Train_Acc: 50.167 Val_Loss: 0.7208  BEST VAL Loss: 0.7208  Val_Acc: 50.000\n",
      "\n",
      "Epoch 928: Validation loss decreased (0.720762 --> 0.720745).  Saving model ...\n",
      "\t Train_Loss: 222051.5016 Train_Acc: 50.167 Val_Loss: 0.7207  BEST VAL Loss: 0.7207  Val_Acc: 50.000\n",
      "\n",
      "Epoch 929: Validation loss decreased (0.720745 --> 0.720727).  Saving model ...\n",
      "\t Train_Loss: 221812.7373 Train_Acc: 50.167 Val_Loss: 0.7207  BEST VAL Loss: 0.7207  Val_Acc: 50.000\n",
      "\n",
      "Epoch 930: Validation loss decreased (0.720727 --> 0.720709).  Saving model ...\n",
      "\t Train_Loss: 221574.4859 Train_Acc: 50.167 Val_Loss: 0.7207  BEST VAL Loss: 0.7207  Val_Acc: 50.000\n",
      "\n",
      "Epoch 931: Validation loss decreased (0.720709 --> 0.720691).  Saving model ...\n",
      "\t Train_Loss: 221336.7458 Train_Acc: 50.167 Val_Loss: 0.7207  BEST VAL Loss: 0.7207  Val_Acc: 50.000\n",
      "\n",
      "Epoch 932: Validation loss decreased (0.720691 --> 0.720674).  Saving model ...\n",
      "\t Train_Loss: 221099.5153 Train_Acc: 50.167 Val_Loss: 0.7207  BEST VAL Loss: 0.7207  Val_Acc: 50.000\n",
      "\n",
      "Epoch 933: Validation loss decreased (0.720674 --> 0.720656).  Saving model ...\n",
      "\t Train_Loss: 220862.7928 Train_Acc: 50.167 Val_Loss: 0.7207  BEST VAL Loss: 0.7207  Val_Acc: 50.000\n",
      "\n",
      "Epoch 934: Validation loss decreased (0.720656 --> 0.720639).  Saving model ...\n",
      "\t Train_Loss: 220626.5767 Train_Acc: 50.167 Val_Loss: 0.7206  BEST VAL Loss: 0.7206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 935: Validation loss decreased (0.720639 --> 0.720621).  Saving model ...\n",
      "\t Train_Loss: 220390.8653 Train_Acc: 50.167 Val_Loss: 0.7206  BEST VAL Loss: 0.7206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 936: Validation loss decreased (0.720621 --> 0.720604).  Saving model ...\n",
      "\t Train_Loss: 220155.6570 Train_Acc: 50.167 Val_Loss: 0.7206  BEST VAL Loss: 0.7206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 937: Validation loss decreased (0.720604 --> 0.720586).  Saving model ...\n",
      "\t Train_Loss: 219920.9502 Train_Acc: 50.167 Val_Loss: 0.7206  BEST VAL Loss: 0.7206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 938: Validation loss decreased (0.720586 --> 0.720569).  Saving model ...\n",
      "\t Train_Loss: 219686.7433 Train_Acc: 50.167 Val_Loss: 0.7206  BEST VAL Loss: 0.7206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 939: Validation loss decreased (0.720569 --> 0.720551).  Saving model ...\n",
      "\t Train_Loss: 219453.0348 Train_Acc: 50.167 Val_Loss: 0.7206  BEST VAL Loss: 0.7206  Val_Acc: 50.000\n",
      "\n",
      "Epoch 940: Validation loss decreased (0.720551 --> 0.720534).  Saving model ...\n",
      "\t Train_Loss: 219219.8230 Train_Acc: 50.167 Val_Loss: 0.7205  BEST VAL Loss: 0.7205  Val_Acc: 50.000\n",
      "\n",
      "Epoch 941: Validation loss decreased (0.720534 --> 0.720517).  Saving model ...\n",
      "\t Train_Loss: 218987.1063 Train_Acc: 50.167 Val_Loss: 0.7205  BEST VAL Loss: 0.7205  Val_Acc: 50.000\n",
      "\n",
      "Epoch 942: Validation loss decreased (0.720517 --> 0.720499).  Saving model ...\n",
      "\t Train_Loss: 218754.8831 Train_Acc: 50.167 Val_Loss: 0.7205  BEST VAL Loss: 0.7205  Val_Acc: 50.000\n",
      "\n",
      "Epoch 943: Validation loss decreased (0.720499 --> 0.720482).  Saving model ...\n",
      "\t Train_Loss: 218523.1520 Train_Acc: 50.167 Val_Loss: 0.7205  BEST VAL Loss: 0.7205  Val_Acc: 50.000\n",
      "\n",
      "Epoch 944: Validation loss decreased (0.720482 --> 0.720465).  Saving model ...\n",
      "\t Train_Loss: 218291.9113 Train_Acc: 50.167 Val_Loss: 0.7205  BEST VAL Loss: 0.7205  Val_Acc: 50.000\n",
      "\n",
      "Epoch 945: Validation loss decreased (0.720465 --> 0.720448).  Saving model ...\n",
      "\t Train_Loss: 218061.1595 Train_Acc: 50.167 Val_Loss: 0.7204  BEST VAL Loss: 0.7204  Val_Acc: 50.000\n",
      "\n",
      "Epoch 946: Validation loss decreased (0.720448 --> 0.720430).  Saving model ...\n",
      "\t Train_Loss: 217830.8951 Train_Acc: 50.167 Val_Loss: 0.7204  BEST VAL Loss: 0.7204  Val_Acc: 50.000\n",
      "\n",
      "Epoch 947: Validation loss decreased (0.720430 --> 0.720413).  Saving model ...\n",
      "\t Train_Loss: 217601.1164 Train_Acc: 50.167 Val_Loss: 0.7204  BEST VAL Loss: 0.7204  Val_Acc: 50.000\n",
      "\n",
      "Epoch 948: Validation loss decreased (0.720413 --> 0.720396).  Saving model ...\n",
      "\t Train_Loss: 217371.8219 Train_Acc: 50.167 Val_Loss: 0.7204  BEST VAL Loss: 0.7204  Val_Acc: 50.000\n",
      "\n",
      "Epoch 949: Validation loss decreased (0.720396 --> 0.720379).  Saving model ...\n",
      "\t Train_Loss: 217143.0102 Train_Acc: 50.167 Val_Loss: 0.7204  BEST VAL Loss: 0.7204  Val_Acc: 50.000\n",
      "\n",
      "Epoch 950: Validation loss decreased (0.720379 --> 0.720362).  Saving model ...\n",
      "\t Train_Loss: 216914.6797 Train_Acc: 50.167 Val_Loss: 0.7204  BEST VAL Loss: 0.7204  Val_Acc: 50.000\n",
      "\n",
      "Epoch 951: Validation loss decreased (0.720362 --> 0.720345).  Saving model ...\n",
      "\t Train_Loss: 216686.8289 Train_Acc: 50.167 Val_Loss: 0.7203  BEST VAL Loss: 0.7203  Val_Acc: 50.000\n",
      "\n",
      "Epoch 952: Validation loss decreased (0.720345 --> 0.720328).  Saving model ...\n",
      "\t Train_Loss: 216459.4563 Train_Acc: 50.167 Val_Loss: 0.7203  BEST VAL Loss: 0.7203  Val_Acc: 50.000\n",
      "\n",
      "Epoch 953: Validation loss decreased (0.720328 --> 0.720311).  Saving model ...\n",
      "\t Train_Loss: 216232.5603 Train_Acc: 50.167 Val_Loss: 0.7203  BEST VAL Loss: 0.7203  Val_Acc: 50.000\n",
      "\n",
      "Epoch 954: Validation loss decreased (0.720311 --> 0.720295).  Saving model ...\n",
      "\t Train_Loss: 216006.1395 Train_Acc: 50.167 Val_Loss: 0.7203  BEST VAL Loss: 0.7203  Val_Acc: 50.000\n",
      "\n",
      "Epoch 955: Validation loss decreased (0.720295 --> 0.720278).  Saving model ...\n",
      "\t Train_Loss: 215780.1924 Train_Acc: 50.167 Val_Loss: 0.7203  BEST VAL Loss: 0.7203  Val_Acc: 50.000\n",
      "\n",
      "Epoch 956: Validation loss decreased (0.720278 --> 0.720261).  Saving model ...\n",
      "\t Train_Loss: 215554.7175 Train_Acc: 50.167 Val_Loss: 0.7203  BEST VAL Loss: 0.7203  Val_Acc: 50.000\n",
      "\n",
      "Epoch 957: Validation loss decreased (0.720261 --> 0.720244).  Saving model ...\n",
      "\t Train_Loss: 215329.7133 Train_Acc: 50.167 Val_Loss: 0.7202  BEST VAL Loss: 0.7202  Val_Acc: 50.000\n",
      "\n",
      "Epoch 958: Validation loss decreased (0.720244 --> 0.720228).  Saving model ...\n",
      "\t Train_Loss: 215105.1784 Train_Acc: 50.167 Val_Loss: 0.7202  BEST VAL Loss: 0.7202  Val_Acc: 50.000\n",
      "\n",
      "Epoch 959: Validation loss decreased (0.720228 --> 0.720211).  Saving model ...\n",
      "\t Train_Loss: 214881.1112 Train_Acc: 50.167 Val_Loss: 0.7202  BEST VAL Loss: 0.7202  Val_Acc: 50.000\n",
      "\n",
      "Epoch 960: Validation loss decreased (0.720211 --> 0.720194).  Saving model ...\n",
      "\t Train_Loss: 214657.5104 Train_Acc: 50.167 Val_Loss: 0.7202  BEST VAL Loss: 0.7202  Val_Acc: 50.000\n",
      "\n",
      "Epoch 961: Validation loss decreased (0.720194 --> 0.720178).  Saving model ...\n",
      "\t Train_Loss: 214434.3744 Train_Acc: 50.167 Val_Loss: 0.7202  BEST VAL Loss: 0.7202  Val_Acc: 50.000\n",
      "\n",
      "Epoch 962: Validation loss decreased (0.720178 --> 0.720161).  Saving model ...\n",
      "\t Train_Loss: 214211.7018 Train_Acc: 50.167 Val_Loss: 0.7202  BEST VAL Loss: 0.7202  Val_Acc: 50.000\n",
      "\n",
      "Epoch 963: Validation loss decreased (0.720161 --> 0.720145).  Saving model ...\n",
      "\t Train_Loss: 213989.4913 Train_Acc: 50.167 Val_Loss: 0.7201  BEST VAL Loss: 0.7201  Val_Acc: 50.000\n",
      "\n",
      "Epoch 964: Validation loss decreased (0.720145 --> 0.720128).  Saving model ...\n",
      "\t Train_Loss: 213767.7412 Train_Acc: 50.167 Val_Loss: 0.7201  BEST VAL Loss: 0.7201  Val_Acc: 50.000\n",
      "\n",
      "Epoch 965: Validation loss decreased (0.720128 --> 0.720112).  Saving model ...\n",
      "\t Train_Loss: 213546.4503 Train_Acc: 50.167 Val_Loss: 0.7201  BEST VAL Loss: 0.7201  Val_Acc: 50.000\n",
      "\n",
      "Epoch 966: Validation loss decreased (0.720112 --> 0.720095).  Saving model ...\n",
      "\t Train_Loss: 213325.6170 Train_Acc: 50.167 Val_Loss: 0.7201  BEST VAL Loss: 0.7201  Val_Acc: 50.000\n",
      "\n",
      "Epoch 967: Validation loss decreased (0.720095 --> 0.720079).  Saving model ...\n",
      "\t Train_Loss: 213105.2401 Train_Acc: 50.167 Val_Loss: 0.7201  BEST VAL Loss: 0.7201  Val_Acc: 50.000\n",
      "\n",
      "Epoch 968: Validation loss decreased (0.720079 --> 0.720062).  Saving model ...\n",
      "\t Train_Loss: 212885.3179 Train_Acc: 50.167 Val_Loss: 0.7201  BEST VAL Loss: 0.7201  Val_Acc: 50.000\n",
      "\n",
      "Epoch 969: Validation loss decreased (0.720062 --> 0.720046).  Saving model ...\n",
      "\t Train_Loss: 212665.8493 Train_Acc: 50.167 Val_Loss: 0.7200  BEST VAL Loss: 0.7200  Val_Acc: 50.000\n",
      "\n",
      "Epoch 970: Validation loss decreased (0.720046 --> 0.720030).  Saving model ...\n",
      "\t Train_Loss: 212446.8326 Train_Acc: 50.167 Val_Loss: 0.7200  BEST VAL Loss: 0.7200  Val_Acc: 50.000\n",
      "\n",
      "Epoch 971: Validation loss decreased (0.720030 --> 0.720013).  Saving model ...\n",
      "\t Train_Loss: 212228.2667 Train_Acc: 50.167 Val_Loss: 0.7200  BEST VAL Loss: 0.7200  Val_Acc: 50.000\n",
      "\n",
      "Epoch 972: Validation loss decreased (0.720013 --> 0.719997).  Saving model ...\n",
      "\t Train_Loss: 212010.1500 Train_Acc: 50.167 Val_Loss: 0.7200  BEST VAL Loss: 0.7200  Val_Acc: 50.000\n",
      "\n",
      "Epoch 973: Validation loss decreased (0.719997 --> 0.719981).  Saving model ...\n",
      "\t Train_Loss: 211792.4811 Train_Acc: 50.167 Val_Loss: 0.7200  BEST VAL Loss: 0.7200  Val_Acc: 50.000\n",
      "\n",
      "Epoch 974: Validation loss decreased (0.719981 --> 0.719965).  Saving model ...\n",
      "\t Train_Loss: 211575.2588 Train_Acc: 50.167 Val_Loss: 0.7200  BEST VAL Loss: 0.7200  Val_Acc: 50.000\n",
      "\n",
      "Epoch 975: Validation loss decreased (0.719965 --> 0.719949).  Saving model ...\n",
      "\t Train_Loss: 211358.4816 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 976: Validation loss decreased (0.719949 --> 0.719933).  Saving model ...\n",
      "\t Train_Loss: 211142.1481 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 977: Validation loss decreased (0.719933 --> 0.719916).  Saving model ...\n",
      "\t Train_Loss: 210926.2571 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 978: Validation loss decreased (0.719916 --> 0.719900).  Saving model ...\n",
      "\t Train_Loss: 210710.8071 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 979: Validation loss decreased (0.719900 --> 0.719884).  Saving model ...\n",
      "\t Train_Loss: 210495.7968 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 980: Validation loss decreased (0.719884 --> 0.719868).  Saving model ...\n",
      "\t Train_Loss: 210281.2248 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 981: Validation loss decreased (0.719868 --> 0.719853).  Saving model ...\n",
      "\t Train_Loss: 210067.0899 Train_Acc: 50.167 Val_Loss: 0.7199  BEST VAL Loss: 0.7199  Val_Acc: 50.000\n",
      "\n",
      "Epoch 982: Validation loss decreased (0.719853 --> 0.719837).  Saving model ...\n",
      "\t Train_Loss: 209853.3906 Train_Acc: 50.167 Val_Loss: 0.7198  BEST VAL Loss: 0.7198  Val_Acc: 50.000\n",
      "\n",
      "Epoch 983: Validation loss decreased (0.719837 --> 0.719821).  Saving model ...\n",
      "\t Train_Loss: 209640.1257 Train_Acc: 50.167 Val_Loss: 0.7198  BEST VAL Loss: 0.7198  Val_Acc: 50.000\n",
      "\n",
      "Epoch 984: Validation loss decreased (0.719821 --> 0.719805).  Saving model ...\n",
      "\t Train_Loss: 209427.2938 Train_Acc: 50.167 Val_Loss: 0.7198  BEST VAL Loss: 0.7198  Val_Acc: 50.000\n",
      "\n",
      "Epoch 985: Validation loss decreased (0.719805 --> 0.719789).  Saving model ...\n",
      "\t Train_Loss: 209214.8936 Train_Acc: 50.167 Val_Loss: 0.7198  BEST VAL Loss: 0.7198  Val_Acc: 50.000\n",
      "\n",
      "Epoch 986: Validation loss decreased (0.719789 --> 0.719773).  Saving model ...\n",
      "\t Train_Loss: 209002.9238 Train_Acc: 50.167 Val_Loss: 0.7198  BEST VAL Loss: 0.7198  Val_Acc: 50.000\n",
      "\n",
      "Epoch 987: Validation loss decreased (0.719773 --> 0.719758).  Saving model ...\n",
      "\t Train_Loss: 208791.3831 Train_Acc: 50.167 Val_Loss: 0.7198  BEST VAL Loss: 0.7198  Val_Acc: 50.000\n",
      "\n",
      "Epoch 988: Validation loss decreased (0.719758 --> 0.719742).  Saving model ...\n",
      "\t Train_Loss: 208580.2701 Train_Acc: 50.167 Val_Loss: 0.7197  BEST VAL Loss: 0.7197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 989: Validation loss decreased (0.719742 --> 0.719726).  Saving model ...\n",
      "\t Train_Loss: 208369.5837 Train_Acc: 50.167 Val_Loss: 0.7197  BEST VAL Loss: 0.7197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 990: Validation loss decreased (0.719726 --> 0.719710).  Saving model ...\n",
      "\t Train_Loss: 208159.3225 Train_Acc: 50.167 Val_Loss: 0.7197  BEST VAL Loss: 0.7197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 991: Validation loss decreased (0.719710 --> 0.719695).  Saving model ...\n",
      "\t Train_Loss: 207949.4851 Train_Acc: 50.167 Val_Loss: 0.7197  BEST VAL Loss: 0.7197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 992: Validation loss decreased (0.719695 --> 0.719679).  Saving model ...\n",
      "\t Train_Loss: 207740.0705 Train_Acc: 50.167 Val_Loss: 0.7197  BEST VAL Loss: 0.7197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 993: Validation loss decreased (0.719679 --> 0.719664).  Saving model ...\n",
      "\t Train_Loss: 207531.0771 Train_Acc: 50.167 Val_Loss: 0.7197  BEST VAL Loss: 0.7197  Val_Acc: 50.000\n",
      "\n",
      "Epoch 994: Validation loss decreased (0.719664 --> 0.719648).  Saving model ...\n",
      "\t Train_Loss: 207322.5039 Train_Acc: 50.167 Val_Loss: 0.7196  BEST VAL Loss: 0.7196  Val_Acc: 50.000\n",
      "\n",
      "Epoch 995: Validation loss decreased (0.719648 --> 0.719633).  Saving model ...\n",
      "\t Train_Loss: 207114.3495 Train_Acc: 50.167 Val_Loss: 0.7196  BEST VAL Loss: 0.7196  Val_Acc: 50.000\n",
      "\n",
      "Epoch 996: Validation loss decreased (0.719633 --> 0.719617).  Saving model ...\n",
      "\t Train_Loss: 206906.6126 Train_Acc: 50.167 Val_Loss: 0.7196  BEST VAL Loss: 0.7196  Val_Acc: 50.000\n",
      "\n",
      "Epoch 997: Validation loss decreased (0.719617 --> 0.719602).  Saving model ...\n",
      "\t Train_Loss: 206699.2921 Train_Acc: 50.167 Val_Loss: 0.7196  BEST VAL Loss: 0.7196  Val_Acc: 50.000\n",
      "\n",
      "Epoch 998: Validation loss decreased (0.719602 --> 0.719586).  Saving model ...\n",
      "\t Train_Loss: 206492.3866 Train_Acc: 50.167 Val_Loss: 0.7196  BEST VAL Loss: 0.7196  Val_Acc: 50.000\n",
      "\n",
      "Epoch 999: Validation loss decreased (0.719586 --> 0.719571).  Saving model ...\n",
      "\t Train_Loss: 206285.8949 Train_Acc: 50.167 Val_Loss: 0.7196  BEST VAL Loss: 0.7196  Val_Acc: 50.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call the optimized training model\n",
    "train_loss, train_acc, valid_loss, valid_acc, epochs_ran, model = train_optimized_model(\n",
    "    params.TRAIN_EPOCHS,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    IN_FEATURES,\n",
    "    OUT_FEATURES,\n",
    "    param_dict,\n",
    "    params,\n",
    ")\n",
    "# create a DataFrame of each stat\n",
    "training_stats = pd.DataFrame(\n",
    "    zip(train_loss, train_acc, valid_loss, valid_acc, epochs_ran),\n",
    "    columns=[\"train_loss\", \"train_acc\", \"valid_loss\", \"valid_acc\", \"epochs_ran\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0f85a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric_vs_epoch(training_stats, \"epochs_ran\", \"train_acc\", \"valid_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f1d90e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric_vs_epoch(training_stats, \"epochs_ran\", \"train_loss\", \"valid_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "103165b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# calling the testing function and outputing list values of tested model\n",
    "y_pred_list, y_pred_prob_list = test_optimized_model(\n",
    "    model, test_loader, IN_FEATURES, OUT_FEATURES, param_dict, params\n",
    ")\n",
    "\n",
    "# un-nest list if nested i.e. length of input data does not match length of output data\n",
    "if len(y_pred_list) != len(Y_test):\n",
    "    y_pred_list = un_nest(y_pred_list)\n",
    "    y_pred_prob_list = un_nest(y_pred_prob_list)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d25467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        75\n",
      "           1       0.50      1.00      0.67        75\n",
      "\n",
      "    accuracy                           0.50       150\n",
      "   macro avg       0.25      0.50      0.33       150\n",
      "weighted avg       0.25      0.50      0.33       150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lippincm/miniconda3/envs/Interstellar/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/home/lippincm/miniconda3/envs/Interstellar/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/home/lippincm/miniconda3/envs/Interstellar/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call visualization function\n",
    "results_output(y_pred_list, y_pred_prob_list, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354e47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8b21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ae02083a9ca7d143c492d1aec380c7bf553ec51bd66e90e72bba65228121b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
