{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "import pathlib\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# plot the predictions\n",
    "import seaborn as sns\n",
    "import toml\n",
    "from joblib import dump\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegression, MultiTaskElasticNetCV\n",
    "\n",
    "# import mse\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# import RepeatedKFold\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    LeaveOneOut,\n",
    "    RepeatedKFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.utils import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cell_type = \"PBMC\"\n",
    "aggregation = True\n",
    "nomic = True\n",
    "flag = True\n",
    "control = \"DMSO_0.100_DMSO_0.025\"\n",
    "treatment = \"LPS_100.000_DMSO_0.025\"\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set shuffle value\n",
    "if shuffle:\n",
    "    shuffle = \"shuffled_baseline\"\n",
    "else:\n",
    "    shuffle = \"final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"regression\"\n",
    "if flag == False:\n",
    "    # read in toml file and get parameters\n",
    "    toml_path = pathlib.Path(\"single_class_config.toml\")\n",
    "    with open(toml_path, \"r\") as f:\n",
    "        config = toml.load(f)\n",
    "    control = config[\"logistic_regression_params\"][\"control\"]\n",
    "    treatment = config[\"logistic_regression_params\"][\"treatments\"]\n",
    "    aggregation = ast.literal_eval(config[\"logistic_regression_params\"][\"aggregation\"])\n",
    "    nomic = ast.literal_eval(config[\"logistic_regression_params\"][\"nomic\"])\n",
    "    cell_type = config[\"logistic_regression_params\"][\"cell_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data from indexes and features dataframe\n",
    "# data_split_path = pathlib.Path(f\"../0.split_data/indexes/data_split_indexes.tsv\")\n",
    "# data_path = pathlib.Path(f\"../../data/{cell_type}_preprocessed_sc_norm.parquet\")\n",
    "data_path = pathlib.Path(\n",
    "    \"../../data/PBMC_subset_sc_norm_DMSO_0.100_DMSO_0.025_LPS_100.000_DMSO_0.025.parquet\"\n",
    ")\n",
    "\n",
    "# dataframe with only the labeled data we want (exclude certain phenotypic classes)\n",
    "data_df = pq.read_table(data_path).to_pandas()\n",
    "\n",
    "# import nomic data\n",
    "nomic_df_path = pathlib.Path(\n",
    "    f\"../../2.Nomic_nELISA_Analysis/Data/clean/Plate2/nELISA_plate_430420_{cell_type}_cleanup4correlation.csv\"\n",
    ")\n",
    "df_nomic = pd.read_csv(nomic_df_path)\n",
    "\n",
    "# clean up nomic data\n",
    "df_nomic = df_nomic.drop(columns=[col for col in df_nomic.columns if \"[pgML]\" in col])\n",
    "# drop first 25 columns (Metadata that is not needed)\n",
    "# df_nomic = df_nomic.drop(columns=df_nomic.columns[3:25])\n",
    "# df_nomic = df_nomic.drop(columns=df_nomic.columns[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (aggregation == True) and (nomic == True):\n",
    "    data_split_path = pathlib.Path(\n",
    "        f\"../0.split_data/indexes/{cell_type}/{MODEL_TYPE}/{control}_{treatment}/aggregated_sc_and_nomic_data_split_indexes.tsv\"\n",
    "    )\n",
    "    data_split_indexes = pd.read_csv(data_split_path, sep=\"\\t\", index_col=0)\n",
    "    # subset each column that contains metadata\n",
    "    metadata = data_df.filter(regex=\"Metadata\")\n",
    "    data_df = data_df.drop(metadata.columns, axis=1)\n",
    "    data_df = pd.concat([data_df, metadata[\"Metadata_Well\"]], axis=1)\n",
    "    # groupby well and take mean of each well\n",
    "    data_df = data_df.groupby(\"Metadata_Well\").mean()\n",
    "    # drop duplicate rows in the metadata_well column\n",
    "    metadata = metadata.drop_duplicates(subset=[\"Metadata_Well\"])\n",
    "    # get the metadata for each well\n",
    "    data_df = pd.merge(\n",
    "        data_df, metadata, left_on=\"Metadata_Well\", right_on=\"Metadata_Well\"\n",
    "    )\n",
    "    data_df = pd.merge(\n",
    "        data_df,\n",
    "        df_nomic,\n",
    "        left_on=[\"Metadata_Well\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
    "        right_on=[\"Metadata_position_x\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
    "    )\n",
    "    data_df = data_df.drop(columns=[\"Metadata_position_x\"])\n",
    "elif (aggregation == True) and (nomic == False):\n",
    "    data_split_path = pathlib.Path(\n",
    "        f\"../0.split_data/indexes/{cell_type}/{MODEL_TYPE}/{control}_{treatment}/aggregated_sc_data_split_indexes.tsv\"\n",
    "    )\n",
    "    data_split_indexes = pd.read_csv(data_split_path, sep=\"\\t\", index_col=0)\n",
    "    # subset each column that contains metadata\n",
    "    metadata = data_df.filter(regex=\"Metadata\")\n",
    "    data_df = data_df.drop(metadata.columns, axis=1)\n",
    "    data_df = pd.concat([data_df, metadata[\"Metadata_Well\"]], axis=1)\n",
    "    # groupby well and take mean of each well\n",
    "    data_df = data_df.groupby(\"Metadata_Well\").mean()\n",
    "    # drop duplicate rows in the metadata_well column\n",
    "    metadata = metadata.drop_duplicates(subset=[\"Metadata_Well\"])\n",
    "    # get the metadata for each well\n",
    "    data_df = pd.merge(\n",
    "        data_df,\n",
    "        df_nomic,\n",
    "        left_on=[\"Metadata_Well\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
    "        right_on=[\"Metadata_position_x\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
    "    )\n",
    "elif (aggregation == False) and (nomic == True):\n",
    "    data_split_path = pathlib.Path(\n",
    "        f\"../0.split_data/indexes/{cell_type}/{MODEL_TYPE}/{control}_{treatment}/sc_and_nomic_data_split_indexes.tsv\"\n",
    "    )\n",
    "    data_split_indexes = pd.read_csv(data_split_path, sep=\"\\t\", index_col=0)\n",
    "    data_df = pd.merge(\n",
    "        data_df,\n",
    "        df_nomic,\n",
    "        left_on=[\"Metadata_Well\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
    "        right_on=[\"Metadata_position_x\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
    "    )\n",
    "    data_df = data_df.drop(columns=[\"Metadata_position_x\"])\n",
    "elif aggregation == False and nomic == False:\n",
    "    data_split_path = pathlib.Path(\n",
    "        f\"../0.split_data/indexes/{cell_type}/{MODEL_TYPE}/{control}_{treatment}/sc_split_indexes.tsv\"\n",
    "    )\n",
    "    data_split_indexes = pd.read_csv(data_split_path, sep=\"\\t\", index_col=0)\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select tht indexes for the training and test set\n",
    "train_indexes = data_split_indexes.loc[data_split_indexes[\"label\"] == \"train\"]\n",
    "test_indexes = data_split_indexes.loc[data_split_indexes[\"label\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data_df by indexes in data_split_indexes\n",
    "training_data = data_df.loc[train_indexes[\"labeled_data_index\"]]\n",
    "testing_data = data_df.loc[test_indexes[\"labeled_data_index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get oneb_Metadata_Treatment_Dose_Inhibitor_Dose  =='DMSO_0.100_DMSO_0.025' and 'LPS_100.000_DMSO_0.025 and Thapsigargin_10.000_DMSO_0.025'\n",
    "training_data = training_data[\n",
    "    training_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(\n",
    "        [control, treatment]\n",
    "    )\n",
    "]\n",
    "testing_data = testing_data[\n",
    "    testing_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(\n",
    "        [control, treatment]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at random downsample the DMSO treatment to match the number of wells in the LPS treatment\n",
    "seed = 0\n",
    "# get the number of wells in the LPS treatment\n",
    "trt_wells = training_data[\n",
    "    training_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"] == treatment\n",
    "].shape[0]\n",
    "# get the number of wells in the DMSO treatment\n",
    "dmso_wells = training_data[\n",
    "    training_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"] == control\n",
    "].shape[0]\n",
    "# downsample the DMSO treatment to match the number of wells in the LPS treatment\n",
    "dmso_holdout = training_data[\n",
    "    training_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"] == control\n",
    "].sample(n=trt_wells, random_state=seed)\n",
    "# remove the downsampled DMSO wells from the data\n",
    "training_data = training_data.drop(dmso_holdout.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metadata columns\n",
    "# subset each column that contains metadata\n",
    "metadata_train = training_data.filter(regex=\"Metadata\")\n",
    "# drop all metadata columns\n",
    "train_data_x = training_data.drop(metadata_train.columns, axis=1)\n",
    "train_treatments = training_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
    "# get all columns that contain \"NSU\" in the column name\n",
    "train_data_y_cols = train_data_x.filter(regex=\"NSU\").columns\n",
    "train_data_y = training_data[train_data_y_cols]\n",
    "train_data_x = train_data_x.drop(train_data_y_cols, axis=1)\n",
    "\n",
    "\n",
    "# define metadata columns\n",
    "# subset each column that contains metadata\n",
    "metadata_test = testing_data.filter(regex=\"Metadata\")\n",
    "# drop all metadata columns\n",
    "test_data_x = testing_data.drop(metadata_test.columns, axis=1)\n",
    "test_treatments = testing_data[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
    "# get all columns that contain \"NSU\" in the column name\n",
    "test_data_y_cols = test_data_x.filter(regex=\"NSU\").columns\n",
    "test_data_y = testing_data[test_data_y_cols]\n",
    "test_data_x = test_data_x.drop(test_data_y_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_x.shape, train_data_y.shape, test_data_x.shape, test_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model path from parameters\n",
    "if (aggregation == True) and (nomic == True):\n",
    "    model_path = pathlib.Path(f\"models/regression/{cell_type}/aggregated_with_nomic/\")\n",
    "elif (aggregation == True) and (nomic == False):\n",
    "    model_path = pathlib.Path(f\"models/regression/{cell_type}/aggregated/\")\n",
    "elif (aggregation == False) and (nomic == True):\n",
    "    model_path = pathlib.Path(f\"models/regression/{cell_type}/sc_with_nomic/\")\n",
    "elif (aggregation == False) and (nomic == False):\n",
    "    model_path = pathlib.Path(f\"models/regression/{cell_type}/sc/\")\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"train_data\": {\n",
    "        \"data_x\": train_data_x,\n",
    "        \"data_y\": train_data_y,\n",
    "        \"col_names\": train_data_y_cols,\n",
    "        \"metadata\": metadata_train,\n",
    "    },\n",
    "    \"test_data\": {\n",
    "        \"data_x\": test_data_x,\n",
    "        \"data_y\": test_data_y,\n",
    "        \"col_names\": test_data_y_cols,\n",
    "        \"metadata\": metadata_test,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation method\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# lsit of metrics to use\n",
    "metrics = [\"explained_variance\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\"]\n",
    "output_metric_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank df for concatenated results\n",
    "results_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"explained_variance\",\n",
    "        \"neg_mean_absolute_error\",\n",
    "        \"neg_mean_squared_error\",\n",
    "        \"well\",\n",
    "        \"treatment\",\n",
    "        \"r2\",\n",
    "        \"cytokine\",\n",
    "        \"data_split\",\n",
    "        \"shuffle\",\n",
    "        \"predicted_value\",\n",
    "        \"actual_value\",\n",
    "        \"log10_neg_mean_absolute_error\",\n",
    "        \"log10_neg_mean_squared_error\",\n",
    "        \"log10_explained_variance\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cytokine in train_data_y_cols:\n",
    "    for data_split in data_dict:\n",
    "        print(data_split)\n",
    "        for i in data_dict[data_split]:\n",
    "            print(i)\n",
    "            data_x = data_dict[data_split][\"data_x\"]\n",
    "            data_y = data_dict[data_split][\"data_y\"]\n",
    "            col_names = data_dict[data_split][\"col_names\"]\n",
    "            metadata = data_dict[data_split][\"metadata\"]\n",
    "        if shuffle == \"shuffled_baseline\":\n",
    "            model = joblib.load(\n",
    "                f\"../1.train_models/{model_path}/{cytokine}_shuffled_baseline__all_nomic.joblib\"\n",
    "            )\n",
    "        elif shuffle == \"final\":\n",
    "            model = joblib.load(\n",
    "                f\"../1.train_models/{model_path}/{cytokine}_final__all_nomic.joblib\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "\n",
    "        # get the cytokine column of choice\n",
    "        y_selected = data_y[cytokine]\n",
    "\n",
    "        if shuffle == \"shuffled_baseline\":\n",
    "            for column in data_x:\n",
    "                np.random.shuffle(data_x[column].values)\n",
    "\n",
    "        # get predictions\n",
    "        predictions = model.predict(data_x)\n",
    "        for metric in metrics:\n",
    "            scores = cross_val_score(\n",
    "                model, data_x, y_selected, scoring=metric, cv=loo, n_jobs=-1\n",
    "            )\n",
    "            output_metric_scores[metric] = scores\n",
    "        r2 = r2_score(y_selected, predictions)\n",
    "        output_metric_scores[\"treatment\"] = metadata[\n",
    "            \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"\n",
    "        ]\n",
    "        output_metric_scores[\"well\"] = metadata[\"Metadata_Well\"]\n",
    "        print(output_metric_scores)\n",
    "        df = pd.DataFrame.from_dict(output_metric_scores)\n",
    "        df[\"r2\"] = r2\n",
    "        df[\"cytokine\"] = cytokine\n",
    "        df[\"data_split\"] = data_split\n",
    "        df[\"shuffle\"] = shuffle\n",
    "        df[\"predicted_value\"] = predictions\n",
    "        df[\"actual_value\"] = y_selected\n",
    "        df[\"log10_neg_mean_absolute_error\"] = -np.log10(-df[\"neg_mean_absolute_error\"])\n",
    "        df[\"log10_neg_mean_squared_error\"] = -np.log10(-df[\"neg_mean_squared_error\"])\n",
    "        df[\"log10_explained_variance\"] = -np.log10(df[\"explained_variance\"])\n",
    "\n",
    "        # replace \"[NSU]\" with \"\"\"\n",
    "        df[\"cytokine\"] = df[\"cytokine\"].replace(\"[ \\[\\]NSU]\", \"\", regex=True)\n",
    "        df[\"cytokine\"] = df[\"cytokine\"].replace(\" \", \"_\", regex=True)\n",
    "\n",
    "        # concat the dataframes\n",
    "        results_df = pd.concat([results_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = results_df.drop(\n",
    "    columns=[\n",
    "        \"explained_variance\",\n",
    "        \"neg_mean_absolute_error\",\n",
    "        \"neg_mean_squared_error\",\n",
    "        \"well\",\n",
    "        \"treatment\",\n",
    "        \"r2\",\n",
    "        \"log10_neg_mean_absolute_error\",\n",
    "        \"log10_neg_mean_squared_error\",\n",
    "        \"log10_explained_variance\",\n",
    "    ]\n",
    ")\n",
    "# calculate the variance of the actual and predicted values per cytokine\n",
    "var_df = var_df.groupby([\"cytokine\", \"data_split\", \"shuffle\"]).var()\n",
    "var_df = pd.merge(\n",
    "    var_df,\n",
    "    results_df.groupby([\"cytokine\", \"data_split\", \"shuffle\"]).r2.unique(),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "var_df.reset_index(inplace=True)\n",
    "var_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model path from parameters\n",
    "if (aggregation == True) and (nomic == True):\n",
    "    results_path = pathlib.Path(\n",
    "        f\"results/regression/{cell_type}/aggregated_with_nomic/{control}__{treatment}\"\n",
    "    )\n",
    "elif (aggregation == True) and (nomic == False):\n",
    "    results_path = pathlib.Path(\n",
    "        f\"results/regression/{cell_type}/aggregated/{control}__{treatment}\"\n",
    "    )\n",
    "elif (aggregation == False) and (nomic == True):\n",
    "    results_path = pathlib.Path(\n",
    "        f\"results/regression/{cell_type}/sc_with_nomic/{control}__{treatment}\"\n",
    "    )\n",
    "elif (aggregation == False) and (nomic == False):\n",
    "    results_path = pathlib.Path(\n",
    "        f\"results/regression/{cell_type}/sc/{control}__{treatment}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Error\")\n",
    "pathlib.Path(results_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model training metrics file exists\n",
    "metrics_file = pathlib.Path(f\"{results_path}/model_stats.csv\")\n",
    "if metrics_file.exists():\n",
    "    metrics_df = pd.read_csv(metrics_file)\n",
    "    if len(metrics_df[\"shuffle\"].unique()) > 1:\n",
    "        pass\n",
    "    else:\n",
    "        metrics_df = pd.concat([metrics_df, results_df], axis=0)\n",
    "        metrics_df.to_csv(metrics_file, index=False)\n",
    "else:\n",
    "    results_df.to_csv(metrics_file, index=False)\n",
    "\n",
    "\n",
    "# do the same for the variance df\n",
    "# check if the model training metrics file exists\n",
    "metrics_file = pathlib.Path(f\"{results_path}/variance_r2_stats.csv\")\n",
    "if metrics_file.exists():\n",
    "    metrics_df = pd.read_csv(metrics_file)\n",
    "    if len(metrics_df[\"shuffle\"].unique()) > 1:\n",
    "        pass\n",
    "    else:\n",
    "        metrics_df = pd.concat([metrics_df, var_df], axis=0)\n",
    "        metrics_df.to_csv(metrics_file, index=False)\n",
    "else:\n",
    "    var_df.to_csv(metrics_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,../scripts//py:percent"
  },
  "kernelspec": {
   "display_name": "Interstellar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
