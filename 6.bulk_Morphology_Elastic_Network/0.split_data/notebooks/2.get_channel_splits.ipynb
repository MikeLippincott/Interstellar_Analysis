{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "import itertools\n",
                "import pathlib\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import toml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "argparser = argparse.ArgumentParser()\n",
                "argparser.add_argument(\"--cell_type\", default=\"all\")\n",
                "\n",
                "args = argparser.parse_args()\n",
                "\n",
                "cell_type = args.cell_type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parameters\n",
                "aggregation = True\n",
                "nomic = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_TYPE = \"regression\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# toml file path\n",
                "TOML_PATH = pathlib.Path(\"../splits.toml\")\n",
                "# read toml file via toml\n",
                "data_splits_by_treatments = toml.load(TOML_PATH)\n",
                "\n",
                "# define the 100% test set data treatments\n",
                "test_100_percent = data_splits_by_treatments[\"splits\"][\"data_splits_100\"]\n",
                "test_75_percent = data_splits_by_treatments[\"splits\"][\"data_splits_75\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "aggregate_and_nomic_path = pathlib.Path(\n",
                "    f\"../../../data/{cell_type}_preprocessed_sc_norm_aggregated_nomic.parquet\"\n",
                ").resolve(strict=True)\n",
                "aggregate_path = pathlib.Path(\n",
                "    f\"../../../data/SHSY5Y_preprocessed_sc_norm_aggregated.parquet\"\n",
                ").resolve(strict=True)\n",
                "data_df = pd.read_parquet(aggregate_and_nomic_path)\n",
                "\n",
                "data_df.head()\n",
                "\n",
                "morphology_df = pd.read_parquet(aggregate_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the NSU columns\n",
                "nsu_cols = [col for col in data_df.columns if \"NSU\" in col]\n",
                "nomic_df = data_df[nsu_cols]\n",
                "nomic_df.loc[\"Metadata_Well\"] = data_df[\"Metadata_Well\"]\n",
                "nomic_df.loc[\"oneb_Treatment_Dose_Inhibitor_Dose\"] = data_df[\n",
                "    \"oneb_Treatment_Dose_Inhibitor_Dose\"\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# subset each column that contains metadata\n",
                "metadata = data_df.filter(regex=\"Metadata\")\n",
                "\n",
                "# get all columns that are not metadata except for metadata_Well\n",
                "data = data_df.drop(metadata.columns, axis=1)\n",
                "\n",
                "# get the metadata_Well column\n",
                "metadata_well = metadata[\n",
                "    [\"Metadata_Well\", \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "]\n",
                "\n",
                "data_df = pd.merge(data, metadata_well, left_index=True, right_index=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# drop morphology metadata\n",
                "morphology_df = morphology_df.drop(\n",
                "    morphology_df.filter(regex=\"Metadata\").columns, axis=1\n",
                ")\n",
                "morphology_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define the list of the channels\n",
                "channel_list = [\"DNA\", \"Gasdermin\", \"ER\", \"Mito\", \"PM\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# combiantions of channels\n",
                "channel_combinations = []\n",
                "for i in range(1, len(channel_list) + 1):\n",
                "    tmp_list = list(itertools.combinations(channel_list, i))\n",
                "    channel_combinations += tmp_list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set up the LOO channel with recursion for dropping multiple channels\n",
                "\n",
                "\n",
                "def channel_drop(df, channel):\n",
                "    df = df.drop(df.filter(regex=channel).columns, axis=1)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# dictionary for each df to go into\n",
                "results_dict = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get all of the the channel combinations\n",
                "for i in channel_combinations:\n",
                "    # keep all channels and drop all channels\n",
                "    if len(i) == 5:\n",
                "        # keep all channels\n",
                "        tmp_df = morphology_df\n",
                "        # get the remaining channels for indexing purposes\n",
                "        channel_list_index = [x for x in channel_list if x in i]\n",
                "        channel_list_index = \"_\".join(channel_list_index)\n",
                "        results_dict[channel_list_index] = tmp_df\n",
                "\n",
                "        # drop all channels\n",
                "        tmp = channel_drop(morphology_df, i[0])\n",
                "        for j in range(1, len(i)):\n",
                "            tmp = channel_drop(tmp, i[j])\n",
                "        tmp_df = tmp\n",
                "        # get the remaining channels for indexing purposes\n",
                "        channel_list_index = \"No Channels\"\n",
                "        results_dict[channel_list_index] = tmp_df\n",
                "    # drop 4 channels\n",
                "    elif len(i) == 4:\n",
                "        tmp = channel_drop(morphology_df, i[0])\n",
                "        for j in range(1, len(i)):\n",
                "            tmp = channel_drop(tmp, i[j])\n",
                "        tmp_df = tmp\n",
                "        # get the remaining channels for indexing purposes\n",
                "        channel_list_index = [x for x in channel_list if x not in i]\n",
                "        channel_list_index = \"_\".join(channel_list_index)\n",
                "        results_dict[channel_list_index] = tmp_df\n",
                "    # drop 3 channels\n",
                "    elif len(i) == 3:\n",
                "        tmp = channel_drop(morphology_df, i[0])\n",
                "        for j in range(1, len(i)):\n",
                "            tmp = channel_drop(tmp, i[j])\n",
                "        tmp_df = tmp\n",
                "        # get the remaining channels for indexing purposes\n",
                "        channel_list_index = [x for x in channel_list if x not in i]\n",
                "        channel_list_index = \"_\".join(channel_list_index)\n",
                "        results_dict[channel_list_index] = tmp_df\n",
                "    # drop 2 channels\n",
                "    elif len(i) == 2:\n",
                "        tmp = channel_drop(morphology_df, i[0])\n",
                "        for j in range(1, len(i)):\n",
                "            tmp = channel_drop(tmp, i[j])\n",
                "        tmp_df = tmp\n",
                "        # get the remaining channels for indexing purposes\n",
                "        channel_list_index = [x for x in channel_list if x not in i]\n",
                "        channel_list_index = \"_\".join(channel_list_index)\n",
                "        results_dict[channel_list_index] = tmp_df\n",
                "    # drop 1 channel\n",
                "    elif len(i) == 1:\n",
                "        tmp = channel_drop(morphology_df, i[0])\n",
                "        tmp_df = tmp\n",
                "        # get the remaining channels for indexing purposes\n",
                "        channel_list_index = [x for x in channel_list if x not in i]\n",
                "        channel_list_index = \"_\".join(channel_list_index)\n",
                "        results_dict[channel_list_index] = tmp_df\n",
                "    else:\n",
                "        print(\"channel length error\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set path to save\n",
                "pathlib.Path(f\"../indexes/{cell_type}/regression/channels\").mkdir(\n",
                "    parents=True, exist_ok=True\n",
                ")\n",
                "\n",
                "# loop through the dictionary and save each dataframe\n",
                "for i in results_dict:\n",
                "    print(i)\n",
                "    print(results_dict[i].shape)\n",
                "    # rename the dictionary keys\n",
                "    # combine the metadata and morphology dataframes\n",
                "    new_df = pd.merge(results_dict[i], metadata_well, left_index=True, right_index=True)\n",
                "    # combine the cytokine dataframes\n",
                "    new_df = pd.merge(new_df, nomic_df, left_index=True, right_index=True)\n",
                "    # set file path\n",
                "    file_path = pathlib.Path(f\"../indexes/{cell_type}/regression/channels/{i}.parquet\")\n",
                "    # save the dataframe\n",
                "    new_df.to_parquet(file_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the list of the dictionary keys\n",
                "index_list = list(results_dict.keys())\n",
                "index_list_new = []\n",
                "for i in index_list:\n",
                "    index_list_new.append(i + \".parquet\")\n",
                "# write the list to a text file\n",
                "# file path\n",
                "file_write_path = pathlib.Path(f\"../cytokine_list/channel_splits.txt\")\n",
                "with open(file_write_path, \"w\") as f:\n",
                "    for i in index_list_new:\n",
                "        f.write(\"%s\\n\" % i)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Interstellar",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
