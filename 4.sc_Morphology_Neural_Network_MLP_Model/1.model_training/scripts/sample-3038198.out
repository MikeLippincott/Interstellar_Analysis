[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '5723512c'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '5295c6a4'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'c6f0fcb5'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '5fa66c43'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_0.010_DMSO_0.025 treatment_name: LPS_100.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_0.010_DMSO_0.025
TREATMENT_NAME: LPS_100.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_0.010_DMSO_0.025' 'LPS_100.000_DMSO_0.025']
The dimensions of the data are: (31846, 1276)
Number of total missing values across all columns: 63692
Data Subset Is Off
Wells held out for testing: ['B20' 'J16']
Wells to use for training, validation, and testing ['B16' 'B17' 'B21' 'J17' 'J20' 'J21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.337845).  Saving model ...
	 Train_Loss: 0.5211 Train_Acc: 69.243 Val_Loss: 0.3378  BEST VAL Loss: 0.3378  Val_Acc: 87.552

Epoch 1: Validation loss decreased (0.337845 --> 0.298073).  Saving model ...
	 Train_Loss: 0.4414 Train_Acc: 78.215 Val_Loss: 0.2981  BEST VAL Loss: 0.2981  Val_Acc: 91.507

Epoch 2: Validation loss decreased (0.298073 --> 0.260598).  Saving model ...
	 Train_Loss: 0.3913 Train_Acc: 89.191 Val_Loss: 0.2606  BEST VAL Loss: 0.2606  Val_Acc: 93.047

Epoch 3: Validation loss decreased (0.260598 --> 0.237281).  Saving model ...
	 Train_Loss: 0.3563 Train_Acc: 90.809 Val_Loss: 0.2373  BEST VAL Loss: 0.2373  Val_Acc: 94.338

Epoch 4: Validation loss decreased (0.237281 --> 0.224105).  Saving model ...
	 Train_Loss: 0.3300 Train_Acc: 91.486 Val_Loss: 0.2241  BEST VAL Loss: 0.2241  Val_Acc: 93.464

Epoch 5: Validation loss decreased (0.224105 --> 0.210500).  Saving model ...
	 Train_Loss: 0.3100 Train_Acc: 91.543 Val_Loss: 0.2105  BEST VAL Loss: 0.2105  Val_Acc: 94.963

Epoch 6: Validation loss decreased (0.210500 --> 0.201307).  Saving model ...
	 Train_Loss: 0.2920 Train_Acc: 92.860 Val_Loss: 0.2013  BEST VAL Loss: 0.2013  Val_Acc: 94.963

Epoch 7: Validation loss decreased (0.201307 --> 0.194660).  Saving model ...
	 Train_Loss: 0.2775 Train_Acc: 92.678 Val_Loss: 0.1947  BEST VAL Loss: 0.1947  Val_Acc: 95.545

Epoch 8: Validation loss decreased (0.194660 --> 0.187403).  Saving model ...
	 Train_Loss: 0.2658 Train_Acc: 92.704 Val_Loss: 0.1874  BEST VAL Loss: 0.1874  Val_Acc: 96.128

Epoch 9: Validation loss decreased (0.187403 --> 0.182776).  Saving model ...
	 Train_Loss: 0.2556 Train_Acc: 93.193 Val_Loss: 0.1828  BEST VAL Loss: 0.1828  Val_Acc: 95.129

Epoch 10: Validation loss decreased (0.182776 --> 0.178679).  Saving model ...
	 Train_Loss: 0.2473 Train_Acc: 93.141 Val_Loss: 0.1787  BEST VAL Loss: 0.1787  Val_Acc: 95.337

Epoch 11: Validation loss decreased (0.178679 --> 0.174343).  Saving model ...
	 Train_Loss: 0.2397 Train_Acc: 93.474 Val_Loss: 0.1743  BEST VAL Loss: 0.1743  Val_Acc: 95.754

Epoch 12: Validation loss decreased (0.174343 --> 0.170551).  Saving model ...
	 Train_Loss: 0.2331 Train_Acc: 93.375 Val_Loss: 0.1706  BEST VAL Loss: 0.1706  Val_Acc: 96.045

Epoch 13: Validation loss decreased (0.170551 --> 0.166124).  Saving model ...
	 Train_Loss: 0.2273 Train_Acc: 93.573 Val_Loss: 0.1661  BEST VAL Loss: 0.1661  Val_Acc: 95.670

Epoch 14: Validation loss decreased (0.166124 --> 0.163269).  Saving model ...
	 Train_Loss: 0.2221 Train_Acc: 93.578 Val_Loss: 0.1633  BEST VAL Loss: 0.1633  Val_Acc: 96.211

Epoch 15: Validation loss decreased (0.163269 --> 0.161304).  Saving model ...
	 Train_Loss: 0.2169 Train_Acc: 94.156 Val_Loss: 0.1613  BEST VAL Loss: 0.1613  Val_Acc: 95.795

Epoch 16: Validation loss decreased (0.161304 --> 0.159051).  Saving model ...
	 Train_Loss: 0.2125 Train_Acc: 93.791 Val_Loss: 0.1591  BEST VAL Loss: 0.1591  Val_Acc: 95.837

Epoch 17: Validation loss decreased (0.159051 --> 0.157132).  Saving model ...
	 Train_Loss: 0.2083 Train_Acc: 93.864 Val_Loss: 0.1571  BEST VAL Loss: 0.1571  Val_Acc: 96.211

Epoch 18: Validation loss decreased (0.157132 --> 0.155203).  Saving model ...
	 Train_Loss: 0.2042 Train_Acc: 94.353 Val_Loss: 0.1552  BEST VAL Loss: 0.1552  Val_Acc: 96.211

Epoch 19: Validation loss decreased (0.155203 --> 0.152723).  Saving model ...
	 Train_Loss: 0.2004 Train_Acc: 94.483 Val_Loss: 0.1527  BEST VAL Loss: 0.1527  Val_Acc: 96.378

Epoch 20: Validation loss decreased (0.152723 --> 0.150884).  Saving model ...
	 Train_Loss: 0.1969 Train_Acc: 94.452 Val_Loss: 0.1509  BEST VAL Loss: 0.1509  Val_Acc: 96.420

Epoch 21: Validation loss decreased (0.150884 --> 0.149253).  Saving model ...
	 Train_Loss: 0.1938 Train_Acc: 94.489 Val_Loss: 0.1493  BEST VAL Loss: 0.1493  Val_Acc: 96.170

Epoch 22: Validation loss decreased (0.149253 --> 0.147737).  Saving model ...
	 Train_Loss: 0.1913 Train_Acc: 94.114 Val_Loss: 0.1477  BEST VAL Loss: 0.1477  Val_Acc: 95.920

Epoch 23: Validation loss decreased (0.147737 --> 0.146083).  Saving model ...
	 Train_Loss: 0.1889 Train_Acc: 93.994 Val_Loss: 0.1461  BEST VAL Loss: 0.1461  Val_Acc: 96.170

Epoch 24: Validation loss decreased (0.146083 --> 0.144534).  Saving model ...
	 Train_Loss: 0.1863 Train_Acc: 94.697 Val_Loss: 0.1445  BEST VAL Loss: 0.1445  Val_Acc: 96.961

Epoch 25: Validation loss decreased (0.144534 --> 0.143333).  Saving model ...
	 Train_Loss: 0.1838 Train_Acc: 94.686 Val_Loss: 0.1433  BEST VAL Loss: 0.1433  Val_Acc: 96.545

Epoch 26: Validation loss decreased (0.143333 --> 0.142594).  Saving model ...
	 Train_Loss: 0.1813 Train_Acc: 94.952 Val_Loss: 0.1426  BEST VAL Loss: 0.1426  Val_Acc: 95.962

Epoch 27: Validation loss decreased (0.142594 --> 0.141932).  Saving model ...
	 Train_Loss: 0.1794 Train_Acc: 94.582 Val_Loss: 0.1419  BEST VAL Loss: 0.1419  Val_Acc: 96.295

Epoch 28: Validation loss decreased (0.141932 --> 0.141162).  Saving model ...
	 Train_Loss: 0.1774 Train_Acc: 94.712 Val_Loss: 0.1412  BEST VAL Loss: 0.1412  Val_Acc: 96.586

Epoch 29: Validation loss decreased (0.141162 --> 0.140435).  Saving model ...
	 Train_Loss: 0.1754 Train_Acc: 94.645 Val_Loss: 0.1404  BEST VAL Loss: 0.1404  Val_Acc: 96.753

Epoch 30: Validation loss decreased (0.140435 --> 0.139248).  Saving model ...
	 Train_Loss: 0.1737 Train_Acc: 94.681 Val_Loss: 0.1392  BEST VAL Loss: 0.1392  Val_Acc: 96.503

Epoch 31: Validation loss decreased (0.139248 --> 0.138325).  Saving model ...
	 Train_Loss: 0.1722 Train_Acc: 94.660 Val_Loss: 0.1383  BEST VAL Loss: 0.1383  Val_Acc: 96.503

Epoch 32: Validation loss decreased (0.138325 --> 0.137795).  Saving model ...
	 Train_Loss: 0.1706 Train_Acc: 94.567 Val_Loss: 0.1378  BEST VAL Loss: 0.1378  Val_Acc: 96.253

Epoch 33: Validation loss decreased (0.137795 --> 0.137032).  Saving model ...
	 Train_Loss: 0.1690 Train_Acc: 94.999 Val_Loss: 0.1370  BEST VAL Loss: 0.1370  Val_Acc: 96.503

Epoch 34: Validation loss decreased (0.137032 --> 0.135946).  Saving model ...
	 Train_Loss: 0.1673 Train_Acc: 95.212 Val_Loss: 0.1359  BEST VAL Loss: 0.1359  Val_Acc: 96.628

Epoch 35: Validation loss decreased (0.135946 --> 0.135327).  Saving model ...
	 Train_Loss: 0.1657 Train_Acc: 95.144 Val_Loss: 0.1353  BEST VAL Loss: 0.1353  Val_Acc: 96.378

Epoch 36: Validation loss decreased (0.135327 --> 0.134523).  Saving model ...
	 Train_Loss: 0.1643 Train_Acc: 94.957 Val_Loss: 0.1345  BEST VAL Loss: 0.1345  Val_Acc: 96.919

Epoch 37: Validation loss decreased (0.134523 --> 0.134364).  Saving model ...
	 Train_Loss: 0.1628 Train_Acc: 95.170 Val_Loss: 0.1344  BEST VAL Loss: 0.1344  Val_Acc: 96.794

Epoch 38: Validation loss decreased (0.134364 --> 0.134264).  Saving model ...
	 Train_Loss: 0.1615 Train_Acc: 95.316 Val_Loss: 0.1343  BEST VAL Loss: 0.1343  Val_Acc: 96.461

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1603 Train_Acc: 94.957 Val_Loss: 0.1344  BEST VAL Loss: 0.1343  Val_Acc: 96.336

Epoch 40: Validation loss decreased (0.134264 --> 0.133807).  Saving model ...
	 Train_Loss: 0.1591 Train_Acc: 95.056 Val_Loss: 0.1338  BEST VAL Loss: 0.1338  Val_Acc: 96.753

Epoch 41: Validation loss decreased (0.133807 --> 0.133370).  Saving model ...
	 Train_Loss: 0.1578 Train_Acc: 95.524 Val_Loss: 0.1334  BEST VAL Loss: 0.1334  Val_Acc: 96.503

Epoch 42: Validation loss decreased (0.133370 --> 0.133012).  Saving model ...
	 Train_Loss: 0.1567 Train_Acc: 95.098 Val_Loss: 0.1330  BEST VAL Loss: 0.1330  Val_Acc: 96.753

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1557 Train_Acc: 95.124 Val_Loss: 0.1331  BEST VAL Loss: 0.1330  Val_Acc: 96.711

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1546 Train_Acc: 95.358 Val_Loss: 0.1333  BEST VAL Loss: 0.1330  Val_Acc: 96.336

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.1536 Train_Acc: 95.306 Val_Loss: 0.1339  BEST VAL Loss: 0.1330  Val_Acc: 96.045

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1526 Train_Acc: 95.436 Val_Loss: 0.1337  BEST VAL Loss: 0.1330  Val_Acc: 96.836

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1517 Train_Acc: 95.451 Val_Loss: 0.1335  BEST VAL Loss: 0.1330  Val_Acc: 97.294

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1506 Train_Acc: 95.483 Val_Loss: 0.1338  BEST VAL Loss: 0.1330  Val_Acc: 96.711

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1497 Train_Acc: 95.321 Val_Loss: 0.1337  BEST VAL Loss: 0.1330  Val_Acc: 96.961

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1489 Train_Acc: 95.311 Val_Loss: 0.1339  BEST VAL Loss: 0.1330  Val_Acc: 96.586

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1480 Train_Acc: 95.493 Val_Loss: 0.1341  BEST VAL Loss: 0.1330  Val_Acc: 96.336

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1471 Train_Acc: 95.696 Val_Loss: 0.1343  BEST VAL Loss: 0.1330  Val_Acc: 96.669

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1464 Train_Acc: 95.165 Val_Loss: 0.1346  BEST VAL Loss: 0.1330  Val_Acc: 96.586

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1458 Train_Acc: 95.009 Val_Loss: 0.1344  BEST VAL Loss: 0.1330  Val_Acc: 96.836

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.1452 Train_Acc: 95.150 Val_Loss: 0.1342  BEST VAL Loss: 0.1330  Val_Acc: 96.836

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.1444 Train_Acc: 95.368 Val_Loss: 0.1340  BEST VAL Loss: 0.1330  Val_Acc: 97.044

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.1437 Train_Acc: 95.249 Val_Loss: 0.1342  BEST VAL Loss: 0.1330  Val_Acc: 96.628

Epoch 58: Validation loss did not decrease
Early stopped at epoch : 58
LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.99      0.99      9707
           1       0.99      0.98      0.99      9508

    accuracy                           0.99     19215
   macro avg       0.99      0.99      0.99     19215
weighted avg       0.99      0.99      0.99     19215

LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.97      0.97      1214
           1       0.97      0.96      0.97      1188

    accuracy                           0.97      2402
   macro avg       0.97      0.97      0.97      2402
weighted avg       0.97      0.97      0.97      2402

LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1214
           1       0.97      0.96      0.96      1188

    accuracy                           0.96      2402
   macro avg       0.96      0.96      0.96      2402
weighted avg       0.96      0.96      0.96      2402

              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1214
           1       0.97      0.96      0.96      1188

    accuracy                           0.96      2402
   macro avg       0.96      0.96      0.96      2402
weighted avg       0.96      0.96      0.96      2402

LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.97      0.96      3724
           1       0.97      0.96      0.96      4103

    accuracy                           0.96      7827
   macro avg       0.96      0.96      0.96      7827
weighted avg       0.96      0.96      0.96      7827

              precision    recall  f1-score   support

           0       0.95      0.97      0.96      3724
           1       0.97      0.96      0.96      4103

    accuracy                           0.96      7827
   macro avg       0.96      0.96      0.96      7827
weighted avg       0.96      0.96      0.96      7827

completed
