[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '6e3552b6'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '7251cd2d'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '2c49018f'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '1fb16265'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Flagellin_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_3.0_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (278808, 1270)
Number of total missing values across all columns: 245710
Data Subset Is Off
Wells held out for testing: ['L08' 'M10']
Wells to use for training, validation, and testing ['L02' 'L03' 'M05' 'L09' 'M11']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.072128).  Saving model ...
	 Train_Loss: 0.1448 Train_Acc: 94.610 Val_Loss: 0.0721  BEST VAL Loss: 0.0721  Val_Acc: 97.732

Epoch 1: Validation loss decreased (0.072128 --> 0.064242).  Saving model ...
	 Train_Loss: 0.1057 Train_Acc: 97.721 Val_Loss: 0.0642  BEST VAL Loss: 0.0642  Val_Acc: 98.319

Epoch 2: Validation loss decreased (0.064242 --> 0.059130).  Saving model ...
	 Train_Loss: 0.0887 Train_Acc: 98.113 Val_Loss: 0.0591  BEST VAL Loss: 0.0591  Val_Acc: 98.573

Epoch 3: Validation loss decreased (0.059130 --> 0.055602).  Saving model ...
	 Train_Loss: 0.0783 Train_Acc: 98.387 Val_Loss: 0.0556  BEST VAL Loss: 0.0556  Val_Acc: 98.760

Epoch 4: Validation loss decreased (0.055602 --> 0.053378).  Saving model ...
	 Train_Loss: 0.0713 Train_Acc: 98.497 Val_Loss: 0.0534  BEST VAL Loss: 0.0534  Val_Acc: 98.709

Epoch 5: Validation loss decreased (0.053378 --> 0.051640).  Saving model ...
	 Train_Loss: 0.0663 Train_Acc: 98.548 Val_Loss: 0.0516  BEST VAL Loss: 0.0516  Val_Acc: 98.709

Epoch 6: Validation loss decreased (0.051640 --> 0.050135).  Saving model ...
	 Train_Loss: 0.0621 Train_Acc: 98.696 Val_Loss: 0.0501  BEST VAL Loss: 0.0501  Val_Acc: 98.826

Epoch 7: Validation loss decreased (0.050135 --> 0.049239).  Saving model ...
	 Train_Loss: 0.0587 Train_Acc: 98.750 Val_Loss: 0.0492  BEST VAL Loss: 0.0492  Val_Acc: 98.831

Epoch 8: Validation loss decreased (0.049239 --> 0.048847).  Saving model ...
	 Train_Loss: 0.0560 Train_Acc: 98.795 Val_Loss: 0.0488  BEST VAL Loss: 0.0488  Val_Acc: 98.841

Epoch 9: Validation loss decreased (0.048847 --> 0.048698).  Saving model ...
	 Train_Loss: 0.0537 Train_Acc: 98.800 Val_Loss: 0.0487  BEST VAL Loss: 0.0487  Val_Acc: 98.795

Epoch 10: Validation loss decreased (0.048698 --> 0.047825).  Saving model ...
	 Train_Loss: 0.0516 Train_Acc: 98.870 Val_Loss: 0.0478  BEST VAL Loss: 0.0478  Val_Acc: 98.907

Epoch 11: Validation loss decreased (0.047825 --> 0.046980).  Saving model ...
	 Train_Loss: 0.0500 Train_Acc: 98.864 Val_Loss: 0.0470  BEST VAL Loss: 0.0470  Val_Acc: 98.962

Epoch 12: Validation loss decreased (0.046980 --> 0.046656).  Saving model ...
	 Train_Loss: 0.0484 Train_Acc: 98.922 Val_Loss: 0.0467  BEST VAL Loss: 0.0467  Val_Acc: 98.750

Epoch 13: Validation loss decreased (0.046656 --> 0.045982).  Saving model ...
	 Train_Loss: 0.0472 Train_Acc: 98.921 Val_Loss: 0.0460  BEST VAL Loss: 0.0460  Val_Acc: 98.942

Epoch 14: Validation loss decreased (0.045982 --> 0.045682).  Saving model ...
	 Train_Loss: 0.0459 Train_Acc: 98.959 Val_Loss: 0.0457  BEST VAL Loss: 0.0457  Val_Acc: 99.018

Epoch 15: Validation loss decreased (0.045682 --> 0.045560).  Saving model ...
	 Train_Loss: 0.0449 Train_Acc: 98.953 Val_Loss: 0.0456  BEST VAL Loss: 0.0456  Val_Acc: 98.902

Epoch 16: Validation loss decreased (0.045560 --> 0.045294).  Saving model ...
	 Train_Loss: 0.0438 Train_Acc: 99.054 Val_Loss: 0.0453  BEST VAL Loss: 0.0453  Val_Acc: 98.912

Epoch 17: Validation loss decreased (0.045294 --> 0.044851).  Saving model ...
	 Train_Loss: 0.0429 Train_Acc: 98.983 Val_Loss: 0.0449  BEST VAL Loss: 0.0449  Val_Acc: 98.942

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.0421 Train_Acc: 98.963 Val_Loss: 0.0452  BEST VAL Loss: 0.0449  Val_Acc: 98.770

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.0414 Train_Acc: 98.998 Val_Loss: 0.0449  BEST VAL Loss: 0.0449  Val_Acc: 98.978

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.0407 Train_Acc: 99.069 Val_Loss: 0.0449  BEST VAL Loss: 0.0449  Val_Acc: 98.983

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.0400 Train_Acc: 99.084 Val_Loss: 0.0450  BEST VAL Loss: 0.0449  Val_Acc: 98.957

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.0393 Train_Acc: 99.092 Val_Loss: 0.0451  BEST VAL Loss: 0.0449  Val_Acc: 98.978

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0388 Train_Acc: 99.077 Val_Loss: 0.0452  BEST VAL Loss: 0.0449  Val_Acc: 98.810

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.0383 Train_Acc: 99.074 Val_Loss: 0.0455  BEST VAL Loss: 0.0449  Val_Acc: 98.907

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.0377 Train_Acc: 99.144 Val_Loss: 0.0457  BEST VAL Loss: 0.0449  Val_Acc: 98.871

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.0371 Train_Acc: 99.197 Val_Loss: 0.0458  BEST VAL Loss: 0.0449  Val_Acc: 99.038

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.0366 Train_Acc: 99.125 Val_Loss: 0.0458  BEST VAL Loss: 0.0449  Val_Acc: 99.028

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.0362 Train_Acc: 99.135 Val_Loss: 0.0460  BEST VAL Loss: 0.0449  Val_Acc: 98.907

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.0358 Train_Acc: 99.171 Val_Loss: 0.0465  BEST VAL Loss: 0.0449  Val_Acc: 98.775

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0354 Train_Acc: 99.152 Val_Loss: 0.0467  BEST VAL Loss: 0.0449  Val_Acc: 98.932

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.0350 Train_Acc: 99.150 Val_Loss: 0.0466  BEST VAL Loss: 0.0449  Val_Acc: 98.937

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0346 Train_Acc: 99.179 Val_Loss: 0.0466  BEST VAL Loss: 0.0449  Val_Acc: 98.891

Epoch 33: Validation loss did not decrease
Early stopped at epoch : 33
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.99      0.99     56122
           1       0.99      1.00      1.00    101921

    accuracy                           0.99    158043
   macro avg       0.99      0.99      0.99    158043
weighted avg       0.99      0.99      0.99    158043

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      7015
           1       0.99      0.99      0.99     12741

    accuracy                           0.99     19756
   macro avg       0.99      0.99      0.99     19756
weighted avg       0.99      0.99      0.99     19756

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      7016
           1       0.99      0.99      0.99     12740

    accuracy                           0.99     19756
   macro avg       0.99      0.99      0.99     19756
weighted avg       0.99      0.99      0.99     19756

              precision    recall  f1-score   support

           0       0.99      0.98      0.99      7016
           1       0.99      0.99      0.99     12740

    accuracy                           0.99     19756
   macro avg       0.99      0.99      0.99     19756
weighted avg       0.99      0.99      0.99     19756

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.82      0.86      0.84     34394
           1       0.89      0.87      0.88     46859

    accuracy                           0.86     81253
   macro avg       0.86      0.86      0.86     81253
weighted avg       0.86      0.86      0.86     81253

              precision    recall  f1-score   support

           0       0.82      0.86      0.84     34394
           1       0.89      0.87      0.88     46859

    accuracy                           0.86     81253
   macro avg       0.86      0.86      0.86     81253
weighted avg       0.86      0.86      0.86     81253

completed
