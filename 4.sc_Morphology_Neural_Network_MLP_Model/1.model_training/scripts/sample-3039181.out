[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '74d9bf98'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '6449f8ed'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '524e7de5'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'b582f042'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (27614, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'M20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'M16' 'M17' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.118992).  Saving model ...
	 Train_Loss: 0.3514 Train_Acc: 84.218 Val_Loss: 0.1190  BEST VAL Loss: 0.1190  Val_Acc: 94.605

Epoch 1: Validation loss decreased (0.118992 --> 0.098757).  Saving model ...
	 Train_Loss: 0.2624 Train_Acc: 94.265 Val_Loss: 0.0988  BEST VAL Loss: 0.0988  Val_Acc: 96.322

Epoch 2: Validation loss decreased (0.098757 --> 0.089385).  Saving model ...
	 Train_Loss: 0.2172 Train_Acc: 95.774 Val_Loss: 0.0894  BEST VAL Loss: 0.0894  Val_Acc: 97.057

Epoch 3: Validation loss decreased (0.089385 --> 0.085448).  Saving model ...
	 Train_Loss: 0.1910 Train_Acc: 96.271 Val_Loss: 0.0854  BEST VAL Loss: 0.0854  Val_Acc: 96.959

Epoch 4: Validation loss decreased (0.085448 --> 0.081382).  Saving model ...
	 Train_Loss: 0.1722 Train_Acc: 96.718 Val_Loss: 0.0814  BEST VAL Loss: 0.0814  Val_Acc: 97.254

Epoch 5: Validation loss did not decrease
	 Train_Loss: 0.1581 Train_Acc: 96.866 Val_Loss: 0.0819  BEST VAL Loss: 0.0814  Val_Acc: 97.008

Epoch 6: Validation loss decreased (0.081382 --> 0.080526).  Saving model ...
	 Train_Loss: 0.1484 Train_Acc: 96.675 Val_Loss: 0.0805  BEST VAL Loss: 0.0805  Val_Acc: 97.548

Epoch 7: Validation loss did not decrease
	 Train_Loss: 0.1394 Train_Acc: 97.350 Val_Loss: 0.0818  BEST VAL Loss: 0.0805  Val_Acc: 97.401

Epoch 8: Validation loss decreased (0.080526 --> 0.079751).  Saving model ...
	 Train_Loss: 0.1320 Train_Acc: 97.405 Val_Loss: 0.0798  BEST VAL Loss: 0.0798  Val_Acc: 97.205

Epoch 9: Validation loss decreased (0.079751 --> 0.077361).  Saving model ...
	 Train_Loss: 0.1255 Train_Acc: 97.847 Val_Loss: 0.0774  BEST VAL Loss: 0.0774  Val_Acc: 97.499

Epoch 10: Validation loss did not decrease
	 Train_Loss: 0.1203 Train_Acc: 97.694 Val_Loss: 0.0791  BEST VAL Loss: 0.0774  Val_Acc: 97.499

Epoch 11: Validation loss did not decrease
	 Train_Loss: 0.1155 Train_Acc: 97.872 Val_Loss: 0.0779  BEST VAL Loss: 0.0774  Val_Acc: 97.744

Epoch 12: Validation loss did not decrease
	 Train_Loss: 0.1115 Train_Acc: 97.951 Val_Loss: 0.0787  BEST VAL Loss: 0.0774  Val_Acc: 97.695

Epoch 13: Validation loss did not decrease
	 Train_Loss: 0.1073 Train_Acc: 98.191 Val_Loss: 0.0810  BEST VAL Loss: 0.0774  Val_Acc: 97.891

Epoch 14: Validation loss did not decrease
	 Train_Loss: 0.1040 Train_Acc: 98.148 Val_Loss: 0.0807  BEST VAL Loss: 0.0774  Val_Acc: 98.087

Epoch 15: Validation loss did not decrease
	 Train_Loss: 0.1009 Train_Acc: 98.301 Val_Loss: 0.0804  BEST VAL Loss: 0.0774  Val_Acc: 97.842

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.0978 Train_Acc: 98.442 Val_Loss: 0.0802  BEST VAL Loss: 0.0774  Val_Acc: 97.842

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.0950 Train_Acc: 98.467 Val_Loss: 0.0812  BEST VAL Loss: 0.0774  Val_Acc: 97.891

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.0925 Train_Acc: 98.375 Val_Loss: 0.0825  BEST VAL Loss: 0.0774  Val_Acc: 97.744

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.0906 Train_Acc: 98.203 Val_Loss: 0.0824  BEST VAL Loss: 0.0774  Val_Acc: 97.793

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.0885 Train_Acc: 98.473 Val_Loss: 0.0840  BEST VAL Loss: 0.0774  Val_Acc: 97.891

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.0866 Train_Acc: 98.368 Val_Loss: 0.0835  BEST VAL Loss: 0.0774  Val_Acc: 98.087

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.0850 Train_Acc: 98.467 Val_Loss: 0.0841  BEST VAL Loss: 0.0774  Val_Acc: 97.940

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0833 Train_Acc: 98.571 Val_Loss: 0.0841  BEST VAL Loss: 0.0774  Val_Acc: 97.548

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.0820 Train_Acc: 98.454 Val_Loss: 0.0838  BEST VAL Loss: 0.0774  Val_Acc: 97.842

Epoch 25: Validation loss did not decrease
Early stopped at epoch : 25
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.99      0.99      8453
           1       0.99      0.99      0.99      7850

    accuracy                           0.99     16303
   macro avg       0.99      0.99      0.99     16303
weighted avg       0.99      0.99      0.99     16303

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1057
           1       0.97      0.97      0.97       982

    accuracy                           0.97      2039
   macro avg       0.97      0.97      0.97      2039
weighted avg       0.97      0.97      0.97      2039

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1057
           1       0.97      0.97      0.97       982

    accuracy                           0.97      2039
   macro avg       0.97      0.97      0.97      2039
weighted avg       0.97      0.97      0.97      2039

              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1057
           1       0.97      0.97      0.97       982

    accuracy                           0.97      2039
   macro avg       0.97      0.97      0.97      2039
weighted avg       0.97      0.97      0.97      2039

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      3835
           1       0.98      0.99      0.98      3398

    accuracy                           0.98      7233
   macro avg       0.98      0.99      0.98      7233
weighted avg       0.98      0.98      0.98      7233

              precision    recall  f1-score   support

           0       0.99      0.98      0.99      3835
           1       0.98      0.99      0.98      3398

    accuracy                           0.98      7233
   macro avg       0.98      0.99      0.98      7233
weighted avg       0.98      0.98      0.98      7233

completed
