[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '22f4fba1'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '6a073420'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '5dc0c0a7'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '56e4bfa5'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_1.0_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (29870, 1276)
Number of total missing values across all columns: 0
Data Subset Is Off
Wells held out for testing: ['L16' 'K20']
Wells to use for training, validation, and testing ['K16' 'K17' 'L17' 'L20' 'K21' 'L21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.357703).  Saving model ...
	 Train_Loss: 0.5040 Train_Acc: 73.294 Val_Loss: 0.3577  BEST VAL Loss: 0.3577  Val_Acc: 84.560

Epoch 1: Validation loss decreased (0.357703 --> 0.328031).  Saving model ...
	 Train_Loss: 0.4431 Train_Acc: 79.554 Val_Loss: 0.3280  BEST VAL Loss: 0.3280  Val_Acc: 87.946

Epoch 2: Validation loss decreased (0.328031 --> 0.300004).  Saving model ...
	 Train_Loss: 0.4021 Train_Acc: 87.175 Val_Loss: 0.3000  BEST VAL Loss: 0.3000  Val_Acc: 89.977

Epoch 3: Validation loss decreased (0.300004 --> 0.282989).  Saving model ...
	 Train_Loss: 0.3705 Train_Acc: 88.874 Val_Loss: 0.2830  BEST VAL Loss: 0.2830  Val_Acc: 90.880

Epoch 4: Validation loss decreased (0.282989 --> 0.271212).  Saving model ...
	 Train_Loss: 0.3438 Train_Acc: 90.234 Val_Loss: 0.2712  BEST VAL Loss: 0.2712  Val_Acc: 91.106

Epoch 5: Validation loss decreased (0.271212 --> 0.260056).  Saving model ...
	 Train_Loss: 0.3240 Train_Acc: 90.793 Val_Loss: 0.2601  BEST VAL Loss: 0.2601  Val_Acc: 91.874

Epoch 6: Validation loss decreased (0.260056 --> 0.252862).  Saving model ...
	 Train_Loss: 0.3073 Train_Acc: 91.493 Val_Loss: 0.2529  BEST VAL Loss: 0.2529  Val_Acc: 92.460

Epoch 7: Validation loss decreased (0.252862 --> 0.244586).  Saving model ...
	 Train_Loss: 0.2947 Train_Acc: 91.657 Val_Loss: 0.2446  BEST VAL Loss: 0.2446  Val_Acc: 93.093

Epoch 8: Validation loss decreased (0.244586 --> 0.238525).  Saving model ...
	 Train_Loss: 0.2838 Train_Acc: 92.283 Val_Loss: 0.2385  BEST VAL Loss: 0.2385  Val_Acc: 93.454

Epoch 9: Validation loss decreased (0.238525 --> 0.232614).  Saving model ...
	 Train_Loss: 0.2740 Train_Acc: 92.554 Val_Loss: 0.2326  BEST VAL Loss: 0.2326  Val_Acc: 93.454

Epoch 10: Validation loss decreased (0.232614 --> 0.226816).  Saving model ...
	 Train_Loss: 0.2641 Train_Acc: 93.395 Val_Loss: 0.2268  BEST VAL Loss: 0.2268  Val_Acc: 93.725

Epoch 11: Validation loss decreased (0.226816 --> 0.224036).  Saving model ...
	 Train_Loss: 0.2550 Train_Acc: 93.497 Val_Loss: 0.2240  BEST VAL Loss: 0.2240  Val_Acc: 93.454

Epoch 12: Validation loss decreased (0.224036 --> 0.220708).  Saving model ...
	 Train_Loss: 0.2470 Train_Acc: 93.943 Val_Loss: 0.2207  BEST VAL Loss: 0.2207  Val_Acc: 92.957

Epoch 13: Validation loss decreased (0.220708 --> 0.218658).  Saving model ...
	 Train_Loss: 0.2399 Train_Acc: 94.095 Val_Loss: 0.2187  BEST VAL Loss: 0.2187  Val_Acc: 93.499

Epoch 14: Validation loss decreased (0.218658 --> 0.216182).  Saving model ...
	 Train_Loss: 0.2339 Train_Acc: 94.022 Val_Loss: 0.2162  BEST VAL Loss: 0.2162  Val_Acc: 93.725

Epoch 15: Validation loss decreased (0.216182 --> 0.213411).  Saving model ...
	 Train_Loss: 0.2280 Train_Acc: 94.349 Val_Loss: 0.2134  BEST VAL Loss: 0.2134  Val_Acc: 94.582

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.2228 Train_Acc: 94.299 Val_Loss: 0.2142  BEST VAL Loss: 0.2134  Val_Acc: 93.499

Epoch 17: Validation loss decreased (0.213411 --> 0.212780).  Saving model ...
	 Train_Loss: 0.2178 Train_Acc: 94.677 Val_Loss: 0.2128  BEST VAL Loss: 0.2128  Val_Acc: 94.673

Epoch 18: Validation loss decreased (0.212780 --> 0.210720).  Saving model ...
	 Train_Loss: 0.2132 Train_Acc: 94.615 Val_Loss: 0.2107  BEST VAL Loss: 0.2107  Val_Acc: 94.176

Epoch 19: Validation loss decreased (0.210720 --> 0.209212).  Saving model ...
	 Train_Loss: 0.2090 Train_Acc: 94.609 Val_Loss: 0.2092  BEST VAL Loss: 0.2092  Val_Acc: 94.763

Epoch 20: Validation loss decreased (0.209212 --> 0.208658).  Saving model ...
	 Train_Loss: 0.2045 Train_Acc: 95.478 Val_Loss: 0.2087  BEST VAL Loss: 0.2087  Val_Acc: 94.537

Epoch 21: Validation loss decreased (0.208658 --> 0.208465).  Saving model ...
	 Train_Loss: 0.2007 Train_Acc: 95.106 Val_Loss: 0.2085  BEST VAL Loss: 0.2085  Val_Acc: 94.537

Epoch 22: Validation loss decreased (0.208465 --> 0.207612).  Saving model ...
	 Train_Loss: 0.1971 Train_Acc: 95.061 Val_Loss: 0.2076  BEST VAL Loss: 0.2076  Val_Acc: 94.763

Epoch 23: Validation loss decreased (0.207612 --> 0.206117).  Saving model ...
	 Train_Loss: 0.1939 Train_Acc: 95.083 Val_Loss: 0.2061  BEST VAL Loss: 0.2061  Val_Acc: 94.718

Epoch 24: Validation loss decreased (0.206117 --> 0.204485).  Saving model ...
	 Train_Loss: 0.1908 Train_Acc: 95.450 Val_Loss: 0.2045  BEST VAL Loss: 0.2045  Val_Acc: 94.402

Epoch 25: Validation loss decreased (0.204485 --> 0.203734).  Saving model ...
	 Train_Loss: 0.1878 Train_Acc: 95.399 Val_Loss: 0.2037  BEST VAL Loss: 0.2037  Val_Acc: 94.357

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1848 Train_Acc: 95.518 Val_Loss: 0.2038  BEST VAL Loss: 0.2037  Val_Acc: 94.537

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1820 Train_Acc: 95.653 Val_Loss: 0.2039  BEST VAL Loss: 0.2037  Val_Acc: 94.628

Epoch 28: Validation loss decreased (0.203734 --> 0.203373).  Saving model ...
	 Train_Loss: 0.1794 Train_Acc: 95.806 Val_Loss: 0.2034  BEST VAL Loss: 0.2034  Val_Acc: 94.808

Epoch 29: Validation loss decreased (0.203373 --> 0.203299).  Saving model ...
	 Train_Loss: 0.1770 Train_Acc: 95.738 Val_Loss: 0.2033  BEST VAL Loss: 0.2033  Val_Acc: 95.169

Epoch 30: Validation loss decreased (0.203299 --> 0.203115).  Saving model ...
	 Train_Loss: 0.1746 Train_Acc: 95.851 Val_Loss: 0.2031  BEST VAL Loss: 0.2031  Val_Acc: 94.537

Epoch 31: Validation loss decreased (0.203115 --> 0.202192).  Saving model ...
	 Train_Loss: 0.1727 Train_Acc: 95.236 Val_Loss: 0.2022  BEST VAL Loss: 0.2022  Val_Acc: 94.944

Epoch 32: Validation loss decreased (0.202192 --> 0.201738).  Saving model ...
	 Train_Loss: 0.1705 Train_Acc: 95.890 Val_Loss: 0.2017  BEST VAL Loss: 0.2017  Val_Acc: 94.537

Epoch 33: Validation loss decreased (0.201738 --> 0.201514).  Saving model ...
	 Train_Loss: 0.1686 Train_Acc: 95.806 Val_Loss: 0.2015  BEST VAL Loss: 0.2015  Val_Acc: 95.260

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1665 Train_Acc: 96.105 Val_Loss: 0.2017  BEST VAL Loss: 0.2015  Val_Acc: 95.079

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1646 Train_Acc: 96.105 Val_Loss: 0.2022  BEST VAL Loss: 0.2015  Val_Acc: 94.853

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1631 Train_Acc: 95.738 Val_Loss: 0.2029  BEST VAL Loss: 0.2015  Val_Acc: 94.221

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1615 Train_Acc: 95.913 Val_Loss: 0.2028  BEST VAL Loss: 0.2015  Val_Acc: 94.537

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1598 Train_Acc: 96.133 Val_Loss: 0.2033  BEST VAL Loss: 0.2015  Val_Acc: 94.718

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1582 Train_Acc: 96.218 Val_Loss: 0.2032  BEST VAL Loss: 0.2015  Val_Acc: 94.808

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1567 Train_Acc: 96.015 Val_Loss: 0.2033  BEST VAL Loss: 0.2015  Val_Acc: 94.673

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1551 Train_Acc: 96.274 Val_Loss: 0.2041  BEST VAL Loss: 0.2015  Val_Acc: 94.853

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1537 Train_Acc: 96.257 Val_Loss: 0.2055  BEST VAL Loss: 0.2015  Val_Acc: 94.447

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1523 Train_Acc: 96.455 Val_Loss: 0.2057  BEST VAL Loss: 0.2015  Val_Acc: 94.944

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1509 Train_Acc: 96.432 Val_Loss: 0.2057  BEST VAL Loss: 0.2015  Val_Acc: 94.808

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.1495 Train_Acc: 96.540 Val_Loss: 0.2060  BEST VAL Loss: 0.2015  Val_Acc: 94.944

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1483 Train_Acc: 96.077 Val_Loss: 0.2066  BEST VAL Loss: 0.2015  Val_Acc: 94.718

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1472 Train_Acc: 96.314 Val_Loss: 0.2079  BEST VAL Loss: 0.2015  Val_Acc: 94.492

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1463 Train_Acc: 95.783 Val_Loss: 0.2087  BEST VAL Loss: 0.2015  Val_Acc: 94.086

Epoch 49: Validation loss did not decrease
Early stopped at epoch : 49
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.97      0.98      9777
           1       0.97      0.99      0.98      7938

    accuracy                           0.98     17715
   macro avg       0.98      0.98      0.98     17715
weighted avg       0.98      0.98      0.98     17715

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.95      0.96      1222
           1       0.94      0.95      0.95       993

    accuracy                           0.95      2215
   macro avg       0.95      0.95      0.95      2215
weighted avg       0.95      0.95      0.95      2215

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.95      0.95      1223
           1       0.94      0.94      0.94       992

    accuracy                           0.95      2215
   macro avg       0.95      0.95      0.95      2215
weighted avg       0.95      0.95      0.95      2215

              precision    recall  f1-score   support

           0       0.95      0.95      0.95      1223
           1       0.94      0.94      0.94       992

    accuracy                           0.95      2215
   macro avg       0.95      0.95      0.95      2215
weighted avg       0.95      0.95      0.95      2215

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.81      0.92      0.86      3996
           1       0.90      0.77      0.83      3729

    accuracy                           0.85      7725
   macro avg       0.86      0.85      0.85      7725
weighted avg       0.86      0.85      0.85      7725

              precision    recall  f1-score   support

           0       0.81      0.92      0.86      3996
           1       0.90      0.77      0.83      3729

    accuracy                           0.85      7725
   macro avg       0.86      0.85      0.85      7725
weighted avg       0.86      0.85      0.85      7725

completed
