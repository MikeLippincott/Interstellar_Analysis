[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18688 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Writing 40207 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:252: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:279: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:583: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:597: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:657: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:866: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:868: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:871: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:902: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:944: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1131: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1134: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1211: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1384: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1386: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1389: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1466: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
SHSY5Y MultiClass_MLP False
[0.954878893196544, 0.4803479499984947, 0.5647731568049614]
Data Subset Is Off
(156696,) (39174,) (207575,) 403445     35856
403446     35857
403447     35858
403448     35859
403449     35860
           ...  
446996    587354
446997    587355
446998    587356
446999    587357
447000    587358
Name: labeled_data_index, Length: 43556, dtype: int64 (150901,)
(156696,) (39174,) (207575,) 403445     35856
403446     35857
403447     35858
403448     35859
403449     35860
           ...  
446996    587354
446997    587355
446998    587356
446999    587357
447000    587358
Name: labeled_data_index, Length: 43556, dtype: int64 (150901,)
597902
(3946,) (89086,) (63664,)
(987,) (22272,) (15915,)
(4933,) (111361,) (91281,)
(10064,) (10941,) (22551,)
(7048,) (77041,) (66812,)
(156696, 1251) (39174, 1251) (207575, 1251) (43556, 1251) (150901, 1251)
(156696,) (39174,) (207575,) (43556,) (150901,)
Number of in features:  1251
Number of out features:  3
Multi_Class
SGD
Epoch 0: Validation loss decreased (inf --> 0.561905).  Saving model ...
	 Train_Loss: 0.6115 Train_Acc: 71.002 Val_Loss: 0.5619  BEST VAL Loss: 0.5619  Val_Acc: 73.434

Epoch 1: Validation loss decreased (0.561905 --> 0.554324).  Saving model ...
	 Train_Loss: 0.5869 Train_Acc: 72.991 Val_Loss: 0.5543  BEST VAL Loss: 0.5543  Val_Acc: 75.057

Epoch 2: Validation loss decreased (0.554324 --> 0.549288).  Saving model ...
	 Train_Loss: 0.5720 Train_Acc: 73.950 Val_Loss: 0.5493  BEST VAL Loss: 0.5493  Val_Acc: 73.855

Epoch 3: Validation loss decreased (0.549288 --> 0.546721).  Saving model ...
	 Train_Loss: 0.5615 Train_Acc: 74.532 Val_Loss: 0.5467  BEST VAL Loss: 0.5467  Val_Acc: 73.237

Epoch 4: Validation loss decreased (0.546721 --> 0.544468).  Saving model ...
	 Train_Loss: 0.5531 Train_Acc: 74.978 Val_Loss: 0.5445  BEST VAL Loss: 0.5445  Val_Acc: 73.551

Epoch 5: Validation loss decreased (0.544468 --> 0.543487).  Saving model ...
	 Train_Loss: 0.5459 Train_Acc: 75.383 Val_Loss: 0.5435  BEST VAL Loss: 0.5435  Val_Acc: 74.151

Epoch 6: Validation loss decreased (0.543487 --> 0.541612).  Saving model ...
	 Train_Loss: 0.5397 Train_Acc: 75.815 Val_Loss: 0.5416  BEST VAL Loss: 0.5416  Val_Acc: 75.627

Epoch 7: Validation loss decreased (0.541612 --> 0.540025).  Saving model ...
	 Train_Loss: 0.5340 Train_Acc: 76.100 Val_Loss: 0.5400  BEST VAL Loss: 0.5400  Val_Acc: 74.675

Epoch 8: Validation loss decreased (0.540025 --> 0.538662).  Saving model ...
	 Train_Loss: 0.5292 Train_Acc: 76.365 Val_Loss: 0.5387  BEST VAL Loss: 0.5387  Val_Acc: 75.126

Epoch 9: Validation loss decreased (0.538662 --> 0.537220).  Saving model ...
	 Train_Loss: 0.5247 Train_Acc: 76.640 Val_Loss: 0.5372  BEST VAL Loss: 0.5372  Val_Acc: 76.025

Epoch 10: Validation loss decreased (0.537220 --> 0.536576).  Saving model ...
	 Train_Loss: 0.5204 Train_Acc: 77.079 Val_Loss: 0.5366  BEST VAL Loss: 0.5366  Val_Acc: 75.037

Epoch 11: Validation loss did not decrease
	 Train_Loss: 0.5167 Train_Acc: 77.278 Val_Loss: 0.5371  BEST VAL Loss: 0.5366  Val_Acc: 75.619

Epoch 12: Validation loss decreased (0.536576 --> 0.536358).  Saving model ...
	 Train_Loss: 0.5131 Train_Acc: 77.578 Val_Loss: 0.5364  BEST VAL Loss: 0.5364  Val_Acc: 76.535

Epoch 13: Validation loss decreased (0.536358 --> 0.535264).  Saving model ...
	 Train_Loss: 0.5098 Train_Acc: 77.664 Val_Loss: 0.5353  BEST VAL Loss: 0.5353  Val_Acc: 75.333

Epoch 14: Validation loss decreased (0.535264 --> 0.534294).  Saving model ...
	 Train_Loss: 0.5068 Train_Acc: 77.841 Val_Loss: 0.5343  BEST VAL Loss: 0.5343  Val_Acc: 76.211

Epoch 15: Validation loss decreased (0.534294 --> 0.533532).  Saving model ...
	 Train_Loss: 0.5039 Train_Acc: 77.869 Val_Loss: 0.5335  BEST VAL Loss: 0.5335  Val_Acc: 75.928

Epoch 16: Validation loss decreased (0.533532 --> 0.533514).  Saving model ...
	 Train_Loss: 0.5011 Train_Acc: 78.110 Val_Loss: 0.5335  BEST VAL Loss: 0.5335  Val_Acc: 75.553

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.4984 Train_Acc: 78.497 Val_Loss: 0.5339  BEST VAL Loss: 0.5335  Val_Acc: 74.304

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.4959 Train_Acc: 78.485 Val_Loss: 0.5335  BEST VAL Loss: 0.5335  Val_Acc: 76.622

Epoch 19: Validation loss decreased (0.533514 --> 0.533337).  Saving model ...
	 Train_Loss: 0.4935 Train_Acc: 78.627 Val_Loss: 0.5333  BEST VAL Loss: 0.5333  Val_Acc: 76.283

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.4911 Train_Acc: 78.826 Val_Loss: 0.5337  BEST VAL Loss: 0.5333  Val_Acc: 76.482

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.4889 Train_Acc: 78.885 Val_Loss: 0.5336  BEST VAL Loss: 0.5333  Val_Acc: 76.505

Epoch 22: Validation loss decreased (0.533337 --> 0.533050).  Saving model ...
	 Train_Loss: 0.4868 Train_Acc: 78.951 Val_Loss: 0.5330  BEST VAL Loss: 0.5330  Val_Acc: 75.844

Epoch 23: Validation loss decreased (0.533050 --> 0.532840).  Saving model ...
	 Train_Loss: 0.4847 Train_Acc: 79.186 Val_Loss: 0.5328  BEST VAL Loss: 0.5328  Val_Acc: 76.602

Epoch 24: Validation loss decreased (0.532840 --> 0.532564).  Saving model ...
	 Train_Loss: 0.4828 Train_Acc: 79.256 Val_Loss: 0.5326  BEST VAL Loss: 0.5326  Val_Acc: 76.847

Epoch 25: Validation loss decreased (0.532564 --> 0.532153).  Saving model ...
	 Train_Loss: 0.4809 Train_Acc: 79.368 Val_Loss: 0.5322  BEST VAL Loss: 0.5322  Val_Acc: 76.436

Epoch 26: Validation loss decreased (0.532153 --> 0.532058).  Saving model ...
	 Train_Loss: 0.4791 Train_Acc: 79.530 Val_Loss: 0.5321  BEST VAL Loss: 0.5321  Val_Acc: 76.303

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.4773 Train_Acc: 79.617 Val_Loss: 0.5323  BEST VAL Loss: 0.5321  Val_Acc: 76.587

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.4756 Train_Acc: 79.670 Val_Loss: 0.5325  BEST VAL Loss: 0.5321  Val_Acc: 75.808

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.4739 Train_Acc: 79.838 Val_Loss: 0.5327  BEST VAL Loss: 0.5321  Val_Acc: 75.856

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.4723 Train_Acc: 79.977 Val_Loss: 0.5330  BEST VAL Loss: 0.5321  Val_Acc: 75.091

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.4707 Train_Acc: 80.104 Val_Loss: 0.5331  BEST VAL Loss: 0.5321  Val_Acc: 75.874

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.4692 Train_Acc: 80.217 Val_Loss: 0.5335  BEST VAL Loss: 0.5321  Val_Acc: 76.474

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.4677 Train_Acc: 80.243 Val_Loss: 0.5339  BEST VAL Loss: 0.5321  Val_Acc: 75.175

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.4662 Train_Acc: 80.347 Val_Loss: 0.5341  BEST VAL Loss: 0.5321  Val_Acc: 76.163

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.4648 Train_Acc: 80.309 Val_Loss: 0.5340  BEST VAL Loss: 0.5321  Val_Acc: 76.170

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.4634 Train_Acc: 80.620 Val_Loss: 0.5344  BEST VAL Loss: 0.5321  Val_Acc: 76.696

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.4621 Train_Acc: 80.504 Val_Loss: 0.5349  BEST VAL Loss: 0.5321  Val_Acc: 75.101

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.4608 Train_Acc: 80.633 Val_Loss: 0.5354  BEST VAL Loss: 0.5321  Val_Acc: 75.272

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.4595 Train_Acc: 80.637 Val_Loss: 0.5356  BEST VAL Loss: 0.5321  Val_Acc: 76.811

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.4582 Train_Acc: 80.834 Val_Loss: 0.5358  BEST VAL Loss: 0.5321  Val_Acc: 76.079

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.4570 Train_Acc: 80.930 Val_Loss: 0.5363  BEST VAL Loss: 0.5321  Val_Acc: 76.615

Epoch 42: Validation loss did not decrease
Early stopped at epoch : 42
MultiClass_MLP
              precision    recall  f1-score   support

           0       0.03      0.03      0.03      3946
           1       0.57      0.59      0.58     89086
           2       0.41      0.38      0.39     63664

    accuracy                           0.49    156696
   macro avg       0.33      0.33      0.33    156696
weighted avg       0.49      0.49      0.49    156696

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.79      0.77      0.78       987
           1       0.78      0.81      0.80     22272
           2       0.73      0.69      0.71     15915

    accuracy                           0.76     39174
   macro avg       0.77      0.76      0.76     39174
weighted avg       0.76      0.76      0.76     39174

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.79      0.77      0.78      4933
           1       0.75      0.82      0.78    111361
           2       0.75      0.67      0.71     91281

    accuracy                           0.75    207575
   macro avg       0.76      0.75      0.76    207575
weighted avg       0.75      0.75      0.75    207575

Precision for class 0: 0.7865979381443299
Recall for class 0: 0.7733630650719643
Precision for class 1: 0.7499897115107618
Recall for class 1: 0.8182397787376191
Precision for class 2: 0.7537116828757848
Recall for class 2: 0.6707200841358004
3
              precision    recall  f1-score   support

           0       0.79      0.77      0.78      4933
           1       0.75      0.82      0.78    111361
           2       0.75      0.67      0.71     91281

    accuracy                           0.75    207575
   macro avg       0.76      0.75      0.76    207575
weighted avg       0.75      0.75      0.75    207575

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.96      0.41      0.57     10064
           1       0.37      0.66      0.47     10941
           2       0.73      0.64      0.68     22551

    accuracy                           0.59     43556
   macro avg       0.68      0.57      0.57     43556
weighted avg       0.69      0.59      0.60     43556

Precision for class 0: 0.9572429906542057
Recall for class 0: 0.40709459459459457
Precision for class 1: 0.3672215108834827
Recall for class 1: 0.655333150534686
Precision for class 2: 0.7278618804111183
Recall for class 2: 0.6374883597179726
3
              precision    recall  f1-score   support

           0       0.96      0.41      0.57     10064
           1       0.37      0.66      0.47     10941
           2       0.73      0.64      0.68     22551

    accuracy                           0.59     43556
   macro avg       0.68      0.57      0.57     43556
weighted avg       0.69      0.59      0.60     43556

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.86      0.58      0.69      7048
           1       0.73      0.80      0.76     77041
           2       0.74      0.68      0.71     66812

    accuracy                           0.74    150901
   macro avg       0.78      0.69      0.72    150901
weighted avg       0.74      0.74      0.74    150901

Precision for class 0: 0.8608585324592937
Recall for class 0: 0.57761066969353
Precision for class 1: 0.7282126856873379
Recall for class 1: 0.8017419296218896
Precision for class 2: 0.744735297952797
Recall for class 2: 0.683874154343531
3
              precision    recall  f1-score   support

           0       0.86      0.58      0.69      7048
           1       0.73      0.80      0.76     77041
           2       0.74      0.68      0.71     66812

    accuracy                           0.74    150901
   macro avg       0.78      0.69      0.72    150901
weighted avg       0.74      0.74      0.74    150901

Done
