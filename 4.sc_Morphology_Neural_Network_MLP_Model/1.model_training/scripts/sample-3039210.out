[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '9f3e272d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e3f8381a'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '0dd269cc'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '94ae1a3e'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Thapsigargin_10.000_DMSO_0.025']
The dimensions of the data are: (26978, 1276)
Number of total missing values across all columns: 53956
Data Subset Is Off
Wells held out for testing: ['E14' 'K14']
Wells to use for training, validation, and testing ['D14' 'D15' 'E15' 'L14' 'K15' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.701780).  Saving model ...
	 Train_Loss: 0.7150 Train_Acc: 48.688 Val_Loss: 0.7018  BEST VAL Loss: 0.7018  Val_Acc: 48.710

Epoch 1: Validation loss decreased (0.701780 --> 0.687289).  Saving model ...
	 Train_Loss: 0.7011 Train_Acc: 48.688 Val_Loss: 0.6873  BEST VAL Loss: 0.6873  Val_Acc: 48.710

Epoch 2: Validation loss decreased (0.687289 --> 0.675146).  Saving model ...
	 Train_Loss: 0.6903 Train_Acc: 59.555 Val_Loss: 0.6751  BEST VAL Loss: 0.6751  Val_Acc: 65.526

Epoch 3: Validation loss decreased (0.675146 --> 0.662663).  Saving model ...
	 Train_Loss: 0.6805 Train_Acc: 62.798 Val_Loss: 0.6627  BEST VAL Loss: 0.6627  Val_Acc: 68.849

Epoch 4: Validation loss decreased (0.662663 --> 0.655441).  Saving model ...
	 Train_Loss: 0.6719 Train_Acc: 64.392 Val_Loss: 0.6554  BEST VAL Loss: 0.6554  Val_Acc: 68.552

Epoch 5: Validation loss decreased (0.655441 --> 0.648250).  Saving model ...
	 Train_Loss: 0.6648 Train_Acc: 65.745 Val_Loss: 0.6482  BEST VAL Loss: 0.6482  Val_Acc: 69.048

Epoch 6: Validation loss decreased (0.648250 --> 0.638796).  Saving model ...
	 Train_Loss: 0.6585 Train_Acc: 67.010 Val_Loss: 0.6388  BEST VAL Loss: 0.6388  Val_Acc: 68.750

Epoch 7: Validation loss decreased (0.638796 --> 0.634222).  Saving model ...
	 Train_Loss: 0.6527 Train_Acc: 67.587 Val_Loss: 0.6342  BEST VAL Loss: 0.6342  Val_Acc: 68.353

Epoch 8: Validation loss decreased (0.634222 --> 0.627160).  Saving model ...
	 Train_Loss: 0.6470 Train_Acc: 68.653 Val_Loss: 0.6272  BEST VAL Loss: 0.6272  Val_Acc: 70.685

Epoch 9: Validation loss decreased (0.627160 --> 0.622305).  Saving model ...
	 Train_Loss: 0.6419 Train_Acc: 69.026 Val_Loss: 0.6223  BEST VAL Loss: 0.6223  Val_Acc: 71.478

Epoch 10: Validation loss decreased (0.622305 --> 0.617069).  Saving model ...
	 Train_Loss: 0.6374 Train_Acc: 69.664 Val_Loss: 0.6171  BEST VAL Loss: 0.6171  Val_Acc: 71.478

Epoch 11: Validation loss decreased (0.617069 --> 0.615312).  Saving model ...
	 Train_Loss: 0.6329 Train_Acc: 70.210 Val_Loss: 0.6153  BEST VAL Loss: 0.6153  Val_Acc: 71.131

Epoch 12: Validation loss decreased (0.615312 --> 0.614356).  Saving model ...
	 Train_Loss: 0.6286 Train_Acc: 70.812 Val_Loss: 0.6144  BEST VAL Loss: 0.6144  Val_Acc: 71.528

Epoch 13: Validation loss decreased (0.614356 --> 0.609938).  Saving model ...
	 Train_Loss: 0.6246 Train_Acc: 71.072 Val_Loss: 0.6099  BEST VAL Loss: 0.6099  Val_Acc: 72.718

Epoch 14: Validation loss decreased (0.609938 --> 0.608013).  Saving model ...
	 Train_Loss: 0.6209 Train_Acc: 71.779 Val_Loss: 0.6080  BEST VAL Loss: 0.6080  Val_Acc: 71.230

Epoch 15: Validation loss decreased (0.608013 --> 0.604469).  Saving model ...
	 Train_Loss: 0.6173 Train_Acc: 71.438 Val_Loss: 0.6045  BEST VAL Loss: 0.6045  Val_Acc: 73.661

Epoch 16: Validation loss decreased (0.604469 --> 0.601624).  Saving model ...
	 Train_Loss: 0.6138 Train_Acc: 73.082 Val_Loss: 0.6016  BEST VAL Loss: 0.6016  Val_Acc: 72.173

Epoch 17: Validation loss decreased (0.601624 --> 0.596846).  Saving model ...
	 Train_Loss: 0.6108 Train_Acc: 72.809 Val_Loss: 0.5968  BEST VAL Loss: 0.5968  Val_Acc: 72.173

Epoch 18: Validation loss decreased (0.596846 --> 0.594228).  Saving model ...
	 Train_Loss: 0.6081 Train_Acc: 73.039 Val_Loss: 0.5942  BEST VAL Loss: 0.5942  Val_Acc: 72.817

Epoch 19: Validation loss decreased (0.594228 --> 0.589708).  Saving model ...
	 Train_Loss: 0.6050 Train_Acc: 73.262 Val_Loss: 0.5897  BEST VAL Loss: 0.5897  Val_Acc: 73.313

Epoch 20: Validation loss decreased (0.589708 --> 0.588038).  Saving model ...
	 Train_Loss: 0.6022 Train_Acc: 73.566 Val_Loss: 0.5880  BEST VAL Loss: 0.5880  Val_Acc: 73.115

Epoch 21: Validation loss decreased (0.588038 --> 0.586638).  Saving model ...
	 Train_Loss: 0.5999 Train_Acc: 73.299 Val_Loss: 0.5866  BEST VAL Loss: 0.5866  Val_Acc: 73.562

Epoch 22: Validation loss decreased (0.586638 --> 0.586211).  Saving model ...
	 Train_Loss: 0.5974 Train_Acc: 73.863 Val_Loss: 0.5862  BEST VAL Loss: 0.5862  Val_Acc: 74.107

Epoch 23: Validation loss decreased (0.586211 --> 0.584857).  Saving model ...
	 Train_Loss: 0.5951 Train_Acc: 73.950 Val_Loss: 0.5849  BEST VAL Loss: 0.5849  Val_Acc: 74.901

Epoch 24: Validation loss decreased (0.584857 --> 0.584288).  Saving model ...
	 Train_Loss: 0.5928 Train_Acc: 74.291 Val_Loss: 0.5843  BEST VAL Loss: 0.5843  Val_Acc: 73.363

Epoch 25: Validation loss decreased (0.584288 --> 0.582084).  Saving model ...
	 Train_Loss: 0.5904 Train_Acc: 74.874 Val_Loss: 0.5821  BEST VAL Loss: 0.5821  Val_Acc: 71.974

Epoch 26: Validation loss decreased (0.582084 --> 0.578997).  Saving model ...
	 Train_Loss: 0.5879 Train_Acc: 75.091 Val_Loss: 0.5790  BEST VAL Loss: 0.5790  Val_Acc: 73.413

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.5856 Train_Acc: 75.222 Val_Loss: 0.5790  BEST VAL Loss: 0.5790  Val_Acc: 73.065

Epoch 28: Validation loss decreased (0.578997 --> 0.577838).  Saving model ...
	 Train_Loss: 0.5835 Train_Acc: 74.428 Val_Loss: 0.5778  BEST VAL Loss: 0.5778  Val_Acc: 73.264

Epoch 29: Validation loss decreased (0.577838 --> 0.577383).  Saving model ...
	 Train_Loss: 0.5816 Train_Acc: 75.067 Val_Loss: 0.5774  BEST VAL Loss: 0.5774  Val_Acc: 72.321

Epoch 30: Validation loss decreased (0.577383 --> 0.575252).  Saving model ...
	 Train_Loss: 0.5794 Train_Acc: 75.575 Val_Loss: 0.5753  BEST VAL Loss: 0.5753  Val_Acc: 72.470

Epoch 31: Validation loss decreased (0.575252 --> 0.574027).  Saving model ...
	 Train_Loss: 0.5773 Train_Acc: 75.321 Val_Loss: 0.5740  BEST VAL Loss: 0.5740  Val_Acc: 73.859

Epoch 32: Validation loss decreased (0.574027 --> 0.570791).  Saving model ...
	 Train_Loss: 0.5753 Train_Acc: 76.140 Val_Loss: 0.5708  BEST VAL Loss: 0.5708  Val_Acc: 73.165

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.5733 Train_Acc: 75.898 Val_Loss: 0.5710  BEST VAL Loss: 0.5708  Val_Acc: 74.008

Epoch 34: Validation loss decreased (0.570791 --> 0.569448).  Saving model ...
	 Train_Loss: 0.5715 Train_Acc: 75.898 Val_Loss: 0.5694  BEST VAL Loss: 0.5694  Val_Acc: 72.669

Epoch 35: Validation loss decreased (0.569448 --> 0.567859).  Saving model ...
	 Train_Loss: 0.5696 Train_Acc: 75.501 Val_Loss: 0.5679  BEST VAL Loss: 0.5679  Val_Acc: 73.512

Epoch 36: Validation loss decreased (0.567859 --> 0.567368).  Saving model ...
	 Train_Loss: 0.5682 Train_Acc: 75.327 Val_Loss: 0.5674  BEST VAL Loss: 0.5674  Val_Acc: 74.107

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.5665 Train_Acc: 75.278 Val_Loss: 0.5690  BEST VAL Loss: 0.5674  Val_Acc: 73.165

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.5648 Train_Acc: 75.966 Val_Loss: 0.5696  BEST VAL Loss: 0.5674  Val_Acc: 73.859

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.5633 Train_Acc: 76.493 Val_Loss: 0.5703  BEST VAL Loss: 0.5674  Val_Acc: 74.008

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.5621 Train_Acc: 75.910 Val_Loss: 0.5703  BEST VAL Loss: 0.5674  Val_Acc: 73.710

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.5608 Train_Acc: 75.730 Val_Loss: 0.5706  BEST VAL Loss: 0.5674  Val_Acc: 73.214

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.5593 Train_Acc: 76.332 Val_Loss: 0.5698  BEST VAL Loss: 0.5674  Val_Acc: 73.512

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.5579 Train_Acc: 76.127 Val_Loss: 0.5691  BEST VAL Loss: 0.5674  Val_Acc: 74.405

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.5567 Train_Acc: 75.470 Val_Loss: 0.5681  BEST VAL Loss: 0.5674  Val_Acc: 73.710

Epoch 45: Validation loss decreased (0.567368 --> 0.566988).  Saving model ...
	 Train_Loss: 0.5553 Train_Acc: 75.916 Val_Loss: 0.5670  BEST VAL Loss: 0.5670  Val_Acc: 74.256

Epoch 46: Validation loss decreased (0.566988 --> 0.566325).  Saving model ...
	 Train_Loss: 0.5540 Train_Acc: 76.561 Val_Loss: 0.5663  BEST VAL Loss: 0.5663  Val_Acc: 73.909

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.5527 Train_Acc: 76.729 Val_Loss: 0.5672  BEST VAL Loss: 0.5663  Val_Acc: 73.512

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.5515 Train_Acc: 76.164 Val_Loss: 0.5683  BEST VAL Loss: 0.5663  Val_Acc: 74.008

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.5502 Train_Acc: 76.586 Val_Loss: 0.5686  BEST VAL Loss: 0.5663  Val_Acc: 74.107

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.5490 Train_Acc: 76.462 Val_Loss: 0.5680  BEST VAL Loss: 0.5663  Val_Acc: 74.504

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.5480 Train_Acc: 76.313 Val_Loss: 0.5682  BEST VAL Loss: 0.5663  Val_Acc: 74.058

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.5469 Train_Acc: 76.512 Val_Loss: 0.5673  BEST VAL Loss: 0.5663  Val_Acc: 74.554

Epoch 53: Validation loss decreased (0.566325 --> 0.566260).  Saving model ...
	 Train_Loss: 0.5456 Train_Acc: 77.312 Val_Loss: 0.5663  BEST VAL Loss: 0.5663  Val_Acc: 74.107

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.5444 Train_Acc: 77.331 Val_Loss: 0.5664  BEST VAL Loss: 0.5663  Val_Acc: 74.653

Epoch 55: Validation loss decreased (0.566260 --> 0.565716).  Saving model ...
	 Train_Loss: 0.5431 Train_Acc: 77.293 Val_Loss: 0.5657  BEST VAL Loss: 0.5657  Val_Acc: 74.157

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.5419 Train_Acc: 76.989 Val_Loss: 0.5678  BEST VAL Loss: 0.5657  Val_Acc: 74.355

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.5410 Train_Acc: 77.393 Val_Loss: 0.5681  BEST VAL Loss: 0.5657  Val_Acc: 74.653

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.5401 Train_Acc: 76.264 Val_Loss: 0.5676  BEST VAL Loss: 0.5657  Val_Acc: 73.909

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.5392 Train_Acc: 76.772 Val_Loss: 0.5698  BEST VAL Loss: 0.5657  Val_Acc: 73.760

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.5382 Train_Acc: 77.275 Val_Loss: 0.5687  BEST VAL Loss: 0.5657  Val_Acc: 73.909

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.5373 Train_Acc: 77.120 Val_Loss: 0.5682  BEST VAL Loss: 0.5657  Val_Acc: 75.050

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.5365 Train_Acc: 76.965 Val_Loss: 0.5686  BEST VAL Loss: 0.5657  Val_Acc: 74.454

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.5358 Train_Acc: 76.859 Val_Loss: 0.5676  BEST VAL Loss: 0.5657  Val_Acc: 74.008

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.5349 Train_Acc: 77.126 Val_Loss: 0.5679  BEST VAL Loss: 0.5657  Val_Acc: 74.306

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.5340 Train_Acc: 77.231 Val_Loss: 0.5670  BEST VAL Loss: 0.5657  Val_Acc: 72.817

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.5330 Train_Acc: 77.951 Val_Loss: 0.5663  BEST VAL Loss: 0.5657  Val_Acc: 74.256

Epoch 67: Validation loss decreased (0.565716 --> 0.564602).  Saving model ...
	 Train_Loss: 0.5320 Train_Acc: 77.715 Val_Loss: 0.5646  BEST VAL Loss: 0.5646  Val_Acc: 74.752

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.5310 Train_Acc: 77.852 Val_Loss: 0.5649  BEST VAL Loss: 0.5646  Val_Acc: 74.058

Epoch 69: Validation loss decreased (0.564602 --> 0.564483).  Saving model ...
	 Train_Loss: 0.5301 Train_Acc: 78.050 Val_Loss: 0.5645  BEST VAL Loss: 0.5645  Val_Acc: 74.851

Epoch 70: Validation loss decreased (0.564483 --> 0.564278).  Saving model ...
	 Train_Loss: 0.5293 Train_Acc: 77.051 Val_Loss: 0.5643  BEST VAL Loss: 0.5643  Val_Acc: 74.752

Epoch 71: Validation loss decreased (0.564278 --> 0.563092).  Saving model ...
	 Train_Loss: 0.5285 Train_Acc: 77.696 Val_Loss: 0.5631  BEST VAL Loss: 0.5631  Val_Acc: 74.901

Epoch 72: Validation loss decreased (0.563092 --> 0.562469).  Saving model ...
	 Train_Loss: 0.5277 Train_Acc: 77.672 Val_Loss: 0.5625  BEST VAL Loss: 0.5625  Val_Acc: 74.950

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.5269 Train_Acc: 77.554 Val_Loss: 0.5628  BEST VAL Loss: 0.5625  Val_Acc: 75.248

Epoch 74: Validation loss decreased (0.562469 --> 0.562063).  Saving model ...
	 Train_Loss: 0.5260 Train_Acc: 78.298 Val_Loss: 0.5621  BEST VAL Loss: 0.5621  Val_Acc: 75.000

Epoch 75: Validation loss decreased (0.562063 --> 0.560983).  Saving model ...
	 Train_Loss: 0.5252 Train_Acc: 78.341 Val_Loss: 0.5610  BEST VAL Loss: 0.5610  Val_Acc: 74.504

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.5243 Train_Acc: 78.552 Val_Loss: 0.5623  BEST VAL Loss: 0.5610  Val_Acc: 75.099

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.5234 Train_Acc: 78.683 Val_Loss: 0.5639  BEST VAL Loss: 0.5610  Val_Acc: 74.554

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.5226 Train_Acc: 78.441 Val_Loss: 0.5627  BEST VAL Loss: 0.5610  Val_Acc: 74.454

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.5218 Train_Acc: 78.404 Val_Loss: 0.5646  BEST VAL Loss: 0.5610  Val_Acc: 75.248

Epoch 80: Validation loss did not decrease
	 Train_Loss: 0.5211 Train_Acc: 78.292 Val_Loss: 0.5652  BEST VAL Loss: 0.5610  Val_Acc: 75.099

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.5204 Train_Acc: 78.304 Val_Loss: 0.5648  BEST VAL Loss: 0.5610  Val_Acc: 75.347

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.5196 Train_Acc: 78.093 Val_Loss: 0.5649  BEST VAL Loss: 0.5610  Val_Acc: 74.454

Epoch 83: Validation loss did not decrease
	 Train_Loss: 0.5188 Train_Acc: 79.656 Val_Loss: 0.5645  BEST VAL Loss: 0.5610  Val_Acc: 74.206

Epoch 84: Validation loss did not decrease
	 Train_Loss: 0.5180 Train_Acc: 78.441 Val_Loss: 0.5637  BEST VAL Loss: 0.5610  Val_Acc: 74.454

Epoch 85: Validation loss did not decrease
	 Train_Loss: 0.5173 Train_Acc: 78.081 Val_Loss: 0.5634  BEST VAL Loss: 0.5610  Val_Acc: 73.859

Epoch 86: Validation loss did not decrease
	 Train_Loss: 0.5168 Train_Acc: 77.988 Val_Loss: 0.5631  BEST VAL Loss: 0.5610  Val_Acc: 74.554

Epoch 87: Validation loss did not decrease
	 Train_Loss: 0.5161 Train_Acc: 78.528 Val_Loss: 0.5621  BEST VAL Loss: 0.5610  Val_Acc: 73.363

Epoch 88: Validation loss did not decrease
	 Train_Loss: 0.5154 Train_Acc: 78.224 Val_Loss: 0.5619  BEST VAL Loss: 0.5610  Val_Acc: 74.405

Epoch 89: Validation loss did not decrease
	 Train_Loss: 0.5147 Train_Acc: 78.596 Val_Loss: 0.5616  BEST VAL Loss: 0.5610  Val_Acc: 74.206

Epoch 90: Validation loss did not decrease
	 Train_Loss: 0.5139 Train_Acc: 79.197 Val_Loss: 0.5615  BEST VAL Loss: 0.5610  Val_Acc: 74.306

Epoch 91: Validation loss decreased (0.560983 --> 0.560605).  Saving model ...
	 Train_Loss: 0.5133 Train_Acc: 78.788 Val_Loss: 0.5606  BEST VAL Loss: 0.5606  Val_Acc: 74.950

Epoch 92: Validation loss did not decrease
	 Train_Loss: 0.5125 Train_Acc: 78.658 Val_Loss: 0.5614  BEST VAL Loss: 0.5606  Val_Acc: 74.802

Epoch 93: Validation loss did not decrease
	 Train_Loss: 0.5119 Train_Acc: 78.416 Val_Loss: 0.5618  BEST VAL Loss: 0.5606  Val_Acc: 75.099

Epoch 94: Validation loss did not decrease
	 Train_Loss: 0.5113 Train_Acc: 77.932 Val_Loss: 0.5624  BEST VAL Loss: 0.5606  Val_Acc: 76.240

Epoch 95: Validation loss did not decrease
	 Train_Loss: 0.5107 Train_Acc: 78.925 Val_Loss: 0.5628  BEST VAL Loss: 0.5606  Val_Acc: 74.653

Epoch 96: Validation loss did not decrease
	 Train_Loss: 0.5100 Train_Acc: 78.726 Val_Loss: 0.5628  BEST VAL Loss: 0.5606  Val_Acc: 74.901

Epoch 97: Validation loss did not decrease
	 Train_Loss: 0.5094 Train_Acc: 78.875 Val_Loss: 0.5622  BEST VAL Loss: 0.5606  Val_Acc: 75.347

Epoch 98: Validation loss did not decrease
	 Train_Loss: 0.5087 Train_Acc: 79.117 Val_Loss: 0.5620  BEST VAL Loss: 0.5606  Val_Acc: 75.694

Epoch 99: Validation loss did not decrease
	 Train_Loss: 0.5081 Train_Acc: 79.154 Val_Loss: 0.5620  BEST VAL Loss: 0.5606  Val_Acc: 76.042

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.61      0.56      8273
           1       0.49      0.40      0.44      7850

    accuracy                           0.51     16123
   macro avg       0.50      0.50      0.50     16123
weighted avg       0.50      0.51      0.50     16123

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.64      0.57      1034
           1       0.50      0.38      0.43       982

    accuracy                           0.51      2016
   macro avg       0.51      0.51      0.50      2016
weighted avg       0.51      0.51      0.50      2016

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.62      0.56      1034
           1       0.48      0.38      0.42       982

    accuracy                           0.50      2016
   macro avg       0.50      0.50      0.49      2016
weighted avg       0.50      0.50      0.49      2016

              precision    recall  f1-score   support

           0       0.51      0.62      0.56      1034
           1       0.48      0.38      0.42       982

    accuracy                           0.50      2016
   macro avg       0.50      0.50      0.49      2016
weighted avg       0.50      0.50      0.49      2016

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.61      0.56      3425
           1       0.51      0.41      0.46      3398

    accuracy                           0.51      6823
   macro avg       0.51      0.51      0.51      6823
weighted avg       0.51      0.51      0.51      6823

              precision    recall  f1-score   support

           0       0.51      0.61      0.56      3425
           1       0.51      0.41      0.46      3398

    accuracy                           0.51      6823
   macro avg       0.51      0.51      0.51      6823
weighted avg       0.51      0.51      0.51      6823

completed
