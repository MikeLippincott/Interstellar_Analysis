[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '50121bbc'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '0a244403'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'b6e7e110'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '36ba246b'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_10.0_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (30211, 1276)
Number of total missing values across all columns: 31618
Data Subset Is Off
Wells held out for testing: ['M16' 'M22']
Wells to use for training, validation, and testing ['M17' 'M18' 'M19' 'M20' 'M21' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.313861).  Saving model ...
	 Train_Loss: 0.4961 Train_Acc: 76.032 Val_Loss: 0.3139  BEST VAL Loss: 0.3139  Val_Acc: 89.884

Epoch 1: Validation loss decreased (0.313861 --> 0.279777).  Saving model ...
	 Train_Loss: 0.4319 Train_Acc: 84.852 Val_Loss: 0.2798  BEST VAL Loss: 0.2798  Val_Acc: 92.346

Epoch 2: Validation loss decreased (0.279777 --> 0.257299).  Saving model ...
	 Train_Loss: 0.3945 Train_Acc: 87.073 Val_Loss: 0.2573  BEST VAL Loss: 0.2573  Val_Acc: 93.151

Epoch 3: Validation loss decreased (0.257299 --> 0.249455).  Saving model ...
	 Train_Loss: 0.3701 Train_Acc: 87.957 Val_Loss: 0.2495  BEST VAL Loss: 0.2495  Val_Acc: 93.465

Epoch 4: Validation loss decreased (0.249455 --> 0.235399).  Saving model ...
	 Train_Loss: 0.3524 Train_Acc: 88.517 Val_Loss: 0.2354  BEST VAL Loss: 0.2354  Val_Acc: 93.868

Epoch 5: Validation loss decreased (0.235399 --> 0.223405).  Saving model ...
	 Train_Loss: 0.3370 Train_Acc: 89.284 Val_Loss: 0.2234  BEST VAL Loss: 0.2234  Val_Acc: 94.673

Epoch 6: Validation loss decreased (0.223405 --> 0.214786).  Saving model ...
	 Train_Loss: 0.3249 Train_Acc: 89.681 Val_Loss: 0.2148  BEST VAL Loss: 0.2148  Val_Acc: 94.897

Epoch 7: Validation loss decreased (0.214786 --> 0.208163).  Saving model ...
	 Train_Loss: 0.3162 Train_Acc: 89.726 Val_Loss: 0.2082  BEST VAL Loss: 0.2082  Val_Acc: 94.315

Epoch 8: Validation loss decreased (0.208163 --> 0.202663).  Saving model ...
	 Train_Loss: 0.3079 Train_Acc: 90.134 Val_Loss: 0.2027  BEST VAL Loss: 0.2027  Val_Acc: 94.539

Epoch 9: Validation loss decreased (0.202663 --> 0.199320).  Saving model ...
	 Train_Loss: 0.3021 Train_Acc: 90.084 Val_Loss: 0.1993  BEST VAL Loss: 0.1993  Val_Acc: 94.136

Epoch 10: Validation loss decreased (0.199320 --> 0.194056).  Saving model ...
	 Train_Loss: 0.2970 Train_Acc: 89.938 Val_Loss: 0.1941  BEST VAL Loss: 0.1941  Val_Acc: 95.524

Epoch 11: Validation loss decreased (0.194056 --> 0.191208).  Saving model ...
	 Train_Loss: 0.2910 Train_Acc: 90.560 Val_Loss: 0.1912  BEST VAL Loss: 0.1912  Val_Acc: 95.345

Epoch 12: Validation loss decreased (0.191208 --> 0.187465).  Saving model ...
	 Train_Loss: 0.2864 Train_Acc: 90.448 Val_Loss: 0.1875  BEST VAL Loss: 0.1875  Val_Acc: 94.584

Epoch 13: Validation loss decreased (0.187465 --> 0.182924).  Saving model ...
	 Train_Loss: 0.2820 Train_Acc: 90.761 Val_Loss: 0.1829  BEST VAL Loss: 0.1829  Val_Acc: 95.792

Epoch 14: Validation loss decreased (0.182924 --> 0.179877).  Saving model ...
	 Train_Loss: 0.2780 Train_Acc: 91.074 Val_Loss: 0.1799  BEST VAL Loss: 0.1799  Val_Acc: 95.389

Epoch 15: Validation loss decreased (0.179877 --> 0.177838).  Saving model ...
	 Train_Loss: 0.2747 Train_Acc: 90.588 Val_Loss: 0.1778  BEST VAL Loss: 0.1778  Val_Acc: 95.121

Epoch 16: Validation loss decreased (0.177838 --> 0.175206).  Saving model ...
	 Train_Loss: 0.2721 Train_Acc: 90.632 Val_Loss: 0.1752  BEST VAL Loss: 0.1752  Val_Acc: 94.987

Epoch 17: Validation loss decreased (0.175206 --> 0.173011).  Saving model ...
	 Train_Loss: 0.2697 Train_Acc: 90.207 Val_Loss: 0.1730  BEST VAL Loss: 0.1730  Val_Acc: 95.524

Epoch 18: Validation loss decreased (0.173011 --> 0.170647).  Saving model ...
	 Train_Loss: 0.2670 Train_Acc: 90.733 Val_Loss: 0.1706  BEST VAL Loss: 0.1706  Val_Acc: 95.837

Epoch 19: Validation loss decreased (0.170647 --> 0.168169).  Saving model ...
	 Train_Loss: 0.2641 Train_Acc: 91.147 Val_Loss: 0.1682  BEST VAL Loss: 0.1682  Val_Acc: 95.792

Epoch 20: Validation loss decreased (0.168169 --> 0.166206).  Saving model ...
	 Train_Loss: 0.2614 Train_Acc: 91.030 Val_Loss: 0.1662  BEST VAL Loss: 0.1662  Val_Acc: 95.389

Epoch 21: Validation loss decreased (0.166206 --> 0.164938).  Saving model ...
	 Train_Loss: 0.2590 Train_Acc: 91.539 Val_Loss: 0.1649  BEST VAL Loss: 0.1649  Val_Acc: 94.852

Epoch 22: Validation loss decreased (0.164938 --> 0.163258).  Saving model ...
	 Train_Loss: 0.2566 Train_Acc: 91.511 Val_Loss: 0.1633  BEST VAL Loss: 0.1633  Val_Acc: 95.345

Epoch 23: Validation loss decreased (0.163258 --> 0.162124).  Saving model ...
	 Train_Loss: 0.2544 Train_Acc: 91.696 Val_Loss: 0.1621  BEST VAL Loss: 0.1621  Val_Acc: 95.434

Epoch 24: Validation loss decreased (0.162124 --> 0.161238).  Saving model ...
	 Train_Loss: 0.2524 Train_Acc: 91.371 Val_Loss: 0.1612  BEST VAL Loss: 0.1612  Val_Acc: 95.121

Epoch 25: Validation loss decreased (0.161238 --> 0.159845).  Saving model ...
	 Train_Loss: 0.2502 Train_Acc: 91.735 Val_Loss: 0.1598  BEST VAL Loss: 0.1598  Val_Acc: 95.613

Epoch 26: Validation loss decreased (0.159845 --> 0.158571).  Saving model ...
	 Train_Loss: 0.2481 Train_Acc: 92.071 Val_Loss: 0.1586  BEST VAL Loss: 0.1586  Val_Acc: 95.703

Epoch 27: Validation loss decreased (0.158571 --> 0.157418).  Saving model ...
	 Train_Loss: 0.2462 Train_Acc: 91.953 Val_Loss: 0.1574  BEST VAL Loss: 0.1574  Val_Acc: 95.703

Epoch 28: Validation loss decreased (0.157418 --> 0.156257).  Saving model ...
	 Train_Loss: 0.2445 Train_Acc: 91.752 Val_Loss: 0.1563  BEST VAL Loss: 0.1563  Val_Acc: 95.748

Epoch 29: Validation loss decreased (0.156257 --> 0.155449).  Saving model ...
	 Train_Loss: 0.2428 Train_Acc: 91.673 Val_Loss: 0.1554  BEST VAL Loss: 0.1554  Val_Acc: 95.434

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.2412 Train_Acc: 91.886 Val_Loss: 0.1555  BEST VAL Loss: 0.1554  Val_Acc: 93.107

Epoch 31: Validation loss decreased (0.155449 --> 0.154967).  Saving model ...
	 Train_Loss: 0.2400 Train_Acc: 91.337 Val_Loss: 0.1550  BEST VAL Loss: 0.1550  Val_Acc: 95.703

Epoch 32: Validation loss decreased (0.154967 --> 0.154088).  Saving model ...
	 Train_Loss: 0.2385 Train_Acc: 91.953 Val_Loss: 0.1541  BEST VAL Loss: 0.1541  Val_Acc: 95.210

Epoch 33: Validation loss decreased (0.154088 --> 0.153933).  Saving model ...
	 Train_Loss: 0.2369 Train_Acc: 91.936 Val_Loss: 0.1539  BEST VAL Loss: 0.1539  Val_Acc: 95.166

Epoch 34: Validation loss decreased (0.153933 --> 0.153398).  Saving model ...
	 Train_Loss: 0.2356 Train_Acc: 91.981 Val_Loss: 0.1534  BEST VAL Loss: 0.1534  Val_Acc: 95.389

Epoch 35: Validation loss decreased (0.153398 --> 0.152604).  Saving model ...
	 Train_Loss: 0.2343 Train_Acc: 92.048 Val_Loss: 0.1526  BEST VAL Loss: 0.1526  Val_Acc: 95.345

Epoch 36: Validation loss decreased (0.152604 --> 0.152495).  Saving model ...
	 Train_Loss: 0.2330 Train_Acc: 91.925 Val_Loss: 0.1525  BEST VAL Loss: 0.1525  Val_Acc: 94.897

Epoch 37: Validation loss decreased (0.152495 --> 0.152294).  Saving model ...
	 Train_Loss: 0.2318 Train_Acc: 92.076 Val_Loss: 0.1523  BEST VAL Loss: 0.1523  Val_Acc: 95.524

Epoch 38: Validation loss decreased (0.152294 --> 0.151877).  Saving model ...
	 Train_Loss: 0.2303 Train_Acc: 92.485 Val_Loss: 0.1519  BEST VAL Loss: 0.1519  Val_Acc: 95.345

Epoch 39: Validation loss decreased (0.151877 --> 0.151317).  Saving model ...
	 Train_Loss: 0.2288 Train_Acc: 92.524 Val_Loss: 0.1513  BEST VAL Loss: 0.1513  Val_Acc: 96.106

Epoch 40: Validation loss decreased (0.151317 --> 0.150491).  Saving model ...
	 Train_Loss: 0.2277 Train_Acc: 92.143 Val_Loss: 0.1505  BEST VAL Loss: 0.1505  Val_Acc: 95.882

Epoch 41: Validation loss decreased (0.150491 --> 0.150032).  Saving model ...
	 Train_Loss: 0.2265 Train_Acc: 92.356 Val_Loss: 0.1500  BEST VAL Loss: 0.1500  Val_Acc: 95.434

Epoch 42: Validation loss decreased (0.150032 --> 0.149951).  Saving model ...
	 Train_Loss: 0.2254 Train_Acc: 92.003 Val_Loss: 0.1500  BEST VAL Loss: 0.1500  Val_Acc: 96.061

Epoch 43: Validation loss decreased (0.149951 --> 0.149575).  Saving model ...
	 Train_Loss: 0.2243 Train_Acc: 92.395 Val_Loss: 0.1496  BEST VAL Loss: 0.1496  Val_Acc: 95.748

Epoch 44: Validation loss decreased (0.149575 --> 0.149065).  Saving model ...
	 Train_Loss: 0.2234 Train_Acc: 92.104 Val_Loss: 0.1491  BEST VAL Loss: 0.1491  Val_Acc: 95.658

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2224 Train_Acc: 92.216 Val_Loss: 0.1495  BEST VAL Loss: 0.1491  Val_Acc: 94.897

Epoch 46: Validation loss decreased (0.149065 --> 0.149021).  Saving model ...
	 Train_Loss: 0.2219 Train_Acc: 91.533 Val_Loss: 0.1490  BEST VAL Loss: 0.1490  Val_Acc: 95.971

Epoch 47: Validation loss decreased (0.149021 --> 0.148997).  Saving model ...
	 Train_Loss: 0.2215 Train_Acc: 91.080 Val_Loss: 0.1490  BEST VAL Loss: 0.1490  Val_Acc: 94.181

Epoch 48: Validation loss decreased (0.148997 --> 0.148755).  Saving model ...
	 Train_Loss: 0.2213 Train_Acc: 90.565 Val_Loss: 0.1488  BEST VAL Loss: 0.1488  Val_Acc: 95.166

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2205 Train_Acc: 91.595 Val_Loss: 0.1489  BEST VAL Loss: 0.1488  Val_Acc: 95.613

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.2200 Train_Acc: 91.729 Val_Loss: 0.1490  BEST VAL Loss: 0.1488  Val_Acc: 95.121

Epoch 51: Validation loss decreased (0.148755 --> 0.148546).  Saving model ...
	 Train_Loss: 0.2195 Train_Acc: 91.550 Val_Loss: 0.1485  BEST VAL Loss: 0.1485  Val_Acc: 94.897

Epoch 52: Validation loss decreased (0.148546 --> 0.148111).  Saving model ...
	 Train_Loss: 0.2189 Train_Acc: 92.451 Val_Loss: 0.1481  BEST VAL Loss: 0.1481  Val_Acc: 95.568

Epoch 53: Validation loss decreased (0.148111 --> 0.147909).  Saving model ...
	 Train_Loss: 0.2182 Train_Acc: 91.964 Val_Loss: 0.1479  BEST VAL Loss: 0.1479  Val_Acc: 95.524

Epoch 54: Validation loss decreased (0.147909 --> 0.147634).  Saving model ...
	 Train_Loss: 0.2177 Train_Acc: 92.115 Val_Loss: 0.1476  BEST VAL Loss: 0.1476  Val_Acc: 95.255

Epoch 55: Validation loss decreased (0.147634 --> 0.147386).  Saving model ...
	 Train_Loss: 0.2173 Train_Acc: 91.785 Val_Loss: 0.1474  BEST VAL Loss: 0.1474  Val_Acc: 96.106

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2165 Train_Acc: 92.647 Val_Loss: 0.1478  BEST VAL Loss: 0.1474  Val_Acc: 95.434

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.2160 Train_Acc: 92.026 Val_Loss: 0.1477  BEST VAL Loss: 0.1474  Val_Acc: 95.613

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.2154 Train_Acc: 92.507 Val_Loss: 0.1475  BEST VAL Loss: 0.1474  Val_Acc: 95.345

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.2146 Train_Acc: 92.552 Val_Loss: 0.1474  BEST VAL Loss: 0.1474  Val_Acc: 95.613

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.2139 Train_Acc: 92.423 Val_Loss: 0.1476  BEST VAL Loss: 0.1474  Val_Acc: 95.658

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.2134 Train_Acc: 92.037 Val_Loss: 0.1480  BEST VAL Loss: 0.1474  Val_Acc: 95.300

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.2133 Train_Acc: 91.959 Val_Loss: 0.1480  BEST VAL Loss: 0.1474  Val_Acc: 96.285

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.2131 Train_Acc: 91.628 Val_Loss: 0.1483  BEST VAL Loss: 0.1474  Val_Acc: 95.613

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.2128 Train_Acc: 92.361 Val_Loss: 0.1482  BEST VAL Loss: 0.1474  Val_Acc: 94.226

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.2124 Train_Acc: 92.250 Val_Loss: 0.1484  BEST VAL Loss: 0.1474  Val_Acc: 95.658

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.2120 Train_Acc: 91.886 Val_Loss: 0.1481  BEST VAL Loss: 0.1474  Val_Acc: 96.195

Epoch 67: Validation loss did not decrease
	 Train_Loss: 0.2118 Train_Acc: 91.780 Val_Loss: 0.1482  BEST VAL Loss: 0.1474  Val_Acc: 95.971

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.2118 Train_Acc: 91.248 Val_Loss: 0.1487  BEST VAL Loss: 0.1474  Val_Acc: 95.748

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.2119 Train_Acc: 91.388 Val_Loss: 0.1489  BEST VAL Loss: 0.1474  Val_Acc: 95.524

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.2124 Train_Acc: 89.619 Val_Loss: 0.1491  BEST VAL Loss: 0.1474  Val_Acc: 94.852

Epoch 71: Validation loss did not decrease
Early stopped at epoch : 71
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.99      0.98      9434
           1       0.99      0.97      0.98      8436

    accuracy                           0.98     17870
   macro avg       0.98      0.98      0.98     17870
weighted avg       0.98      0.98      0.98     17870

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.98      0.96      1179
           1       0.97      0.95      0.96      1055

    accuracy                           0.96      2234
   macro avg       0.96      0.96      0.96      2234
weighted avg       0.96      0.96      0.96      2234

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1179
           1       0.96      0.95      0.96      1055

    accuracy                           0.96      2234
   macro avg       0.96      0.96      0.96      2234
weighted avg       0.96      0.96      0.96      2234

              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1179
           1       0.96      0.95      0.96      1055

    accuracy                           0.96      2234
   macro avg       0.96      0.96      0.96      2234
weighted avg       0.96      0.96      0.96      2234

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.94      0.95      4017
           1       0.94      0.95      0.95      3856

    accuracy                           0.95      7873
   macro avg       0.95      0.95      0.95      7873
weighted avg       0.95      0.95      0.95      7873

              precision    recall  f1-score   support

           0       0.95      0.94      0.95      4017
           1       0.94      0.95      0.95      3856

    accuracy                           0.95      7873
   macro avg       0.95      0.95      0.95      7873
weighted avg       0.95      0.95      0.95      7873

completed
