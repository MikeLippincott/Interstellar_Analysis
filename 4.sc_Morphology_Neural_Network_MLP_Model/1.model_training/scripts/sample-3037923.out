[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd0f04327'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '413db8af'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'dffbcfdf'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'fad75b38'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (29575, 1276)
Number of total missing values across all columns: 59150
Data Subset Is Off
Wells held out for testing: ['D14' 'M22']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.463561).  Saving model ...
	 Train_Loss: 0.6145 Train_Acc: 65.933 Val_Loss: 0.4636  BEST VAL Loss: 0.4636  Val_Acc: 80.368

Epoch 1: Validation loss decreased (0.463561 --> 0.426527).  Saving model ...
	 Train_Loss: 0.5559 Train_Acc: 76.234 Val_Loss: 0.4265  BEST VAL Loss: 0.4265  Val_Acc: 84.681

Epoch 2: Validation loss decreased (0.426527 --> 0.400585).  Saving model ...
	 Train_Loss: 0.5240 Train_Acc: 78.363 Val_Loss: 0.4006  BEST VAL Loss: 0.4006  Val_Acc: 85.310

Epoch 3: Validation loss decreased (0.400585 --> 0.393294).  Saving model ...
	 Train_Loss: 0.5045 Train_Acc: 79.200 Val_Loss: 0.3933  BEST VAL Loss: 0.3933  Val_Acc: 86.298

Epoch 4: Validation loss decreased (0.393294 --> 0.382603).  Saving model ...
	 Train_Loss: 0.4900 Train_Acc: 79.874 Val_Loss: 0.3826  BEST VAL Loss: 0.3826  Val_Acc: 86.164

Epoch 5: Validation loss decreased (0.382603 --> 0.373367).  Saving model ...
	 Train_Loss: 0.4768 Train_Acc: 81.132 Val_Loss: 0.3734  BEST VAL Loss: 0.3734  Val_Acc: 86.837

Epoch 6: Validation loss decreased (0.373367 --> 0.368894).  Saving model ...
	 Train_Loss: 0.4682 Train_Acc: 81.571 Val_Loss: 0.3689  BEST VAL Loss: 0.3689  Val_Acc: 86.253

Epoch 7: Validation loss decreased (0.368894 --> 0.363159).  Saving model ...
	 Train_Loss: 0.4609 Train_Acc: 81.408 Val_Loss: 0.3632  BEST VAL Loss: 0.3632  Val_Acc: 86.792

Epoch 8: Validation loss decreased (0.363159 --> 0.355392).  Saving model ...
	 Train_Loss: 0.4543 Train_Acc: 82.677 Val_Loss: 0.3554  BEST VAL Loss: 0.3554  Val_Acc: 87.556

Epoch 9: Validation loss decreased (0.355392 --> 0.350850).  Saving model ...
	 Train_Loss: 0.4488 Train_Acc: 81.475 Val_Loss: 0.3508  BEST VAL Loss: 0.3508  Val_Acc: 87.556

Epoch 10: Validation loss decreased (0.350850 --> 0.344936).  Saving model ...
	 Train_Loss: 0.4430 Train_Acc: 83.419 Val_Loss: 0.3449  BEST VAL Loss: 0.3449  Val_Acc: 87.601

Epoch 11: Validation loss decreased (0.344936 --> 0.341324).  Saving model ...
	 Train_Loss: 0.4380 Train_Acc: 81.818 Val_Loss: 0.3413  BEST VAL Loss: 0.3413  Val_Acc: 87.197

Epoch 12: Validation loss decreased (0.341324 --> 0.337020).  Saving model ...
	 Train_Loss: 0.4343 Train_Acc: 83.823 Val_Loss: 0.3370  BEST VAL Loss: 0.3370  Val_Acc: 86.882

Epoch 13: Validation loss did not decrease
	 Train_Loss: 0.4319 Train_Acc: 81.694 Val_Loss: 0.3393  BEST VAL Loss: 0.3370  Val_Acc: 87.871

Epoch 14: Validation loss did not decrease
	 Train_Loss: 0.4289 Train_Acc: 83.368 Val_Loss: 0.3377  BEST VAL Loss: 0.3370  Val_Acc: 88.410

Epoch 15: Validation loss decreased (0.337020 --> 0.334354).  Saving model ...
	 Train_Loss: 0.4258 Train_Acc: 83.413 Val_Loss: 0.3344  BEST VAL Loss: 0.3344  Val_Acc: 88.455

Epoch 16: Validation loss decreased (0.334354 --> 0.332959).  Saving model ...
	 Train_Loss: 0.4239 Train_Acc: 82.402 Val_Loss: 0.3330  BEST VAL Loss: 0.3330  Val_Acc: 88.005

Epoch 17: Validation loss decreased (0.332959 --> 0.332092).  Saving model ...
	 Train_Loss: 0.4220 Train_Acc: 82.273 Val_Loss: 0.3321  BEST VAL Loss: 0.3321  Val_Acc: 88.724

Epoch 18: Validation loss decreased (0.332092 --> 0.328626).  Saving model ...
	 Train_Loss: 0.4203 Train_Acc: 81.655 Val_Loss: 0.3286  BEST VAL Loss: 0.3286  Val_Acc: 87.916

Epoch 19: Validation loss decreased (0.328626 --> 0.326532).  Saving model ...
	 Train_Loss: 0.4180 Train_Acc: 82.806 Val_Loss: 0.3265  BEST VAL Loss: 0.3265  Val_Acc: 88.185

Epoch 20: Validation loss decreased (0.326532 --> 0.324326).  Saving model ...
	 Train_Loss: 0.4162 Train_Acc: 82.683 Val_Loss: 0.3243  BEST VAL Loss: 0.3243  Val_Acc: 88.679

Epoch 21: Validation loss decreased (0.324326 --> 0.321965).  Saving model ...
	 Train_Loss: 0.4145 Train_Acc: 83.677 Val_Loss: 0.3220  BEST VAL Loss: 0.3220  Val_Acc: 87.736

Epoch 22: Validation loss decreased (0.321965 --> 0.320861).  Saving model ...
	 Train_Loss: 0.4131 Train_Acc: 83.143 Val_Loss: 0.3209  BEST VAL Loss: 0.3209  Val_Acc: 86.568

Epoch 23: Validation loss decreased (0.320861 --> 0.319613).  Saving model ...
	 Train_Loss: 0.4118 Train_Acc: 83.739 Val_Loss: 0.3196  BEST VAL Loss: 0.3196  Val_Acc: 88.185

Epoch 24: Validation loss decreased (0.319613 --> 0.318165).  Saving model ...
	 Train_Loss: 0.4100 Train_Acc: 82.823 Val_Loss: 0.3182  BEST VAL Loss: 0.3182  Val_Acc: 88.904

Epoch 25: Validation loss decreased (0.318165 --> 0.316199).  Saving model ...
	 Train_Loss: 0.4087 Train_Acc: 82.402 Val_Loss: 0.3162  BEST VAL Loss: 0.3162  Val_Acc: 88.814

Epoch 26: Validation loss decreased (0.316199 --> 0.315302).  Saving model ...
	 Train_Loss: 0.4074 Train_Acc: 82.649 Val_Loss: 0.3153  BEST VAL Loss: 0.3153  Val_Acc: 89.623

Epoch 27: Validation loss decreased (0.315302 --> 0.314194).  Saving model ...
	 Train_Loss: 0.4064 Train_Acc: 82.761 Val_Loss: 0.3142  BEST VAL Loss: 0.3142  Val_Acc: 88.410

Epoch 28: Validation loss decreased (0.314194 --> 0.312919).  Saving model ...
	 Train_Loss: 0.4056 Train_Acc: 83.188 Val_Loss: 0.3129  BEST VAL Loss: 0.3129  Val_Acc: 88.994

Epoch 29: Validation loss decreased (0.312919 --> 0.311527).  Saving model ...
	 Train_Loss: 0.4045 Train_Acc: 82.773 Val_Loss: 0.3115  BEST VAL Loss: 0.3115  Val_Acc: 88.140

Epoch 30: Validation loss decreased (0.311527 --> 0.310630).  Saving model ...
	 Train_Loss: 0.4036 Train_Acc: 82.784 Val_Loss: 0.3106  BEST VAL Loss: 0.3106  Val_Acc: 88.095

Epoch 31: Validation loss decreased (0.310630 --> 0.310444).  Saving model ...
	 Train_Loss: 0.4028 Train_Acc: 83.059 Val_Loss: 0.3104  BEST VAL Loss: 0.3104  Val_Acc: 87.646

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.4023 Train_Acc: 83.542 Val_Loss: 0.3108  BEST VAL Loss: 0.3104  Val_Acc: 88.589

Epoch 33: Validation loss decreased (0.310444 --> 0.309741).  Saving model ...
	 Train_Loss: 0.4027 Train_Acc: 80.750 Val_Loss: 0.3097  BEST VAL Loss: 0.3097  Val_Acc: 88.634

Epoch 34: Validation loss decreased (0.309741 --> 0.309278).  Saving model ...
	 Train_Loss: 0.4023 Train_Acc: 81.177 Val_Loss: 0.3093  BEST VAL Loss: 0.3093  Val_Acc: 89.218

Epoch 35: Validation loss decreased (0.309278 --> 0.308808).  Saving model ...
	 Train_Loss: 0.4016 Train_Acc: 83.806 Val_Loss: 0.3088  BEST VAL Loss: 0.3088  Val_Acc: 88.589

Epoch 36: Validation loss decreased (0.308808 --> 0.307499).  Saving model ...
	 Train_Loss: 0.4008 Train_Acc: 83.879 Val_Loss: 0.3075  BEST VAL Loss: 0.3075  Val_Acc: 89.084

Epoch 37: Validation loss decreased (0.307499 --> 0.306087).  Saving model ...
	 Train_Loss: 0.3997 Train_Acc: 84.345 Val_Loss: 0.3061  BEST VAL Loss: 0.3061  Val_Acc: 88.275

Epoch 38: Validation loss decreased (0.306087 --> 0.304911).  Saving model ...
	 Train_Loss: 0.3988 Train_Acc: 83.947 Val_Loss: 0.3049  BEST VAL Loss: 0.3049  Val_Acc: 89.084

Epoch 39: Validation loss decreased (0.304911 --> 0.303396).  Saving model ...
	 Train_Loss: 0.3978 Train_Acc: 83.997 Val_Loss: 0.3034  BEST VAL Loss: 0.3034  Val_Acc: 88.949

Epoch 40: Validation loss decreased (0.303396 --> 0.302364).  Saving model ...
	 Train_Loss: 0.3971 Train_Acc: 84.048 Val_Loss: 0.3024  BEST VAL Loss: 0.3024  Val_Acc: 88.949

Epoch 41: Validation loss decreased (0.302364 --> 0.301984).  Saving model ...
	 Train_Loss: 0.3967 Train_Acc: 83.272 Val_Loss: 0.3020  BEST VAL Loss: 0.3020  Val_Acc: 88.230

Epoch 42: Validation loss decreased (0.301984 --> 0.301975).  Saving model ...
	 Train_Loss: 0.3966 Train_Acc: 82.357 Val_Loss: 0.3020  BEST VAL Loss: 0.3020  Val_Acc: 87.556

Epoch 43: Validation loss decreased (0.301975 --> 0.301652).  Saving model ...
	 Train_Loss: 0.3969 Train_Acc: 81.790 Val_Loss: 0.3017  BEST VAL Loss: 0.3017  Val_Acc: 87.601

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.3979 Train_Acc: 80.492 Val_Loss: 0.3026  BEST VAL Loss: 0.3017  Val_Acc: 84.322

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.3985 Train_Acc: 80.880 Val_Loss: 0.3034  BEST VAL Loss: 0.3017  Val_Acc: 85.669

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.3994 Train_Acc: 79.498 Val_Loss: 0.3039  BEST VAL Loss: 0.3017  Val_Acc: 86.613

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.4000 Train_Acc: 80.537 Val_Loss: 0.3045  BEST VAL Loss: 0.3017  Val_Acc: 85.984

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.4004 Train_Acc: 80.779 Val_Loss: 0.3047  BEST VAL Loss: 0.3017  Val_Acc: 88.140

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.4009 Train_Acc: 81.930 Val_Loss: 0.3055  BEST VAL Loss: 0.3017  Val_Acc: 88.095

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.4015 Train_Acc: 80.548 Val_Loss: 0.3058  BEST VAL Loss: 0.3017  Val_Acc: 87.332

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.4024 Train_Acc: 79.818 Val_Loss: 0.3072  BEST VAL Loss: 0.3017  Val_Acc: 85.490

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.4037 Train_Acc: 78.723 Val_Loss: 0.3092  BEST VAL Loss: 0.3017  Val_Acc: 84.681

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.4051 Train_Acc: 78.734 Val_Loss: 0.3110  BEST VAL Loss: 0.3017  Val_Acc: 84.501

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.4066 Train_Acc: 78.801 Val_Loss: 0.3134  BEST VAL Loss: 0.3017  Val_Acc: 78.392

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.4086 Train_Acc: 76.240 Val_Loss: 0.3152  BEST VAL Loss: 0.3017  Val_Acc: 83.109

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.4103 Train_Acc: 76.936 Val_Loss: 0.3170  BEST VAL Loss: 0.3017  Val_Acc: 83.872

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.4117 Train_Acc: 77.481 Val_Loss: 0.3182  BEST VAL Loss: 0.3017  Val_Acc: 85.535

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.4133 Train_Acc: 78.043 Val_Loss: 0.3204  BEST VAL Loss: 0.3017  Val_Acc: 86.388

Epoch 59: Validation loss did not decrease
Early stopped at epoch : 59
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.51      0.52      9433
           1       0.47      0.49      0.48      8370

    accuracy                           0.50     17803
   macro avg       0.50      0.50      0.50     17803
weighted avg       0.50      0.50      0.50     17803

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.51      0.52      1179
           1       0.47      0.49      0.48      1047

    accuracy                           0.50      2226
   macro avg       0.50      0.50      0.50      2226
weighted avg       0.50      0.50      0.50      2226

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.49      0.51      1180
           1       0.47      0.50      0.48      1046

    accuracy                           0.50      2226
   macro avg       0.50      0.50      0.50      2226
weighted avg       0.50      0.50      0.50      2226

              precision    recall  f1-score   support

           0       0.53      0.49      0.51      1180
           1       0.47      0.50      0.48      1046

    accuracy                           0.50      2226
   macro avg       0.50      0.50      0.50      2226
weighted avg       0.50      0.50      0.50      2226

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.47      0.51      4017
           1       0.45      0.53      0.49      3303

    accuracy                           0.50      7320
   macro avg       0.50      0.50      0.50      7320
weighted avg       0.51      0.50      0.50      7320

              precision    recall  f1-score   support

           0       0.55      0.47      0.51      4017
           1       0.45      0.53      0.49      3303

    accuracy                           0.50      7320
   macro avg       0.50      0.50      0.50      7320
weighted avg       0.51      0.50      0.50      7320

completed
