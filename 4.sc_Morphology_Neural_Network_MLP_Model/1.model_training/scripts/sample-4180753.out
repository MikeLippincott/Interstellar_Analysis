[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18688 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
[NbConvertApp] Writing 52448 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:252: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:279: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1019: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1019: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:663: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:668: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:686: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:723: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:737: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:834: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:840: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:890: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1001: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1007: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1174: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1176: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1179: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1204: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1240: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1360: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1366: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1124: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1548: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1665: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1671: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1820: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1822: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1825: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1896: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
PBMC MultiClass_MLP False
[0.9436581681188537, 0.5416430152668075, 0.5146988166143389]
Data Subset Is Off
(1411998,) (353000,) (1877429,) 3642427     858323
3642428     858324
3642429     858325
3642430     858326
3642431     858327
            ...
4061834    5498161
4061835    5498162
4061836    5498163
4061837    5498164
4061838    5498165
Name: labeled_data_index, Length: 419412, dtype: int64 (1536843,)
(1411998,) (353000,) (1877429,) 3642427     858323
3642428     858324
3642429     858325
3642430     858326
3642431     858327
            ...
4061834    5498161
4061835    5498162
4061836    5498163
4061837    5498164
4061838    5498165
Name: labeled_data_index, Length: 419412, dtype: int64 (1536843,)
5598682
(95928,) (722887,) (593183,)
(23982,) (180722,) (148296,)
(119911,) (903609,) (853909,)
(0,) (0,) (419412,)
(75619,) (758977,) (702247,)
(1411998, 1245) (353000, 1245) (1877429, 1245) (419412, 1245) (1536843, 1245)
(1411998,) (353000,) (1877429,) (419412,) (1536843,)
3
Number of in features:  1245
Number of out features:  3
Multi_Class
Adam
Epoch 0: Validation loss decreased (inf --> 0.312827).  Saving model ...
	 Train_Loss: 0.3599 Train_Acc: 0.002 Val_Loss: 0.3128  BEST VAL Loss: 0.3128  Val_Acc: 0.000

Epoch 1: Validation loss decreased (0.312827 --> 0.302025).  Saving model ...
	 Train_Loss: 0.3419 Train_Acc: 0.001 Val_Loss: 0.3020  BEST VAL Loss: 0.3020  Val_Acc: 0.000

Epoch 2: Validation loss decreased (0.302025 --> 0.295637).  Saving model ...
	 Train_Loss: 0.3314 Train_Acc: 0.001 Val_Loss: 0.2956  BEST VAL Loss: 0.2956  Val_Acc: 0.000

Epoch 3: Validation loss decreased (0.295637 --> 0.290545).  Saving model ...
	 Train_Loss: 0.3244 Train_Acc: 0.001 Val_Loss: 0.2905  BEST VAL Loss: 0.2905  Val_Acc: 0.000

Epoch 4: Validation loss decreased (0.290545 --> 0.286949).  Saving model ...
	 Train_Loss: 0.3190 Train_Acc: 0.000 Val_Loss: 0.2869  BEST VAL Loss: 0.2869  Val_Acc: 0.000

Epoch 5: Validation loss decreased (0.286949 --> 0.283507).  Saving model ...
	 Train_Loss: 0.3146 Train_Acc: 0.000 Val_Loss: 0.2835  BEST VAL Loss: 0.2835  Val_Acc: 0.000

Epoch 6: Validation loss decreased (0.283507 --> 0.280648).  Saving model ...
	 Train_Loss: 0.3110 Train_Acc: 0.000 Val_Loss: 0.2806  BEST VAL Loss: 0.2806  Val_Acc: 0.000

Epoch 7: Validation loss decreased (0.280648 --> 0.278039).  Saving model ...
	 Train_Loss: 0.3079 Train_Acc: 0.001 Val_Loss: 0.2780  BEST VAL Loss: 0.2780  Val_Acc: 0.000

Epoch 8: Validation loss decreased (0.278039 --> 0.275724).  Saving model ...
	 Train_Loss: 0.3053 Train_Acc: 0.000 Val_Loss: 0.2757  BEST VAL Loss: 0.2757  Val_Acc: 0.001

Epoch 9: Validation loss decreased (0.275724 --> 0.273806).  Saving model ...
	 Train_Loss: 0.3029 Train_Acc: 0.001 Val_Loss: 0.2738  BEST VAL Loss: 0.2738  Val_Acc: 0.000

Epoch 10: Validation loss decreased (0.273806 --> 0.272088).  Saving model ...
	 Train_Loss: 0.3008 Train_Acc: 0.001 Val_Loss: 0.2721  BEST VAL Loss: 0.2721  Val_Acc: 0.000

Epoch 11: Validation loss decreased (0.272088 --> 0.270454).  Saving model ...
	 Train_Loss: 0.2990 Train_Acc: 0.001 Val_Loss: 0.2705  BEST VAL Loss: 0.2705  Val_Acc: 0.001

Epoch 12: Validation loss decreased (0.270454 --> 0.269164).  Saving model ...
	 Train_Loss: 0.2972 Train_Acc: 0.001 Val_Loss: 0.2692  BEST VAL Loss: 0.2692  Val_Acc: 0.000

Epoch 13: Validation loss decreased (0.269164 --> 0.267658).  Saving model ...
	 Train_Loss: 0.2957 Train_Acc: 0.000 Val_Loss: 0.2677  BEST VAL Loss: 0.2677  Val_Acc: 0.001

Epoch 14: Validation loss decreased (0.267658 --> 0.266419).  Saving model ...
	 Train_Loss: 0.2942 Train_Acc: 0.001 Val_Loss: 0.2664  BEST VAL Loss: 0.2664  Val_Acc: 0.000

Epoch 15: Validation loss decreased (0.266419 --> 0.265256).  Saving model ...
	 Train_Loss: 0.2929 Train_Acc: 0.000 Val_Loss: 0.2653  BEST VAL Loss: 0.2653  Val_Acc: 0.000

Epoch 16: Validation loss decreased (0.265256 --> 0.264200).  Saving model ...
	 Train_Loss: 0.2917 Train_Acc: 0.000 Val_Loss: 0.2642  BEST VAL Loss: 0.2642  Val_Acc: 0.000

Epoch 17: Validation loss decreased (0.264200 --> 0.263214).  Saving model ...
	 Train_Loss: 0.2905 Train_Acc: 0.001 Val_Loss: 0.2632  BEST VAL Loss: 0.2632  Val_Acc: 0.000

Epoch 18: Validation loss decreased (0.263214 --> 0.262234).  Saving model ...
	 Train_Loss: 0.2894 Train_Acc: 0.001 Val_Loss: 0.2622  BEST VAL Loss: 0.2622  Val_Acc: 0.000

Epoch 19: Validation loss decreased (0.262234 --> 0.261360).  Saving model ...
	 Train_Loss: 0.2884 Train_Acc: 0.000 Val_Loss: 0.2614  BEST VAL Loss: 0.2614  Val_Acc: 0.000

Epoch 20: Validation loss decreased (0.261360 --> 0.260610).  Saving model ...
	 Train_Loss: 0.2875 Train_Acc: 0.000 Val_Loss: 0.2606  BEST VAL Loss: 0.2606  Val_Acc: 0.000

Epoch 21: Validation loss decreased (0.260610 --> 0.259790).  Saving model ...
	 Train_Loss: 0.2866 Train_Acc: 0.000 Val_Loss: 0.2598  BEST VAL Loss: 0.2598  Val_Acc: 0.000

Epoch 22: Validation loss decreased (0.259790 --> 0.259094).  Saving model ...
	 Train_Loss: 0.2858 Train_Acc: 0.000 Val_Loss: 0.2591  BEST VAL Loss: 0.2591  Val_Acc: 0.000

Epoch 23: Validation loss decreased (0.259094 --> 0.258475).  Saving model ...
	 Train_Loss: 0.2850 Train_Acc: 0.000 Val_Loss: 0.2585  BEST VAL Loss: 0.2585  Val_Acc: 0.001

Epoch 24: Validation loss decreased (0.258475 --> 0.257757).  Saving model ...
	 Train_Loss: 0.2842 Train_Acc: 0.001 Val_Loss: 0.2578  BEST VAL Loss: 0.2578  Val_Acc: 0.001

Epoch 25: Validation loss decreased (0.257757 --> 0.257093).  Saving model ...
	 Train_Loss: 0.2835 Train_Acc: 0.001 Val_Loss: 0.2571  BEST VAL Loss: 0.2571  Val_Acc: 0.000

Epoch 26: Validation loss decreased (0.257093 --> 0.256435).  Saving model ...
	 Train_Loss: 0.2828 Train_Acc: 0.000 Val_Loss: 0.2564  BEST VAL Loss: 0.2564  Val_Acc: 0.000

Epoch 27: Validation loss decreased (0.256435 --> 0.255876).  Saving model ...
	 Train_Loss: 0.2822 Train_Acc: 0.001 Val_Loss: 0.2559  BEST VAL Loss: 0.2559  Val_Acc: 0.000

Epoch 28: Validation loss decreased (0.255876 --> 0.255337).  Saving model ...
	 Train_Loss: 0.2815 Train_Acc: 0.001 Val_Loss: 0.2553  BEST VAL Loss: 0.2553  Val_Acc: 0.000

Epoch 29: Validation loss decreased (0.255337 --> 0.254801).  Saving model ...
	 Train_Loss: 0.2809 Train_Acc: 0.001 Val_Loss: 0.2548  BEST VAL Loss: 0.2548  Val_Acc: 0.001

Epoch 30: Validation loss decreased (0.254801 --> 0.254314).  Saving model ...
	 Train_Loss: 0.2803 Train_Acc: 0.001 Val_Loss: 0.2543  BEST VAL Loss: 0.2543  Val_Acc: 0.000

Epoch 31: Validation loss decreased (0.254314 --> 0.253850).  Saving model ...
	 Train_Loss: 0.2798 Train_Acc: 0.001 Val_Loss: 0.2538  BEST VAL Loss: 0.2538  Val_Acc: 0.000

Epoch 32: Validation loss decreased (0.253850 --> 0.253424).  Saving model ...
	 Train_Loss: 0.2792 Train_Acc: 0.001 Val_Loss: 0.2534  BEST VAL Loss: 0.2534  Val_Acc: 0.001

Epoch 33: Validation loss decreased (0.253424 --> 0.252962).  Saving model ...
	 Train_Loss: 0.2787 Train_Acc: 0.001 Val_Loss: 0.2530  BEST VAL Loss: 0.2530  Val_Acc: 0.001

Epoch 34: Validation loss decreased (0.252962 --> 0.252507).  Saving model ...
	 Train_Loss: 0.2782 Train_Acc: 0.000 Val_Loss: 0.2525  BEST VAL Loss: 0.2525  Val_Acc: 0.001

Epoch 35: Validation loss decreased (0.252507 --> 0.252025).  Saving model ...
	 Train_Loss: 0.2777 Train_Acc: 0.001 Val_Loss: 0.2520  BEST VAL Loss: 0.2520  Val_Acc: 0.001

Epoch 36: Validation loss decreased (0.252025 --> 0.251600).  Saving model ...
	 Train_Loss: 0.2773 Train_Acc: 0.001 Val_Loss: 0.2516  BEST VAL Loss: 0.2516  Val_Acc: 0.000

Epoch 37: Validation loss decreased (0.251600 --> 0.251195).  Saving model ...
	 Train_Loss: 0.2768 Train_Acc: 0.001 Val_Loss: 0.2512  BEST VAL Loss: 0.2512  Val_Acc: 0.000

Epoch 38: Validation loss decreased (0.251195 --> 0.250814).  Saving model ...
	 Train_Loss: 0.2764 Train_Acc: 0.001 Val_Loss: 0.2508  BEST VAL Loss: 0.2508  Val_Acc: 0.000

Epoch 39: Validation loss decreased (0.250814 --> 0.250399).  Saving model ...
	 Train_Loss: 0.2759 Train_Acc: 0.000 Val_Loss: 0.2504  BEST VAL Loss: 0.2504  Val_Acc: 0.001

Epoch 40: Validation loss decreased (0.250399 --> 0.250066).  Saving model ...
	 Train_Loss: 0.2755 Train_Acc: 0.001 Val_Loss: 0.2501  BEST VAL Loss: 0.2501  Val_Acc: 0.001

Epoch 41: Validation loss decreased (0.250066 --> 0.249714).  Saving model ...
	 Train_Loss: 0.2751 Train_Acc: 0.001 Val_Loss: 0.2497  BEST VAL Loss: 0.2497  Val_Acc: 0.001

Epoch 42: Validation loss decreased (0.249714 --> 0.249413).  Saving model ...
	 Train_Loss: 0.2747 Train_Acc: 0.000 Val_Loss: 0.2494  BEST VAL Loss: 0.2494  Val_Acc: 0.001

Epoch 43: Validation loss decreased (0.249413 --> 0.249073).  Saving model ...
	 Train_Loss: 0.2744 Train_Acc: 0.001 Val_Loss: 0.2491  BEST VAL Loss: 0.2491  Val_Acc: 0.001

Epoch 44: Validation loss decreased (0.249073 --> 0.248714).  Saving model ...
	 Train_Loss: 0.2740 Train_Acc: 0.000 Val_Loss: 0.2487  BEST VAL Loss: 0.2487  Val_Acc: 0.001

Epoch 45: Validation loss decreased (0.248714 --> 0.248422).  Saving model ...
	 Train_Loss: 0.2736 Train_Acc: 0.001 Val_Loss: 0.2484  BEST VAL Loss: 0.2484  Val_Acc: 0.001

Epoch 46: Validation loss decreased (0.248422 --> 0.248096).  Saving model ...
	 Train_Loss: 0.2733 Train_Acc: 0.001 Val_Loss: 0.2481  BEST VAL Loss: 0.2481  Val_Acc: 0.001

Epoch 47: Validation loss decreased (0.248096 --> 0.247796).  Saving model ...
	 Train_Loss: 0.2729 Train_Acc: 0.001 Val_Loss: 0.2478  BEST VAL Loss: 0.2478  Val_Acc: 0.000

Epoch 48: Validation loss decreased (0.247796 --> 0.247518).  Saving model ...
	 Train_Loss: 0.2726 Train_Acc: 0.001 Val_Loss: 0.2475  BEST VAL Loss: 0.2475  Val_Acc: 0.000

Epoch 49: Validation loss decreased (0.247518 --> 0.247228).  Saving model ...
	 Train_Loss: 0.2723 Train_Acc: 0.001 Val_Loss: 0.2472  BEST VAL Loss: 0.2472  Val_Acc: 0.001

Epoch 50: Validation loss decreased (0.247228 --> 0.246956).  Saving model ...
	 Train_Loss: 0.2720 Train_Acc: 0.001 Val_Loss: 0.2470  BEST VAL Loss: 0.2470  Val_Acc: 0.000

Epoch 51: Validation loss decreased (0.246956 --> 0.246696).  Saving model ...
	 Train_Loss: 0.2716 Train_Acc: 0.001 Val_Loss: 0.2467  BEST VAL Loss: 0.2467  Val_Acc: 0.001

Epoch 52: Validation loss decreased (0.246696 --> 0.246429).  Saving model ...
	 Train_Loss: 0.2713 Train_Acc: 0.001 Val_Loss: 0.2464  BEST VAL Loss: 0.2464  Val_Acc: 0.001

Epoch 53: Validation loss decreased (0.246429 --> 0.246162).  Saving model ...
	 Train_Loss: 0.2711 Train_Acc: 0.001 Val_Loss: 0.2462  BEST VAL Loss: 0.2462  Val_Acc: 0.001

Epoch 54: Validation loss decreased (0.246162 --> 0.245906).  Saving model ...
	 Train_Loss: 0.2708 Train_Acc: 0.001 Val_Loss: 0.2459  BEST VAL Loss: 0.2459  Val_Acc: 0.001

Epoch 55: Validation loss decreased (0.245906 --> 0.245616).  Saving model ...
	 Train_Loss: 0.2705 Train_Acc: 0.000 Val_Loss: 0.2456  BEST VAL Loss: 0.2456  Val_Acc: 0.001

Epoch 56: Validation loss decreased (0.245616 --> 0.245376).  Saving model ...
	 Train_Loss: 0.2702 Train_Acc: 0.001 Val_Loss: 0.2454  BEST VAL Loss: 0.2454  Val_Acc: 0.000

Epoch 57: Validation loss decreased (0.245376 --> 0.245143).  Saving model ...
	 Train_Loss: 0.2699 Train_Acc: 0.001 Val_Loss: 0.2451  BEST VAL Loss: 0.2451  Val_Acc: 0.001

Epoch 58: Validation loss decreased (0.245143 --> 0.244899).  Saving model ...
	 Train_Loss: 0.2697 Train_Acc: 0.001 Val_Loss: 0.2449  BEST VAL Loss: 0.2449  Val_Acc: 0.001

Epoch 59: Validation loss decreased (0.244899 --> 0.244690).  Saving model ...
	 Train_Loss: 0.2694 Train_Acc: 0.001 Val_Loss: 0.2447  BEST VAL Loss: 0.2447  Val_Acc: 0.001

Epoch 60: Validation loss decreased (0.244690 --> 0.244502).  Saving model ...
	 Train_Loss: 0.2692 Train_Acc: 0.001 Val_Loss: 0.2445  BEST VAL Loss: 0.2445  Val_Acc: 0.001

Epoch 61: Validation loss decreased (0.244502 --> 0.244295).  Saving model ...
	 Train_Loss: 0.2689 Train_Acc: 0.001 Val_Loss: 0.2443  BEST VAL Loss: 0.2443  Val_Acc: 0.001

Epoch 62: Validation loss decreased (0.244295 --> 0.244087).  Saving model ...
	 Train_Loss: 0.2687 Train_Acc: 0.001 Val_Loss: 0.2441  BEST VAL Loss: 0.2441  Val_Acc: 0.001

Epoch 63: Validation loss decreased (0.244087 --> 0.243865).  Saving model ...
	 Train_Loss: 0.2684 Train_Acc: 0.001 Val_Loss: 0.2439  BEST VAL Loss: 0.2439  Val_Acc: 0.001

Epoch 64: Validation loss decreased (0.243865 --> 0.243658).  Saving model ...
	 Train_Loss: 0.2682 Train_Acc: 0.001 Val_Loss: 0.2437  BEST VAL Loss: 0.2437  Val_Acc: 0.001

Epoch 65: Validation loss decreased (0.243658 --> 0.243462).  Saving model ...
	 Train_Loss: 0.2680 Train_Acc: 0.001 Val_Loss: 0.2435  BEST VAL Loss: 0.2435  Val_Acc: 0.001

Epoch 66: Validation loss decreased (0.243462 --> 0.243275).  Saving model ...
	 Train_Loss: 0.2678 Train_Acc: 0.001 Val_Loss: 0.2433  BEST VAL Loss: 0.2433  Val_Acc: 0.001

Epoch 67: Validation loss decreased (0.243275 --> 0.243098).  Saving model ...
	 Train_Loss: 0.2675 Train_Acc: 0.000 Val_Loss: 0.2431  BEST VAL Loss: 0.2431  Val_Acc: 0.001

Epoch 68: Validation loss decreased (0.243098 --> 0.242908).  Saving model ...
	 Train_Loss: 0.2673 Train_Acc: 0.001 Val_Loss: 0.2429  BEST VAL Loss: 0.2429  Val_Acc: 0.001

Epoch 69: Validation loss decreased (0.242908 --> 0.242722).  Saving model ...
	 Train_Loss: 0.2671 Train_Acc: 0.001 Val_Loss: 0.2427  BEST VAL Loss: 0.2427  Val_Acc: 0.001

Epoch 70: Validation loss decreased (0.242722 --> 0.242548).  Saving model ...
	 Train_Loss: 0.2669 Train_Acc: 0.001 Val_Loss: 0.2425  BEST VAL Loss: 0.2425  Val_Acc: 0.001

Epoch 71: Validation loss decreased (0.242548 --> 0.242403).  Saving model ...
	 Train_Loss: 0.2667 Train_Acc: 0.001 Val_Loss: 0.2424  BEST VAL Loss: 0.2424  Val_Acc: 0.001

Epoch 72: Validation loss decreased (0.242403 --> 0.242209).  Saving model ...
	 Train_Loss: 0.2665 Train_Acc: 0.001 Val_Loss: 0.2422  BEST VAL Loss: 0.2422  Val_Acc: 0.001

Epoch 73: Validation loss decreased (0.242209 --> 0.242042).  Saving model ...
	 Train_Loss: 0.2663 Train_Acc: 0.001 Val_Loss: 0.2420  BEST VAL Loss: 0.2420  Val_Acc: 0.001

Epoch 74: Validation loss decreased (0.242042 --> 0.241877).  Saving model ...
	 Train_Loss: 0.2661 Train_Acc: 0.001 Val_Loss: 0.2419  BEST VAL Loss: 0.2419  Val_Acc: 0.001

Epoch 75: Validation loss decreased (0.241877 --> 0.241717).  Saving model ...
	 Train_Loss: 0.2659 Train_Acc: 0.001 Val_Loss: 0.2417  BEST VAL Loss: 0.2417  Val_Acc: 0.001

Epoch 76: Validation loss decreased (0.241717 --> 0.241542).  Saving model ...
	 Train_Loss: 0.2657 Train_Acc: 0.001 Val_Loss: 0.2415  BEST VAL Loss: 0.2415  Val_Acc: 0.001

Epoch 77: Validation loss decreased (0.241542 --> 0.241382).  Saving model ...
	 Train_Loss: 0.2655 Train_Acc: 0.001 Val_Loss: 0.2414  BEST VAL Loss: 0.2414  Val_Acc: 0.001

Epoch 78: Validation loss decreased (0.241382 --> 0.241215).  Saving model ...
	 Train_Loss: 0.2654 Train_Acc: 0.001 Val_Loss: 0.2412  BEST VAL Loss: 0.2412  Val_Acc: 0.001

Epoch 79: Validation loss decreased (0.241215 --> 0.241053).  Saving model ...
	 Train_Loss: 0.2652 Train_Acc: 0.001 Val_Loss: 0.2411  BEST VAL Loss: 0.2411  Val_Acc: 0.001

Epoch 80: Validation loss decreased (0.241053 --> 0.240917).  Saving model ...
	 Train_Loss: 0.2650 Train_Acc: 0.001 Val_Loss: 0.2409  BEST VAL Loss: 0.2409  Val_Acc: 0.002

Epoch 81: Validation loss decreased (0.240917 --> 0.240788).  Saving model ...
	 Train_Loss: 0.2648 Train_Acc: 0.001 Val_Loss: 0.2408  BEST VAL Loss: 0.2408  Val_Acc: 0.001

Epoch 82: Validation loss decreased (0.240788 --> 0.240648).  Saving model ...
	 Train_Loss: 0.2647 Train_Acc: 0.001 Val_Loss: 0.2406  BEST VAL Loss: 0.2406  Val_Acc: 0.002

Epoch 83: Validation loss decreased (0.240648 --> 0.240507).  Saving model ...
	 Train_Loss: 0.2645 Train_Acc: 0.001 Val_Loss: 0.2405  BEST VAL Loss: 0.2405  Val_Acc: 0.001

Epoch 84: Validation loss decreased (0.240507 --> 0.240376).  Saving model ...
	 Train_Loss: 0.2643 Train_Acc: 0.001 Val_Loss: 0.2404  BEST VAL Loss: 0.2404  Val_Acc: 0.001

Epoch 85: Validation loss decreased (0.240376 --> 0.240256).  Saving model ...
	 Train_Loss: 0.2642 Train_Acc: 0.001 Val_Loss: 0.2403  BEST VAL Loss: 0.2403  Val_Acc: 0.002

Epoch 86: Validation loss decreased (0.240256 --> 0.240114).  Saving model ...
	 Train_Loss: 0.2640 Train_Acc: 0.001 Val_Loss: 0.2401  BEST VAL Loss: 0.2401  Val_Acc: 0.001

Epoch 87: Validation loss decreased (0.240114 --> 0.239983).  Saving model ...
	 Train_Loss: 0.2638 Train_Acc: 0.001 Val_Loss: 0.2400  BEST VAL Loss: 0.2400  Val_Acc: 0.001

Epoch 88: Validation loss decreased (0.239983 --> 0.239837).  Saving model ...
	 Train_Loss: 0.2637 Train_Acc: 0.001 Val_Loss: 0.2398  BEST VAL Loss: 0.2398  Val_Acc: 0.001

Epoch 89: Validation loss decreased (0.239837 --> 0.239689).  Saving model ...
	 Train_Loss: 0.2635 Train_Acc: 0.000 Val_Loss: 0.2397  BEST VAL Loss: 0.2397  Val_Acc: 0.001

Epoch 90: Validation loss decreased (0.239689 --> 0.239563).  Saving model ...
	 Train_Loss: 0.2634 Train_Acc: 0.001 Val_Loss: 0.2396  BEST VAL Loss: 0.2396  Val_Acc: 0.001

Epoch 91: Validation loss decreased (0.239563 --> 0.239432).  Saving model ...
	 Train_Loss: 0.2632 Train_Acc: 0.001 Val_Loss: 0.2394  BEST VAL Loss: 0.2394  Val_Acc: 0.001

Epoch 92: Validation loss decreased (0.239432 --> 0.239302).  Saving model ...
	 Train_Loss: 0.2631 Train_Acc: 0.001 Val_Loss: 0.2393  BEST VAL Loss: 0.2393  Val_Acc: 0.001

Epoch 93: Validation loss decreased (0.239302 --> 0.239187).  Saving model ...
	 Train_Loss: 0.2629 Train_Acc: 0.001 Val_Loss: 0.2392  BEST VAL Loss: 0.2392  Val_Acc: 0.002

Epoch 94: Validation loss decreased (0.239187 --> 0.239074).  Saving model ...
	 Train_Loss: 0.2628 Train_Acc: 0.001 Val_Loss: 0.2391  BEST VAL Loss: 0.2391  Val_Acc: 0.002

Epoch 95: Validation loss decreased (0.239074 --> 0.238964).  Saving model ...
	 Train_Loss: 0.2626 Train_Acc: 0.001 Val_Loss: 0.2390  BEST VAL Loss: 0.2390  Val_Acc: 0.002

Epoch 96: Validation loss decreased (0.238964 --> 0.238850).  Saving model ...
	 Train_Loss: 0.2625 Train_Acc: 0.001 Val_Loss: 0.2389  BEST VAL Loss: 0.2389  Val_Acc: 0.001

Epoch 97: Validation loss decreased (0.238850 --> 0.238734).  Saving model ...
	 Train_Loss: 0.2624 Train_Acc: 0.002 Val_Loss: 0.2387  BEST VAL Loss: 0.2387  Val_Acc: 0.001

Epoch 98: Validation loss decreased (0.238734 --> 0.238613).  Saving model ...
	 Train_Loss: 0.2622 Train_Acc: 0.001 Val_Loss: 0.2386  BEST VAL Loss: 0.2386  Val_Acc: 0.001

Epoch 99: Validation loss decreased (0.238613 --> 0.238503).  Saving model ...
	 Train_Loss: 0.2621 Train_Acc: 0.001 Val_Loss: 0.2385  BEST VAL Loss: 0.2385  Val_Acc: 0.001

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.74      0.63      0.68     95928
           1       0.85      0.91      0.88    722887
           2       0.88      0.82      0.85    593183

    accuracy                           0.86   1411998
   macro avg       0.82      0.79      0.80   1411998
weighted avg       0.86      0.86      0.85   1411998

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.71      0.60      0.65     23982
           1       0.84      0.90      0.87    180722
           2       0.87      0.81      0.84    148296

    accuracy                           0.84    353000
   macro avg       0.81      0.77      0.79    353000
weighted avg       0.84      0.84      0.84    353000

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.59      0.67      0.63    106202
           1       0.90      0.81      0.86   1007425
           2       0.79      0.88      0.83    763802

    accuracy                           0.83   1877429
   macro avg       0.76      0.79      0.77   1877429
weighted avg       0.84      0.83      0.83   1877429

Precision for class 0: 0.667623961883957
Recall for class 0: 0.5912968785182343
Precision for class 1: 0.8112345832195944
Recall for class 1: 0.9044376494700694
Precision for class 2: 0.8841218011997873
Recall for class 2: 0.7908266571730711
3
              precision    recall  f1-score   support

           0       0.67      0.59      0.63    119911
           1       0.81      0.90      0.86    903609
           2       0.88      0.79      0.83    853909

    accuracy                           0.83   1877429
   macro avg       0.79      0.76      0.77   1877429
weighted avg       0.84      0.83      0.83   1877429

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.00      0.00      0.00     26712
           1       0.00      0.00      0.00    131364
           2       0.62      1.00      0.77    261336

    accuracy                           0.62    419412
   macro avg       0.21      0.33      0.26    419412
weighted avg       0.39      0.62      0.48    419412

Precision for class 0: 0.0
Recall for class 0: 0.0
Precision for class 1: 0.0
Recall for class 1: 0.0
Precision for class 2: 1.0
Recall for class 2: 0.6231009127063604
3
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.62      0.77    419412

    accuracy                           0.62    419412
   macro avg       0.33      0.21      0.26    419412
weighted avg       1.00      0.62      0.77    419412

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.34      0.30      0.32     83336
           1       0.79      0.77      0.78    779418
           2       0.73      0.76      0.75    674089

    accuracy                           0.74   1536843
   macro avg       0.62      0.61      0.62   1536843
weighted avg       0.74      0.74      0.74   1536843

Precision for class 0: 0.3044182586157243
Recall for class 0: 0.3354844681892117
Precision for class 1: 0.7683502305566461
Recall for class 1: 0.789043673260191
Precision for class 2: 0.7639525344576161
Recall for class 2: 0.733320327463129
3
              precision    recall  f1-score   support

           0       0.30      0.34      0.32     75619
           1       0.77      0.79      0.78    758977
           2       0.76      0.73      0.75    702247

    accuracy                           0.74   1536843
   macro avg       0.61      0.62      0.62   1536843
weighted avg       0.74      0.74      0.74   1536843

Done
