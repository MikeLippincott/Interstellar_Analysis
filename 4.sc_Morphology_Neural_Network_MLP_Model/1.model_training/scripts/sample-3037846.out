[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '2c58e859'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '6ed53594'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '7f8c8c66'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f828eb43'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: Flagellin_0.100_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: Flagellin_0.100_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'Flagellin_0.100_DMSO_0.025']
The dimensions of the data are: (28140, 1276)
Number of total missing values across all columns: 56280
Data Subset Is Off
Wells held out for testing: ['E14' 'L22']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'L18' 'L19' 'L23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.322718).  Saving model ...
	 Train_Loss: 0.4614 Train_Acc: 78.690 Val_Loss: 0.3227  BEST VAL Loss: 0.3227  Val_Acc: 85.492

Epoch 1: Validation loss decreased (0.322718 --> 0.291390).  Saving model ...
	 Train_Loss: 0.3910 Train_Acc: 86.721 Val_Loss: 0.2914  BEST VAL Loss: 0.2914  Val_Acc: 87.870

Epoch 2: Validation loss decreased (0.291390 --> 0.269013).  Saving model ...
	 Train_Loss: 0.3485 Train_Acc: 88.948 Val_Loss: 0.2690  BEST VAL Loss: 0.2690  Val_Acc: 89.762

Epoch 3: Validation loss decreased (0.269013 --> 0.254396).  Saving model ...
	 Train_Loss: 0.3179 Train_Acc: 90.822 Val_Loss: 0.2544  BEST VAL Loss: 0.2544  Val_Acc: 90.490

Epoch 4: Validation loss decreased (0.254396 --> 0.242839).  Saving model ...
	 Train_Loss: 0.2947 Train_Acc: 91.714 Val_Loss: 0.2428  BEST VAL Loss: 0.2428  Val_Acc: 91.460

Epoch 5: Validation loss decreased (0.242839 --> 0.234536).  Saving model ...
	 Train_Loss: 0.2759 Train_Acc: 92.769 Val_Loss: 0.2345  BEST VAL Loss: 0.2345  Val_Acc: 91.897

Epoch 6: Validation loss decreased (0.234536 --> 0.227775).  Saving model ...
	 Train_Loss: 0.2606 Train_Acc: 93.206 Val_Loss: 0.2278  BEST VAL Loss: 0.2278  Val_Acc: 91.557

Epoch 7: Validation loss decreased (0.227775 --> 0.222172).  Saving model ...
	 Train_Loss: 0.2474 Train_Acc: 93.843 Val_Loss: 0.2222  BEST VAL Loss: 0.2222  Val_Acc: 92.479

Epoch 8: Validation loss decreased (0.222172 --> 0.217175).  Saving model ...
	 Train_Loss: 0.2360 Train_Acc: 94.365 Val_Loss: 0.2172  BEST VAL Loss: 0.2172  Val_Acc: 92.382

Epoch 9: Validation loss decreased (0.217175 --> 0.213800).  Saving model ...
	 Train_Loss: 0.2261 Train_Acc: 94.631 Val_Loss: 0.2138  BEST VAL Loss: 0.2138  Val_Acc: 92.528

Epoch 10: Validation loss decreased (0.213800 --> 0.210904).  Saving model ...
	 Train_Loss: 0.2171 Train_Acc: 95.014 Val_Loss: 0.2109  BEST VAL Loss: 0.2109  Val_Acc: 92.819

Epoch 11: Validation loss decreased (0.210904 --> 0.208036).  Saving model ...
	 Train_Loss: 0.2091 Train_Acc: 95.305 Val_Loss: 0.2080  BEST VAL Loss: 0.2080  Val_Acc: 92.576

Epoch 12: Validation loss decreased (0.208036 --> 0.206016).  Saving model ...
	 Train_Loss: 0.2019 Train_Acc: 95.311 Val_Loss: 0.2060  BEST VAL Loss: 0.2060  Val_Acc: 92.868

Epoch 13: Validation loss decreased (0.206016 --> 0.205176).  Saving model ...
	 Train_Loss: 0.1955 Train_Acc: 95.590 Val_Loss: 0.2052  BEST VAL Loss: 0.2052  Val_Acc: 92.625

Epoch 14: Validation loss decreased (0.205176 --> 0.203788).  Saving model ...
	 Train_Loss: 0.1895 Train_Acc: 95.948 Val_Loss: 0.2038  BEST VAL Loss: 0.2038  Val_Acc: 93.159

Epoch 15: Validation loss decreased (0.203788 --> 0.202557).  Saving model ...
	 Train_Loss: 0.1845 Train_Acc: 95.620 Val_Loss: 0.2026  BEST VAL Loss: 0.2026  Val_Acc: 92.965

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.1799 Train_Acc: 95.736 Val_Loss: 0.2043  BEST VAL Loss: 0.2026  Val_Acc: 92.528

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.1757 Train_Acc: 95.911 Val_Loss: 0.2041  BEST VAL Loss: 0.2026  Val_Acc: 93.595

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.1714 Train_Acc: 96.318 Val_Loss: 0.2031  BEST VAL Loss: 0.2026  Val_Acc: 93.595

Epoch 19: Validation loss decreased (0.202557 --> 0.201881).  Saving model ...
	 Train_Loss: 0.1674 Train_Acc: 96.421 Val_Loss: 0.2019  BEST VAL Loss: 0.2019  Val_Acc: 93.886

Epoch 20: Validation loss decreased (0.201881 --> 0.201178).  Saving model ...
	 Train_Loss: 0.1639 Train_Acc: 96.354 Val_Loss: 0.2012  BEST VAL Loss: 0.2012  Val_Acc: 93.644

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1604 Train_Acc: 96.779 Val_Loss: 0.2012  BEST VAL Loss: 0.2012  Val_Acc: 93.547

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1571 Train_Acc: 96.615 Val_Loss: 0.2013  BEST VAL Loss: 0.2012  Val_Acc: 93.450

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1541 Train_Acc: 96.712 Val_Loss: 0.2017  BEST VAL Loss: 0.2012  Val_Acc: 93.692

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1511 Train_Acc: 96.761 Val_Loss: 0.2025  BEST VAL Loss: 0.2012  Val_Acc: 93.256

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1483 Train_Acc: 96.870 Val_Loss: 0.2033  BEST VAL Loss: 0.2012  Val_Acc: 93.207

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1458 Train_Acc: 96.755 Val_Loss: 0.2035  BEST VAL Loss: 0.2012  Val_Acc: 92.965

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1434 Train_Acc: 96.979 Val_Loss: 0.2040  BEST VAL Loss: 0.2012  Val_Acc: 93.304

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1411 Train_Acc: 96.809 Val_Loss: 0.2051  BEST VAL Loss: 0.2012  Val_Acc: 93.644

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1391 Train_Acc: 96.858 Val_Loss: 0.2061  BEST VAL Loss: 0.2012  Val_Acc: 93.256

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1371 Train_Acc: 96.803 Val_Loss: 0.2079  BEST VAL Loss: 0.2012  Val_Acc: 93.159

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1352 Train_Acc: 97.064 Val_Loss: 0.2086  BEST VAL Loss: 0.2012  Val_Acc: 93.207

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1333 Train_Acc: 96.931 Val_Loss: 0.2095  BEST VAL Loss: 0.2012  Val_Acc: 93.595

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1315 Train_Acc: 96.997 Val_Loss: 0.2110  BEST VAL Loss: 0.2012  Val_Acc: 93.498

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1298 Train_Acc: 96.918 Val_Loss: 0.2123  BEST VAL Loss: 0.2012  Val_Acc: 93.304

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1281 Train_Acc: 97.228 Val_Loss: 0.2140  BEST VAL Loss: 0.2012  Val_Acc: 93.207

Epoch 36: Validation loss did not decrease
Early stopped at epoch : 36
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      8634
           1       0.98      0.98      0.98      7851

    accuracy                           0.98     16485
   macro avg       0.98      0.98      0.98     16485
weighted avg       0.98      0.98      0.98     16485

Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.94      0.94      1079
           1       0.93      0.93      0.93       982

    accuracy                           0.94      2061
   macro avg       0.94      0.94      0.94      2061
weighted avg       0.94      0.94      0.94      2061

Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.94      0.94      1080
           1       0.93      0.93      0.93       981

    accuracy                           0.93      2061
   macro avg       0.93      0.93      0.93      2061
weighted avg       0.93      0.93      0.93      2061

              precision    recall  f1-score   support

           0       0.94      0.94      0.94      1080
           1       0.93      0.93      0.93       981

    accuracy                           0.93      2061
   macro avg       0.93      0.93      0.93      2061
weighted avg       0.93      0.93      0.93      2061

Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.94      0.95      4135
           1       0.93      0.95      0.94      3398

    accuracy                           0.95      7533
   macro avg       0.95      0.95      0.95      7533
weighted avg       0.95      0.95      0.95      7533

              precision    recall  f1-score   support

           0       0.96      0.94      0.95      4135
           1       0.93      0.95      0.94      3398

    accuracy                           0.95      7533
   macro avg       0.95      0.95      0.95      7533
weighted avg       0.95      0.95      0.95      7533

completed
