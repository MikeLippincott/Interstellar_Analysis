[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '3bc02eeb'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '97c254ed'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '5c322070'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '504ad9fa'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: DMSO_0.100_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: DMSO_0.100_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['DMSO_0.100_DMSO_0.025' 'Thapsigargin_1.000_DMSO_0.025']
The dimensions of the data are: (363586, 1270)
Number of total missing values across all columns: 727172
Data Subset Is Off
Wells held out for testing: ['J06' 'K07']
Wells to use for training, validation, and testing ['B06' 'C06' 'B07' 'C07' 'D06' 'D07' 'I06' 'I07' 'K06' 'J07']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.603476).  Saving model ...
	 Train_Loss: 0.6337 Train_Acc: 62.806 Val_Loss: 0.6035  BEST VAL Loss: 0.6035  Val_Acc: 63.010

Epoch 1: Validation loss decreased (0.603476 --> 0.581021).  Saving model ...
	 Train_Loss: 0.6150 Train_Acc: 65.295 Val_Loss: 0.5810  BEST VAL Loss: 0.5810  Val_Acc: 72.100

Epoch 2: Validation loss decreased (0.581021 --> 0.561113).  Saving model ...
	 Train_Loss: 0.6008 Train_Acc: 68.431 Val_Loss: 0.5611  BEST VAL Loss: 0.5611  Val_Acc: 74.054

Epoch 3: Validation loss decreased (0.561113 --> 0.546101).  Saving model ...
	 Train_Loss: 0.5903 Train_Acc: 69.242 Val_Loss: 0.5461  BEST VAL Loss: 0.5461  Val_Acc: 76.001

Epoch 4: Validation loss decreased (0.546101 --> 0.539256).  Saving model ...
	 Train_Loss: 0.5823 Train_Acc: 69.987 Val_Loss: 0.5393  BEST VAL Loss: 0.5393  Val_Acc: 76.829

Epoch 5: Validation loss decreased (0.539256 --> 0.529788).  Saving model ...
	 Train_Loss: 0.5757 Train_Acc: 70.246 Val_Loss: 0.5298  BEST VAL Loss: 0.5298  Val_Acc: 77.807

Epoch 6: Validation loss decreased (0.529788 --> 0.521828).  Saving model ...
	 Train_Loss: 0.5702 Train_Acc: 70.741 Val_Loss: 0.5218  BEST VAL Loss: 0.5218  Val_Acc: 78.285

Epoch 7: Validation loss decreased (0.521828 --> 0.516216).  Saving model ...
	 Train_Loss: 0.5656 Train_Acc: 70.981 Val_Loss: 0.5162  BEST VAL Loss: 0.5162  Val_Acc: 78.658

Epoch 8: Validation loss decreased (0.516216 --> 0.510408).  Saving model ...
	 Train_Loss: 0.5614 Train_Acc: 71.350 Val_Loss: 0.5104  BEST VAL Loss: 0.5104  Val_Acc: 78.625

Epoch 9: Validation loss decreased (0.510408 --> 0.504564).  Saving model ...
	 Train_Loss: 0.5576 Train_Acc: 71.560 Val_Loss: 0.5046  BEST VAL Loss: 0.5046  Val_Acc: 79.892

Epoch 10: Validation loss decreased (0.504564 --> 0.500313).  Saving model ...
	 Train_Loss: 0.5542 Train_Acc: 71.958 Val_Loss: 0.5003  BEST VAL Loss: 0.5003  Val_Acc: 79.832

Epoch 11: Validation loss decreased (0.500313 --> 0.495982).  Saving model ...
	 Train_Loss: 0.5512 Train_Acc: 72.242 Val_Loss: 0.4960  BEST VAL Loss: 0.4960  Val_Acc: 80.800

Epoch 12: Validation loss decreased (0.495982 --> 0.491846).  Saving model ...
	 Train_Loss: 0.5483 Train_Acc: 72.305 Val_Loss: 0.4918  BEST VAL Loss: 0.4918  Val_Acc: 80.868

Epoch 13: Validation loss decreased (0.491846 --> 0.487555).  Saving model ...
	 Train_Loss: 0.5456 Train_Acc: 72.323 Val_Loss: 0.4876  BEST VAL Loss: 0.4876  Val_Acc: 81.507

Epoch 14: Validation loss decreased (0.487555 --> 0.484025).  Saving model ...
	 Train_Loss: 0.5432 Train_Acc: 72.691 Val_Loss: 0.4840  BEST VAL Loss: 0.4840  Val_Acc: 81.174

Epoch 15: Validation loss decreased (0.484025 --> 0.480813).  Saving model ...
	 Train_Loss: 0.5408 Train_Acc: 72.593 Val_Loss: 0.4808  BEST VAL Loss: 0.4808  Val_Acc: 81.402

Epoch 16: Validation loss decreased (0.480813 --> 0.477470).  Saving model ...
	 Train_Loss: 0.5387 Train_Acc: 72.767 Val_Loss: 0.4775  BEST VAL Loss: 0.4775  Val_Acc: 81.557

Epoch 17: Validation loss decreased (0.477470 --> 0.474965).  Saving model ...
	 Train_Loss: 0.5368 Train_Acc: 72.889 Val_Loss: 0.4750  BEST VAL Loss: 0.4750  Val_Acc: 80.985

Epoch 18: Validation loss decreased (0.474965 --> 0.472636).  Saving model ...
	 Train_Loss: 0.5351 Train_Acc: 72.870 Val_Loss: 0.4726  BEST VAL Loss: 0.4726  Val_Acc: 81.180

Epoch 19: Validation loss decreased (0.472636 --> 0.470248).  Saving model ...
	 Train_Loss: 0.5335 Train_Acc: 72.855 Val_Loss: 0.4702  BEST VAL Loss: 0.4702  Val_Acc: 81.614

Epoch 20: Validation loss decreased (0.470248 --> 0.468259).  Saving model ...
	 Train_Loss: 0.5320 Train_Acc: 72.886 Val_Loss: 0.4683  BEST VAL Loss: 0.4683  Val_Acc: 81.365

Epoch 21: Validation loss decreased (0.468259 --> 0.466222).  Saving model ...
	 Train_Loss: 0.5306 Train_Acc: 72.900 Val_Loss: 0.4662  BEST VAL Loss: 0.4662  Val_Acc: 81.584

Epoch 22: Validation loss decreased (0.466222 --> 0.464491).  Saving model ...
	 Train_Loss: 0.5293 Train_Acc: 73.052 Val_Loss: 0.4645  BEST VAL Loss: 0.4645  Val_Acc: 82.112

Epoch 23: Validation loss decreased (0.464491 --> 0.462746).  Saving model ...
	 Train_Loss: 0.5281 Train_Acc: 73.087 Val_Loss: 0.4627  BEST VAL Loss: 0.4627  Val_Acc: 81.803

Epoch 24: Validation loss decreased (0.462746 --> 0.461167).  Saving model ...
	 Train_Loss: 0.5269 Train_Acc: 73.018 Val_Loss: 0.4612  BEST VAL Loss: 0.4612  Val_Acc: 82.324

Epoch 25: Validation loss decreased (0.461167 --> 0.459256).  Saving model ...
	 Train_Loss: 0.5259 Train_Acc: 73.011 Val_Loss: 0.4593  BEST VAL Loss: 0.4593  Val_Acc: 81.547

Epoch 26: Validation loss decreased (0.459256 --> 0.457842).  Saving model ...
	 Train_Loss: 0.5249 Train_Acc: 73.051 Val_Loss: 0.4578  BEST VAL Loss: 0.4578  Val_Acc: 81.349

Epoch 27: Validation loss decreased (0.457842 --> 0.456538).  Saving model ...
	 Train_Loss: 0.5239 Train_Acc: 73.002 Val_Loss: 0.4565  BEST VAL Loss: 0.4565  Val_Acc: 81.873

Epoch 28: Validation loss decreased (0.456538 --> 0.455343).  Saving model ...
	 Train_Loss: 0.5230 Train_Acc: 73.039 Val_Loss: 0.4553  BEST VAL Loss: 0.4553  Val_Acc: 81.762

Epoch 29: Validation loss decreased (0.455343 --> 0.454199).  Saving model ...
	 Train_Loss: 0.5221 Train_Acc: 73.042 Val_Loss: 0.4542  BEST VAL Loss: 0.4542  Val_Acc: 81.944

Epoch 30: Validation loss decreased (0.454199 --> 0.453340).  Saving model ...
	 Train_Loss: 0.5212 Train_Acc: 73.130 Val_Loss: 0.4533  BEST VAL Loss: 0.4533  Val_Acc: 82.119

Epoch 31: Validation loss decreased (0.453340 --> 0.452397).  Saving model ...
	 Train_Loss: 0.5204 Train_Acc: 73.155 Val_Loss: 0.4524  BEST VAL Loss: 0.4524  Val_Acc: 81.523

Epoch 32: Validation loss decreased (0.452397 --> 0.451612).  Saving model ...
	 Train_Loss: 0.5197 Train_Acc: 73.054 Val_Loss: 0.4516  BEST VAL Loss: 0.4516  Val_Acc: 80.884

Epoch 33: Validation loss decreased (0.451612 --> 0.450688).  Saving model ...
	 Train_Loss: 0.5190 Train_Acc: 72.979 Val_Loss: 0.4507  BEST VAL Loss: 0.4507  Val_Acc: 81.739

Epoch 34: Validation loss decreased (0.450688 --> 0.449751).  Saving model ...
	 Train_Loss: 0.5183 Train_Acc: 73.087 Val_Loss: 0.4498  BEST VAL Loss: 0.4498  Val_Acc: 82.260

Epoch 35: Validation loss decreased (0.449751 --> 0.448922).  Saving model ...
	 Train_Loss: 0.5176 Train_Acc: 73.260 Val_Loss: 0.4489  BEST VAL Loss: 0.4489  Val_Acc: 81.951

Epoch 36: Validation loss decreased (0.448922 --> 0.448278).  Saving model ...
	 Train_Loss: 0.5170 Train_Acc: 73.165 Val_Loss: 0.4483  BEST VAL Loss: 0.4483  Val_Acc: 82.613

Epoch 37: Validation loss decreased (0.448278 --> 0.447403).  Saving model ...
	 Train_Loss: 0.5163 Train_Acc: 73.269 Val_Loss: 0.4474  BEST VAL Loss: 0.4474  Val_Acc: 82.485

Epoch 38: Validation loss decreased (0.447403 --> 0.446514).  Saving model ...
	 Train_Loss: 0.5158 Train_Acc: 73.215 Val_Loss: 0.4465  BEST VAL Loss: 0.4465  Val_Acc: 81.537

Epoch 39: Validation loss decreased (0.446514 --> 0.445786).  Saving model ...
	 Train_Loss: 0.5152 Train_Acc: 73.377 Val_Loss: 0.4458  BEST VAL Loss: 0.4458  Val_Acc: 82.337

Epoch 40: Validation loss decreased (0.445786 --> 0.445082).  Saving model ...
	 Train_Loss: 0.5147 Train_Acc: 73.308 Val_Loss: 0.4451  BEST VAL Loss: 0.4451  Val_Acc: 82.394

Epoch 41: Validation loss decreased (0.445082 --> 0.444338).  Saving model ...
	 Train_Loss: 0.5141 Train_Acc: 73.389 Val_Loss: 0.4443  BEST VAL Loss: 0.4443  Val_Acc: 82.260

Epoch 42: Validation loss decreased (0.444338 --> 0.443585).  Saving model ...
	 Train_Loss: 0.5136 Train_Acc: 73.390 Val_Loss: 0.4436  BEST VAL Loss: 0.4436  Val_Acc: 82.307

Epoch 43: Validation loss decreased (0.443585 --> 0.443008).  Saving model ...
	 Train_Loss: 0.5131 Train_Acc: 73.295 Val_Loss: 0.4430  BEST VAL Loss: 0.4430  Val_Acc: 82.004

Epoch 44: Validation loss decreased (0.443008 --> 0.442316).  Saving model ...
	 Train_Loss: 0.5127 Train_Acc: 73.311 Val_Loss: 0.4423  BEST VAL Loss: 0.4423  Val_Acc: 82.297

Epoch 45: Validation loss decreased (0.442316 --> 0.441803).  Saving model ...
	 Train_Loss: 0.5122 Train_Acc: 73.502 Val_Loss: 0.4418  BEST VAL Loss: 0.4418  Val_Acc: 82.310

Epoch 46: Validation loss decreased (0.441803 --> 0.441092).  Saving model ...
	 Train_Loss: 0.5118 Train_Acc: 73.452 Val_Loss: 0.4411  BEST VAL Loss: 0.4411  Val_Acc: 82.327

Epoch 47: Validation loss decreased (0.441092 --> 0.440473).  Saving model ...
	 Train_Loss: 0.5113 Train_Acc: 73.459 Val_Loss: 0.4405  BEST VAL Loss: 0.4405  Val_Acc: 82.546

Epoch 48: Validation loss decreased (0.440473 --> 0.439942).  Saving model ...
	 Train_Loss: 0.5109 Train_Acc: 73.472 Val_Loss: 0.4399  BEST VAL Loss: 0.4399  Val_Acc: 81.779

Epoch 49: Validation loss decreased (0.439942 --> 0.439325).  Saving model ...
	 Train_Loss: 0.5105 Train_Acc: 73.371 Val_Loss: 0.4393  BEST VAL Loss: 0.4393  Val_Acc: 82.310

Epoch 50: Validation loss decreased (0.439325 --> 0.438645).  Saving model ...
	 Train_Loss: 0.5101 Train_Acc: 73.305 Val_Loss: 0.4386  BEST VAL Loss: 0.4386  Val_Acc: 82.176

Epoch 51: Validation loss decreased (0.438645 --> 0.438269).  Saving model ...
	 Train_Loss: 0.5097 Train_Acc: 73.417 Val_Loss: 0.4383  BEST VAL Loss: 0.4383  Val_Acc: 82.445

Epoch 52: Validation loss decreased (0.438269 --> 0.437708).  Saving model ...
	 Train_Loss: 0.5093 Train_Acc: 73.527 Val_Loss: 0.4377  BEST VAL Loss: 0.4377  Val_Acc: 82.452

Epoch 53: Validation loss decreased (0.437708 --> 0.437068).  Saving model ...
	 Train_Loss: 0.5090 Train_Acc: 73.528 Val_Loss: 0.4371  BEST VAL Loss: 0.4371  Val_Acc: 82.307

Epoch 54: Validation loss decreased (0.437068 --> 0.436670).  Saving model ...
	 Train_Loss: 0.5086 Train_Acc: 73.422 Val_Loss: 0.4367  BEST VAL Loss: 0.4367  Val_Acc: 82.445

Epoch 55: Validation loss decreased (0.436670 --> 0.436160).  Saving model ...
	 Train_Loss: 0.5083 Train_Acc: 73.476 Val_Loss: 0.4362  BEST VAL Loss: 0.4362  Val_Acc: 82.748

Epoch 56: Validation loss decreased (0.436160 --> 0.435823).  Saving model ...
	 Train_Loss: 0.5079 Train_Acc: 73.457 Val_Loss: 0.4358  BEST VAL Loss: 0.4358  Val_Acc: 82.647

Epoch 57: Validation loss decreased (0.435823 --> 0.435283).  Saving model ...
	 Train_Loss: 0.5076 Train_Acc: 73.575 Val_Loss: 0.4353  BEST VAL Loss: 0.4353  Val_Acc: 82.499

Epoch 58: Validation loss decreased (0.435283 --> 0.434938).  Saving model ...
	 Train_Loss: 0.5073 Train_Acc: 73.403 Val_Loss: 0.4349  BEST VAL Loss: 0.4349  Val_Acc: 82.986

Epoch 59: Validation loss decreased (0.434938 --> 0.434548).  Saving model ...
	 Train_Loss: 0.5070 Train_Acc: 73.349 Val_Loss: 0.4345  BEST VAL Loss: 0.4345  Val_Acc: 82.502

Epoch 60: Validation loss decreased (0.434548 --> 0.434059).  Saving model ...
	 Train_Loss: 0.5067 Train_Acc: 73.565 Val_Loss: 0.4341  BEST VAL Loss: 0.4341  Val_Acc: 82.711

Epoch 61: Validation loss decreased (0.434059 --> 0.433648).  Saving model ...
	 Train_Loss: 0.5064 Train_Acc: 73.401 Val_Loss: 0.4336  BEST VAL Loss: 0.4336  Val_Acc: 82.472

Epoch 62: Validation loss decreased (0.433648 --> 0.433242).  Saving model ...
	 Train_Loss: 0.5061 Train_Acc: 73.614 Val_Loss: 0.4332  BEST VAL Loss: 0.4332  Val_Acc: 82.751

Epoch 63: Validation loss decreased (0.433242 --> 0.432834).  Saving model ...
	 Train_Loss: 0.5058 Train_Acc: 73.450 Val_Loss: 0.4328  BEST VAL Loss: 0.4328  Val_Acc: 82.405

Epoch 64: Validation loss decreased (0.432834 --> 0.432425).  Saving model ...
	 Train_Loss: 0.5055 Train_Acc: 73.492 Val_Loss: 0.4324  BEST VAL Loss: 0.4324  Val_Acc: 82.495

Epoch 65: Validation loss decreased (0.432425 --> 0.432016).  Saving model ...
	 Train_Loss: 0.5052 Train_Acc: 73.482 Val_Loss: 0.4320  BEST VAL Loss: 0.4320  Val_Acc: 82.694

Epoch 66: Validation loss decreased (0.432016 --> 0.431739).  Saving model ...
	 Train_Loss: 0.5050 Train_Acc: 73.641 Val_Loss: 0.4317  BEST VAL Loss: 0.4317  Val_Acc: 82.324

Epoch 67: Validation loss decreased (0.431739 --> 0.431421).  Saving model ...
	 Train_Loss: 0.5047 Train_Acc: 73.421 Val_Loss: 0.4314  BEST VAL Loss: 0.4314  Val_Acc: 82.529

Epoch 68: Validation loss decreased (0.431421 --> 0.430982).  Saving model ...
	 Train_Loss: 0.5044 Train_Acc: 73.520 Val_Loss: 0.4310  BEST VAL Loss: 0.4310  Val_Acc: 82.613

Epoch 69: Validation loss decreased (0.430982 --> 0.430666).  Saving model ...
	 Train_Loss: 0.5042 Train_Acc: 73.308 Val_Loss: 0.4307  BEST VAL Loss: 0.4307  Val_Acc: 82.919

Epoch 70: Validation loss decreased (0.430666 --> 0.430261).  Saving model ...
	 Train_Loss: 0.5040 Train_Acc: 73.454 Val_Loss: 0.4303  BEST VAL Loss: 0.4303  Val_Acc: 82.704

Epoch 71: Validation loss decreased (0.430261 --> 0.429908).  Saving model ...
	 Train_Loss: 0.5037 Train_Acc: 73.547 Val_Loss: 0.4299  BEST VAL Loss: 0.4299  Val_Acc: 82.569

Epoch 72: Validation loss decreased (0.429908 --> 0.429674).  Saving model ...
	 Train_Loss: 0.5035 Train_Acc: 73.434 Val_Loss: 0.4297  BEST VAL Loss: 0.4297  Val_Acc: 82.680

Epoch 73: Validation loss decreased (0.429674 --> 0.429386).  Saving model ...
	 Train_Loss: 0.5033 Train_Acc: 73.465 Val_Loss: 0.4294  BEST VAL Loss: 0.4294  Val_Acc: 82.519

Epoch 74: Validation loss decreased (0.429386 --> 0.429115).  Saving model ...
	 Train_Loss: 0.5030 Train_Acc: 73.583 Val_Loss: 0.4291  BEST VAL Loss: 0.4291  Val_Acc: 82.818

Epoch 75: Validation loss decreased (0.429115 --> 0.428734).  Saving model ...
	 Train_Loss: 0.5028 Train_Acc: 73.401 Val_Loss: 0.4287  BEST VAL Loss: 0.4287  Val_Acc: 82.909

Epoch 76: Validation loss decreased (0.428734 --> 0.428499).  Saving model ...
	 Train_Loss: 0.5026 Train_Acc: 73.440 Val_Loss: 0.4285  BEST VAL Loss: 0.4285  Val_Acc: 82.630

Epoch 77: Validation loss decreased (0.428499 --> 0.428204).  Saving model ...
	 Train_Loss: 0.5024 Train_Acc: 73.657 Val_Loss: 0.4282  BEST VAL Loss: 0.4282  Val_Acc: 82.526

Epoch 78: Validation loss decreased (0.428204 --> 0.427840).  Saving model ...
	 Train_Loss: 0.5022 Train_Acc: 73.678 Val_Loss: 0.4278  BEST VAL Loss: 0.4278  Val_Acc: 82.536

Epoch 79: Validation loss decreased (0.427840 --> 0.427581).  Saving model ...
	 Train_Loss: 0.5020 Train_Acc: 73.613 Val_Loss: 0.4276  BEST VAL Loss: 0.4276  Val_Acc: 82.781

Epoch 80: Validation loss decreased (0.427581 --> 0.427279).  Saving model ...
	 Train_Loss: 0.5018 Train_Acc: 73.616 Val_Loss: 0.4273  BEST VAL Loss: 0.4273  Val_Acc: 82.734

Epoch 81: Validation loss decreased (0.427279 --> 0.426991).  Saving model ...
	 Train_Loss: 0.5016 Train_Acc: 73.609 Val_Loss: 0.4270  BEST VAL Loss: 0.4270  Val_Acc: 82.418

Epoch 82: Validation loss decreased (0.426991 --> 0.426817).  Saving model ...
	 Train_Loss: 0.5014 Train_Acc: 73.601 Val_Loss: 0.4268  BEST VAL Loss: 0.4268  Val_Acc: 82.852

Epoch 83: Validation loss decreased (0.426817 --> 0.426544).  Saving model ...
	 Train_Loss: 0.5012 Train_Acc: 73.669 Val_Loss: 0.4265  BEST VAL Loss: 0.4265  Val_Acc: 82.842

Epoch 84: Validation loss decreased (0.426544 --> 0.426288).  Saving model ...
	 Train_Loss: 0.5010 Train_Acc: 73.539 Val_Loss: 0.4263  BEST VAL Loss: 0.4263  Val_Acc: 82.680

Epoch 85: Validation loss decreased (0.426288 --> 0.426027).  Saving model ...
	 Train_Loss: 0.5008 Train_Acc: 73.366 Val_Loss: 0.4260  BEST VAL Loss: 0.4260  Val_Acc: 82.815

Epoch 86: Validation loss decreased (0.426027 --> 0.425717).  Saving model ...
	 Train_Loss: 0.5006 Train_Acc: 73.615 Val_Loss: 0.4257  BEST VAL Loss: 0.4257  Val_Acc: 82.966

Epoch 87: Validation loss decreased (0.425717 --> 0.425435).  Saving model ...
	 Train_Loss: 0.5005 Train_Acc: 73.579 Val_Loss: 0.4254  BEST VAL Loss: 0.4254  Val_Acc: 82.643

Epoch 88: Validation loss decreased (0.425435 --> 0.425214).  Saving model ...
	 Train_Loss: 0.5003 Train_Acc: 73.520 Val_Loss: 0.4252  BEST VAL Loss: 0.4252  Val_Acc: 82.993

Epoch 89: Validation loss decreased (0.425214 --> 0.424982).  Saving model ...
	 Train_Loss: 0.5001 Train_Acc: 73.579 Val_Loss: 0.4250  BEST VAL Loss: 0.4250  Val_Acc: 82.754

Epoch 90: Validation loss decreased (0.424982 --> 0.424739).  Saving model ...
	 Train_Loss: 0.4999 Train_Acc: 73.533 Val_Loss: 0.4247  BEST VAL Loss: 0.4247  Val_Acc: 82.553

Epoch 91: Validation loss decreased (0.424739 --> 0.424584).  Saving model ...
	 Train_Loss: 0.4998 Train_Acc: 73.690 Val_Loss: 0.4246  BEST VAL Loss: 0.4246  Val_Acc: 82.973

Epoch 92: Validation loss decreased (0.424584 --> 0.424283).  Saving model ...
	 Train_Loss: 0.4996 Train_Acc: 73.644 Val_Loss: 0.4243  BEST VAL Loss: 0.4243  Val_Acc: 83.017

Epoch 93: Validation loss decreased (0.424283 --> 0.424024).  Saving model ...
	 Train_Loss: 0.4994 Train_Acc: 73.610 Val_Loss: 0.4240  BEST VAL Loss: 0.4240  Val_Acc: 83.044

Epoch 94: Validation loss decreased (0.424024 --> 0.423787).  Saving model ...
	 Train_Loss: 0.4993 Train_Acc: 73.634 Val_Loss: 0.4238  BEST VAL Loss: 0.4238  Val_Acc: 82.263

Epoch 95: Validation loss decreased (0.423787 --> 0.423544).  Saving model ...
	 Train_Loss: 0.4991 Train_Acc: 73.626 Val_Loss: 0.4235  BEST VAL Loss: 0.4235  Val_Acc: 82.986

Epoch 96: Validation loss decreased (0.423544 --> 0.423363).  Saving model ...
	 Train_Loss: 0.4990 Train_Acc: 73.555 Val_Loss: 0.4234  BEST VAL Loss: 0.4234  Val_Acc: 82.694

Epoch 97: Validation loss decreased (0.423363 --> 0.423106).  Saving model ...
	 Train_Loss: 0.4988 Train_Acc: 73.597 Val_Loss: 0.4231  BEST VAL Loss: 0.4231  Val_Acc: 82.842

Epoch 98: Validation loss decreased (0.423106 --> 0.422957).  Saving model ...
	 Train_Loss: 0.4987 Train_Acc: 73.554 Val_Loss: 0.4230  BEST VAL Loss: 0.4230  Val_Acc: 82.331

Epoch 99: Validation loss decreased (0.422957 --> 0.422719).  Saving model ...
	 Train_Loss: 0.4985 Train_Acc: 73.665 Val_Loss: 0.4227  BEST VAL Loss: 0.4227  Val_Acc: 82.946

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.85      0.91      0.88    149884
           1       0.82      0.72      0.77     87993

    accuracy                           0.84    237877
   macro avg       0.83      0.81      0.82    237877
weighted avg       0.84      0.84      0.84    237877

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.84      0.90      0.87     18736
           1       0.81      0.71      0.75     10999

    accuracy                           0.83     29735
   macro avg       0.82      0.80      0.81     29735
weighted avg       0.83      0.83      0.83     29735

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.84      0.90      0.87     18736
           1       0.81      0.71      0.76     10999

    accuracy                           0.83     29735
   macro avg       0.82      0.81      0.81     29735
weighted avg       0.83      0.83      0.83     29735

              precision    recall  f1-score   support

           0       0.84      0.90      0.87     18736
           1       0.81      0.71      0.76     10999

    accuracy                           0.83     29735
   macro avg       0.82      0.81      0.81     29735
weighted avg       0.83      0.83      0.83     29735

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.74      0.83      0.78     27774
           1       0.87      0.79      0.82     38465

    accuracy                           0.81     66239
   macro avg       0.80      0.81      0.80     66239
weighted avg       0.81      0.81      0.81     66239

              precision    recall  f1-score   support

           0       0.74      0.83      0.78     27774
           1       0.87      0.79      0.82     38465

    accuracy                           0.81     66239
   macro avg       0.80      0.81      0.80     66239
weighted avg       0.81      0.81      0.81     66239

completed
