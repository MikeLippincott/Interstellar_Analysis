[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd66b576f'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '1c76b0a8'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '91d7c075'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '918b58fd'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (29021, 1276)
Number of total missing values across all columns: 58042
Data Subset Is Off
Wells held out for testing: ['E14' 'M22']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.401882).  Saving model ...
	 Train_Loss: 0.6006 Train_Acc: 73.056 Val_Loss: 0.4019  BEST VAL Loss: 0.4019  Val_Acc: 83.711

Epoch 1: Validation loss decreased (0.401882 --> 0.353173).  Saving model ...
	 Train_Loss: 0.5008 Train_Acc: 82.221 Val_Loss: 0.3532  BEST VAL Loss: 0.3532  Val_Acc: 85.562

Epoch 2: Validation loss decreased (0.353173 --> 0.316107).  Saving model ...
	 Train_Loss: 0.4415 Train_Acc: 83.893 Val_Loss: 0.3161  BEST VAL Loss: 0.3161  Val_Acc: 88.894

Epoch 3: Validation loss decreased (0.316107 --> 0.294006).  Saving model ...
	 Train_Loss: 0.4021 Train_Acc: 84.726 Val_Loss: 0.2940  BEST VAL Loss: 0.2940  Val_Acc: 90.051

Epoch 4: Validation loss decreased (0.294006 --> 0.278370).  Saving model ...
	 Train_Loss: 0.3751 Train_Acc: 85.501 Val_Loss: 0.2784  BEST VAL Loss: 0.2784  Val_Acc: 90.699

Epoch 5: Validation loss decreased (0.278370 --> 0.263191).  Saving model ...
	 Train_Loss: 0.3543 Train_Acc: 87.960 Val_Loss: 0.2632  BEST VAL Loss: 0.2632  Val_Acc: 92.272

Epoch 6: Validation loss decreased (0.263191 --> 0.253019).  Saving model ...
	 Train_Loss: 0.3361 Train_Acc: 87.248 Val_Loss: 0.2530  BEST VAL Loss: 0.2530  Val_Acc: 92.457

Epoch 7: Validation loss decreased (0.253019 --> 0.245524).  Saving model ...
	 Train_Loss: 0.3219 Train_Acc: 89.597 Val_Loss: 0.2455  BEST VAL Loss: 0.2455  Val_Acc: 90.884

Epoch 8: Validation loss decreased (0.245524 --> 0.239902).  Saving model ...
	 Train_Loss: 0.3102 Train_Acc: 89.852 Val_Loss: 0.2399  BEST VAL Loss: 0.2399  Val_Acc: 91.208

Epoch 9: Validation loss decreased (0.239902 --> 0.234935).  Saving model ...
	 Train_Loss: 0.3006 Train_Acc: 90.159 Val_Loss: 0.2349  BEST VAL Loss: 0.2349  Val_Acc: 91.347

Epoch 10: Validation loss decreased (0.234935 --> 0.230414).  Saving model ...
	 Train_Loss: 0.2913 Train_Acc: 90.795 Val_Loss: 0.2304  BEST VAL Loss: 0.2304  Val_Acc: 92.272

Epoch 11: Validation loss decreased (0.230414 --> 0.225375).  Saving model ...
	 Train_Loss: 0.2836 Train_Acc: 90.760 Val_Loss: 0.2254  BEST VAL Loss: 0.2254  Val_Acc: 92.272

Epoch 12: Validation loss decreased (0.225375 --> 0.222935).  Saving model ...
	 Train_Loss: 0.2758 Train_Acc: 91.489 Val_Loss: 0.2229  BEST VAL Loss: 0.2229  Val_Acc: 91.948

Epoch 13: Validation loss decreased (0.222935 --> 0.220674).  Saving model ...
	 Train_Loss: 0.2692 Train_Acc: 91.588 Val_Loss: 0.2207  BEST VAL Loss: 0.2207  Val_Acc: 92.689

Epoch 14: Validation loss decreased (0.220674 --> 0.217722).  Saving model ...
	 Train_Loss: 0.2628 Train_Acc: 92.062 Val_Loss: 0.2177  BEST VAL Loss: 0.2177  Val_Acc: 92.781

Epoch 15: Validation loss decreased (0.217722 --> 0.215410).  Saving model ...
	 Train_Loss: 0.2573 Train_Acc: 91.987 Val_Loss: 0.2154  BEST VAL Loss: 0.2154  Val_Acc: 92.226

Epoch 16: Validation loss decreased (0.215410 --> 0.213245).  Saving model ...
	 Train_Loss: 0.2519 Train_Acc: 92.178 Val_Loss: 0.2132  BEST VAL Loss: 0.2132  Val_Acc: 93.151

Epoch 17: Validation loss decreased (0.213245 --> 0.210507).  Saving model ...
	 Train_Loss: 0.2476 Train_Acc: 91.929 Val_Loss: 0.2105  BEST VAL Loss: 0.2105  Val_Acc: 92.411

Epoch 18: Validation loss decreased (0.210507 --> 0.208152).  Saving model ...
	 Train_Loss: 0.2432 Train_Acc: 92.282 Val_Loss: 0.2082  BEST VAL Loss: 0.2082  Val_Acc: 92.735

Epoch 19: Validation loss decreased (0.208152 --> 0.206196).  Saving model ...
	 Train_Loss: 0.2392 Train_Acc: 92.340 Val_Loss: 0.2062  BEST VAL Loss: 0.2062  Val_Acc: 93.059

Epoch 20: Validation loss decreased (0.206196 --> 0.203599).  Saving model ...
	 Train_Loss: 0.2351 Train_Acc: 92.913 Val_Loss: 0.2036  BEST VAL Loss: 0.2036  Val_Acc: 93.429

Epoch 21: Validation loss decreased (0.203599 --> 0.201833).  Saving model ...
	 Train_Loss: 0.2313 Train_Acc: 92.895 Val_Loss: 0.2018  BEST VAL Loss: 0.2018  Val_Acc: 93.012

Epoch 22: Validation loss decreased (0.201833 --> 0.199754).  Saving model ...
	 Train_Loss: 0.2281 Train_Acc: 92.698 Val_Loss: 0.1998  BEST VAL Loss: 0.1998  Val_Acc: 93.198

Epoch 23: Validation loss decreased (0.199754 --> 0.198738).  Saving model ...
	 Train_Loss: 0.2247 Train_Acc: 93.202 Val_Loss: 0.1987  BEST VAL Loss: 0.1987  Val_Acc: 93.012

Epoch 24: Validation loss decreased (0.198738 --> 0.197384).  Saving model ...
	 Train_Loss: 0.2218 Train_Acc: 93.127 Val_Loss: 0.1974  BEST VAL Loss: 0.1974  Val_Acc: 93.336

Epoch 25: Validation loss decreased (0.197384 --> 0.196284).  Saving model ...
	 Train_Loss: 0.2189 Train_Acc: 93.121 Val_Loss: 0.1963  BEST VAL Loss: 0.1963  Val_Acc: 92.689

Epoch 26: Validation loss decreased (0.196284 --> 0.196125).  Saving model ...
	 Train_Loss: 0.2160 Train_Acc: 93.260 Val_Loss: 0.1961  BEST VAL Loss: 0.1961  Val_Acc: 93.475

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.2135 Train_Acc: 93.242 Val_Loss: 0.1966  BEST VAL Loss: 0.1961  Val_Acc: 92.642

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.2111 Train_Acc: 93.335 Val_Loss: 0.1969  BEST VAL Loss: 0.1961  Val_Acc: 93.198

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.2090 Train_Acc: 93.491 Val_Loss: 0.1974  BEST VAL Loss: 0.1961  Val_Acc: 92.827

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.2069 Train_Acc: 93.549 Val_Loss: 0.1969  BEST VAL Loss: 0.1961  Val_Acc: 92.874

Epoch 31: Validation loss decreased (0.196125 --> 0.196103).  Saving model ...
	 Train_Loss: 0.2048 Train_Acc: 93.462 Val_Loss: 0.1961  BEST VAL Loss: 0.1961  Val_Acc: 93.290

Epoch 32: Validation loss decreased (0.196103 --> 0.195958).  Saving model ...
	 Train_Loss: 0.2028 Train_Acc: 93.514 Val_Loss: 0.1960  BEST VAL Loss: 0.1960  Val_Acc: 93.475

Epoch 33: Validation loss decreased (0.195958 --> 0.194904).  Saving model ...
	 Train_Loss: 0.2010 Train_Acc: 93.688 Val_Loss: 0.1949  BEST VAL Loss: 0.1949  Val_Acc: 93.614

Epoch 34: Validation loss decreased (0.194904 --> 0.194625).  Saving model ...
	 Train_Loss: 0.1992 Train_Acc: 93.757 Val_Loss: 0.1946  BEST VAL Loss: 0.1946  Val_Acc: 93.012

Epoch 35: Validation loss decreased (0.194625 --> 0.194445).  Saving model ...
	 Train_Loss: 0.1973 Train_Acc: 93.925 Val_Loss: 0.1944  BEST VAL Loss: 0.1944  Val_Acc: 93.336

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1956 Train_Acc: 93.879 Val_Loss: 0.1945  BEST VAL Loss: 0.1944  Val_Acc: 93.012

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1941 Train_Acc: 93.873 Val_Loss: 0.1947  BEST VAL Loss: 0.1944  Val_Acc: 93.845

Epoch 38: Validation loss decreased (0.194445 --> 0.194442).  Saving model ...
	 Train_Loss: 0.1926 Train_Acc: 94.075 Val_Loss: 0.1944  BEST VAL Loss: 0.1944  Val_Acc: 93.614

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1910 Train_Acc: 94.122 Val_Loss: 0.1951  BEST VAL Loss: 0.1944  Val_Acc: 92.827

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1894 Train_Acc: 94.272 Val_Loss: 0.1946  BEST VAL Loss: 0.1944  Val_Acc: 93.522

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1881 Train_Acc: 93.896 Val_Loss: 0.1948  BEST VAL Loss: 0.1944  Val_Acc: 93.105

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1867 Train_Acc: 94.209 Val_Loss: 0.1954  BEST VAL Loss: 0.1944  Val_Acc: 93.290

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1852 Train_Acc: 94.423 Val_Loss: 0.1956  BEST VAL Loss: 0.1944  Val_Acc: 93.244

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1840 Train_Acc: 94.162 Val_Loss: 0.1954  BEST VAL Loss: 0.1944  Val_Acc: 93.475

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.1829 Train_Acc: 94.023 Val_Loss: 0.1958  BEST VAL Loss: 0.1944  Val_Acc: 93.522

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1818 Train_Acc: 94.226 Val_Loss: 0.1964  BEST VAL Loss: 0.1944  Val_Acc: 93.845

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1808 Train_Acc: 94.093 Val_Loss: 0.1972  BEST VAL Loss: 0.1944  Val_Acc: 93.336

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1797 Train_Acc: 94.365 Val_Loss: 0.1973  BEST VAL Loss: 0.1944  Val_Acc: 93.614

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1786 Train_Acc: 94.486 Val_Loss: 0.1985  BEST VAL Loss: 0.1944  Val_Acc: 92.874

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1778 Train_Acc: 94.139 Val_Loss: 0.1992  BEST VAL Loss: 0.1944  Val_Acc: 93.383

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1774 Train_Acc: 93.937 Val_Loss: 0.2018  BEST VAL Loss: 0.1944  Val_Acc: 93.012

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1780 Train_Acc: 93.827 Val_Loss: 0.2023  BEST VAL Loss: 0.1944  Val_Acc: 93.383

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1778 Train_Acc: 93.289 Val_Loss: 0.2023  BEST VAL Loss: 0.1944  Val_Acc: 93.198

Epoch 54: Validation loss did not decrease
Early stopped at epoch : 54
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.97      0.98      9434
           1       0.96      0.99      0.97      7850

    accuracy                           0.97     17284
   macro avg       0.97      0.98      0.97     17284
weighted avg       0.98      0.97      0.97     17284

Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.93      0.94      1179
           1       0.92      0.94      0.93       982

    accuracy                           0.94      2161
   macro avg       0.94      0.94      0.94      2161
weighted avg       0.94      0.94      0.94      2161

Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.94      0.95      1179
           1       0.93      0.95      0.94       982

    accuracy                           0.95      2161
   macro avg       0.95      0.95      0.95      2161
weighted avg       0.95      0.95      0.95      2161

              precision    recall  f1-score   support

           0       0.96      0.94      0.95      1179
           1       0.93      0.95      0.94       982

    accuracy                           0.95      2161
   macro avg       0.95      0.95      0.95      2161
weighted avg       0.95      0.95      0.95      2161

Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.91      0.94      4017
           1       0.90      0.96      0.93      3398

    accuracy                           0.94      7415
   macro avg       0.93      0.94      0.94      7415
weighted avg       0.94      0.94      0.94      7415

              precision    recall  f1-score   support

           0       0.97      0.91      0.94      4017
           1       0.90      0.96      0.93      3398

    accuracy                           0.94      7415
   macro avg       0.93      0.94      0.94      7415
weighted avg       0.94      0.94      0.94      7415

completed
