[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'bc33fb94'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'bb58c985'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '7c601671'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f620b5ac'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: Flagellin_0.100_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: Flagellin_0.100_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_Nigericin_1.000_1.0_DMSO_0.025' 'Flagellin_0.100_DMSO_0.025']
The dimensions of the data are: (31146, 1276)
Number of total missing values across all columns: 29856
Data Subset Is Off
Wells held out for testing: ['K16' 'L22']
Wells to use for training, validation, and testing ['K17' 'L18' 'L19' 'K20' 'K21' 'L23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.510282).  Saving model ...
	 Train_Loss: 0.5907 Train_Acc: 70.574 Val_Loss: 0.5103  BEST VAL Loss: 0.5103  Val_Acc: 77.719

Epoch 1: Validation loss decreased (0.510282 --> 0.503404).  Saving model ...
	 Train_Loss: 0.5296 Train_Acc: 79.555 Val_Loss: 0.5034  BEST VAL Loss: 0.5034  Val_Acc: 80.219

Epoch 2: Validation loss decreased (0.503404 --> 0.480712).  Saving model ...
	 Train_Loss: 0.4924 Train_Acc: 82.642 Val_Loss: 0.4807  BEST VAL Loss: 0.4807  Val_Acc: 82.588

Epoch 3: Validation loss decreased (0.480712 --> 0.452842).  Saving model ...
	 Train_Loss: 0.4643 Train_Acc: 84.555 Val_Loss: 0.4528  BEST VAL Loss: 0.4528  Val_Acc: 85.877

Epoch 4: Validation loss decreased (0.452842 --> 0.431939).  Saving model ...
	 Train_Loss: 0.4434 Train_Acc: 85.668 Val_Loss: 0.4319  BEST VAL Loss: 0.4319  Val_Acc: 86.623

Epoch 5: Validation loss decreased (0.431939 --> 0.416676).  Saving model ...
	 Train_Loss: 0.4256 Train_Acc: 86.907 Val_Loss: 0.4167  BEST VAL Loss: 0.4167  Val_Acc: 86.667

Epoch 6: Validation loss decreased (0.416676 --> 0.404966).  Saving model ...
	 Train_Loss: 0.4106 Train_Acc: 87.713 Val_Loss: 0.4050  BEST VAL Loss: 0.4050  Val_Acc: 88.333

Epoch 7: Validation loss decreased (0.404966 --> 0.394729).  Saving model ...
	 Train_Loss: 0.3985 Train_Acc: 88.239 Val_Loss: 0.3947  BEST VAL Loss: 0.3947  Val_Acc: 87.982

Epoch 8: Validation loss decreased (0.394729 --> 0.384728).  Saving model ...
	 Train_Loss: 0.3878 Train_Acc: 88.530 Val_Loss: 0.3847  BEST VAL Loss: 0.3847  Val_Acc: 89.649

Epoch 9: Validation loss decreased (0.384728 --> 0.380310).  Saving model ...
	 Train_Loss: 0.3767 Train_Acc: 89.956 Val_Loss: 0.3803  BEST VAL Loss: 0.3803  Val_Acc: 88.114

Epoch 10: Validation loss decreased (0.380310 --> 0.374943).  Saving model ...
	 Train_Loss: 0.3679 Train_Acc: 89.868 Val_Loss: 0.3749  BEST VAL Loss: 0.3749  Val_Acc: 89.254

Epoch 11: Validation loss decreased (0.374943 --> 0.367562).  Saving model ...
	 Train_Loss: 0.3598 Train_Acc: 90.191 Val_Loss: 0.3676  BEST VAL Loss: 0.3676  Val_Acc: 89.693

Epoch 12: Validation loss decreased (0.367562 --> 0.364115).  Saving model ...
	 Train_Loss: 0.3528 Train_Acc: 90.147 Val_Loss: 0.3641  BEST VAL Loss: 0.3641  Val_Acc: 88.289

Epoch 13: Validation loss decreased (0.364115 --> 0.358119).  Saving model ...
	 Train_Loss: 0.3455 Train_Acc: 91.189 Val_Loss: 0.3581  BEST VAL Loss: 0.3581  Val_Acc: 90.175

Epoch 14: Validation loss decreased (0.358119 --> 0.355550).  Saving model ...
	 Train_Loss: 0.3395 Train_Acc: 90.986 Val_Loss: 0.3555  BEST VAL Loss: 0.3555  Val_Acc: 89.956

Epoch 15: Validation loss decreased (0.355550 --> 0.352527).  Saving model ...
	 Train_Loss: 0.3337 Train_Acc: 91.282 Val_Loss: 0.3525  BEST VAL Loss: 0.3525  Val_Acc: 90.263

Epoch 16: Validation loss decreased (0.352527 --> 0.350460).  Saving model ...
	 Train_Loss: 0.3285 Train_Acc: 91.480 Val_Loss: 0.3505  BEST VAL Loss: 0.3505  Val_Acc: 90.263

Epoch 17: Validation loss decreased (0.350460 --> 0.348716).  Saving model ...
	 Train_Loss: 0.3233 Train_Acc: 92.138 Val_Loss: 0.3487  BEST VAL Loss: 0.3487  Val_Acc: 90.614

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.3187 Train_Acc: 91.896 Val_Loss: 0.3514  BEST VAL Loss: 0.3487  Val_Acc: 89.474

Epoch 19: Validation loss decreased (0.348716 --> 0.348059).  Saving model ...
	 Train_Loss: 0.3146 Train_Acc: 92.023 Val_Loss: 0.3481  BEST VAL Loss: 0.3481  Val_Acc: 91.360

Epoch 20: Validation loss decreased (0.348059 --> 0.344016).  Saving model ...
	 Train_Loss: 0.3105 Train_Acc: 92.231 Val_Loss: 0.3440  BEST VAL Loss: 0.3440  Val_Acc: 91.096

Epoch 21: Validation loss decreased (0.344016 --> 0.339888).  Saving model ...
	 Train_Loss: 0.3060 Train_Acc: 92.812 Val_Loss: 0.3399  BEST VAL Loss: 0.3399  Val_Acc: 92.018

Epoch 22: Validation loss decreased (0.339888 --> 0.337649).  Saving model ...
	 Train_Loss: 0.3022 Train_Acc: 92.708 Val_Loss: 0.3376  BEST VAL Loss: 0.3376  Val_Acc: 91.711

Epoch 23: Validation loss decreased (0.337649 --> 0.335091).  Saving model ...
	 Train_Loss: 0.2986 Train_Acc: 92.785 Val_Loss: 0.3351  BEST VAL Loss: 0.3351  Val_Acc: 91.404

Epoch 24: Validation loss decreased (0.335091 --> 0.333407).  Saving model ...
	 Train_Loss: 0.2951 Train_Acc: 93.015 Val_Loss: 0.3334  BEST VAL Loss: 0.3334  Val_Acc: 91.360

Epoch 25: Validation loss decreased (0.333407 --> 0.330915).  Saving model ...
	 Train_Loss: 0.2920 Train_Acc: 92.922 Val_Loss: 0.3309  BEST VAL Loss: 0.3309  Val_Acc: 91.228

Epoch 26: Validation loss decreased (0.330915 --> 0.328814).  Saving model ...
	 Train_Loss: 0.2890 Train_Acc: 93.136 Val_Loss: 0.3288  BEST VAL Loss: 0.3288  Val_Acc: 91.667

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.2862 Train_Acc: 93.152 Val_Loss: 0.3294  BEST VAL Loss: 0.3288  Val_Acc: 89.518

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.2834 Train_Acc: 93.158 Val_Loss: 0.3294  BEST VAL Loss: 0.3288  Val_Acc: 91.754

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.2806 Train_Acc: 93.662 Val_Loss: 0.3298  BEST VAL Loss: 0.3288  Val_Acc: 90.658

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.2781 Train_Acc: 93.366 Val_Loss: 0.3288  BEST VAL Loss: 0.3288  Val_Acc: 91.579

Epoch 31: Validation loss decreased (0.328814 --> 0.326944).  Saving model ...
	 Train_Loss: 0.2754 Train_Acc: 93.826 Val_Loss: 0.3269  BEST VAL Loss: 0.3269  Val_Acc: 92.675

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.2728 Train_Acc: 93.925 Val_Loss: 0.3284  BEST VAL Loss: 0.3269  Val_Acc: 90.614

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.2705 Train_Acc: 93.651 Val_Loss: 0.3280  BEST VAL Loss: 0.3269  Val_Acc: 91.623

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.2682 Train_Acc: 93.909 Val_Loss: 0.3272  BEST VAL Loss: 0.3269  Val_Acc: 91.535

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.2659 Train_Acc: 93.985 Val_Loss: 0.3289  BEST VAL Loss: 0.3269  Val_Acc: 91.272

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.2640 Train_Acc: 93.777 Val_Loss: 0.3292  BEST VAL Loss: 0.3269  Val_Acc: 91.096

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.2620 Train_Acc: 93.963 Val_Loss: 0.3303  BEST VAL Loss: 0.3269  Val_Acc: 91.316

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2603 Train_Acc: 93.931 Val_Loss: 0.3297  BEST VAL Loss: 0.3269  Val_Acc: 91.623

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.2586 Train_Acc: 93.865 Val_Loss: 0.3289  BEST VAL Loss: 0.3269  Val_Acc: 91.404

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2568 Train_Acc: 94.287 Val_Loss: 0.3299  BEST VAL Loss: 0.3269  Val_Acc: 90.526

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2552 Train_Acc: 93.876 Val_Loss: 0.3306  BEST VAL Loss: 0.3269  Val_Acc: 91.228

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2537 Train_Acc: 94.106 Val_Loss: 0.3297  BEST VAL Loss: 0.3269  Val_Acc: 92.237

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.2520 Train_Acc: 94.364 Val_Loss: 0.3295  BEST VAL Loss: 0.3269  Val_Acc: 92.237

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.2503 Train_Acc: 94.484 Val_Loss: 0.3306  BEST VAL Loss: 0.3269  Val_Acc: 92.412

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2486 Train_Acc: 94.435 Val_Loss: 0.3310  BEST VAL Loss: 0.3269  Val_Acc: 91.886

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.2471 Train_Acc: 94.342 Val_Loss: 0.3302  BEST VAL Loss: 0.3269  Val_Acc: 92.412

Epoch 47: Validation loss did not decrease
Early stopped at epoch : 47
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.46      0.47      8635
           1       0.53      0.54      0.54      9604

    accuracy                           0.50     18239
   macro avg       0.50      0.50      0.50     18239
weighted avg       0.50      0.50      0.50     18239

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.47      0.48      1079
           1       0.54      0.54      0.54      1201

    accuracy                           0.51      2280
   macro avg       0.51      0.51      0.51      2280
weighted avg       0.51      0.51      0.51      2280

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.49      0.47      0.48      1079
           1       0.54      0.56      0.55      1201

    accuracy                           0.52      2280
   macro avg       0.52      0.52      0.52      2280
weighted avg       0.52      0.52      0.52      2280

              precision    recall  f1-score   support

           0       0.49      0.47      0.48      1079
           1       0.54      0.56      0.55      1201

    accuracy                           0.52      2280
   macro avg       0.52      0.52      0.52      2280
weighted avg       0.52      0.52      0.52      2280

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.50      0.48      0.49      4135
           1       0.51      0.52      0.52      4212

    accuracy                           0.50      8347
   macro avg       0.50      0.50      0.50      8347
weighted avg       0.50      0.50      0.50      8347

              precision    recall  f1-score   support

           0       0.50      0.48      0.49      4135
           1       0.51      0.52      0.52      4212

    accuracy                           0.50      8347
   macro avg       0.50      0.50      0.50      8347
weighted avg       0.50      0.50      0.50      8347

completed
