[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '8666dbed'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '3fee8ee8'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '7fba95cb'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '181c116d'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Flagellin_1.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (29575, 1276)
Number of total missing values across all columns: 59150
Data Subset Is Off
Wells held out for testing: ['D14' 'M22']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.494093).  Saving model ...
	 Train_Loss: 0.6139 Train_Acc: 65.871 Val_Loss: 0.4941  BEST VAL Loss: 0.4941  Val_Acc: 78.302

Epoch 1: Validation loss decreased (0.494093 --> 0.435160).  Saving model ...
	 Train_Loss: 0.5498 Train_Acc: 76.156 Val_Loss: 0.4352  BEST VAL Loss: 0.4352  Val_Acc: 82.884

Epoch 2: Validation loss decreased (0.435160 --> 0.402176).  Saving model ...
	 Train_Loss: 0.5076 Train_Acc: 81.087 Val_Loss: 0.4022  BEST VAL Loss: 0.4022  Val_Acc: 85.759

Epoch 3: Validation loss decreased (0.402176 --> 0.385617).  Saving model ...
	 Train_Loss: 0.4795 Train_Acc: 82.402 Val_Loss: 0.3856  BEST VAL Loss: 0.3856  Val_Acc: 85.310

Epoch 4: Validation loss decreased (0.385617 --> 0.372695).  Saving model ...
	 Train_Loss: 0.4570 Train_Acc: 84.036 Val_Loss: 0.3727  BEST VAL Loss: 0.3727  Val_Acc: 86.882

Epoch 5: Validation loss decreased (0.372695 --> 0.359112).  Saving model ...
	 Train_Loss: 0.4362 Train_Acc: 86.356 Val_Loss: 0.3591  BEST VAL Loss: 0.3591  Val_Acc: 87.197

Epoch 6: Validation loss decreased (0.359112 --> 0.349827).  Saving model ...
	 Train_Loss: 0.4213 Train_Acc: 86.008 Val_Loss: 0.3498  BEST VAL Loss: 0.3498  Val_Acc: 87.781

Epoch 7: Validation loss decreased (0.349827 --> 0.338289).  Saving model ...
	 Train_Loss: 0.4083 Train_Acc: 86.946 Val_Loss: 0.3383  BEST VAL Loss: 0.3383  Val_Acc: 87.916

Epoch 8: Validation loss decreased (0.338289 --> 0.330194).  Saving model ...
	 Train_Loss: 0.3963 Train_Acc: 87.564 Val_Loss: 0.3302  BEST VAL Loss: 0.3302  Val_Acc: 88.814

Epoch 9: Validation loss decreased (0.330194 --> 0.324413).  Saving model ...
	 Train_Loss: 0.3866 Train_Acc: 88.283 Val_Loss: 0.3244  BEST VAL Loss: 0.3244  Val_Acc: 88.859

Epoch 10: Validation loss decreased (0.324413 --> 0.318156).  Saving model ...
	 Train_Loss: 0.3776 Train_Acc: 88.435 Val_Loss: 0.3182  BEST VAL Loss: 0.3182  Val_Acc: 89.398

Epoch 11: Validation loss decreased (0.318156 --> 0.312566).  Saving model ...
	 Train_Loss: 0.3699 Train_Acc: 88.530 Val_Loss: 0.3126  BEST VAL Loss: 0.3126  Val_Acc: 88.904

Epoch 12: Validation loss decreased (0.312566 --> 0.307407).  Saving model ...
	 Train_Loss: 0.3629 Train_Acc: 88.491 Val_Loss: 0.3074  BEST VAL Loss: 0.3074  Val_Acc: 89.308

Epoch 13: Validation loss decreased (0.307407 --> 0.303326).  Saving model ...
	 Train_Loss: 0.3568 Train_Acc: 88.878 Val_Loss: 0.3033  BEST VAL Loss: 0.3033  Val_Acc: 89.847

Epoch 14: Validation loss decreased (0.303326 --> 0.299160).  Saving model ...
	 Train_Loss: 0.3513 Train_Acc: 88.665 Val_Loss: 0.2992  BEST VAL Loss: 0.2992  Val_Acc: 89.488

Epoch 15: Validation loss decreased (0.299160 --> 0.295773).  Saving model ...
	 Train_Loss: 0.3460 Train_Acc: 89.019 Val_Loss: 0.2958  BEST VAL Loss: 0.2958  Val_Acc: 90.027

Epoch 16: Validation loss decreased (0.295773 --> 0.291473).  Saving model ...
	 Train_Loss: 0.3407 Train_Acc: 89.451 Val_Loss: 0.2915  BEST VAL Loss: 0.2915  Val_Acc: 89.712

Epoch 17: Validation loss decreased (0.291473 --> 0.289873).  Saving model ...
	 Train_Loss: 0.3362 Train_Acc: 89.176 Val_Loss: 0.2899  BEST VAL Loss: 0.2899  Val_Acc: 90.072

Epoch 18: Validation loss decreased (0.289873 --> 0.287290).  Saving model ...
	 Train_Loss: 0.3317 Train_Acc: 89.333 Val_Loss: 0.2873  BEST VAL Loss: 0.2873  Val_Acc: 90.162

Epoch 19: Validation loss decreased (0.287290 --> 0.286439).  Saving model ...
	 Train_Loss: 0.3275 Train_Acc: 89.760 Val_Loss: 0.2864  BEST VAL Loss: 0.2864  Val_Acc: 90.252

Epoch 20: Validation loss decreased (0.286439 --> 0.284599).  Saving model ...
	 Train_Loss: 0.3237 Train_Acc: 89.884 Val_Loss: 0.2846  BEST VAL Loss: 0.2846  Val_Acc: 89.982

Epoch 21: Validation loss decreased (0.284599 --> 0.283364).  Saving model ...
	 Train_Loss: 0.3199 Train_Acc: 90.114 Val_Loss: 0.2834  BEST VAL Loss: 0.2834  Val_Acc: 89.847

Epoch 22: Validation loss decreased (0.283364 --> 0.281648).  Saving model ...
	 Train_Loss: 0.3163 Train_Acc: 90.153 Val_Loss: 0.2816  BEST VAL Loss: 0.2816  Val_Acc: 90.611

Epoch 23: Validation loss decreased (0.281648 --> 0.279758).  Saving model ...
	 Train_Loss: 0.3131 Train_Acc: 90.198 Val_Loss: 0.2798  BEST VAL Loss: 0.2798  Val_Acc: 89.937

Epoch 24: Validation loss decreased (0.279758 --> 0.278307).  Saving model ...
	 Train_Loss: 0.3102 Train_Acc: 90.086 Val_Loss: 0.2783  BEST VAL Loss: 0.2783  Val_Acc: 90.117

Epoch 25: Validation loss decreased (0.278307 --> 0.276798).  Saving model ...
	 Train_Loss: 0.3072 Train_Acc: 90.007 Val_Loss: 0.2768  BEST VAL Loss: 0.2768  Val_Acc: 89.982

Epoch 26: Validation loss decreased (0.276798 --> 0.276665).  Saving model ...
	 Train_Loss: 0.3044 Train_Acc: 90.636 Val_Loss: 0.2767  BEST VAL Loss: 0.2767  Val_Acc: 90.431

Epoch 27: Validation loss decreased (0.276665 --> 0.275927).  Saving model ...
	 Train_Loss: 0.3016 Train_Acc: 90.535 Val_Loss: 0.2759  BEST VAL Loss: 0.2759  Val_Acc: 90.521

Epoch 28: Validation loss decreased (0.275927 --> 0.274351).  Saving model ...
	 Train_Loss: 0.2991 Train_Acc: 90.423 Val_Loss: 0.2744  BEST VAL Loss: 0.2744  Val_Acc: 90.476

Epoch 29: Validation loss decreased (0.274351 --> 0.274012).  Saving model ...
	 Train_Loss: 0.2967 Train_Acc: 90.339 Val_Loss: 0.2740  BEST VAL Loss: 0.2740  Val_Acc: 90.027

Epoch 30: Validation loss decreased (0.274012 --> 0.273421).  Saving model ...
	 Train_Loss: 0.2946 Train_Acc: 90.440 Val_Loss: 0.2734  BEST VAL Loss: 0.2734  Val_Acc: 90.881

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.2927 Train_Acc: 90.007 Val_Loss: 0.2739  BEST VAL Loss: 0.2734  Val_Acc: 90.566

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.2905 Train_Acc: 90.591 Val_Loss: 0.2741  BEST VAL Loss: 0.2734  Val_Acc: 90.476

Epoch 33: Validation loss decreased (0.273421 --> 0.273294).  Saving model ...
	 Train_Loss: 0.2885 Train_Acc: 90.670 Val_Loss: 0.2733  BEST VAL Loss: 0.2733  Val_Acc: 90.431

Epoch 34: Validation loss decreased (0.273294 --> 0.273199).  Saving model ...
	 Train_Loss: 0.2867 Train_Acc: 90.344 Val_Loss: 0.2732  BEST VAL Loss: 0.2732  Val_Acc: 90.611

Epoch 35: Validation loss decreased (0.273199 --> 0.271882).  Saving model ...
	 Train_Loss: 0.2848 Train_Acc: 90.496 Val_Loss: 0.2719  BEST VAL Loss: 0.2719  Val_Acc: 90.611

Epoch 36: Validation loss decreased (0.271882 --> 0.271559).  Saving model ...
	 Train_Loss: 0.2831 Train_Acc: 90.625 Val_Loss: 0.2716  BEST VAL Loss: 0.2716  Val_Acc: 90.386

Epoch 37: Validation loss decreased (0.271559 --> 0.271367).  Saving model ...
	 Train_Loss: 0.2813 Train_Acc: 91.221 Val_Loss: 0.2714  BEST VAL Loss: 0.2714  Val_Acc: 90.117

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2797 Train_Acc: 90.788 Val_Loss: 0.2727  BEST VAL Loss: 0.2714  Val_Acc: 90.296

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.2782 Train_Acc: 90.715 Val_Loss: 0.2731  BEST VAL Loss: 0.2714  Val_Acc: 90.476

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2768 Train_Acc: 90.457 Val_Loss: 0.2727  BEST VAL Loss: 0.2714  Val_Acc: 90.162

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2754 Train_Acc: 90.698 Val_Loss: 0.2726  BEST VAL Loss: 0.2714  Val_Acc: 90.296

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2740 Train_Acc: 90.979 Val_Loss: 0.2720  BEST VAL Loss: 0.2714  Val_Acc: 90.881

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.2725 Train_Acc: 91.075 Val_Loss: 0.2723  BEST VAL Loss: 0.2714  Val_Acc: 89.892

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.2713 Train_Acc: 90.653 Val_Loss: 0.2717  BEST VAL Loss: 0.2714  Val_Acc: 89.757

Epoch 45: Validation loss decreased (0.271367 --> 0.271141).  Saving model ...
	 Train_Loss: 0.2699 Train_Acc: 91.395 Val_Loss: 0.2711  BEST VAL Loss: 0.2711  Val_Acc: 91.554

Epoch 46: Validation loss decreased (0.271141 --> 0.270955).  Saving model ...
	 Train_Loss: 0.2685 Train_Acc: 91.288 Val_Loss: 0.2710  BEST VAL Loss: 0.2710  Val_Acc: 91.195

Epoch 47: Validation loss decreased (0.270955 --> 0.270642).  Saving model ...
	 Train_Loss: 0.2671 Train_Acc: 91.501 Val_Loss: 0.2706  BEST VAL Loss: 0.2706  Val_Acc: 90.791

Epoch 48: Validation loss decreased (0.270642 --> 0.270279).  Saving model ...
	 Train_Loss: 0.2656 Train_Acc: 91.574 Val_Loss: 0.2703  BEST VAL Loss: 0.2703  Val_Acc: 90.881

Epoch 49: Validation loss decreased (0.270279 --> 0.270045).  Saving model ...
	 Train_Loss: 0.2644 Train_Acc: 91.507 Val_Loss: 0.2700  BEST VAL Loss: 0.2700  Val_Acc: 90.746

Epoch 50: Validation loss decreased (0.270045 --> 0.269434).  Saving model ...
	 Train_Loss: 0.2632 Train_Acc: 91.282 Val_Loss: 0.2694  BEST VAL Loss: 0.2694  Val_Acc: 90.611

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.2622 Train_Acc: 90.957 Val_Loss: 0.2700  BEST VAL Loss: 0.2694  Val_Acc: 90.431

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.2611 Train_Acc: 91.030 Val_Loss: 0.2702  BEST VAL Loss: 0.2694  Val_Acc: 90.746

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.2601 Train_Acc: 91.041 Val_Loss: 0.2700  BEST VAL Loss: 0.2694  Val_Acc: 90.925

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.2592 Train_Acc: 90.872 Val_Loss: 0.2702  BEST VAL Loss: 0.2694  Val_Acc: 91.015

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.2582 Train_Acc: 91.013 Val_Loss: 0.2704  BEST VAL Loss: 0.2694  Val_Acc: 90.836

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2572 Train_Acc: 91.288 Val_Loss: 0.2699  BEST VAL Loss: 0.2694  Val_Acc: 90.701

Epoch 57: Validation loss decreased (0.269434 --> 0.269143).  Saving model ...
	 Train_Loss: 0.2561 Train_Acc: 91.642 Val_Loss: 0.2691  BEST VAL Loss: 0.2691  Val_Acc: 91.060

Epoch 58: Validation loss decreased (0.269143 --> 0.269000).  Saving model ...
	 Train_Loss: 0.2551 Train_Acc: 91.619 Val_Loss: 0.2690  BEST VAL Loss: 0.2690  Val_Acc: 90.296

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.2540 Train_Acc: 91.754 Val_Loss: 0.2692  BEST VAL Loss: 0.2690  Val_Acc: 90.117

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.2531 Train_Acc: 91.479 Val_Loss: 0.2695  BEST VAL Loss: 0.2690  Val_Acc: 90.611

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.2521 Train_Acc: 91.687 Val_Loss: 0.2691  BEST VAL Loss: 0.2690  Val_Acc: 90.881

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.2512 Train_Acc: 91.782 Val_Loss: 0.2694  BEST VAL Loss: 0.2690  Val_Acc: 90.746

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.2503 Train_Acc: 91.389 Val_Loss: 0.2692  BEST VAL Loss: 0.2690  Val_Acc: 91.779

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.2495 Train_Acc: 91.636 Val_Loss: 0.2693  BEST VAL Loss: 0.2690  Val_Acc: 90.970

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.2486 Train_Acc: 91.530 Val_Loss: 0.2693  BEST VAL Loss: 0.2690  Val_Acc: 91.240

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.2478 Train_Acc: 91.569 Val_Loss: 0.2691  BEST VAL Loss: 0.2690  Val_Acc: 91.195

Epoch 67: Validation loss decreased (0.269000 --> 0.268968).  Saving model ...
	 Train_Loss: 0.2470 Train_Acc: 91.760 Val_Loss: 0.2690  BEST VAL Loss: 0.2690  Val_Acc: 91.240

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.2461 Train_Acc: 91.822 Val_Loss: 0.2691  BEST VAL Loss: 0.2690  Val_Acc: 91.330

Epoch 69: Validation loss decreased (0.268968 --> 0.268820).  Saving model ...
	 Train_Loss: 0.2453 Train_Acc: 91.709 Val_Loss: 0.2688  BEST VAL Loss: 0.2688  Val_Acc: 91.105

Epoch 70: Validation loss decreased (0.268820 --> 0.268798).  Saving model ...
	 Train_Loss: 0.2445 Train_Acc: 91.653 Val_Loss: 0.2688  BEST VAL Loss: 0.2688  Val_Acc: 90.476

Epoch 71: Validation loss decreased (0.268798 --> 0.268231).  Saving model ...
	 Train_Loss: 0.2439 Train_Acc: 91.406 Val_Loss: 0.2682  BEST VAL Loss: 0.2682  Val_Acc: 90.431

Epoch 72: Validation loss decreased (0.268231 --> 0.267729).  Saving model ...
	 Train_Loss: 0.2433 Train_Acc: 91.030 Val_Loss: 0.2677  BEST VAL Loss: 0.2677  Val_Acc: 90.746

Epoch 73: Validation loss decreased (0.267729 --> 0.267600).  Saving model ...
	 Train_Loss: 0.2427 Train_Acc: 91.445 Val_Loss: 0.2676  BEST VAL Loss: 0.2676  Val_Acc: 91.105

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.2420 Train_Acc: 91.653 Val_Loss: 0.2680  BEST VAL Loss: 0.2676  Val_Acc: 91.060

Epoch 75: Validation loss did not decrease
	 Train_Loss: 0.2414 Train_Acc: 91.726 Val_Loss: 0.2686  BEST VAL Loss: 0.2676  Val_Acc: 90.970

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.2406 Train_Acc: 91.990 Val_Loss: 0.2682  BEST VAL Loss: 0.2676  Val_Acc: 91.060

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.2399 Train_Acc: 91.541 Val_Loss: 0.2689  BEST VAL Loss: 0.2676  Val_Acc: 91.330

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.2394 Train_Acc: 91.462 Val_Loss: 0.2690  BEST VAL Loss: 0.2676  Val_Acc: 90.791

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.2386 Train_Acc: 92.046 Val_Loss: 0.2693  BEST VAL Loss: 0.2676  Val_Acc: 91.285

Epoch 80: Validation loss did not decrease
	 Train_Loss: 0.2379 Train_Acc: 92.164 Val_Loss: 0.2701  BEST VAL Loss: 0.2676  Val_Acc: 91.105

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.2374 Train_Acc: 91.687 Val_Loss: 0.2704  BEST VAL Loss: 0.2676  Val_Acc: 90.701

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.2367 Train_Acc: 91.900 Val_Loss: 0.2703  BEST VAL Loss: 0.2676  Val_Acc: 91.015

Epoch 83: Validation loss did not decrease
	 Train_Loss: 0.2361 Train_Acc: 91.996 Val_Loss: 0.2707  BEST VAL Loss: 0.2676  Val_Acc: 90.611

Epoch 84: Validation loss did not decrease
	 Train_Loss: 0.2355 Train_Acc: 91.383 Val_Loss: 0.2705  BEST VAL Loss: 0.2676  Val_Acc: 91.105

Epoch 85: Validation loss did not decrease
	 Train_Loss: 0.2350 Train_Acc: 91.855 Val_Loss: 0.2705  BEST VAL Loss: 0.2676  Val_Acc: 90.566

Epoch 86: Validation loss did not decrease
	 Train_Loss: 0.2344 Train_Acc: 91.996 Val_Loss: 0.2706  BEST VAL Loss: 0.2676  Val_Acc: 90.701

Epoch 87: Validation loss did not decrease
	 Train_Loss: 0.2339 Train_Acc: 91.872 Val_Loss: 0.2705  BEST VAL Loss: 0.2676  Val_Acc: 91.240

Epoch 88: Validation loss did not decrease
	 Train_Loss: 0.2333 Train_Acc: 91.838 Val_Loss: 0.2710  BEST VAL Loss: 0.2676  Val_Acc: 91.465

Epoch 89: Validation loss did not decrease
Early stopped at epoch : 89
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.54      0.53      9433
           1       0.46      0.44      0.45      8370

    accuracy                           0.50     17803
   macro avg       0.49      0.49      0.49     17803
weighted avg       0.50      0.50      0.50     17803

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.55      0.54      1179
           1       0.48      0.46      0.47      1047

    accuracy                           0.51      2226
   macro avg       0.51      0.51      0.51      2226
weighted avg       0.51      0.51      0.51      2226

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.55      0.54      1180
           1       0.48      0.46      0.47      1046

    accuracy                           0.51      2226
   macro avg       0.51      0.51      0.51      2226
weighted avg       0.51      0.51      0.51      2226

              precision    recall  f1-score   support

           0       0.54      0.55      0.54      1180
           1       0.48      0.46      0.47      1046

    accuracy                           0.51      2226
   macro avg       0.51      0.51      0.51      2226
weighted avg       0.51      0.51      0.51      2226

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.52      0.53      4017
           1       0.44      0.46      0.45      3303

    accuracy                           0.50      7320
   macro avg       0.49      0.49      0.49      7320
weighted avg       0.50      0.50      0.50      7320

              precision    recall  f1-score   support

           0       0.54      0.52      0.53      4017
           1       0.44      0.46      0.45      3303

    accuracy                           0.50      7320
   macro avg       0.49      0.49      0.49      7320
weighted avg       0.50      0.50      0.50      7320

completed
