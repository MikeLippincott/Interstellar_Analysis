[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd19bdab4'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '85b801c9'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '697f7559'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e3a609b3'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Flagellin_1.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (29575, 1276)
Number of total missing values across all columns: 59150
Data Subset Is Off
Wells held out for testing: ['D14' 'M22']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.465210).  Saving model ...
	 Train_Loss: 0.5955 Train_Acc: 69.977 Val_Loss: 0.4652  BEST VAL Loss: 0.4652  Val_Acc: 79.919

Epoch 1: Validation loss decreased (0.465210 --> 0.451678).  Saving model ...
	 Train_Loss: 0.5407 Train_Acc: 78.358 Val_Loss: 0.4517  BEST VAL Loss: 0.4517  Val_Acc: 80.997

Epoch 2: Validation loss decreased (0.451678 --> 0.427025).  Saving model ...
	 Train_Loss: 0.5124 Train_Acc: 78.661 Val_Loss: 0.4270  BEST VAL Loss: 0.4270  Val_Acc: 82.570

Epoch 3: Validation loss decreased (0.427025 --> 0.413468).  Saving model ...
	 Train_Loss: 0.4916 Train_Acc: 79.211 Val_Loss: 0.4135  BEST VAL Loss: 0.4135  Val_Acc: 83.243

Epoch 4: Validation loss decreased (0.413468 --> 0.405309).  Saving model ...
	 Train_Loss: 0.4788 Train_Acc: 79.498 Val_Loss: 0.4053  BEST VAL Loss: 0.4053  Val_Acc: 82.031

Epoch 5: Validation loss decreased (0.405309 --> 0.391903).  Saving model ...
	 Train_Loss: 0.4658 Train_Acc: 79.307 Val_Loss: 0.3919  BEST VAL Loss: 0.3919  Val_Acc: 84.951

Epoch 6: Validation loss decreased (0.391903 --> 0.380658).  Saving model ...
	 Train_Loss: 0.4543 Train_Acc: 80.368 Val_Loss: 0.3807  BEST VAL Loss: 0.3807  Val_Acc: 86.119

Epoch 7: Validation loss decreased (0.380658 --> 0.370241).  Saving model ...
	 Train_Loss: 0.4436 Train_Acc: 81.441 Val_Loss: 0.3702  BEST VAL Loss: 0.3702  Val_Acc: 86.298

Epoch 8: Validation loss decreased (0.370241 --> 0.364888).  Saving model ...
	 Train_Loss: 0.4360 Train_Acc: 81.110 Val_Loss: 0.3649  BEST VAL Loss: 0.3649  Val_Acc: 85.759

Epoch 9: Validation loss decreased (0.364888 --> 0.358128).  Saving model ...
	 Train_Loss: 0.4301 Train_Acc: 81.065 Val_Loss: 0.3581  BEST VAL Loss: 0.3581  Val_Acc: 85.984

Epoch 10: Validation loss decreased (0.358128 --> 0.352321).  Saving model ...
	 Train_Loss: 0.4246 Train_Acc: 81.773 Val_Loss: 0.3523  BEST VAL Loss: 0.3523  Val_Acc: 85.984

Epoch 11: Validation loss decreased (0.352321 --> 0.346575).  Saving model ...
	 Train_Loss: 0.4196 Train_Acc: 81.351 Val_Loss: 0.3466  BEST VAL Loss: 0.3466  Val_Acc: 86.703

Epoch 12: Validation loss decreased (0.346575 --> 0.340435).  Saving model ...
	 Train_Loss: 0.4143 Train_Acc: 82.576 Val_Loss: 0.3404  BEST VAL Loss: 0.3404  Val_Acc: 87.421

Epoch 13: Validation loss decreased (0.340435 --> 0.336870).  Saving model ...
	 Train_Loss: 0.4099 Train_Acc: 82.458 Val_Loss: 0.3369  BEST VAL Loss: 0.3369  Val_Acc: 86.613

Epoch 14: Validation loss decreased (0.336870 --> 0.332205).  Saving model ...
	 Train_Loss: 0.4063 Train_Acc: 82.565 Val_Loss: 0.3322  BEST VAL Loss: 0.3322  Val_Acc: 87.960

Epoch 15: Validation loss decreased (0.332205 --> 0.327790).  Saving model ...
	 Train_Loss: 0.4031 Train_Acc: 82.694 Val_Loss: 0.3278  BEST VAL Loss: 0.3278  Val_Acc: 87.960

Epoch 16: Validation loss decreased (0.327790 --> 0.325978).  Saving model ...
	 Train_Loss: 0.3997 Train_Acc: 82.913 Val_Loss: 0.3260  BEST VAL Loss: 0.3260  Val_Acc: 86.568

Epoch 17: Validation loss decreased (0.325978 --> 0.323012).  Saving model ...
	 Train_Loss: 0.3970 Train_Acc: 82.480 Val_Loss: 0.3230  BEST VAL Loss: 0.3230  Val_Acc: 86.658

Epoch 18: Validation loss decreased (0.323012 --> 0.319666).  Saving model ...
	 Train_Loss: 0.3940 Train_Acc: 83.351 Val_Loss: 0.3197  BEST VAL Loss: 0.3197  Val_Acc: 87.826

Epoch 19: Validation loss decreased (0.319666 --> 0.317344).  Saving model ...
	 Train_Loss: 0.3913 Train_Acc: 83.424 Val_Loss: 0.3173  BEST VAL Loss: 0.3173  Val_Acc: 87.871

Epoch 20: Validation loss decreased (0.317344 --> 0.314466).  Saving model ...
	 Train_Loss: 0.3889 Train_Acc: 83.076 Val_Loss: 0.3145  BEST VAL Loss: 0.3145  Val_Acc: 87.466

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.3876 Train_Acc: 82.222 Val_Loss: 0.3146  BEST VAL Loss: 0.3145  Val_Acc: 87.736

Epoch 22: Validation loss decreased (0.314466 --> 0.313203).  Saving model ...
	 Train_Loss: 0.3866 Train_Acc: 82.172 Val_Loss: 0.3132  BEST VAL Loss: 0.3132  Val_Acc: 87.107

Epoch 23: Validation loss decreased (0.313203 --> 0.312146).  Saving model ...
	 Train_Loss: 0.3853 Train_Acc: 83.160 Val_Loss: 0.3121  BEST VAL Loss: 0.3121  Val_Acc: 87.107

Epoch 24: Validation loss decreased (0.312146 --> 0.311744).  Saving model ...
	 Train_Loss: 0.3838 Train_Acc: 83.368 Val_Loss: 0.3117  BEST VAL Loss: 0.3117  Val_Acc: 87.556

Epoch 25: Validation loss decreased (0.311744 --> 0.310375).  Saving model ...
	 Train_Loss: 0.3819 Train_Acc: 84.239 Val_Loss: 0.3104  BEST VAL Loss: 0.3104  Val_Acc: 87.646

Epoch 26: Validation loss decreased (0.310375 --> 0.308939).  Saving model ...
	 Train_Loss: 0.3805 Train_Acc: 83.379 Val_Loss: 0.3089  BEST VAL Loss: 0.3089  Val_Acc: 88.050

Epoch 27: Validation loss decreased (0.308939 --> 0.307337).  Saving model ...
	 Train_Loss: 0.3785 Train_Acc: 84.446 Val_Loss: 0.3073  BEST VAL Loss: 0.3073  Val_Acc: 87.916

Epoch 28: Validation loss decreased (0.307337 --> 0.306031).  Saving model ...
	 Train_Loss: 0.3773 Train_Acc: 82.531 Val_Loss: 0.3060  BEST VAL Loss: 0.3060  Val_Acc: 87.781

Epoch 29: Validation loss decreased (0.306031 --> 0.305704).  Saving model ...
	 Train_Loss: 0.3763 Train_Acc: 82.683 Val_Loss: 0.3057  BEST VAL Loss: 0.3057  Val_Acc: 86.523

Epoch 30: Validation loss decreased (0.305704 --> 0.304621).  Saving model ...
	 Train_Loss: 0.3753 Train_Acc: 82.812 Val_Loss: 0.3046  BEST VAL Loss: 0.3046  Val_Acc: 87.646

Epoch 31: Validation loss decreased (0.304621 --> 0.304061).  Saving model ...
	 Train_Loss: 0.3742 Train_Acc: 83.845 Val_Loss: 0.3041  BEST VAL Loss: 0.3041  Val_Acc: 88.544

Epoch 32: Validation loss decreased (0.304061 --> 0.303092).  Saving model ...
	 Train_Loss: 0.3733 Train_Acc: 82.874 Val_Loss: 0.3031  BEST VAL Loss: 0.3031  Val_Acc: 87.781

Epoch 33: Validation loss decreased (0.303092 --> 0.301855).  Saving model ...
	 Train_Loss: 0.3723 Train_Acc: 83.480 Val_Loss: 0.3019  BEST VAL Loss: 0.3019  Val_Acc: 87.960

Epoch 34: Validation loss decreased (0.301855 --> 0.301385).  Saving model ...
	 Train_Loss: 0.3717 Train_Acc: 83.076 Val_Loss: 0.3014  BEST VAL Loss: 0.3014  Val_Acc: 87.332

Epoch 35: Validation loss decreased (0.301385 --> 0.300057).  Saving model ...
	 Train_Loss: 0.3710 Train_Acc: 82.862 Val_Loss: 0.3001  BEST VAL Loss: 0.3001  Val_Acc: 88.634

Epoch 36: Validation loss decreased (0.300057 --> 0.299487).  Saving model ...
	 Train_Loss: 0.3702 Train_Acc: 83.435 Val_Loss: 0.2995  BEST VAL Loss: 0.2995  Val_Acc: 87.736

Epoch 37: Validation loss decreased (0.299487 --> 0.298501).  Saving model ...
	 Train_Loss: 0.3694 Train_Acc: 84.205 Val_Loss: 0.2985  BEST VAL Loss: 0.2985  Val_Acc: 87.736

Epoch 38: Validation loss decreased (0.298501 --> 0.297762).  Saving model ...
	 Train_Loss: 0.3683 Train_Acc: 84.486 Val_Loss: 0.2978  BEST VAL Loss: 0.2978  Val_Acc: 88.050

Epoch 39: Validation loss decreased (0.297762 --> 0.296994).  Saving model ...
	 Train_Loss: 0.3672 Train_Acc: 83.986 Val_Loss: 0.2970  BEST VAL Loss: 0.2970  Val_Acc: 87.871

Epoch 40: Validation loss decreased (0.296994 --> 0.296292).  Saving model ...
	 Train_Loss: 0.3666 Train_Acc: 83.671 Val_Loss: 0.2963  BEST VAL Loss: 0.2963  Val_Acc: 88.095

Epoch 41: Validation loss decreased (0.296292 --> 0.295570).  Saving model ...
	 Train_Loss: 0.3663 Train_Acc: 84.385 Val_Loss: 0.2956  BEST VAL Loss: 0.2956  Val_Acc: 87.197

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.3670 Train_Acc: 83.604 Val_Loss: 0.2976  BEST VAL Loss: 0.2956  Val_Acc: 86.748

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.3684 Train_Acc: 82.801 Val_Loss: 0.3001  BEST VAL Loss: 0.2956  Val_Acc: 82.075

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.3698 Train_Acc: 82.020 Val_Loss: 0.3010  BEST VAL Loss: 0.2956  Val_Acc: 86.882

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.3711 Train_Acc: 82.200 Val_Loss: 0.3020  BEST VAL Loss: 0.2956  Val_Acc: 87.152

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.3722 Train_Acc: 82.615 Val_Loss: 0.3025  BEST VAL Loss: 0.2956  Val_Acc: 87.556

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.3731 Train_Acc: 82.773 Val_Loss: 0.3028  BEST VAL Loss: 0.2956  Val_Acc: 87.107

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.3741 Train_Acc: 82.110 Val_Loss: 0.3045  BEST VAL Loss: 0.2956  Val_Acc: 82.659

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.3751 Train_Acc: 82.205 Val_Loss: 0.3054  BEST VAL Loss: 0.2956  Val_Acc: 86.119

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.3760 Train_Acc: 82.329 Val_Loss: 0.3062  BEST VAL Loss: 0.2956  Val_Acc: 86.882

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.3769 Train_Acc: 82.093 Val_Loss: 0.3070  BEST VAL Loss: 0.2956  Val_Acc: 87.242

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.3776 Train_Acc: 82.458 Val_Loss: 0.3072  BEST VAL Loss: 0.2956  Val_Acc: 87.960

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.3785 Train_Acc: 82.452 Val_Loss: 0.3084  BEST VAL Loss: 0.2956  Val_Acc: 85.849

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.3793 Train_Acc: 82.306 Val_Loss: 0.3093  BEST VAL Loss: 0.2956  Val_Acc: 85.580

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.3800 Train_Acc: 82.598 Val_Loss: 0.3102  BEST VAL Loss: 0.2956  Val_Acc: 85.984

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.3808 Train_Acc: 82.115 Val_Loss: 0.3116  BEST VAL Loss: 0.2956  Val_Acc: 84.232

Epoch 57: Validation loss did not decrease
Early stopped at epoch : 57
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.88      0.94      0.91      9433
           1       0.93      0.86      0.89      8370

    accuracy                           0.90     17803
   macro avg       0.91      0.90      0.90     17803
weighted avg       0.90      0.90      0.90     17803

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.86      0.91      0.88      1179
           1       0.89      0.83      0.86      1047

    accuracy                           0.87      2226
   macro avg       0.87      0.87      0.87      2226
weighted avg       0.87      0.87      0.87      2226

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.87      0.91      0.89      1180
           1       0.89      0.85      0.87      1046

    accuracy                           0.88      2226
   macro avg       0.88      0.88      0.88      2226
weighted avg       0.88      0.88      0.88      2226

              precision    recall  f1-score   support

           0       0.87      0.91      0.89      1180
           1       0.89      0.85      0.87      1046

    accuracy                           0.88      2226
   macro avg       0.88      0.88      0.88      2226
weighted avg       0.88      0.88      0.88      2226

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.89      0.87      0.88      4017
           1       0.85      0.86      0.85      3303

    accuracy                           0.87      7320
   macro avg       0.87      0.87      0.87      7320
weighted avg       0.87      0.87      0.87      7320

              precision    recall  f1-score   support

           0       0.89      0.87      0.88      4017
           1       0.85      0.86      0.85      3303

    accuracy                           0.87      7320
   macro avg       0.87      0.87      0.87      7320
weighted avg       0.87      0.87      0.87      7320

completed
