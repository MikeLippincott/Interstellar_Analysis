[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '80d89bab'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c7b52a1b'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '373a8e89'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e89d64b7'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_0.100_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_0.100_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_0.100_DMSO_0.025']
The dimensions of the data are: (30722, 1276)
Number of total missing values across all columns: 61444
Data Subset Is Off
Wells held out for testing: ['E14' 'C20']
Wells to use for training, validation, and testing ['E15' 'C16' 'C17' 'C21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.473502).  Saving model ...
	 Train_Loss: 0.7721 Train_Acc: 66.530 Val_Loss: 0.4735  BEST VAL Loss: 0.4735  Val_Acc: 78.628

Epoch 1: Validation loss decreased (0.473502 --> 0.425404).  Saving model ...
	 Train_Loss: 0.6069 Train_Acc: 80.227 Val_Loss: 0.4254  BEST VAL Loss: 0.4254  Val_Acc: 82.386

Epoch 2: Validation loss decreased (0.425404 --> 0.394149).  Saving model ...
	 Train_Loss: 0.5259 Train_Acc: 83.227 Val_Loss: 0.3941  BEST VAL Loss: 0.3941  Val_Acc: 84.484

Epoch 3: Validation loss decreased (0.394149 --> 0.363949).  Saving model ...
	 Train_Loss: 0.4719 Train_Acc: 85.549 Val_Loss: 0.3639  BEST VAL Loss: 0.3639  Val_Acc: 88.243

Epoch 4: Validation loss decreased (0.363949 --> 0.338569).  Saving model ...
	 Train_Loss: 0.4305 Train_Acc: 86.784 Val_Loss: 0.3386  BEST VAL Loss: 0.3386  Val_Acc: 89.117

Epoch 5: Validation loss decreased (0.338569 --> 0.320246).  Saving model ...
	 Train_Loss: 0.3991 Train_Acc: 87.691 Val_Loss: 0.3202  BEST VAL Loss: 0.3202  Val_Acc: 90.385

Epoch 6: Validation loss decreased (0.320246 --> 0.304660).  Saving model ...
	 Train_Loss: 0.3746 Train_Acc: 88.674 Val_Loss: 0.3047  BEST VAL Loss: 0.3047  Val_Acc: 90.516

Epoch 7: Validation loss decreased (0.304660 --> 0.290356).  Saving model ...
	 Train_Loss: 0.3553 Train_Acc: 88.543 Val_Loss: 0.2904  BEST VAL Loss: 0.2904  Val_Acc: 91.171

Epoch 8: Validation loss decreased (0.290356 --> 0.282819).  Saving model ...
	 Train_Loss: 0.3388 Train_Acc: 89.559 Val_Loss: 0.2828  BEST VAL Loss: 0.2828  Val_Acc: 90.953

Epoch 9: Validation loss decreased (0.282819 --> 0.275497).  Saving model ...
	 Train_Loss: 0.3255 Train_Acc: 89.718 Val_Loss: 0.2755  BEST VAL Loss: 0.2755  Val_Acc: 91.696

Epoch 10: Validation loss decreased (0.275497 --> 0.267867).  Saving model ...
	 Train_Loss: 0.3136 Train_Acc: 89.920 Val_Loss: 0.2679  BEST VAL Loss: 0.2679  Val_Acc: 92.220

Epoch 11: Validation loss decreased (0.267867 --> 0.261435).  Saving model ...
	 Train_Loss: 0.3029 Train_Acc: 90.215 Val_Loss: 0.2614  BEST VAL Loss: 0.2614  Val_Acc: 92.788

Epoch 12: Validation loss decreased (0.261435 --> 0.256384).  Saving model ...
	 Train_Loss: 0.2929 Train_Acc: 90.947 Val_Loss: 0.2564  BEST VAL Loss: 0.2564  Val_Acc: 92.570

Epoch 13: Validation loss decreased (0.256384 --> 0.251820).  Saving model ...
	 Train_Loss: 0.2843 Train_Acc: 90.783 Val_Loss: 0.2518  BEST VAL Loss: 0.2518  Val_Acc: 92.701

Epoch 14: Validation loss decreased (0.251820 --> 0.247670).  Saving model ...
	 Train_Loss: 0.2769 Train_Acc: 90.706 Val_Loss: 0.2477  BEST VAL Loss: 0.2477  Val_Acc: 92.876

Epoch 15: Validation loss decreased (0.247670 --> 0.244224).  Saving model ...
	 Train_Loss: 0.2705 Train_Acc: 90.870 Val_Loss: 0.2442  BEST VAL Loss: 0.2442  Val_Acc: 92.876

Epoch 16: Validation loss decreased (0.244224 --> 0.240510).  Saving model ...
	 Train_Loss: 0.2642 Train_Acc: 91.253 Val_Loss: 0.2405  BEST VAL Loss: 0.2405  Val_Acc: 93.357

Epoch 17: Validation loss decreased (0.240510 --> 0.236524).  Saving model ...
	 Train_Loss: 0.2583 Train_Acc: 91.460 Val_Loss: 0.2365  BEST VAL Loss: 0.2365  Val_Acc: 93.269

Epoch 18: Validation loss decreased (0.236524 --> 0.234651).  Saving model ...
	 Train_Loss: 0.2528 Train_Acc: 91.422 Val_Loss: 0.2347  BEST VAL Loss: 0.2347  Val_Acc: 93.051

Epoch 19: Validation loss decreased (0.234651 --> 0.231962).  Saving model ...
	 Train_Loss: 0.2477 Train_Acc: 91.947 Val_Loss: 0.2320  BEST VAL Loss: 0.2320  Val_Acc: 93.488

Epoch 20: Validation loss decreased (0.231962 --> 0.229542).  Saving model ...
	 Train_Loss: 0.2428 Train_Acc: 92.012 Val_Loss: 0.2295  BEST VAL Loss: 0.2295  Val_Acc: 93.094

Epoch 21: Validation loss decreased (0.229542 --> 0.227474).  Saving model ...
	 Train_Loss: 0.2385 Train_Acc: 92.799 Val_Loss: 0.2275  BEST VAL Loss: 0.2275  Val_Acc: 92.657

Epoch 22: Validation loss decreased (0.227474 --> 0.226538).  Saving model ...
	 Train_Loss: 0.2343 Train_Acc: 92.318 Val_Loss: 0.2265  BEST VAL Loss: 0.2265  Val_Acc: 93.138

Epoch 23: Validation loss decreased (0.226538 --> 0.225794).  Saving model ...
	 Train_Loss: 0.2306 Train_Acc: 92.739 Val_Loss: 0.2258  BEST VAL Loss: 0.2258  Val_Acc: 92.351

Epoch 24: Validation loss decreased (0.225794 --> 0.225625).  Saving model ...
	 Train_Loss: 0.2271 Train_Acc: 93.028 Val_Loss: 0.2256  BEST VAL Loss: 0.2256  Val_Acc: 93.094

Epoch 25: Validation loss decreased (0.225625 --> 0.224662).  Saving model ...
	 Train_Loss: 0.2237 Train_Acc: 93.395 Val_Loss: 0.2247  BEST VAL Loss: 0.2247  Val_Acc: 93.138

Epoch 26: Validation loss decreased (0.224662 --> 0.222894).  Saving model ...
	 Train_Loss: 0.2206 Train_Acc: 93.007 Val_Loss: 0.2229  BEST VAL Loss: 0.2229  Val_Acc: 93.007

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.2174 Train_Acc: 93.526 Val_Loss: 0.2239  BEST VAL Loss: 0.2229  Val_Acc: 93.357

Epoch 28: Validation loss decreased (0.222894 --> 0.222122).  Saving model ...
	 Train_Loss: 0.2144 Train_Acc: 93.640 Val_Loss: 0.2221  BEST VAL Loss: 0.2221  Val_Acc: 93.226

Epoch 29: Validation loss decreased (0.222122 --> 0.219879).  Saving model ...
	 Train_Loss: 0.2117 Train_Acc: 93.640 Val_Loss: 0.2199  BEST VAL Loss: 0.2199  Val_Acc: 94.100

Epoch 30: Validation loss decreased (0.219879 --> 0.218197).  Saving model ...
	 Train_Loss: 0.2093 Train_Acc: 93.345 Val_Loss: 0.2182  BEST VAL Loss: 0.2182  Val_Acc: 93.488

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.2068 Train_Acc: 93.772 Val_Loss: 0.2204  BEST VAL Loss: 0.2182  Val_Acc: 92.963

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.2047 Train_Acc: 93.307 Val_Loss: 0.2186  BEST VAL Loss: 0.2182  Val_Acc: 93.357

Epoch 33: Validation loss decreased (0.218197 --> 0.217851).  Saving model ...
	 Train_Loss: 0.2027 Train_Acc: 93.340 Val_Loss: 0.2179  BEST VAL Loss: 0.2179  Val_Acc: 93.182

Epoch 34: Validation loss decreased (0.217851 --> 0.216441).  Saving model ...
	 Train_Loss: 0.2005 Train_Acc: 93.673 Val_Loss: 0.2164  BEST VAL Loss: 0.2164  Val_Acc: 93.837

Epoch 35: Validation loss decreased (0.216441 --> 0.216078).  Saving model ...
	 Train_Loss: 0.1985 Train_Acc: 93.832 Val_Loss: 0.2161  BEST VAL Loss: 0.2161  Val_Acc: 92.876

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1965 Train_Acc: 93.695 Val_Loss: 0.2168  BEST VAL Loss: 0.2161  Val_Acc: 93.051

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1945 Train_Acc: 94.203 Val_Loss: 0.2166  BEST VAL Loss: 0.2161  Val_Acc: 93.444

Epoch 38: Validation loss decreased (0.216078 --> 0.215983).  Saving model ...
	 Train_Loss: 0.1927 Train_Acc: 94.176 Val_Loss: 0.2160  BEST VAL Loss: 0.2160  Val_Acc: 93.138

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1908 Train_Acc: 94.312 Val_Loss: 0.2161  BEST VAL Loss: 0.2160  Val_Acc: 93.400

Epoch 40: Validation loss decreased (0.215983 --> 0.215880).  Saving model ...
	 Train_Loss: 0.1890 Train_Acc: 94.220 Val_Loss: 0.2159  BEST VAL Loss: 0.2159  Val_Acc: 93.182

Epoch 41: Validation loss decreased (0.215880 --> 0.215494).  Saving model ...
	 Train_Loss: 0.1873 Train_Acc: 94.285 Val_Loss: 0.2155  BEST VAL Loss: 0.2155  Val_Acc: 92.788

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1855 Train_Acc: 94.569 Val_Loss: 0.2167  BEST VAL Loss: 0.2155  Val_Acc: 93.619

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1838 Train_Acc: 94.558 Val_Loss: 0.2169  BEST VAL Loss: 0.2155  Val_Acc: 92.876

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1825 Train_Acc: 94.159 Val_Loss: 0.2171  BEST VAL Loss: 0.2155  Val_Acc: 93.794

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.1811 Train_Acc: 94.127 Val_Loss: 0.2165  BEST VAL Loss: 0.2155  Val_Acc: 93.794

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1798 Train_Acc: 94.400 Val_Loss: 0.2161  BEST VAL Loss: 0.2155  Val_Acc: 93.444

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1783 Train_Acc: 94.651 Val_Loss: 0.2171  BEST VAL Loss: 0.2155  Val_Acc: 93.226

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1772 Train_Acc: 93.979 Val_Loss: 0.2178  BEST VAL Loss: 0.2155  Val_Acc: 93.444

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1762 Train_Acc: 94.296 Val_Loss: 0.2172  BEST VAL Loss: 0.2155  Val_Acc: 93.706

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1750 Train_Acc: 94.220 Val_Loss: 0.2182  BEST VAL Loss: 0.2155  Val_Acc: 93.269

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1737 Train_Acc: 94.733 Val_Loss: 0.2183  BEST VAL Loss: 0.2155  Val_Acc: 92.963

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1726 Train_Acc: 94.504 Val_Loss: 0.2179  BEST VAL Loss: 0.2155  Val_Acc: 93.226

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1715 Train_Acc: 94.509 Val_Loss: 0.2198  BEST VAL Loss: 0.2155  Val_Acc: 93.007

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1707 Train_Acc: 94.116 Val_Loss: 0.2198  BEST VAL Loss: 0.2155  Val_Acc: 93.488

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.1697 Train_Acc: 94.367 Val_Loss: 0.2195  BEST VAL Loss: 0.2155  Val_Acc: 93.094

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.1687 Train_Acc: 94.515 Val_Loss: 0.2198  BEST VAL Loss: 0.2155  Val_Acc: 93.925

Epoch 57: Validation loss did not decrease
Early stopped at epoch : 57
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.57      0.55      0.56     10451
           1       0.43      0.45      0.44      7852

    accuracy                           0.51     18303
   macro avg       0.50      0.50      0.50     18303
weighted avg       0.51      0.51      0.51     18303

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.55      0.56      1307
           1       0.42      0.43      0.43       981

    accuracy                           0.50      2288
   macro avg       0.49      0.49      0.49      2288
weighted avg       0.50      0.50      0.50      2288

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.57      0.56      0.56      1307
           1       0.42      0.44      0.43       981

    accuracy                           0.50      2288
   macro avg       0.50      0.50      0.50      2288
weighted avg       0.51      0.50      0.51      2288

              precision    recall  f1-score   support

           0       0.57      0.56      0.56      1307
           1       0.42      0.44      0.43       981

    accuracy                           0.50      2288
   macro avg       0.50      0.50      0.50      2288
weighted avg       0.51      0.50      0.51      2288

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.57      0.55      0.56      4445
           1       0.43      0.44      0.44      3398

    accuracy                           0.51      7843
   macro avg       0.50      0.50      0.50      7843
weighted avg       0.51      0.51      0.51      7843

              precision    recall  f1-score   support

           0       0.57      0.55      0.56      4445
           1       0.43      0.44      0.44      3398

    accuracy                           0.51      7843
   macro avg       0.50      0.50      0.50      7843
weighted avg       0.51      0.51      0.51      7843

completed
