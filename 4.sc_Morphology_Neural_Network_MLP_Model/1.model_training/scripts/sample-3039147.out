[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '63b43255'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '8a990f7a'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '8215bc8e'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2d59b84b'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_0.010_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_0.010_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_0.010_DMSO_0.025']
The dimensions of the data are: (29071, 1276)
Number of total missing values across all columns: 58142
Data Subset Is Off
Wells held out for testing: ['E14' 'B20']
Wells to use for training, validation, and testing ['E15' 'B16' 'B17' 'B21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.378472).  Saving model ...
	 Train_Loss: 0.6188 Train_Acc: 66.143 Val_Loss: 0.3785  BEST VAL Loss: 0.3785  Val_Acc: 85.877

Epoch 1: Validation loss decreased (0.378472 --> 0.342141).  Saving model ...
	 Train_Loss: 0.5353 Train_Acc: 79.515 Val_Loss: 0.3421  BEST VAL Loss: 0.3421  Val_Acc: 89.021

Epoch 2: Validation loss decreased (0.342141 --> 0.304124).  Saving model ...
	 Train_Loss: 0.4853 Train_Acc: 84.111 Val_Loss: 0.3041  BEST VAL Loss: 0.3041  Val_Acc: 91.526

Epoch 3: Validation loss decreased (0.304124 --> 0.280386).  Saving model ...
	 Train_Loss: 0.4501 Train_Acc: 85.905 Val_Loss: 0.2804  BEST VAL Loss: 0.2804  Val_Acc: 91.800

Epoch 4: Validation loss decreased (0.280386 --> 0.264447).  Saving model ...
	 Train_Loss: 0.4254 Train_Acc: 86.987 Val_Loss: 0.2644  BEST VAL Loss: 0.2644  Val_Acc: 92.027

Epoch 5: Validation loss decreased (0.264447 --> 0.256500).  Saving model ...
	 Train_Loss: 0.4077 Train_Acc: 86.890 Val_Loss: 0.2565  BEST VAL Loss: 0.2565  Val_Acc: 91.708

Epoch 6: Validation loss decreased (0.256500 --> 0.247524).  Saving model ...
	 Train_Loss: 0.3940 Train_Acc: 87.653 Val_Loss: 0.2475  BEST VAL Loss: 0.2475  Val_Acc: 92.164

Epoch 7: Validation loss decreased (0.247524 --> 0.240258).  Saving model ...
	 Train_Loss: 0.3836 Train_Acc: 87.801 Val_Loss: 0.2403  BEST VAL Loss: 0.2403  Val_Acc: 92.301

Epoch 8: Validation loss decreased (0.240258 --> 0.234404).  Saving model ...
	 Train_Loss: 0.3744 Train_Acc: 88.485 Val_Loss: 0.2344  BEST VAL Loss: 0.2344  Val_Acc: 93.030

Epoch 9: Validation loss decreased (0.234404 --> 0.229204).  Saving model ...
	 Train_Loss: 0.3662 Train_Acc: 88.877 Val_Loss: 0.2292  BEST VAL Loss: 0.2292  Val_Acc: 92.847

Epoch 10: Validation loss decreased (0.229204 --> 0.224669).  Saving model ...
	 Train_Loss: 0.3590 Train_Acc: 89.060 Val_Loss: 0.2247  BEST VAL Loss: 0.2247  Val_Acc: 92.802

Epoch 11: Validation loss decreased (0.224669 --> 0.221595).  Saving model ...
	 Train_Loss: 0.3529 Train_Acc: 88.929 Val_Loss: 0.2216  BEST VAL Loss: 0.2216  Val_Acc: 93.667

Epoch 12: Validation loss decreased (0.221595 --> 0.218518).  Saving model ...
	 Train_Loss: 0.3466 Train_Acc: 89.658 Val_Loss: 0.2185  BEST VAL Loss: 0.2185  Val_Acc: 93.030

Epoch 13: Validation loss decreased (0.218518 --> 0.217184).  Saving model ...
	 Train_Loss: 0.3411 Train_Acc: 89.709 Val_Loss: 0.2172  BEST VAL Loss: 0.2172  Val_Acc: 92.528

Epoch 14: Validation loss decreased (0.217184 --> 0.215824).  Saving model ...
	 Train_Loss: 0.3363 Train_Acc: 89.891 Val_Loss: 0.2158  BEST VAL Loss: 0.2158  Val_Acc: 93.394

Epoch 15: Validation loss decreased (0.215824 --> 0.212656).  Saving model ...
	 Train_Loss: 0.3318 Train_Acc: 89.908 Val_Loss: 0.2127  BEST VAL Loss: 0.2127  Val_Acc: 93.531

Epoch 16: Validation loss decreased (0.212656 --> 0.210643).  Saving model ...
	 Train_Loss: 0.3278 Train_Acc: 89.971 Val_Loss: 0.2106  BEST VAL Loss: 0.2106  Val_Acc: 93.121

Epoch 17: Validation loss decreased (0.210643 --> 0.208564).  Saving model ...
	 Train_Loss: 0.3243 Train_Acc: 89.755 Val_Loss: 0.2086  BEST VAL Loss: 0.2086  Val_Acc: 93.576

Epoch 18: Validation loss decreased (0.208564 --> 0.207099).  Saving model ...
	 Train_Loss: 0.3208 Train_Acc: 90.056 Val_Loss: 0.2071  BEST VAL Loss: 0.2071  Val_Acc: 93.622

Epoch 19: Validation loss decreased (0.207099 --> 0.205817).  Saving model ...
	 Train_Loss: 0.3175 Train_Acc: 90.256 Val_Loss: 0.2058  BEST VAL Loss: 0.2058  Val_Acc: 92.893

Epoch 20: Validation loss decreased (0.205817 --> 0.204091).  Saving model ...
	 Train_Loss: 0.3143 Train_Acc: 90.540 Val_Loss: 0.2041  BEST VAL Loss: 0.2041  Val_Acc: 92.711

Epoch 21: Validation loss decreased (0.204091 --> 0.202970).  Saving model ...
	 Train_Loss: 0.3113 Train_Acc: 90.358 Val_Loss: 0.2030  BEST VAL Loss: 0.2030  Val_Acc: 93.166

Epoch 22: Validation loss decreased (0.202970 --> 0.201275).  Saving model ...
	 Train_Loss: 0.3083 Train_Acc: 90.671 Val_Loss: 0.2013  BEST VAL Loss: 0.2013  Val_Acc: 93.759

Epoch 23: Validation loss decreased (0.201275 --> 0.199465).  Saving model ...
	 Train_Loss: 0.3058 Train_Acc: 90.387 Val_Loss: 0.1995  BEST VAL Loss: 0.1995  Val_Acc: 93.394

Epoch 24: Validation loss decreased (0.199465 --> 0.199000).  Saving model ...
	 Train_Loss: 0.3031 Train_Acc: 90.865 Val_Loss: 0.1990  BEST VAL Loss: 0.1990  Val_Acc: 92.756

Epoch 25: Validation loss decreased (0.199000 --> 0.197253).  Saving model ...
	 Train_Loss: 0.3011 Train_Acc: 90.444 Val_Loss: 0.1973  BEST VAL Loss: 0.1973  Val_Acc: 94.077

Epoch 26: Validation loss decreased (0.197253 --> 0.196608).  Saving model ...
	 Train_Loss: 0.2986 Train_Acc: 91.002 Val_Loss: 0.1966  BEST VAL Loss: 0.1966  Val_Acc: 93.713

Epoch 27: Validation loss decreased (0.196608 --> 0.196437).  Saving model ...
	 Train_Loss: 0.2966 Train_Acc: 90.563 Val_Loss: 0.1964  BEST VAL Loss: 0.1964  Val_Acc: 93.713

Epoch 28: Validation loss decreased (0.196437 --> 0.196390).  Saving model ...
	 Train_Loss: 0.2946 Train_Acc: 90.791 Val_Loss: 0.1964  BEST VAL Loss: 0.1964  Val_Acc: 93.349

Epoch 29: Validation loss decreased (0.196390 --> 0.195618).  Saving model ...
	 Train_Loss: 0.2925 Train_Acc: 91.047 Val_Loss: 0.1956  BEST VAL Loss: 0.1956  Val_Acc: 93.303

Epoch 30: Validation loss decreased (0.195618 --> 0.194601).  Saving model ...
	 Train_Loss: 0.2906 Train_Acc: 91.361 Val_Loss: 0.1946  BEST VAL Loss: 0.1946  Val_Acc: 93.349

Epoch 31: Validation loss decreased (0.194601 --> 0.193475).  Saving model ...
	 Train_Loss: 0.2888 Train_Acc: 90.928 Val_Loss: 0.1935  BEST VAL Loss: 0.1935  Val_Acc: 94.487

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.2871 Train_Acc: 91.127 Val_Loss: 0.1935  BEST VAL Loss: 0.1935  Val_Acc: 94.260

Epoch 33: Validation loss decreased (0.193475 --> 0.192744).  Saving model ...
	 Train_Loss: 0.2854 Train_Acc: 90.916 Val_Loss: 0.1927  BEST VAL Loss: 0.1927  Val_Acc: 94.123

Epoch 34: Validation loss decreased (0.192744 --> 0.192698).  Saving model ...
	 Train_Loss: 0.2837 Train_Acc: 91.617 Val_Loss: 0.1927  BEST VAL Loss: 0.1927  Val_Acc: 94.442

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.2824 Train_Acc: 91.093 Val_Loss: 0.1929  BEST VAL Loss: 0.1927  Val_Acc: 93.394

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.2809 Train_Acc: 91.201 Val_Loss: 0.1928  BEST VAL Loss: 0.1927  Val_Acc: 93.759

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.2794 Train_Acc: 91.212 Val_Loss: 0.1928  BEST VAL Loss: 0.1927  Val_Acc: 93.485

Epoch 38: Validation loss decreased (0.192698 --> 0.192300).  Saving model ...
	 Train_Loss: 0.2780 Train_Acc: 91.355 Val_Loss: 0.1923  BEST VAL Loss: 0.1923  Val_Acc: 93.895

Epoch 39: Validation loss decreased (0.192300 --> 0.191502).  Saving model ...
	 Train_Loss: 0.2765 Train_Acc: 91.771 Val_Loss: 0.1915  BEST VAL Loss: 0.1915  Val_Acc: 94.214

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2751 Train_Acc: 91.651 Val_Loss: 0.1917  BEST VAL Loss: 0.1915  Val_Acc: 93.440

Epoch 41: Validation loss decreased (0.191502 --> 0.191344).  Saving model ...
	 Train_Loss: 0.2739 Train_Acc: 91.509 Val_Loss: 0.1913  BEST VAL Loss: 0.1913  Val_Acc: 93.850

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2725 Train_Acc: 91.839 Val_Loss: 0.1915  BEST VAL Loss: 0.1913  Val_Acc: 93.576

Epoch 43: Validation loss decreased (0.191344 --> 0.190908).  Saving model ...
	 Train_Loss: 0.2712 Train_Acc: 91.719 Val_Loss: 0.1909  BEST VAL Loss: 0.1909  Val_Acc: 93.941

Epoch 44: Validation loss decreased (0.190908 --> 0.190753).  Saving model ...
	 Train_Loss: 0.2701 Train_Acc: 91.423 Val_Loss: 0.1908  BEST VAL Loss: 0.1908  Val_Acc: 93.986

Epoch 45: Validation loss decreased (0.190753 --> 0.190433).  Saving model ...
	 Train_Loss: 0.2690 Train_Acc: 91.497 Val_Loss: 0.1904  BEST VAL Loss: 0.1904  Val_Acc: 93.895

Epoch 46: Validation loss decreased (0.190433 --> 0.190025).  Saving model ...
	 Train_Loss: 0.2678 Train_Acc: 91.839 Val_Loss: 0.1900  BEST VAL Loss: 0.1900  Val_Acc: 93.440

Epoch 47: Validation loss decreased (0.190025 --> 0.189673).  Saving model ...
	 Train_Loss: 0.2668 Train_Acc: 91.605 Val_Loss: 0.1897  BEST VAL Loss: 0.1897  Val_Acc: 93.713

Epoch 48: Validation loss decreased (0.189673 --> 0.189589).  Saving model ...
	 Train_Loss: 0.2657 Train_Acc: 91.725 Val_Loss: 0.1896  BEST VAL Loss: 0.1896  Val_Acc: 93.759

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2645 Train_Acc: 92.295 Val_Loss: 0.1898  BEST VAL Loss: 0.1896  Val_Acc: 94.214

Epoch 50: Validation loss decreased (0.189589 --> 0.189450).  Saving model ...
	 Train_Loss: 0.2635 Train_Acc: 91.702 Val_Loss: 0.1895  BEST VAL Loss: 0.1895  Val_Acc: 93.804

Epoch 51: Validation loss decreased (0.189450 --> 0.188812).  Saving model ...
	 Train_Loss: 0.2626 Train_Acc: 91.423 Val_Loss: 0.1888  BEST VAL Loss: 0.1888  Val_Acc: 94.123

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.2615 Train_Acc: 92.055 Val_Loss: 0.1895  BEST VAL Loss: 0.1888  Val_Acc: 93.622

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.2606 Train_Acc: 91.793 Val_Loss: 0.1893  BEST VAL Loss: 0.1888  Val_Acc: 94.077

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.2596 Train_Acc: 92.232 Val_Loss: 0.1890  BEST VAL Loss: 0.1888  Val_Acc: 93.804

Epoch 55: Validation loss decreased (0.188812 --> 0.188466).  Saving model ...
	 Train_Loss: 0.2588 Train_Acc: 91.662 Val_Loss: 0.1885  BEST VAL Loss: 0.1885  Val_Acc: 94.077

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2580 Train_Acc: 91.537 Val_Loss: 0.1888  BEST VAL Loss: 0.1885  Val_Acc: 93.804

Epoch 57: Validation loss decreased (0.188466 --> 0.188439).  Saving model ...
	 Train_Loss: 0.2571 Train_Acc: 91.879 Val_Loss: 0.1884  BEST VAL Loss: 0.1884  Val_Acc: 93.667

Epoch 58: Validation loss decreased (0.188439 --> 0.188295).  Saving model ...
	 Train_Loss: 0.2563 Train_Acc: 91.822 Val_Loss: 0.1883  BEST VAL Loss: 0.1883  Val_Acc: 93.804

Epoch 59: Validation loss decreased (0.188295 --> 0.188073).  Saving model ...
	 Train_Loss: 0.2554 Train_Acc: 92.613 Val_Loss: 0.1881  BEST VAL Loss: 0.1881  Val_Acc: 93.895

Epoch 60: Validation loss decreased (0.188073 --> 0.187647).  Saving model ...
	 Train_Loss: 0.2545 Train_Acc: 91.936 Val_Loss: 0.1876  BEST VAL Loss: 0.1876  Val_Acc: 94.123

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.2537 Train_Acc: 92.112 Val_Loss: 0.1878  BEST VAL Loss: 0.1876  Val_Acc: 93.986

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.2529 Train_Acc: 92.249 Val_Loss: 0.1880  BEST VAL Loss: 0.1876  Val_Acc: 93.759

Epoch 63: Validation loss decreased (0.187647 --> 0.187622).  Saving model ...
	 Train_Loss: 0.2521 Train_Acc: 92.357 Val_Loss: 0.1876  BEST VAL Loss: 0.1876  Val_Acc: 94.260

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.2513 Train_Acc: 91.907 Val_Loss: 0.1877  BEST VAL Loss: 0.1876  Val_Acc: 94.305

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.2507 Train_Acc: 91.913 Val_Loss: 0.1878  BEST VAL Loss: 0.1876  Val_Acc: 93.440

Epoch 66: Validation loss decreased (0.187622 --> 0.187582).  Saving model ...
	 Train_Loss: 0.2501 Train_Acc: 92.146 Val_Loss: 0.1876  BEST VAL Loss: 0.1876  Val_Acc: 93.986

Epoch 67: Validation loss decreased (0.187582 --> 0.187295).  Saving model ...
	 Train_Loss: 0.2494 Train_Acc: 92.141 Val_Loss: 0.1873  BEST VAL Loss: 0.1873  Val_Acc: 94.214

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.2487 Train_Acc: 92.312 Val_Loss: 0.1882  BEST VAL Loss: 0.1873  Val_Acc: 94.579

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.2481 Train_Acc: 92.181 Val_Loss: 0.1878  BEST VAL Loss: 0.1873  Val_Acc: 94.442

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.2473 Train_Acc: 92.443 Val_Loss: 0.1881  BEST VAL Loss: 0.1873  Val_Acc: 94.260

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.2468 Train_Acc: 92.044 Val_Loss: 0.1883  BEST VAL Loss: 0.1873  Val_Acc: 93.576

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.2461 Train_Acc: 92.482 Val_Loss: 0.1887  BEST VAL Loss: 0.1873  Val_Acc: 93.485

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.2456 Train_Acc: 91.981 Val_Loss: 0.1884  BEST VAL Loss: 0.1873  Val_Acc: 94.123

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.2450 Train_Acc: 92.312 Val_Loss: 0.1890  BEST VAL Loss: 0.1873  Val_Acc: 93.895

Epoch 75: Validation loss did not decrease
	 Train_Loss: 0.2444 Train_Acc: 92.181 Val_Loss: 0.1887  BEST VAL Loss: 0.1873  Val_Acc: 93.850

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.2438 Train_Acc: 92.243 Val_Loss: 0.1888  BEST VAL Loss: 0.1873  Val_Acc: 94.032

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.2432 Train_Acc: 92.465 Val_Loss: 0.1889  BEST VAL Loss: 0.1873  Val_Acc: 94.260

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.2426 Train_Acc: 92.238 Val_Loss: 0.1894  BEST VAL Loss: 0.1873  Val_Acc: 93.986

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.2421 Train_Acc: 92.175 Val_Loss: 0.1891  BEST VAL Loss: 0.1873  Val_Acc: 94.351

Epoch 80: Validation loss did not decrease
	 Train_Loss: 0.2416 Train_Acc: 92.232 Val_Loss: 0.1891  BEST VAL Loss: 0.1873  Val_Acc: 94.351

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.2411 Train_Acc: 92.095 Val_Loss: 0.1893  BEST VAL Loss: 0.1873  Val_Acc: 94.305

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.2405 Train_Acc: 92.557 Val_Loss: 0.1894  BEST VAL Loss: 0.1873  Val_Acc: 93.531

Epoch 83: Validation loss did not decrease
Early stopped at epoch : 83
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.57      0.56      9707
           1       0.45      0.44      0.44      7852

    accuracy                           0.51     17559
   macro avg       0.50      0.50      0.50     17559
weighted avg       0.51      0.51      0.51     17559

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.57      0.56      1214
           1       0.44      0.43      0.43       981

    accuracy                           0.51      2195
   macro avg       0.50      0.50      0.50      2195
weighted avg       0.50      0.51      0.50      2195

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.57      0.56      1214
           1       0.44      0.42      0.43       981

    accuracy                           0.50      2195
   macro avg       0.49      0.49      0.49      2195
weighted avg       0.50      0.50      0.50      2195

              precision    recall  f1-score   support

           0       0.55      0.57      0.56      1214
           1       0.44      0.42      0.43       981

    accuracy                           0.50      2195
   macro avg       0.49      0.49      0.49      2195
weighted avg       0.50      0.50      0.50      2195

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.010_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.55      0.54      3724
           1       0.49      0.47      0.48      3398

    accuracy                           0.51      7122
   macro avg       0.51      0.51      0.51      7122
weighted avg       0.51      0.51      0.51      7122

              precision    recall  f1-score   support

           0       0.53      0.55      0.54      3724
           1       0.49      0.47      0.48      3398

    accuracy                           0.51      7122
   macro avg       0.51      0.51      0.51      7122
weighted avg       0.51      0.51      0.51      7122

completed
