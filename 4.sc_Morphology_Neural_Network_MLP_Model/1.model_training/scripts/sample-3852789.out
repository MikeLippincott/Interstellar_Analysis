[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18688 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Writing 40895 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:254: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:281: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:571: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:585: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:645: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:854: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:856: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:859: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:890: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:932: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1124: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
  warnings.warn(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1131: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1133: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1136: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1213: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1400: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1402: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1405: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1482: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
PBMC MultiClass_MLP True
[0.9436581681188537, 0.5245113046249099, 0.5318305272562364]
Data Subset Is Off
(1483474,) (370869,) (1966775,) 3821118     858323
3821119     858324
3821120     858325
3821121     858326
3821122     858327
            ...   
4061834    4538951
4061835    4538952
4061836    4538953
4061837    4538954
4061838    4538955
Name: labeled_data_index, Length: 240721, dtype: int64 (1536843,)
(1483474,) (370869,) (1966775,) 3821118     858323
3821119     858324
3821120     858325
3821121     858326
3821122     858327
            ...   
4061834    4538951
4061835    4538952
4061836    4538953
4061837    4538954
4061838    4538955
Name: labeled_data_index, Length: 240721, dtype: int64 (1536843,)
5598682
(95928,) (749319,) (638227,)
(23982,) (187329,) (159558,)
(119911,) (936644,) (910220,)
(0,) (0,) (240721,)
(75619,) (788818,) (672406,)
(1483474, 1245) (370869, 1245) (1966775, 1245) (240721, 1245) (1536843, 1245)
(1483474,) (370869,) (1966775,) (240721,) (1536843,)
Number of in features:  1245
Number of out features:  3
Multi_Class
Adam
Epoch 0: Validation loss decreased (inf --> 0.574813).  Saving model ...
	 Train_Loss: 0.6648 Train_Acc: 72.857 Val_Loss: 0.5748  BEST VAL Loss: 0.5748  Val_Acc: 76.480

Epoch 1: Validation loss decreased (0.574813 --> 0.560745).  Saving model ...
	 Train_Loss: 0.6314 Train_Acc: 75.757 Val_Loss: 0.5607  BEST VAL Loss: 0.5607  Val_Acc: 78.375

Epoch 2: Validation loss decreased (0.560745 --> 0.552291).  Saving model ...
	 Train_Loss: 0.6129 Train_Acc: 76.743 Val_Loss: 0.5523  BEST VAL Loss: 0.5523  Val_Acc: 78.804

Epoch 3: Validation loss decreased (0.552291 --> 0.544396).  Saving model ...
	 Train_Loss: 0.6002 Train_Acc: 77.329 Val_Loss: 0.5444  BEST VAL Loss: 0.5444  Val_Acc: 79.315

Epoch 4: Validation loss decreased (0.544396 --> 0.539715).  Saving model ...
	 Train_Loss: 0.5907 Train_Acc: 77.750 Val_Loss: 0.5397  BEST VAL Loss: 0.5397  Val_Acc: 79.387

Epoch 5: Validation loss decreased (0.539715 --> 0.535336).  Saving model ...
	 Train_Loss: 0.5831 Train_Acc: 78.071 Val_Loss: 0.5353  BEST VAL Loss: 0.5353  Val_Acc: 80.145

Epoch 6: Validation loss decreased (0.535336 --> 0.530521).  Saving model ...
	 Train_Loss: 0.5769 Train_Acc: 78.334 Val_Loss: 0.5305  BEST VAL Loss: 0.5305  Val_Acc: 80.432

Epoch 7: Validation loss decreased (0.530521 --> 0.526494).  Saving model ...
	 Train_Loss: 0.5715 Train_Acc: 78.581 Val_Loss: 0.5265  BEST VAL Loss: 0.5265  Val_Acc: 80.300

Epoch 8: Validation loss decreased (0.526494 --> 0.523741).  Saving model ...
	 Train_Loss: 0.5669 Train_Acc: 78.764 Val_Loss: 0.5237  BEST VAL Loss: 0.5237  Val_Acc: 80.329

Epoch 9: Validation loss decreased (0.523741 --> 0.520482).  Saving model ...
	 Train_Loss: 0.5628 Train_Acc: 78.883 Val_Loss: 0.5205  BEST VAL Loss: 0.5205  Val_Acc: 80.753

Epoch 10: Validation loss decreased (0.520482 --> 0.517834).  Saving model ...
	 Train_Loss: 0.5592 Train_Acc: 79.077 Val_Loss: 0.5178  BEST VAL Loss: 0.5178  Val_Acc: 80.954

Epoch 11: Validation loss decreased (0.517834 --> 0.515213).  Saving model ...
	 Train_Loss: 0.5560 Train_Acc: 79.154 Val_Loss: 0.5152  BEST VAL Loss: 0.5152  Val_Acc: 81.033

Epoch 12: Validation loss decreased (0.515213 --> 0.513566).  Saving model ...
	 Train_Loss: 0.5531 Train_Acc: 79.265 Val_Loss: 0.5136  BEST VAL Loss: 0.5136  Val_Acc: 81.220

Epoch 13: Validation loss decreased (0.513566 --> 0.511832).  Saving model ...
	 Train_Loss: 0.5505 Train_Acc: 79.438 Val_Loss: 0.5118  BEST VAL Loss: 0.5118  Val_Acc: 81.177

Epoch 14: Validation loss decreased (0.511832 --> 0.510763).  Saving model ...
	 Train_Loss: 0.5480 Train_Acc: 79.519 Val_Loss: 0.5108  BEST VAL Loss: 0.5108  Val_Acc: 81.274

Epoch 15: Validation loss decreased (0.510763 --> 0.509015).  Saving model ...
	 Train_Loss: 0.5458 Train_Acc: 79.523 Val_Loss: 0.5090  BEST VAL Loss: 0.5090  Val_Acc: 81.512

Epoch 16: Validation loss decreased (0.509015 --> 0.507531).  Saving model ...
	 Train_Loss: 0.5438 Train_Acc: 79.571 Val_Loss: 0.5075  BEST VAL Loss: 0.5075  Val_Acc: 81.368

Epoch 17: Validation loss decreased (0.507531 --> 0.506535).  Saving model ...
	 Train_Loss: 0.5418 Train_Acc: 79.710 Val_Loss: 0.5065  BEST VAL Loss: 0.5065  Val_Acc: 81.310

Epoch 18: Validation loss decreased (0.506535 --> 0.505598).  Saving model ...
	 Train_Loss: 0.5400 Train_Acc: 79.755 Val_Loss: 0.5056  BEST VAL Loss: 0.5056  Val_Acc: 81.422

Epoch 19: Validation loss decreased (0.505598 --> 0.504372).  Saving model ...
	 Train_Loss: 0.5384 Train_Acc: 79.783 Val_Loss: 0.5044  BEST VAL Loss: 0.5044  Val_Acc: 81.558

Epoch 20: Validation loss decreased (0.504372 --> 0.503682).  Saving model ...
	 Train_Loss: 0.5369 Train_Acc: 79.768 Val_Loss: 0.5037  BEST VAL Loss: 0.5037  Val_Acc: 81.417

Epoch 21: Validation loss decreased (0.503682 --> 0.502748).  Saving model ...
	 Train_Loss: 0.5354 Train_Acc: 79.873 Val_Loss: 0.5027  BEST VAL Loss: 0.5027  Val_Acc: 81.605

Epoch 22: Validation loss decreased (0.502748 --> 0.501478).  Saving model ...
	 Train_Loss: 0.5340 Train_Acc: 79.905 Val_Loss: 0.5015  BEST VAL Loss: 0.5015  Val_Acc: 81.747

Epoch 23: Validation loss decreased (0.501478 --> 0.500588).  Saving model ...
	 Train_Loss: 0.5328 Train_Acc: 79.940 Val_Loss: 0.5006  BEST VAL Loss: 0.5006  Val_Acc: 81.615

Epoch 24: Validation loss decreased (0.500588 --> 0.499443).  Saving model ...
	 Train_Loss: 0.5316 Train_Acc: 80.013 Val_Loss: 0.4994  BEST VAL Loss: 0.4994  Val_Acc: 81.729

Epoch 25: Validation loss decreased (0.499443 --> 0.498544).  Saving model ...
	 Train_Loss: 0.5304 Train_Acc: 80.029 Val_Loss: 0.4985  BEST VAL Loss: 0.4985  Val_Acc: 81.559

Epoch 26: Validation loss decreased (0.498544 --> 0.497803).  Saving model ...
	 Train_Loss: 0.5293 Train_Acc: 80.029 Val_Loss: 0.4978  BEST VAL Loss: 0.4978  Val_Acc: 81.973

Epoch 27: Validation loss decreased (0.497803 --> 0.497036).  Saving model ...
	 Train_Loss: 0.5283 Train_Acc: 80.085 Val_Loss: 0.4970  BEST VAL Loss: 0.4970  Val_Acc: 81.780

Epoch 28: Validation loss decreased (0.497036 --> 0.496239).  Saving model ...
	 Train_Loss: 0.5273 Train_Acc: 80.090 Val_Loss: 0.4962  BEST VAL Loss: 0.4962  Val_Acc: 82.052

Epoch 29: Validation loss decreased (0.496239 --> 0.495694).  Saving model ...
	 Train_Loss: 0.5264 Train_Acc: 80.077 Val_Loss: 0.4957  BEST VAL Loss: 0.4957  Val_Acc: 81.839

Epoch 30: Validation loss decreased (0.495694 --> 0.495162).  Saving model ...
	 Train_Loss: 0.5255 Train_Acc: 80.115 Val_Loss: 0.4952  BEST VAL Loss: 0.4952  Val_Acc: 81.656

Epoch 31: Validation loss decreased (0.495162 --> 0.494494).  Saving model ...
	 Train_Loss: 0.5246 Train_Acc: 80.189 Val_Loss: 0.4945  BEST VAL Loss: 0.4945  Val_Acc: 81.531

Epoch 32: Validation loss decreased (0.494494 --> 0.493805).  Saving model ...
	 Train_Loss: 0.5238 Train_Acc: 80.155 Val_Loss: 0.4938  BEST VAL Loss: 0.4938  Val_Acc: 81.839

Epoch 33: Validation loss decreased (0.493805 --> 0.493380).  Saving model ...
	 Train_Loss: 0.5230 Train_Acc: 80.219 Val_Loss: 0.4934  BEST VAL Loss: 0.4934  Val_Acc: 81.990

Epoch 34: Validation loss decreased (0.493380 --> 0.492954).  Saving model ...
	 Train_Loss: 0.5223 Train_Acc: 80.255 Val_Loss: 0.4930  BEST VAL Loss: 0.4930  Val_Acc: 81.890

Epoch 35: Validation loss decreased (0.492954 --> 0.492407).  Saving model ...
	 Train_Loss: 0.5216 Train_Acc: 80.276 Val_Loss: 0.4924  BEST VAL Loss: 0.4924  Val_Acc: 81.900

Epoch 36: Validation loss decreased (0.492407 --> 0.491709).  Saving model ...
	 Train_Loss: 0.5209 Train_Acc: 80.269 Val_Loss: 0.4917  BEST VAL Loss: 0.4917  Val_Acc: 82.045

Epoch 37: Validation loss decreased (0.491709 --> 0.491008).  Saving model ...
	 Train_Loss: 0.5202 Train_Acc: 80.276 Val_Loss: 0.4910  BEST VAL Loss: 0.4910  Val_Acc: 82.147

Epoch 38: Validation loss decreased (0.491008 --> 0.490570).  Saving model ...
	 Train_Loss: 0.5196 Train_Acc: 80.275 Val_Loss: 0.4906  BEST VAL Loss: 0.4906  Val_Acc: 81.943

Epoch 39: Validation loss decreased (0.490570 --> 0.489966).  Saving model ...
	 Train_Loss: 0.5190 Train_Acc: 80.250 Val_Loss: 0.4900  BEST VAL Loss: 0.4900  Val_Acc: 81.888

Epoch 40: Validation loss decreased (0.489966 --> 0.489529).  Saving model ...
	 Train_Loss: 0.5184 Train_Acc: 80.267 Val_Loss: 0.4895  BEST VAL Loss: 0.4895  Val_Acc: 81.932

Epoch 41: Validation loss decreased (0.489529 --> 0.489045).  Saving model ...
	 Train_Loss: 0.5179 Train_Acc: 80.309 Val_Loss: 0.4890  BEST VAL Loss: 0.4890  Val_Acc: 81.818

Epoch 42: Validation loss decreased (0.489045 --> 0.488499).  Saving model ...
	 Train_Loss: 0.5173 Train_Acc: 80.352 Val_Loss: 0.4885  BEST VAL Loss: 0.4885  Val_Acc: 82.013

Epoch 43: Validation loss decreased (0.488499 --> 0.487941).  Saving model ...
	 Train_Loss: 0.5168 Train_Acc: 80.385 Val_Loss: 0.4879  BEST VAL Loss: 0.4879  Val_Acc: 82.127

Epoch 44: Validation loss decreased (0.487941 --> 0.487323).  Saving model ...
	 Train_Loss: 0.5163 Train_Acc: 80.412 Val_Loss: 0.4873  BEST VAL Loss: 0.4873  Val_Acc: 82.131

Epoch 45: Validation loss decreased (0.487323 --> 0.487240).  Saving model ...
	 Train_Loss: 0.5158 Train_Acc: 80.365 Val_Loss: 0.4872  BEST VAL Loss: 0.4872  Val_Acc: 81.493

Epoch 46: Validation loss decreased (0.487240 --> 0.486879).  Saving model ...
	 Train_Loss: 0.5154 Train_Acc: 80.357 Val_Loss: 0.4869  BEST VAL Loss: 0.4869  Val_Acc: 82.061

Epoch 47: Validation loss decreased (0.486879 --> 0.486414).  Saving model ...
	 Train_Loss: 0.5149 Train_Acc: 80.383 Val_Loss: 0.4864  BEST VAL Loss: 0.4864  Val_Acc: 81.874

Epoch 48: Validation loss decreased (0.486414 --> 0.486020).  Saving model ...
	 Train_Loss: 0.5145 Train_Acc: 80.441 Val_Loss: 0.4860  BEST VAL Loss: 0.4860  Val_Acc: 81.799

Epoch 49: Validation loss decreased (0.486020 --> 0.485539).  Saving model ...
	 Train_Loss: 0.5140 Train_Acc: 80.374 Val_Loss: 0.4855  BEST VAL Loss: 0.4855  Val_Acc: 82.047

Epoch 50: Validation loss decreased (0.485539 --> 0.485260).  Saving model ...
	 Train_Loss: 0.5136 Train_Acc: 80.388 Val_Loss: 0.4853  BEST VAL Loss: 0.4853  Val_Acc: 82.078

Epoch 51: Validation loss decreased (0.485260 --> 0.484949).  Saving model ...
	 Train_Loss: 0.5132 Train_Acc: 80.391 Val_Loss: 0.4849  BEST VAL Loss: 0.4849  Val_Acc: 82.088

Epoch 52: Validation loss decreased (0.484949 --> 0.484427).  Saving model ...
	 Train_Loss: 0.5129 Train_Acc: 80.415 Val_Loss: 0.4844  BEST VAL Loss: 0.4844  Val_Acc: 82.095

Epoch 53: Validation loss decreased (0.484427 --> 0.484011).  Saving model ...
	 Train_Loss: 0.5125 Train_Acc: 80.391 Val_Loss: 0.4840  BEST VAL Loss: 0.4840  Val_Acc: 82.107

Epoch 54: Validation loss decreased (0.484011 --> 0.483746).  Saving model ...
	 Train_Loss: 0.5122 Train_Acc: 80.415 Val_Loss: 0.4837  BEST VAL Loss: 0.4837  Val_Acc: 82.055

Epoch 55: Validation loss decreased (0.483746 --> 0.483537).  Saving model ...
	 Train_Loss: 0.5118 Train_Acc: 80.424 Val_Loss: 0.4835  BEST VAL Loss: 0.4835  Val_Acc: 81.955

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.5115 Train_Acc: 80.422 Val_Loss: 0.4836  BEST VAL Loss: 0.4835  Val_Acc: 82.135

Epoch 57: Validation loss decreased (0.483537 --> 0.483229).  Saving model ...
	 Train_Loss: 0.5112 Train_Acc: 80.405 Val_Loss: 0.4832  BEST VAL Loss: 0.4832  Val_Acc: 82.160

Epoch 58: Validation loss decreased (0.483229 --> 0.482996).  Saving model ...
	 Train_Loss: 0.5108 Train_Acc: 80.431 Val_Loss: 0.4830  BEST VAL Loss: 0.4830  Val_Acc: 82.192

Epoch 59: Validation loss decreased (0.482996 --> 0.482637).  Saving model ...
	 Train_Loss: 0.5105 Train_Acc: 80.399 Val_Loss: 0.4826  BEST VAL Loss: 0.4826  Val_Acc: 82.199

Epoch 60: Validation loss decreased (0.482637 --> 0.482377).  Saving model ...
	 Train_Loss: 0.5103 Train_Acc: 80.473 Val_Loss: 0.4824  BEST VAL Loss: 0.4824  Val_Acc: 82.158

Epoch 61: Validation loss decreased (0.482377 --> 0.482288).  Saving model ...
	 Train_Loss: 0.5100 Train_Acc: 80.414 Val_Loss: 0.4823  BEST VAL Loss: 0.4823  Val_Acc: 81.853

Epoch 62: Validation loss decreased (0.482288 --> 0.482039).  Saving model ...
	 Train_Loss: 0.5097 Train_Acc: 80.376 Val_Loss: 0.4820  BEST VAL Loss: 0.4820  Val_Acc: 82.093

Epoch 63: Validation loss decreased (0.482039 --> 0.481695).  Saving model ...
	 Train_Loss: 0.5094 Train_Acc: 80.407 Val_Loss: 0.4817  BEST VAL Loss: 0.4817  Val_Acc: 82.144

Epoch 64: Validation loss decreased (0.481695 --> 0.481485).  Saving model ...
	 Train_Loss: 0.5092 Train_Acc: 80.438 Val_Loss: 0.4815  BEST VAL Loss: 0.4815  Val_Acc: 82.059

Epoch 65: Validation loss decreased (0.481485 --> 0.481201).  Saving model ...
	 Train_Loss: 0.5089 Train_Acc: 80.450 Val_Loss: 0.4812  BEST VAL Loss: 0.4812  Val_Acc: 82.129

Epoch 66: Validation loss decreased (0.481201 --> 0.480972).  Saving model ...
	 Train_Loss: 0.5087 Train_Acc: 80.469 Val_Loss: 0.4810  BEST VAL Loss: 0.4810  Val_Acc: 81.995

Epoch 67: Validation loss decreased (0.480972 --> 0.480785).  Saving model ...
	 Train_Loss: 0.5084 Train_Acc: 80.405 Val_Loss: 0.4808  BEST VAL Loss: 0.4808  Val_Acc: 81.904

Epoch 68: Validation loss decreased (0.480785 --> 0.480502).  Saving model ...
	 Train_Loss: 0.5082 Train_Acc: 80.423 Val_Loss: 0.4805  BEST VAL Loss: 0.4805  Val_Acc: 82.090

Epoch 69: Validation loss decreased (0.480502 --> 0.480279).  Saving model ...
	 Train_Loss: 0.5080 Train_Acc: 80.440 Val_Loss: 0.4803  BEST VAL Loss: 0.4803  Val_Acc: 81.850

Epoch 70: Validation loss decreased (0.480279 --> 0.479971).  Saving model ...
	 Train_Loss: 0.5077 Train_Acc: 80.515 Val_Loss: 0.4800  BEST VAL Loss: 0.4800  Val_Acc: 82.342

Epoch 71: Validation loss decreased (0.479971 --> 0.479770).  Saving model ...
	 Train_Loss: 0.5075 Train_Acc: 80.464 Val_Loss: 0.4798  BEST VAL Loss: 0.4798  Val_Acc: 82.073

Epoch 72: Validation loss decreased (0.479770 --> 0.479551).  Saving model ...
	 Train_Loss: 0.5073 Train_Acc: 80.480 Val_Loss: 0.4796  BEST VAL Loss: 0.4796  Val_Acc: 81.699

Epoch 73: Validation loss decreased (0.479551 --> 0.479379).  Saving model ...
	 Train_Loss: 0.5071 Train_Acc: 80.442 Val_Loss: 0.4794  BEST VAL Loss: 0.4794  Val_Acc: 82.018

Epoch 74: Validation loss decreased (0.479379 --> 0.479348).  Saving model ...
	 Train_Loss: 0.5068 Train_Acc: 80.456 Val_Loss: 0.4793  BEST VAL Loss: 0.4793  Val_Acc: 82.204

Epoch 75: Validation loss decreased (0.479348 --> 0.479180).  Saving model ...
	 Train_Loss: 0.5066 Train_Acc: 80.508 Val_Loss: 0.4792  BEST VAL Loss: 0.4792  Val_Acc: 82.224

Epoch 76: Validation loss decreased (0.479180 --> 0.479105).  Saving model ...
	 Train_Loss: 0.5064 Train_Acc: 80.421 Val_Loss: 0.4791  BEST VAL Loss: 0.4791  Val_Acc: 81.633

Epoch 77: Validation loss decreased (0.479105 --> 0.478960).  Saving model ...
	 Train_Loss: 0.5063 Train_Acc: 80.452 Val_Loss: 0.4790  BEST VAL Loss: 0.4790  Val_Acc: 82.082

Epoch 78: Validation loss decreased (0.478960 --> 0.478684).  Saving model ...
	 Train_Loss: 0.5061 Train_Acc: 80.492 Val_Loss: 0.4787  BEST VAL Loss: 0.4787  Val_Acc: 82.394

Epoch 79: Validation loss decreased (0.478684 --> 0.478404).  Saving model ...
	 Train_Loss: 0.5059 Train_Acc: 80.524 Val_Loss: 0.4784  BEST VAL Loss: 0.4784  Val_Acc: 82.193

Epoch 80: Validation loss decreased (0.478404 --> 0.478258).  Saving model ...
	 Train_Loss: 0.5057 Train_Acc: 80.455 Val_Loss: 0.4783  BEST VAL Loss: 0.4783  Val_Acc: 82.299

Epoch 81: Validation loss decreased (0.478258 --> 0.478129).  Saving model ...
	 Train_Loss: 0.5055 Train_Acc: 80.497 Val_Loss: 0.4781  BEST VAL Loss: 0.4781  Val_Acc: 82.186

Epoch 82: Validation loss decreased (0.478129 --> 0.478016).  Saving model ...
	 Train_Loss: 0.5053 Train_Acc: 80.531 Val_Loss: 0.4780  BEST VAL Loss: 0.4780  Val_Acc: 82.102

Epoch 83: Validation loss decreased (0.478016 --> 0.477990).  Saving model ...
	 Train_Loss: 0.5052 Train_Acc: 80.521 Val_Loss: 0.4780  BEST VAL Loss: 0.4780  Val_Acc: 81.811

Epoch 84: Validation loss decreased (0.477990 --> 0.477862).  Saving model ...
	 Train_Loss: 0.5050 Train_Acc: 80.562 Val_Loss: 0.4779  BEST VAL Loss: 0.4779  Val_Acc: 82.347

Epoch 85: Validation loss decreased (0.477862 --> 0.477678).  Saving model ...
	 Train_Loss: 0.5048 Train_Acc: 80.559 Val_Loss: 0.4777  BEST VAL Loss: 0.4777  Val_Acc: 81.670

Epoch 86: Validation loss decreased (0.477678 --> 0.477572).  Saving model ...
	 Train_Loss: 0.5046 Train_Acc: 80.547 Val_Loss: 0.4776  BEST VAL Loss: 0.4776  Val_Acc: 82.261

Epoch 87: Validation loss decreased (0.477572 --> 0.477464).  Saving model ...
	 Train_Loss: 0.5045 Train_Acc: 80.463 Val_Loss: 0.4775  BEST VAL Loss: 0.4775  Val_Acc: 82.092

Epoch 88: Validation loss did not decrease
	 Train_Loss: 0.5043 Train_Acc: 80.604 Val_Loss: 0.4775  BEST VAL Loss: 0.4775  Val_Acc: 82.101

Epoch 89: Validation loss decreased (0.477464 --> 0.477437).  Saving model ...
	 Train_Loss: 0.5042 Train_Acc: 80.539 Val_Loss: 0.4774  BEST VAL Loss: 0.4774  Val_Acc: 82.231

Epoch 90: Validation loss decreased (0.477437 --> 0.477212).  Saving model ...
	 Train_Loss: 0.5040 Train_Acc: 80.544 Val_Loss: 0.4772  BEST VAL Loss: 0.4772  Val_Acc: 82.328

Epoch 91: Validation loss did not decrease
	 Train_Loss: 0.5039 Train_Acc: 80.579 Val_Loss: 0.4773  BEST VAL Loss: 0.4772  Val_Acc: 82.215

Epoch 92: Validation loss decreased (0.477212 --> 0.477078).  Saving model ...
	 Train_Loss: 0.5037 Train_Acc: 80.570 Val_Loss: 0.4771  BEST VAL Loss: 0.4771  Val_Acc: 82.415

Epoch 93: Validation loss decreased (0.477078 --> 0.476895).  Saving model ...
	 Train_Loss: 0.5035 Train_Acc: 80.578 Val_Loss: 0.4769  BEST VAL Loss: 0.4769  Val_Acc: 82.296

Epoch 94: Validation loss decreased (0.476895 --> 0.476644).  Saving model ...
	 Train_Loss: 0.5034 Train_Acc: 80.609 Val_Loss: 0.4766  BEST VAL Loss: 0.4766  Val_Acc: 82.292

Epoch 95: Validation loss did not decrease
	 Train_Loss: 0.5033 Train_Acc: 80.627 Val_Loss: 0.4767  BEST VAL Loss: 0.4766  Val_Acc: 82.468

Epoch 96: Validation loss decreased (0.476644 --> 0.476501).  Saving model ...
	 Train_Loss: 0.5031 Train_Acc: 80.618 Val_Loss: 0.4765  BEST VAL Loss: 0.4765  Val_Acc: 82.410

Epoch 97: Validation loss decreased (0.476501 --> 0.476258).  Saving model ...
	 Train_Loss: 0.5030 Train_Acc: 80.585 Val_Loss: 0.4763  BEST VAL Loss: 0.4763  Val_Acc: 82.364

Epoch 98: Validation loss decreased (0.476258 --> 0.476143).  Saving model ...
	 Train_Loss: 0.5028 Train_Acc: 80.560 Val_Loss: 0.4761  BEST VAL Loss: 0.4761  Val_Acc: 81.968

Epoch 99: Validation loss decreased (0.476143 --> 0.475988).  Saving model ...
	 Train_Loss: 0.5027 Train_Acc: 80.564 Val_Loss: 0.4760  BEST VAL Loss: 0.4760  Val_Acc: 82.338

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.06      0.06      0.06     95928
           1       0.51      0.53      0.52    749319
           2       0.43      0.41      0.42    638227

    accuracy                           0.45   1483474
   macro avg       0.33      0.33      0.33   1483474
weighted avg       0.44      0.45      0.45   1483474

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.07      0.06      0.07     23982
           1       0.50      0.53      0.52    187329
           2       0.43      0.41      0.42    159558

    accuracy                           0.45    370869
   macro avg       0.33      0.33      0.33    370869
weighted avg       0.44      0.45      0.45    370869

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.06      0.06      0.06    119911
           1       0.48      0.52      0.50    936644
           2       0.46      0.42      0.44    910220

    accuracy                           0.45   1966775
   macro avg       0.33      0.33      0.33   1966775
weighted avg       0.44      0.45      0.44   1966775

Precision for class 0: 0.05929315133854779
Recall for class 0: 0.05803470907589796
Precision for class 1: 0.47625600645278693
Recall for class 1: 0.5169189147637736
Precision for class 2: 0.46293561192803984
Recall for class 2: 0.4235569422776911
3
              precision    recall  f1-score   support

           0       0.06      0.06      0.06    119911
           1       0.48      0.52      0.50    936644
           2       0.46      0.42      0.44    910220

    accuracy                           0.45   1966775
   macro avg       0.33      0.33      0.33   1966775
weighted avg       0.44      0.45      0.44   1966775

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.84      0.91    240721

    accuracy                           0.84    240721
   macro avg       0.33      0.28      0.30    240721
weighted avg       1.00      0.84      0.91    240721

Precision for class 0: 0.0
Recall for class 0: 0.0
Precision for class 1: 0.0
Recall for class 1: 0.0
Precision for class 2: 1.0
Recall for class 2: 0.8351410969545656
3
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.84      0.91    240721

    accuracy                           0.84    240721
   macro avg       0.33      0.28      0.30    240721
weighted avg       1.00      0.84      0.91    240721

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.05      0.05      0.05     75619
           1       0.51      0.48      0.50    788818
           2       0.44      0.47      0.45    672406

    accuracy                           0.45   1536843
   macro avg       0.33      0.33      0.33   1536843
weighted avg       0.46      0.45      0.46   1536843

Precision for class 0: 0.04978492612680008
Recall for class 0: 0.05280418942329309
Precision for class 1: 0.5133317276988191
Recall for class 1: 0.4809537814806457
Precision for class 2: 0.4373862486656429
Recall for class 2: 0.46676710201872085
3
              precision    recall  f1-score   support

           0       0.05      0.05      0.05     75619
           1       0.51      0.48      0.50    788818
           2       0.44      0.47      0.45    672406

    accuracy                           0.45   1536843
   macro avg       0.33      0.33      0.33   1536843
weighted avg       0.46      0.45      0.46   1536843

Done
