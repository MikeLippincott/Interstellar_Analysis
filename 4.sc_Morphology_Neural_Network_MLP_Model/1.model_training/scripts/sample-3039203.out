[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '297543a4'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2d29dc07'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'ca374b10'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '4fac5a4c'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_0.010_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_0.010_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_0.010_DMSO_0.025']
The dimensions of the data are: (29071, 1276)
Number of total missing values across all columns: 58142
Data Subset Is Off
Wells held out for testing: ['E14' 'B20']
Wells to use for training, validation, and testing ['E15' 'B16' 'B17' 'B21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.450044).  Saving model ...
	 Train_Loss: 0.5743 Train_Acc: 75.864 Val_Loss: 0.4500  BEST VAL Loss: 0.4500  Val_Acc: 84.237

Epoch 1: Validation loss decreased (0.450044 --> 0.388144).  Saving model ...
	 Train_Loss: 0.4940 Train_Acc: 85.153 Val_Loss: 0.3881  BEST VAL Loss: 0.3881  Val_Acc: 88.155

Epoch 2: Validation loss decreased (0.388144 --> 0.347550).  Saving model ...
	 Train_Loss: 0.4377 Train_Acc: 87.938 Val_Loss: 0.3476  BEST VAL Loss: 0.3476  Val_Acc: 89.613

Epoch 3: Validation loss decreased (0.347550 --> 0.318196).  Saving model ...
	 Train_Loss: 0.3948 Train_Acc: 89.891 Val_Loss: 0.3182  BEST VAL Loss: 0.3182  Val_Acc: 90.661

Epoch 4: Validation loss decreased (0.318196 --> 0.295242).  Saving model ...
	 Train_Loss: 0.3612 Train_Acc: 91.452 Val_Loss: 0.2952  BEST VAL Loss: 0.2952  Val_Acc: 91.982

Epoch 5: Validation loss decreased (0.295242 --> 0.278035).  Saving model ...
	 Train_Loss: 0.3350 Train_Acc: 92.323 Val_Loss: 0.2780  BEST VAL Loss: 0.2780  Val_Acc: 92.893

Epoch 6: Validation loss decreased (0.278035 --> 0.263856).  Saving model ...
	 Train_Loss: 0.3128 Train_Acc: 93.126 Val_Loss: 0.2639  BEST VAL Loss: 0.2639  Val_Acc: 93.121

Epoch 7: Validation loss decreased (0.263856 --> 0.253065).  Saving model ...
	 Train_Loss: 0.2944 Train_Acc: 93.815 Val_Loss: 0.2531  BEST VAL Loss: 0.2531  Val_Acc: 93.030

Epoch 8: Validation loss decreased (0.253065 --> 0.243973).  Saving model ...
	 Train_Loss: 0.2786 Train_Acc: 94.089 Val_Loss: 0.2440  BEST VAL Loss: 0.2440  Val_Acc: 93.030

Epoch 9: Validation loss decreased (0.243973 --> 0.236784).  Saving model ...
	 Train_Loss: 0.2651 Train_Acc: 94.487 Val_Loss: 0.2368  BEST VAL Loss: 0.2368  Val_Acc: 92.893

Epoch 10: Validation loss decreased (0.236784 --> 0.230925).  Saving model ...
	 Train_Loss: 0.2533 Train_Acc: 94.806 Val_Loss: 0.2309  BEST VAL Loss: 0.2309  Val_Acc: 93.212

Epoch 11: Validation loss decreased (0.230925 --> 0.225200).  Saving model ...
	 Train_Loss: 0.2431 Train_Acc: 94.920 Val_Loss: 0.2252  BEST VAL Loss: 0.2252  Val_Acc: 93.303

Epoch 12: Validation loss decreased (0.225200 --> 0.219963).  Saving model ...
	 Train_Loss: 0.2339 Train_Acc: 95.210 Val_Loss: 0.2200  BEST VAL Loss: 0.2200  Val_Acc: 93.941

Epoch 13: Validation loss decreased (0.219963 --> 0.215978).  Saving model ...
	 Train_Loss: 0.2256 Train_Acc: 95.512 Val_Loss: 0.2160  BEST VAL Loss: 0.2160  Val_Acc: 93.576

Epoch 14: Validation loss decreased (0.215978 --> 0.212621).  Saving model ...
	 Train_Loss: 0.2177 Train_Acc: 95.917 Val_Loss: 0.2126  BEST VAL Loss: 0.2126  Val_Acc: 93.622

Epoch 15: Validation loss decreased (0.212621 --> 0.211036).  Saving model ...
	 Train_Loss: 0.2106 Train_Acc: 95.911 Val_Loss: 0.2110  BEST VAL Loss: 0.2110  Val_Acc: 93.394

Epoch 16: Validation loss decreased (0.211036 --> 0.208672).  Saving model ...
	 Train_Loss: 0.2043 Train_Acc: 95.951 Val_Loss: 0.2087  BEST VAL Loss: 0.2087  Val_Acc: 93.303

Epoch 17: Validation loss decreased (0.208672 --> 0.206921).  Saving model ...
	 Train_Loss: 0.1983 Train_Acc: 96.304 Val_Loss: 0.2069  BEST VAL Loss: 0.2069  Val_Acc: 93.895

Epoch 18: Validation loss decreased (0.206921 --> 0.205819).  Saving model ...
	 Train_Loss: 0.1929 Train_Acc: 96.344 Val_Loss: 0.2058  BEST VAL Loss: 0.2058  Val_Acc: 93.895

Epoch 19: Validation loss decreased (0.205819 --> 0.204568).  Saving model ...
	 Train_Loss: 0.1881 Train_Acc: 96.224 Val_Loss: 0.2046  BEST VAL Loss: 0.2046  Val_Acc: 93.713

Epoch 20: Validation loss decreased (0.204568 --> 0.203077).  Saving model ...
	 Train_Loss: 0.1835 Train_Acc: 96.367 Val_Loss: 0.2031  BEST VAL Loss: 0.2031  Val_Acc: 94.169

Epoch 21: Validation loss decreased (0.203077 --> 0.202583).  Saving model ...
	 Train_Loss: 0.1793 Train_Acc: 96.543 Val_Loss: 0.2026  BEST VAL Loss: 0.2026  Val_Acc: 93.895

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1752 Train_Acc: 96.765 Val_Loss: 0.2033  BEST VAL Loss: 0.2026  Val_Acc: 93.303

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1715 Train_Acc: 96.663 Val_Loss: 0.2033  BEST VAL Loss: 0.2026  Val_Acc: 93.622

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1680 Train_Acc: 96.759 Val_Loss: 0.2031  BEST VAL Loss: 0.2026  Val_Acc: 94.214

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1647 Train_Acc: 96.771 Val_Loss: 0.2034  BEST VAL Loss: 0.2026  Val_Acc: 93.804

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1614 Train_Acc: 97.204 Val_Loss: 0.2034  BEST VAL Loss: 0.2026  Val_Acc: 94.032

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1582 Train_Acc: 97.255 Val_Loss: 0.2027  BEST VAL Loss: 0.2026  Val_Acc: 93.895

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1551 Train_Acc: 97.466 Val_Loss: 0.2029  BEST VAL Loss: 0.2026  Val_Acc: 93.895

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1522 Train_Acc: 97.454 Val_Loss: 0.2033  BEST VAL Loss: 0.2026  Val_Acc: 93.850

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1494 Train_Acc: 97.830 Val_Loss: 0.2039  BEST VAL Loss: 0.2026  Val_Acc: 93.667

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1467 Train_Acc: 97.682 Val_Loss: 0.2041  BEST VAL Loss: 0.2026  Val_Acc: 93.895

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1441 Train_Acc: 97.938 Val_Loss: 0.2050  BEST VAL Loss: 0.2026  Val_Acc: 93.531

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1418 Train_Acc: 97.665 Val_Loss: 0.2051  BEST VAL Loss: 0.2026  Val_Acc: 93.531

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1396 Train_Acc: 97.699 Val_Loss: 0.2053  BEST VAL Loss: 0.2026  Val_Acc: 93.531

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1375 Train_Acc: 97.705 Val_Loss: 0.2057  BEST VAL Loss: 0.2026  Val_Acc: 93.531

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1354 Train_Acc: 97.802 Val_Loss: 0.2060  BEST VAL Loss: 0.2026  Val_Acc: 93.349

Epoch 37: Validation loss did not decrease
Early stopped at epoch : 37
LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      9707
           1       0.98      0.97      0.98      7852

    accuracy                           0.98     17559
   macro avg       0.98      0.98      0.98     17559
weighted avg       0.98      0.98      0.98     17559

LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.96      0.95      1214
           1       0.95      0.92      0.93       981

    accuracy                           0.94      2195
   macro avg       0.94      0.94      0.94      2195
weighted avg       0.94      0.94      0.94      2195

LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.96      0.94      1214
           1       0.95      0.91      0.93       981

    accuracy                           0.94      2195
   macro avg       0.94      0.93      0.94      2195
weighted avg       0.94      0.94      0.94      2195

              precision    recall  f1-score   support

           0       0.93      0.96      0.94      1214
           1       0.95      0.91      0.93       981

    accuracy                           0.94      2195
   macro avg       0.94      0.93      0.94      2195
weighted avg       0.94      0.94      0.94      2195

LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.95      0.94      3724
           1       0.95      0.93      0.94      3398

    accuracy                           0.94      7122
   macro avg       0.94      0.94      0.94      7122
weighted avg       0.94      0.94      0.94      7122

              precision    recall  f1-score   support

           0       0.94      0.95      0.94      3724
           1       0.95      0.93      0.94      3398

    accuracy                           0.94      7122
   macro avg       0.94      0.94      0.94      7122
weighted avg       0.94      0.94      0.94      7122

completed
