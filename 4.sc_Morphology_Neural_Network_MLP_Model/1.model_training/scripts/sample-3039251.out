[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '3cd7c068'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f6fbf698'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '0f3db007'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '34ed62f3'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_10.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_10.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_10.000_DMSO_0.025']
The dimensions of the data are: (30576, 1276)
Number of total missing values across all columns: 61152
Data Subset Is Off
Wells held out for testing: ['D14' 'E20']
Wells to use for training, validation, and testing ['D15' 'E16' 'E17' 'E21' 'K14' 'K15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.519876).  Saving model ...
	 Train_Loss: 0.6310 Train_Acc: 62.928 Val_Loss: 0.5199  BEST VAL Loss: 0.5199  Val_Acc: 77.759

Epoch 1: Validation loss decreased (0.519876 --> 0.494133).  Saving model ...
	 Train_Loss: 0.6022 Train_Acc: 70.362 Val_Loss: 0.4941  BEST VAL Loss: 0.4941  Val_Acc: 80.182

Epoch 2: Validation loss decreased (0.494133 --> 0.493544).  Saving model ...
	 Train_Loss: 0.5874 Train_Acc: 72.158 Val_Loss: 0.4935  BEST VAL Loss: 0.4935  Val_Acc: 79.230

Epoch 3: Validation loss decreased (0.493544 --> 0.484589).  Saving model ...
	 Train_Loss: 0.5780 Train_Acc: 73.100 Val_Loss: 0.4846  BEST VAL Loss: 0.4846  Val_Acc: 81.307

Epoch 4: Validation loss decreased (0.484589 --> 0.478076).  Saving model ...
	 Train_Loss: 0.5702 Train_Acc: 73.814 Val_Loss: 0.4781  BEST VAL Loss: 0.4781  Val_Acc: 81.350

Epoch 5: Validation loss decreased (0.478076 --> 0.469829).  Saving model ...
	 Train_Loss: 0.5641 Train_Acc: 74.322 Val_Loss: 0.4698  BEST VAL Loss: 0.4698  Val_Acc: 81.783

Epoch 6: Validation loss decreased (0.469829 --> 0.460870).  Saving model ...
	 Train_Loss: 0.5584 Train_Acc: 74.993 Val_Loss: 0.4609  BEST VAL Loss: 0.4609  Val_Acc: 83.903

Epoch 7: Validation loss decreased (0.460870 --> 0.455220).  Saving model ...
	 Train_Loss: 0.5522 Train_Acc: 75.853 Val_Loss: 0.4552  BEST VAL Loss: 0.4552  Val_Acc: 82.908

Epoch 8: Validation loss decreased (0.455220 --> 0.450560).  Saving model ...
	 Train_Loss: 0.5482 Train_Acc: 75.231 Val_Loss: 0.4506  BEST VAL Loss: 0.4506  Val_Acc: 83.643

Epoch 9: Validation loss decreased (0.450560 --> 0.446887).  Saving model ...
	 Train_Loss: 0.5439 Train_Acc: 76.086 Val_Loss: 0.4469  BEST VAL Loss: 0.4469  Val_Acc: 84.293

Epoch 10: Validation loss decreased (0.446887 --> 0.442601).  Saving model ...
	 Train_Loss: 0.5402 Train_Acc: 75.821 Val_Loss: 0.4426  BEST VAL Loss: 0.4426  Val_Acc: 84.293

Epoch 11: Validation loss decreased (0.442601 --> 0.440789).  Saving model ...
	 Train_Loss: 0.5391 Train_Acc: 73.841 Val_Loss: 0.4408  BEST VAL Loss: 0.4408  Val_Acc: 83.773

Epoch 12: Validation loss decreased (0.440789 --> 0.438511).  Saving model ...
	 Train_Loss: 0.5375 Train_Acc: 75.399 Val_Loss: 0.4385  BEST VAL Loss: 0.4385  Val_Acc: 84.206

Epoch 13: Validation loss decreased (0.438511 --> 0.437125).  Saving model ...
	 Train_Loss: 0.5361 Train_Acc: 75.188 Val_Loss: 0.4371  BEST VAL Loss: 0.4371  Val_Acc: 83.081

Epoch 14: Validation loss decreased (0.437125 --> 0.435065).  Saving model ...
	 Train_Loss: 0.5350 Train_Acc: 74.625 Val_Loss: 0.4351  BEST VAL Loss: 0.4351  Val_Acc: 84.206

Epoch 15: Validation loss decreased (0.435065 --> 0.432476).  Saving model ...
	 Train_Loss: 0.5331 Train_Acc: 75.929 Val_Loss: 0.4325  BEST VAL Loss: 0.4325  Val_Acc: 84.639

Epoch 16: Validation loss decreased (0.432476 --> 0.430244).  Saving model ...
	 Train_Loss: 0.5316 Train_Acc: 76.367 Val_Loss: 0.4302  BEST VAL Loss: 0.4302  Val_Acc: 84.552

Epoch 17: Validation loss decreased (0.430244 --> 0.427303).  Saving model ...
	 Train_Loss: 0.5294 Train_Acc: 76.979 Val_Loss: 0.4273  BEST VAL Loss: 0.4273  Val_Acc: 85.115

Epoch 18: Validation loss decreased (0.427303 --> 0.424392).  Saving model ...
	 Train_Loss: 0.5275 Train_Acc: 76.541 Val_Loss: 0.4244  BEST VAL Loss: 0.4244  Val_Acc: 85.028

Epoch 19: Validation loss decreased (0.424392 --> 0.421715).  Saving model ...
	 Train_Loss: 0.5260 Train_Acc: 76.146 Val_Loss: 0.4217  BEST VAL Loss: 0.4217  Val_Acc: 84.855

Epoch 20: Validation loss decreased (0.421715 --> 0.420105).  Saving model ...
	 Train_Loss: 0.5242 Train_Acc: 77.547 Val_Loss: 0.4201  BEST VAL Loss: 0.4201  Val_Acc: 84.768

Epoch 21: Validation loss decreased (0.420105 --> 0.419092).  Saving model ...
	 Train_Loss: 0.5234 Train_Acc: 76.995 Val_Loss: 0.4191  BEST VAL Loss: 0.4191  Val_Acc: 83.557

Epoch 22: Validation loss decreased (0.419092 --> 0.418584).  Saving model ...
	 Train_Loss: 0.5231 Train_Acc: 75.415 Val_Loss: 0.4186  BEST VAL Loss: 0.4186  Val_Acc: 84.033

Epoch 23: Validation loss decreased (0.418584 --> 0.417602).  Saving model ...
	 Train_Loss: 0.5223 Train_Acc: 76.752 Val_Loss: 0.4176  BEST VAL Loss: 0.4176  Val_Acc: 84.466

Epoch 24: Validation loss decreased (0.417602 --> 0.417377).  Saving model ...
	 Train_Loss: 0.5218 Train_Acc: 76.822 Val_Loss: 0.4174  BEST VAL Loss: 0.4174  Val_Acc: 84.249

Epoch 25: Validation loss decreased (0.417377 --> 0.416473).  Saving model ...
	 Train_Loss: 0.5216 Train_Acc: 76.324 Val_Loss: 0.4165  BEST VAL Loss: 0.4165  Val_Acc: 84.336

Epoch 26: Validation loss decreased (0.416473 --> 0.415466).  Saving model ...
	 Train_Loss: 0.5210 Train_Acc: 76.827 Val_Loss: 0.4155  BEST VAL Loss: 0.4155  Val_Acc: 84.898

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.5207 Train_Acc: 76.211 Val_Loss: 0.4161  BEST VAL Loss: 0.4155  Val_Acc: 81.566

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.5212 Train_Acc: 75.069 Val_Loss: 0.4163  BEST VAL Loss: 0.4155  Val_Acc: 83.860

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.5216 Train_Acc: 76.346 Val_Loss: 0.4160  BEST VAL Loss: 0.4155  Val_Acc: 84.119

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.5215 Train_Acc: 75.886 Val_Loss: 0.4157  BEST VAL Loss: 0.4155  Val_Acc: 84.552

Epoch 31: Validation loss decreased (0.415466 --> 0.415039).  Saving model ...
	 Train_Loss: 0.5211 Train_Acc: 75.767 Val_Loss: 0.4150  BEST VAL Loss: 0.4150  Val_Acc: 84.379

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.5208 Train_Acc: 75.670 Val_Loss: 0.4154  BEST VAL Loss: 0.4150  Val_Acc: 82.345

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.5208 Train_Acc: 75.475 Val_Loss: 0.4152  BEST VAL Loss: 0.4150  Val_Acc: 83.470

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.5207 Train_Acc: 76.129 Val_Loss: 0.4151  BEST VAL Loss: 0.4150  Val_Acc: 83.211

Epoch 35: Validation loss decreased (0.415039 --> 0.414909).  Saving model ...
	 Train_Loss: 0.5206 Train_Acc: 75.312 Val_Loss: 0.4149  BEST VAL Loss: 0.4149  Val_Acc: 83.427

Epoch 36: Validation loss decreased (0.414909 --> 0.414281).  Saving model ...
	 Train_Loss: 0.5203 Train_Acc: 76.470 Val_Loss: 0.4143  BEST VAL Loss: 0.4143  Val_Acc: 84.855

Epoch 37: Validation loss decreased (0.414281 --> 0.414132).  Saving model ...
	 Train_Loss: 0.5200 Train_Acc: 76.248 Val_Loss: 0.4141  BEST VAL Loss: 0.4141  Val_Acc: 84.033

Epoch 38: Validation loss decreased (0.414132 --> 0.413783).  Saving model ...
	 Train_Loss: 0.5199 Train_Acc: 76.005 Val_Loss: 0.4138  BEST VAL Loss: 0.4138  Val_Acc: 84.163

Epoch 39: Validation loss decreased (0.413783 --> 0.413405).  Saving model ...
	 Train_Loss: 0.5197 Train_Acc: 76.881 Val_Loss: 0.4134  BEST VAL Loss: 0.4134  Val_Acc: 84.033

Epoch 40: Validation loss decreased (0.413405 --> 0.413175).  Saving model ...
	 Train_Loss: 0.5195 Train_Acc: 76.373 Val_Loss: 0.4132  BEST VAL Loss: 0.4132  Val_Acc: 83.600

Epoch 41: Validation loss decreased (0.413175 --> 0.413076).  Saving model ...
	 Train_Loss: 0.5194 Train_Acc: 76.102 Val_Loss: 0.4131  BEST VAL Loss: 0.4131  Val_Acc: 83.297

Epoch 42: Validation loss decreased (0.413076 --> 0.412574).  Saving model ...
	 Train_Loss: 0.5193 Train_Acc: 76.330 Val_Loss: 0.4126  BEST VAL Loss: 0.4126  Val_Acc: 83.990

Epoch 43: Validation loss decreased (0.412574 --> 0.411658).  Saving model ...
	 Train_Loss: 0.5190 Train_Acc: 76.833 Val_Loss: 0.4117  BEST VAL Loss: 0.4117  Val_Acc: 85.244

Epoch 44: Validation loss decreased (0.411658 --> 0.411073).  Saving model ...
	 Train_Loss: 0.5187 Train_Acc: 76.957 Val_Loss: 0.4111  BEST VAL Loss: 0.4111  Val_Acc: 84.855

Epoch 45: Validation loss decreased (0.411073 --> 0.410373).  Saving model ...
	 Train_Loss: 0.5185 Train_Acc: 76.476 Val_Loss: 0.4104  BEST VAL Loss: 0.4104  Val_Acc: 84.422

Epoch 46: Validation loss decreased (0.410373 --> 0.409864).  Saving model ...
	 Train_Loss: 0.5184 Train_Acc: 76.324 Val_Loss: 0.4099  BEST VAL Loss: 0.4099  Val_Acc: 84.509

Epoch 47: Validation loss decreased (0.409864 --> 0.409342).  Saving model ...
	 Train_Loss: 0.5182 Train_Acc: 76.005 Val_Loss: 0.4093  BEST VAL Loss: 0.4093  Val_Acc: 83.990

Epoch 48: Validation loss decreased (0.409342 --> 0.409083).  Saving model ...
	 Train_Loss: 0.5180 Train_Acc: 75.918 Val_Loss: 0.4091  BEST VAL Loss: 0.4091  Val_Acc: 83.167

Epoch 49: Validation loss decreased (0.409083 --> 0.408811).  Saving model ...
	 Train_Loss: 0.5179 Train_Acc: 76.043 Val_Loss: 0.4088  BEST VAL Loss: 0.4088  Val_Acc: 84.033

Epoch 50: Validation loss decreased (0.408811 --> 0.408420).  Saving model ...
	 Train_Loss: 0.5178 Train_Acc: 76.373 Val_Loss: 0.4084  BEST VAL Loss: 0.4084  Val_Acc: 84.033

Epoch 51: Validation loss decreased (0.408420 --> 0.407969).  Saving model ...
	 Train_Loss: 0.5177 Train_Acc: 76.562 Val_Loss: 0.4080  BEST VAL Loss: 0.4080  Val_Acc: 84.422

Epoch 52: Validation loss decreased (0.407969 --> 0.407545).  Saving model ...
	 Train_Loss: 0.5174 Train_Acc: 76.844 Val_Loss: 0.4075  BEST VAL Loss: 0.4075  Val_Acc: 84.422

Epoch 53: Validation loss decreased (0.407545 --> 0.407315).  Saving model ...
	 Train_Loss: 0.5171 Train_Acc: 76.562 Val_Loss: 0.4073  BEST VAL Loss: 0.4073  Val_Acc: 84.163

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.5172 Train_Acc: 74.544 Val_Loss: 0.4076  BEST VAL Loss: 0.4073  Val_Acc: 81.740

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.5174 Train_Acc: 74.133 Val_Loss: 0.4076  BEST VAL Loss: 0.4073  Val_Acc: 83.860

Epoch 56: Validation loss decreased (0.407315 --> 0.407227).  Saving model ...
	 Train_Loss: 0.5171 Train_Acc: 76.844 Val_Loss: 0.4072  BEST VAL Loss: 0.4072  Val_Acc: 84.682

Epoch 57: Validation loss decreased (0.407227 --> 0.406910).  Saving model ...
	 Train_Loss: 0.5168 Train_Acc: 77.352 Val_Loss: 0.4069  BEST VAL Loss: 0.4069  Val_Acc: 84.509

Epoch 58: Validation loss decreased (0.406910 --> 0.406588).  Saving model ...
	 Train_Loss: 0.5166 Train_Acc: 77.525 Val_Loss: 0.4066  BEST VAL Loss: 0.4066  Val_Acc: 84.855

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.5165 Train_Acc: 77.190 Val_Loss: 0.4067  BEST VAL Loss: 0.4066  Val_Acc: 83.687

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.5166 Train_Acc: 75.551 Val_Loss: 0.4069  BEST VAL Loss: 0.4066  Val_Acc: 82.994

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.5167 Train_Acc: 75.913 Val_Loss: 0.4069  BEST VAL Loss: 0.4066  Val_Acc: 83.341

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.5167 Train_Acc: 76.059 Val_Loss: 0.4070  BEST VAL Loss: 0.4066  Val_Acc: 83.514

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.5168 Train_Acc: 76.048 Val_Loss: 0.4072  BEST VAL Loss: 0.4066  Val_Acc: 82.302

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.5175 Train_Acc: 75.794 Val_Loss: 0.4085  BEST VAL Loss: 0.4066  Val_Acc: 82.778

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.5183 Train_Acc: 75.513 Val_Loss: 0.4099  BEST VAL Loss: 0.4066  Val_Acc: 80.182

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.5193 Train_Acc: 73.386 Val_Loss: 0.4114  BEST VAL Loss: 0.4066  Val_Acc: 80.614

Epoch 67: Validation loss did not decrease
	 Train_Loss: 0.5202 Train_Acc: 74.966 Val_Loss: 0.4125  BEST VAL Loss: 0.4066  Val_Acc: 82.172

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.5209 Train_Acc: 75.794 Val_Loss: 0.4133  BEST VAL Loss: 0.4066  Val_Acc: 83.254

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.5216 Train_Acc: 75.810 Val_Loss: 0.4142  BEST VAL Loss: 0.4066  Val_Acc: 83.687

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.5222 Train_Acc: 76.800 Val_Loss: 0.4150  BEST VAL Loss: 0.4066  Val_Acc: 83.470

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.5228 Train_Acc: 76.227 Val_Loss: 0.4160  BEST VAL Loss: 0.4066  Val_Acc: 82.821

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.5234 Train_Acc: 75.534 Val_Loss: 0.4169  BEST VAL Loss: 0.4066  Val_Acc: 82.735

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.5239 Train_Acc: 75.475 Val_Loss: 0.4177  BEST VAL Loss: 0.4066  Val_Acc: 82.951

Epoch 74: Validation loss did not decrease
Early stopped at epoch : 74
LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.86      0.86      0.86     10113
           1       0.83      0.83      0.83      8370

    accuracy                           0.85     18483
   macro avg       0.85      0.85      0.85     18483
weighted avg       0.85      0.85      0.85     18483

LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.86      0.86      0.86      1264
           1       0.83      0.83      0.83      1047

    accuracy                           0.85      2311
   macro avg       0.85      0.85      0.85      2311
weighted avg       0.85      0.85      0.85      2311

LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.84      0.86      0.85      1265
           1       0.82      0.80      0.81      1046

    accuracy                           0.83      2311
   macro avg       0.83      0.83      0.83      2311
weighted avg       0.83      0.83      0.83      2311

              precision    recall  f1-score   support

           0       0.84      0.86      0.85      1265
           1       0.82      0.80      0.81      1046

    accuracy                           0.83      2311
   macro avg       0.83      0.83      0.83      2311
weighted avg       0.83      0.83      0.83      2311

LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.86      0.87      0.86      4168
           1       0.83      0.82      0.83      3303

    accuracy                           0.85      7471
   macro avg       0.85      0.84      0.85      7471
weighted avg       0.85      0.85      0.85      7471

              precision    recall  f1-score   support

           0       0.86      0.87      0.86      4168
           1       0.83      0.82      0.83      3303

    accuracy                           0.85      7471
   macro avg       0.85      0.84      0.85      7471
weighted avg       0.85      0.85      0.85      7471

completed
