[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '62749182'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '8ac18105'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'c2b22058'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '020bdf6f'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (26864, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'L20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'L16' 'L17' 'L21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.083054).  Saving model ...
	 Train_Loss: 0.2572 Train_Acc: 88.182 Val_Loss: 0.0831  BEST VAL Loss: 0.0831  Val_Acc: 95.695

Epoch 1: Validation loss decreased (0.083054 --> 0.075710).  Saving model ...
	 Train_Loss: 0.1874 Train_Acc: 95.539 Val_Loss: 0.0757  BEST VAL Loss: 0.0757  Val_Acc: 95.695

Epoch 2: Validation loss did not decrease
	 Train_Loss: 0.1567 Train_Acc: 96.306 Val_Loss: 0.0811  BEST VAL Loss: 0.0757  Val_Acc: 96.388

Epoch 3: Validation loss did not decrease
	 Train_Loss: 0.1380 Train_Acc: 96.795 Val_Loss: 0.0782  BEST VAL Loss: 0.0757  Val_Acc: 96.982

Epoch 4: Validation loss decreased (0.075710 --> 0.072534).  Saving model ...
	 Train_Loss: 0.1244 Train_Acc: 97.123 Val_Loss: 0.0725  BEST VAL Loss: 0.0725  Val_Acc: 96.982

Epoch 5: Validation loss decreased (0.072534 --> 0.070344).  Saving model ...
	 Train_Loss: 0.1138 Train_Acc: 97.451 Val_Loss: 0.0703  BEST VAL Loss: 0.0703  Val_Acc: 96.685

Epoch 6: Validation loss decreased (0.070344 --> 0.068425).  Saving model ...
	 Train_Loss: 0.1059 Train_Acc: 97.575 Val_Loss: 0.0684  BEST VAL Loss: 0.0684  Val_Acc: 97.081

Epoch 7: Validation loss did not decrease
	 Train_Loss: 0.0996 Train_Acc: 97.729 Val_Loss: 0.0715  BEST VAL Loss: 0.0684  Val_Acc: 97.724

Epoch 8: Validation loss did not decrease
	 Train_Loss: 0.0942 Train_Acc: 97.766 Val_Loss: 0.0709  BEST VAL Loss: 0.0684  Val_Acc: 96.883

Epoch 9: Validation loss decreased (0.068425 --> 0.068127).  Saving model ...
	 Train_Loss: 0.0899 Train_Acc: 97.865 Val_Loss: 0.0681  BEST VAL Loss: 0.0681  Val_Acc: 97.180

Epoch 10: Validation loss decreased (0.068127 --> 0.066068).  Saving model ...
	 Train_Loss: 0.0859 Train_Acc: 97.884 Val_Loss: 0.0661  BEST VAL Loss: 0.0661  Val_Acc: 97.378

Epoch 11: Validation loss decreased (0.066068 --> 0.065259).  Saving model ...
	 Train_Loss: 0.0825 Train_Acc: 98.045 Val_Loss: 0.0653  BEST VAL Loss: 0.0653  Val_Acc: 97.229

Epoch 12: Validation loss decreased (0.065259 --> 0.063721).  Saving model ...
	 Train_Loss: 0.0796 Train_Acc: 97.964 Val_Loss: 0.0637  BEST VAL Loss: 0.0637  Val_Acc: 97.625

Epoch 13: Validation loss decreased (0.063721 --> 0.062866).  Saving model ...
	 Train_Loss: 0.0767 Train_Acc: 98.261 Val_Loss: 0.0629  BEST VAL Loss: 0.0629  Val_Acc: 97.625

Epoch 14: Validation loss decreased (0.062866 --> 0.061672).  Saving model ...
	 Train_Loss: 0.0743 Train_Acc: 98.181 Val_Loss: 0.0617  BEST VAL Loss: 0.0617  Val_Acc: 97.476

Epoch 15: Validation loss did not decrease
	 Train_Loss: 0.0720 Train_Acc: 98.292 Val_Loss: 0.0622  BEST VAL Loss: 0.0617  Val_Acc: 97.378

Epoch 16: Validation loss decreased (0.061672 --> 0.061285).  Saving model ...
	 Train_Loss: 0.0699 Train_Acc: 98.292 Val_Loss: 0.0613  BEST VAL Loss: 0.0613  Val_Acc: 97.378

Epoch 17: Validation loss decreased (0.061285 --> 0.060229).  Saving model ...
	 Train_Loss: 0.0681 Train_Acc: 98.292 Val_Loss: 0.0602  BEST VAL Loss: 0.0602  Val_Acc: 97.427

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.0664 Train_Acc: 98.410 Val_Loss: 0.0604  BEST VAL Loss: 0.0602  Val_Acc: 97.674

Epoch 19: Validation loss decreased (0.060229 --> 0.059546).  Saving model ...
	 Train_Loss: 0.0647 Train_Acc: 98.360 Val_Loss: 0.0595  BEST VAL Loss: 0.0595  Val_Acc: 97.674

Epoch 20: Validation loss decreased (0.059546 --> 0.059410).  Saving model ...
	 Train_Loss: 0.0632 Train_Acc: 98.472 Val_Loss: 0.0594  BEST VAL Loss: 0.0594  Val_Acc: 97.328

Epoch 21: Validation loss decreased (0.059410 --> 0.058491).  Saving model ...
	 Train_Loss: 0.0618 Train_Acc: 98.404 Val_Loss: 0.0585  BEST VAL Loss: 0.0585  Val_Acc: 98.021

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.0606 Train_Acc: 98.410 Val_Loss: 0.0591  BEST VAL Loss: 0.0585  Val_Acc: 97.476

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0595 Train_Acc: 98.317 Val_Loss: 0.0586  BEST VAL Loss: 0.0585  Val_Acc: 97.575

Epoch 24: Validation loss decreased (0.058491 --> 0.058192).  Saving model ...
	 Train_Loss: 0.0583 Train_Acc: 98.391 Val_Loss: 0.0582  BEST VAL Loss: 0.0582  Val_Acc: 97.674

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.0573 Train_Acc: 98.447 Val_Loss: 0.0584  BEST VAL Loss: 0.0582  Val_Acc: 97.476

Epoch 26: Validation loss decreased (0.058192 --> 0.057934).  Saving model ...
	 Train_Loss: 0.0563 Train_Acc: 98.447 Val_Loss: 0.0579  BEST VAL Loss: 0.0579  Val_Acc: 98.021

Epoch 27: Validation loss decreased (0.057934 --> 0.057736).  Saving model ...
	 Train_Loss: 0.0553 Train_Acc: 98.558 Val_Loss: 0.0577  BEST VAL Loss: 0.0577  Val_Acc: 97.922

Epoch 28: Validation loss decreased (0.057736 --> 0.057681).  Saving model ...
	 Train_Loss: 0.0544 Train_Acc: 98.503 Val_Loss: 0.0577  BEST VAL Loss: 0.0577  Val_Acc: 97.922

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.0536 Train_Acc: 98.583 Val_Loss: 0.0586  BEST VAL Loss: 0.0577  Val_Acc: 97.922

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0528 Train_Acc: 98.478 Val_Loss: 0.0594  BEST VAL Loss: 0.0577  Val_Acc: 97.724

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.0523 Train_Acc: 98.490 Val_Loss: 0.0597  BEST VAL Loss: 0.0577  Val_Acc: 97.674

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0517 Train_Acc: 98.515 Val_Loss: 0.0595  BEST VAL Loss: 0.0577  Val_Acc: 97.971

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0510 Train_Acc: 98.595 Val_Loss: 0.0592  BEST VAL Loss: 0.0577  Val_Acc: 97.922

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0504 Train_Acc: 98.645 Val_Loss: 0.0596  BEST VAL Loss: 0.0577  Val_Acc: 97.427

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.0499 Train_Acc: 98.546 Val_Loss: 0.0626  BEST VAL Loss: 0.0577  Val_Acc: 97.328

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.0493 Train_Acc: 98.744 Val_Loss: 0.0681  BEST VAL Loss: 0.0577  Val_Acc: 97.872

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.0487 Train_Acc: 98.670 Val_Loss: 0.0680  BEST VAL Loss: 0.0577  Val_Acc: 97.724

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0482 Train_Acc: 98.664 Val_Loss: 0.0679  BEST VAL Loss: 0.0577  Val_Acc: 97.773

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0478 Train_Acc: 98.534 Val_Loss: 0.0675  BEST VAL Loss: 0.0577  Val_Acc: 97.971

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0473 Train_Acc: 98.688 Val_Loss: 0.0708  BEST VAL Loss: 0.0577  Val_Acc: 97.823

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0468 Train_Acc: 98.800 Val_Loss: 0.0727  BEST VAL Loss: 0.0577  Val_Acc: 97.773

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0464 Train_Acc: 98.571 Val_Loss: 0.0724  BEST VAL Loss: 0.0577  Val_Acc: 97.773

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0459 Train_Acc: 98.571 Val_Loss: 0.0726  BEST VAL Loss: 0.0577  Val_Acc: 97.674

Epoch 44: Validation loss did not decrease
Early stopped at epoch : 44
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.52      0.52      8312
           1       0.49      0.48      0.48      7850

    accuracy                           0.50     16162
   macro avg       0.50      0.50      0.50     16162
weighted avg       0.50      0.50      0.50     16162

LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.51      0.51      1039
           1       0.48      0.48      0.48       982

    accuracy                           0.50      2021
   macro avg       0.49      0.49      0.49      2021
weighted avg       0.50      0.50      0.50      2021

LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.52      0.52      1039
           1       0.48      0.48      0.48       982

    accuracy                           0.50      2021
   macro avg       0.50      0.50      0.50      2021
weighted avg       0.50      0.50      0.50      2021

              precision    recall  f1-score   support

           0       0.51      0.52      0.52      1039
           1       0.48      0.48      0.48       982

    accuracy                           0.50      2021
   macro avg       0.50      0.50      0.50      2021
weighted avg       0.50      0.50      0.50      2021

LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.49      0.49      0.49      3262
           1       0.51      0.50      0.50      3398

    accuracy                           0.50      6660
   macro avg       0.50      0.50      0.50      6660
weighted avg       0.50      0.50      0.50      6660

              precision    recall  f1-score   support

           0       0.49      0.49      0.49      3262
           1       0.51      0.50      0.50      3398

    accuracy                           0.50      6660
   macro avg       0.50      0.50      0.50      6660
weighted avg       0.50      0.50      0.50      6660

completed
