[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'f3da3a5d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '6c2644a6'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '79658fcf'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '6a447aa1'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_1.000_DMSO_0.025']
The dimensions of the data are: (29670, 1276)
Number of total missing values across all columns: 59340
Data Subset Is Off
Wells held out for testing: ['E14' 'D20']
Wells to use for training, validation, and testing ['E15' 'D16' 'D17' 'D21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.304738).  Saving model ...
	 Train_Loss: 0.4470 Train_Acc: 77.191 Val_Loss: 0.3047  BEST VAL Loss: 0.3047  Val_Acc: 86.205

Epoch 1: Validation loss decreased (0.304738 --> 0.266885).  Saving model ...
	 Train_Loss: 0.3714 Train_Acc: 86.500 Val_Loss: 0.2669  BEST VAL Loss: 0.2669  Val_Acc: 90.095

Epoch 2: Validation loss decreased (0.266885 --> 0.248307).  Saving model ...
	 Train_Loss: 0.3297 Train_Acc: 89.215 Val_Loss: 0.2483  BEST VAL Loss: 0.2483  Val_Acc: 90.502

Epoch 3: Validation loss decreased (0.248307 --> 0.235796).  Saving model ...
	 Train_Loss: 0.3004 Train_Acc: 90.980 Val_Loss: 0.2358  BEST VAL Loss: 0.2358  Val_Acc: 91.000

Epoch 4: Validation loss decreased (0.235796 --> 0.226442).  Saving model ...
	 Train_Loss: 0.2789 Train_Acc: 92.099 Val_Loss: 0.2264  BEST VAL Loss: 0.2264  Val_Acc: 92.130

Epoch 5: Validation loss decreased (0.226442 --> 0.219914).  Saving model ...
	 Train_Loss: 0.2623 Train_Acc: 92.546 Val_Loss: 0.2199  BEST VAL Loss: 0.2199  Val_Acc: 92.175

Epoch 6: Validation loss decreased (0.219914 --> 0.215228).  Saving model ...
	 Train_Loss: 0.2484 Train_Acc: 93.032 Val_Loss: 0.2152  BEST VAL Loss: 0.2152  Val_Acc: 92.130

Epoch 7: Validation loss decreased (0.215228 --> 0.210711).  Saving model ...
	 Train_Loss: 0.2376 Train_Acc: 93.163 Val_Loss: 0.2107  BEST VAL Loss: 0.2107  Val_Acc: 92.447

Epoch 8: Validation loss decreased (0.210711 --> 0.207227).  Saving model ...
	 Train_Loss: 0.2281 Train_Acc: 93.547 Val_Loss: 0.2072  BEST VAL Loss: 0.2072  Val_Acc: 92.266

Epoch 9: Validation loss decreased (0.207227 --> 0.205767).  Saving model ...
	 Train_Loss: 0.2200 Train_Acc: 93.711 Val_Loss: 0.2058  BEST VAL Loss: 0.2058  Val_Acc: 92.673

Epoch 10: Validation loss decreased (0.205767 --> 0.203177).  Saving model ...
	 Train_Loss: 0.2135 Train_Acc: 93.643 Val_Loss: 0.2032  BEST VAL Loss: 0.2032  Val_Acc: 92.130

Epoch 11: Validation loss decreased (0.203177 --> 0.200744).  Saving model ...
	 Train_Loss: 0.2075 Train_Acc: 94.022 Val_Loss: 0.2007  BEST VAL Loss: 0.2007  Val_Acc: 92.673

Epoch 12: Validation loss decreased (0.200744 --> 0.199187).  Saving model ...
	 Train_Loss: 0.2018 Train_Acc: 94.141 Val_Loss: 0.1992  BEST VAL Loss: 0.1992  Val_Acc: 92.854

Epoch 13: Validation loss decreased (0.199187 --> 0.198456).  Saving model ...
	 Train_Loss: 0.1967 Train_Acc: 94.537 Val_Loss: 0.1985  BEST VAL Loss: 0.1985  Val_Acc: 93.035

Epoch 14: Validation loss decreased (0.198456 --> 0.198106).  Saving model ...
	 Train_Loss: 0.1917 Train_Acc: 94.825 Val_Loss: 0.1981  BEST VAL Loss: 0.1981  Val_Acc: 92.718

Epoch 15: Validation loss decreased (0.198106 --> 0.197809).  Saving model ...
	 Train_Loss: 0.1874 Train_Acc: 94.972 Val_Loss: 0.1978  BEST VAL Loss: 0.1978  Val_Acc: 92.899

Epoch 16: Validation loss decreased (0.197809 --> 0.197715).  Saving model ...
	 Train_Loss: 0.1835 Train_Acc: 94.859 Val_Loss: 0.1977  BEST VAL Loss: 0.1977  Val_Acc: 93.080

Epoch 17: Validation loss decreased (0.197715 --> 0.197665).  Saving model ...
	 Train_Loss: 0.1796 Train_Acc: 95.227 Val_Loss: 0.1977  BEST VAL Loss: 0.1977  Val_Acc: 93.080

Epoch 18: Validation loss decreased (0.197665 --> 0.197194).  Saving model ...
	 Train_Loss: 0.1764 Train_Acc: 95.080 Val_Loss: 0.1972  BEST VAL Loss: 0.1972  Val_Acc: 93.035

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1734 Train_Acc: 95.227 Val_Loss: 0.1976  BEST VAL Loss: 0.1972  Val_Acc: 93.080

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1704 Train_Acc: 95.312 Val_Loss: 0.1991  BEST VAL Loss: 0.1972  Val_Acc: 92.583

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1679 Train_Acc: 94.740 Val_Loss: 0.1996  BEST VAL Loss: 0.1972  Val_Acc: 92.899

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1652 Train_Acc: 95.634 Val_Loss: 0.1997  BEST VAL Loss: 0.1972  Val_Acc: 92.718

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1626 Train_Acc: 95.809 Val_Loss: 0.1998  BEST VAL Loss: 0.1972  Val_Acc: 93.125

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1603 Train_Acc: 95.374 Val_Loss: 0.2004  BEST VAL Loss: 0.1972  Val_Acc: 92.899

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1581 Train_Acc: 95.747 Val_Loss: 0.2001  BEST VAL Loss: 0.1972  Val_Acc: 93.171

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1559 Train_Acc: 95.939 Val_Loss: 0.2006  BEST VAL Loss: 0.1972  Val_Acc: 92.944

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1536 Train_Acc: 96.081 Val_Loss: 0.2010  BEST VAL Loss: 0.1972  Val_Acc: 92.763

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1516 Train_Acc: 95.990 Val_Loss: 0.2020  BEST VAL Loss: 0.1972  Val_Acc: 93.351

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1495 Train_Acc: 96.166 Val_Loss: 0.2030  BEST VAL Loss: 0.1972  Val_Acc: 92.944

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1477 Train_Acc: 96.222 Val_Loss: 0.2044  BEST VAL Loss: 0.1972  Val_Acc: 92.763

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1460 Train_Acc: 95.973 Val_Loss: 0.2052  BEST VAL Loss: 0.1972  Val_Acc: 92.899

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1443 Train_Acc: 96.036 Val_Loss: 0.2064  BEST VAL Loss: 0.1972  Val_Acc: 92.854

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1427 Train_Acc: 96.267 Val_Loss: 0.2074  BEST VAL Loss: 0.1972  Val_Acc: 92.944

Epoch 34: Validation loss did not decrease
Early stopped at epoch : 34
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.98      9832
           1       0.98      0.96      0.97      7850

    accuracy                           0.97     17682
   macro avg       0.97      0.97      0.97     17682
weighted avg       0.97      0.97      0.97     17682

Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.94      0.94      1229
           1       0.93      0.92      0.92       982

    accuracy                           0.93      2211
   macro avg       0.93      0.93      0.93      2211
weighted avg       0.93      0.93      0.93      2211

Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.92      0.96      0.94      1229
           1       0.94      0.89      0.92       982

    accuracy                           0.93      2211
   macro avg       0.93      0.92      0.93      2211
weighted avg       0.93      0.93      0.93      2211

              precision    recall  f1-score   support

           0       0.92      0.96      0.94      1229
           1       0.94      0.89      0.92       982

    accuracy                           0.93      2211
   macro avg       0.93      0.92      0.93      2211
weighted avg       0.93      0.93      0.93      2211

Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.95      0.95      4168
           1       0.94      0.93      0.93      3398

    accuracy                           0.94      7566
   macro avg       0.94      0.94      0.94      7566
weighted avg       0.94      0.94      0.94      7566

              precision    recall  f1-score   support

           0       0.94      0.95      0.95      4168
           1       0.94      0.93      0.93      3398

    accuracy                           0.94      7566
   macro avg       0.94      0.94      0.94      7566
weighted avg       0.94      0.94      0.94      7566

completed
