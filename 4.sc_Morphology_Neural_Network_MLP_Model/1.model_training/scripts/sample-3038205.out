[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '80fb9902'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e4c60be9'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '30de4eee'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '4d90af8a'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: LPS_100.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_100.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_100.000_DMSO_0.025']
The dimensions of the data are: (29753, 1276)
Number of total missing values across all columns: 59506
Data Subset Is Off
Wells held out for testing: ['D14' 'J20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'J16' 'J17' 'J21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.175834).  Saving model ...
	 Train_Loss: 0.3923 Train_Acc: 80.966 Val_Loss: 0.1758  BEST VAL Loss: 0.1758  Val_Acc: 93.035

Epoch 1: Validation loss decreased (0.175834 --> 0.168111).  Saving model ...
	 Train_Loss: 0.3296 Train_Acc: 88.955 Val_Loss: 0.1681  BEST VAL Loss: 0.1681  Val_Acc: 95.226

Epoch 2: Validation loss decreased (0.168111 --> 0.148953).  Saving model ...
	 Train_Loss: 0.2973 Train_Acc: 90.291 Val_Loss: 0.1490  BEST VAL Loss: 0.1490  Val_Acc: 95.926

Epoch 3: Validation loss decreased (0.148953 --> 0.140875).  Saving model ...
	 Train_Loss: 0.2758 Train_Acc: 90.872 Val_Loss: 0.1409  BEST VAL Loss: 0.1409  Val_Acc: 96.014

Epoch 4: Validation loss decreased (0.140875 --> 0.134819).  Saving model ...
	 Train_Loss: 0.2609 Train_Acc: 91.989 Val_Loss: 0.1348  BEST VAL Loss: 0.1348  Val_Acc: 96.233

Epoch 5: Validation loss decreased (0.134819 --> 0.130082).  Saving model ...
	 Train_Loss: 0.2491 Train_Acc: 92.219 Val_Loss: 0.1301  BEST VAL Loss: 0.1301  Val_Acc: 96.540

Epoch 6: Validation loss decreased (0.130082 --> 0.126878).  Saving model ...
	 Train_Loss: 0.2410 Train_Acc: 92.432 Val_Loss: 0.1269  BEST VAL Loss: 0.1269  Val_Acc: 96.583

Epoch 7: Validation loss decreased (0.126878 --> 0.121957).  Saving model ...
	 Train_Loss: 0.2325 Train_Acc: 92.723 Val_Loss: 0.1220  BEST VAL Loss: 0.1220  Val_Acc: 97.065

Epoch 8: Validation loss decreased (0.121957 --> 0.118351).  Saving model ...
	 Train_Loss: 0.2256 Train_Acc: 93.172 Val_Loss: 0.1184  BEST VAL Loss: 0.1184  Val_Acc: 97.021

Epoch 9: Validation loss decreased (0.118351 --> 0.115926).  Saving model ...
	 Train_Loss: 0.2200 Train_Acc: 92.969 Val_Loss: 0.1159  BEST VAL Loss: 0.1159  Val_Acc: 96.802

Epoch 10: Validation loss decreased (0.115926 --> 0.114172).  Saving model ...
	 Train_Loss: 0.2156 Train_Acc: 92.931 Val_Loss: 0.1142  BEST VAL Loss: 0.1142  Val_Acc: 96.759

Epoch 11: Validation loss decreased (0.114172 --> 0.112157).  Saving model ...
	 Train_Loss: 0.2115 Train_Acc: 93.073 Val_Loss: 0.1122  BEST VAL Loss: 0.1122  Val_Acc: 97.284

Epoch 12: Validation loss decreased (0.112157 --> 0.110440).  Saving model ...
	 Train_Loss: 0.2080 Train_Acc: 92.925 Val_Loss: 0.1104  BEST VAL Loss: 0.1104  Val_Acc: 96.934

Epoch 13: Validation loss decreased (0.110440 --> 0.108836).  Saving model ...
	 Train_Loss: 0.2043 Train_Acc: 93.456 Val_Loss: 0.1088  BEST VAL Loss: 0.1088  Val_Acc: 96.978

Epoch 14: Validation loss did not decrease
	 Train_Loss: 0.2009 Train_Acc: 93.517 Val_Loss: 0.1091  BEST VAL Loss: 0.1088  Val_Acc: 97.153

Epoch 15: Validation loss decreased (0.108836 --> 0.107984).  Saving model ...
	 Train_Loss: 0.1980 Train_Acc: 93.517 Val_Loss: 0.1080  BEST VAL Loss: 0.1080  Val_Acc: 97.197

Epoch 16: Validation loss decreased (0.107984 --> 0.107251).  Saving model ...
	 Train_Loss: 0.1955 Train_Acc: 93.484 Val_Loss: 0.1073  BEST VAL Loss: 0.1073  Val_Acc: 97.284

Epoch 17: Validation loss decreased (0.107251 --> 0.105986).  Saving model ...
	 Train_Loss: 0.1932 Train_Acc: 93.621 Val_Loss: 0.1060  BEST VAL Loss: 0.1060  Val_Acc: 97.547

Epoch 18: Validation loss decreased (0.105986 --> 0.105569).  Saving model ...
	 Train_Loss: 0.1908 Train_Acc: 93.763 Val_Loss: 0.1056  BEST VAL Loss: 0.1056  Val_Acc: 97.459

Epoch 19: Validation loss decreased (0.105569 --> 0.104502).  Saving model ...
	 Train_Loss: 0.1890 Train_Acc: 93.495 Val_Loss: 0.1045  BEST VAL Loss: 0.1045  Val_Acc: 97.503

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1873 Train_Acc: 93.396 Val_Loss: 0.1045  BEST VAL Loss: 0.1045  Val_Acc: 97.328

Epoch 21: Validation loss decreased (0.104502 --> 0.102989).  Saving model ...
	 Train_Loss: 0.1857 Train_Acc: 93.615 Val_Loss: 0.1030  BEST VAL Loss: 0.1030  Val_Acc: 97.722

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1839 Train_Acc: 93.851 Val_Loss: 0.1042  BEST VAL Loss: 0.1030  Val_Acc: 97.328

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1825 Train_Acc: 93.593 Val_Loss: 0.1034  BEST VAL Loss: 0.1030  Val_Acc: 97.722

Epoch 24: Validation loss decreased (0.102989 --> 0.102665).  Saving model ...
	 Train_Loss: 0.1811 Train_Acc: 93.829 Val_Loss: 0.1027  BEST VAL Loss: 0.1027  Val_Acc: 97.547

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1798 Train_Acc: 93.741 Val_Loss: 0.1029  BEST VAL Loss: 0.1027  Val_Acc: 97.328

Epoch 26: Validation loss decreased (0.102665 --> 0.102482).  Saving model ...
	 Train_Loss: 0.1785 Train_Acc: 93.741 Val_Loss: 0.1025  BEST VAL Loss: 0.1025  Val_Acc: 97.328

Epoch 27: Validation loss decreased (0.102482 --> 0.102405).  Saving model ...
	 Train_Loss: 0.1774 Train_Acc: 93.697 Val_Loss: 0.1024  BEST VAL Loss: 0.1024  Val_Acc: 97.503

Epoch 28: Validation loss decreased (0.102405 --> 0.101630).  Saving model ...
	 Train_Loss: 0.1766 Train_Acc: 93.681 Val_Loss: 0.1016  BEST VAL Loss: 0.1016  Val_Acc: 97.503

Epoch 29: Validation loss decreased (0.101630 --> 0.101223).  Saving model ...
	 Train_Loss: 0.1757 Train_Acc: 93.681 Val_Loss: 0.1012  BEST VAL Loss: 0.1012  Val_Acc: 97.459

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1748 Train_Acc: 93.834 Val_Loss: 0.1013  BEST VAL Loss: 0.1012  Val_Acc: 97.635

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1737 Train_Acc: 94.031 Val_Loss: 0.1018  BEST VAL Loss: 0.1012  Val_Acc: 97.459

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1731 Train_Acc: 93.445 Val_Loss: 0.1016  BEST VAL Loss: 0.1012  Val_Acc: 97.328

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1724 Train_Acc: 93.621 Val_Loss: 0.1019  BEST VAL Loss: 0.1012  Val_Acc: 96.890

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1716 Train_Acc: 93.714 Val_Loss: 0.1017  BEST VAL Loss: 0.1012  Val_Acc: 97.547

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1710 Train_Acc: 93.697 Val_Loss: 0.1017  BEST VAL Loss: 0.1012  Val_Acc: 97.591

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1702 Train_Acc: 93.916 Val_Loss: 0.1019  BEST VAL Loss: 0.1012  Val_Acc: 97.459

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1694 Train_Acc: 93.927 Val_Loss: 0.1017  BEST VAL Loss: 0.1012  Val_Acc: 97.459

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1687 Train_Acc: 94.037 Val_Loss: 0.1017  BEST VAL Loss: 0.1012  Val_Acc: 97.898

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1680 Train_Acc: 93.966 Val_Loss: 0.1015  BEST VAL Loss: 0.1012  Val_Acc: 97.635

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1675 Train_Acc: 93.752 Val_Loss: 0.1021  BEST VAL Loss: 0.1012  Val_Acc: 97.722

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1669 Train_Acc: 93.900 Val_Loss: 0.1020  BEST VAL Loss: 0.1012  Val_Acc: 97.766

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1663 Train_Acc: 93.862 Val_Loss: 0.1018  BEST VAL Loss: 0.1012  Val_Acc: 97.678

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1658 Train_Acc: 93.840 Val_Loss: 0.1023  BEST VAL Loss: 0.1012  Val_Acc: 97.503

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1653 Train_Acc: 94.020 Val_Loss: 0.1023  BEST VAL Loss: 0.1012  Val_Acc: 97.547

Epoch 45: Validation loss did not decrease
Early stopped at epoch : 45
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.54      0.54      9891
           1       0.46      0.45      0.45      8371

    accuracy                           0.50     18262
   macro avg       0.50      0.50      0.50     18262
weighted avg       0.50      0.50      0.50     18262

Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1237
           1       0.47      0.47      0.47      1046

    accuracy                           0.51      2283
   macro avg       0.51      0.51      0.51      2283
weighted avg       0.51      0.51      0.51      2283

Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.53      0.53      1237
           1       0.44      0.44      0.44      1046

    accuracy                           0.49      2283
   macro avg       0.48      0.48      0.48      2283
weighted avg       0.49      0.49      0.49      2283

              precision    recall  f1-score   support

           0       0.53      0.53      0.53      1237
           1       0.44      0.44      0.44      1046

    accuracy                           0.49      2283
   macro avg       0.48      0.48      0.48      2283
weighted avg       0.49      0.49      0.49      2283

Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.51      0.51      3622
           1       0.47      0.49      0.48      3303

    accuracy                           0.50      6925
   macro avg       0.50      0.50      0.50      6925
weighted avg       0.50      0.50      0.50      6925

              precision    recall  f1-score   support

           0       0.52      0.51      0.51      3622
           1       0.47      0.49      0.48      3303

    accuracy                           0.50      6925
   macro avg       0.50      0.50      0.50      6925
weighted avg       0.50      0.50      0.50      6925

completed
