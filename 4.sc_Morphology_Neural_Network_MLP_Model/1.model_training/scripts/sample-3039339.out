[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '0afc4807'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2a255ad3'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '1d8c8dac'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e84ae2a0'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: H2O2_100.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: H2O2_100.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'H2O2_100.000_DMSO_0.025']
The dimensions of the data are: (388992, 1270)
Number of total missing values across all columns: 777984
Data Subset Is Off
Wells held out for testing: ['I10' 'K07']
Wells to use for training, validation, and testing ['D06' 'D07' 'H04' 'I04' 'H05' 'I05' 'H10' 'H11' 'I11' 'K06']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.569188).  Saving model ...
	 Train_Loss: 0.6450 Train_Acc: 56.833 Val_Loss: 0.5692  BEST VAL Loss: 0.5692  Val_Acc: 71.570

Epoch 1: Validation loss decreased (0.569188 --> 0.533987).  Saving model ...
	 Train_Loss: 0.6007 Train_Acc: 71.293 Val_Loss: 0.5340  BEST VAL Loss: 0.5340  Val_Acc: 75.999

Epoch 2: Validation loss decreased (0.533987 --> 0.509278).  Saving model ...
	 Train_Loss: 0.5706 Train_Acc: 74.043 Val_Loss: 0.5093  BEST VAL Loss: 0.5093  Val_Acc: 78.230

Epoch 3: Validation loss decreased (0.509278 --> 0.493068).  Saving model ...
	 Train_Loss: 0.5491 Train_Acc: 75.475 Val_Loss: 0.4931  BEST VAL Loss: 0.4931  Val_Acc: 79.059

Epoch 4: Validation loss decreased (0.493068 --> 0.480784).  Saving model ...
	 Train_Loss: 0.5332 Train_Acc: 76.370 Val_Loss: 0.4808  BEST VAL Loss: 0.4808  Val_Acc: 79.776

Epoch 5: Validation loss decreased (0.480784 --> 0.470880).  Saving model ...
	 Train_Loss: 0.5209 Train_Acc: 76.749 Val_Loss: 0.4709  BEST VAL Loss: 0.4709  Val_Acc: 80.118

Epoch 6: Validation loss decreased (0.470880 --> 0.462716).  Saving model ...
	 Train_Loss: 0.5111 Train_Acc: 77.046 Val_Loss: 0.4627  BEST VAL Loss: 0.4627  Val_Acc: 80.544

Epoch 7: Validation loss decreased (0.462716 --> 0.456202).  Saving model ...
	 Train_Loss: 0.5032 Train_Acc: 77.230 Val_Loss: 0.4562  BEST VAL Loss: 0.4562  Val_Acc: 80.780

Epoch 8: Validation loss decreased (0.456202 --> 0.450244).  Saving model ...
	 Train_Loss: 0.4964 Train_Acc: 77.509 Val_Loss: 0.4502  BEST VAL Loss: 0.4502  Val_Acc: 80.950

Epoch 9: Validation loss decreased (0.450244 --> 0.445298).  Saving model ...
	 Train_Loss: 0.4906 Train_Acc: 77.628 Val_Loss: 0.4453  BEST VAL Loss: 0.4453  Val_Acc: 81.081

Epoch 10: Validation loss decreased (0.445298 --> 0.440802).  Saving model ...
	 Train_Loss: 0.4856 Train_Acc: 77.837 Val_Loss: 0.4408  BEST VAL Loss: 0.4408  Val_Acc: 81.326

Epoch 11: Validation loss decreased (0.440802 --> 0.436685).  Saving model ...
	 Train_Loss: 0.4812 Train_Acc: 77.862 Val_Loss: 0.4367  BEST VAL Loss: 0.4367  Val_Acc: 81.621

Epoch 12: Validation loss decreased (0.436685 --> 0.433289).  Saving model ...
	 Train_Loss: 0.4773 Train_Acc: 77.948 Val_Loss: 0.4333  BEST VAL Loss: 0.4333  Val_Acc: 81.590

Epoch 13: Validation loss decreased (0.433289 --> 0.430010).  Saving model ...
	 Train_Loss: 0.4737 Train_Acc: 78.475 Val_Loss: 0.4300  BEST VAL Loss: 0.4300  Val_Acc: 81.609

Epoch 14: Validation loss decreased (0.430010 --> 0.427027).  Saving model ...
	 Train_Loss: 0.4704 Train_Acc: 78.726 Val_Loss: 0.4270  BEST VAL Loss: 0.4270  Val_Acc: 81.861

Epoch 15: Validation loss decreased (0.427027 --> 0.424222).  Saving model ...
	 Train_Loss: 0.4674 Train_Acc: 78.875 Val_Loss: 0.4242  BEST VAL Loss: 0.4242  Val_Acc: 82.041

Epoch 16: Validation loss decreased (0.424222 --> 0.421833).  Saving model ...
	 Train_Loss: 0.4647 Train_Acc: 78.781 Val_Loss: 0.4218  BEST VAL Loss: 0.4218  Val_Acc: 81.786

Epoch 17: Validation loss decreased (0.421833 --> 0.419816).  Saving model ...
	 Train_Loss: 0.4623 Train_Acc: 78.737 Val_Loss: 0.4198  BEST VAL Loss: 0.4198  Val_Acc: 81.820

Epoch 18: Validation loss decreased (0.419816 --> 0.417774).  Saving model ...
	 Train_Loss: 0.4600 Train_Acc: 78.776 Val_Loss: 0.4178  BEST VAL Loss: 0.4178  Val_Acc: 81.979

Epoch 19: Validation loss decreased (0.417774 --> 0.415847).  Saving model ...
	 Train_Loss: 0.4579 Train_Acc: 78.854 Val_Loss: 0.4158  BEST VAL Loss: 0.4158  Val_Acc: 81.982

Epoch 20: Validation loss decreased (0.415847 --> 0.414092).  Saving model ...
	 Train_Loss: 0.4560 Train_Acc: 78.953 Val_Loss: 0.4141  BEST VAL Loss: 0.4141  Val_Acc: 82.066

Epoch 21: Validation loss decreased (0.414092 --> 0.412348).  Saving model ...
	 Train_Loss: 0.4542 Train_Acc: 78.958 Val_Loss: 0.4123  BEST VAL Loss: 0.4123  Val_Acc: 82.370

Epoch 22: Validation loss decreased (0.412348 --> 0.410659).  Saving model ...
	 Train_Loss: 0.4524 Train_Acc: 79.045 Val_Loss: 0.4107  BEST VAL Loss: 0.4107  Val_Acc: 82.410

Epoch 23: Validation loss decreased (0.410659 --> 0.409131).  Saving model ...
	 Train_Loss: 0.4509 Train_Acc: 79.090 Val_Loss: 0.4091  BEST VAL Loss: 0.4091  Val_Acc: 82.392

Epoch 24: Validation loss decreased (0.409131 --> 0.407656).  Saving model ...
	 Train_Loss: 0.4494 Train_Acc: 79.139 Val_Loss: 0.4077  BEST VAL Loss: 0.4077  Val_Acc: 82.320

Epoch 25: Validation loss decreased (0.407656 --> 0.406466).  Saving model ...
	 Train_Loss: 0.4480 Train_Acc: 79.081 Val_Loss: 0.4065  BEST VAL Loss: 0.4065  Val_Acc: 82.174

Epoch 26: Validation loss decreased (0.406466 --> 0.405212).  Saving model ...
	 Train_Loss: 0.4467 Train_Acc: 79.204 Val_Loss: 0.4052  BEST VAL Loss: 0.4052  Val_Acc: 82.342

Epoch 27: Validation loss decreased (0.405212 --> 0.403969).  Saving model ...
	 Train_Loss: 0.4454 Train_Acc: 79.238 Val_Loss: 0.4040  BEST VAL Loss: 0.4040  Val_Acc: 82.556

Epoch 28: Validation loss decreased (0.403969 --> 0.402812).  Saving model ...
	 Train_Loss: 0.4442 Train_Acc: 79.276 Val_Loss: 0.4028  BEST VAL Loss: 0.4028  Val_Acc: 82.622

Epoch 29: Validation loss decreased (0.402812 --> 0.401668).  Saving model ...
	 Train_Loss: 0.4430 Train_Acc: 79.355 Val_Loss: 0.4017  BEST VAL Loss: 0.4017  Val_Acc: 82.628

Epoch 30: Validation loss decreased (0.401668 --> 0.400624).  Saving model ...
	 Train_Loss: 0.4419 Train_Acc: 79.346 Val_Loss: 0.4006  BEST VAL Loss: 0.4006  Val_Acc: 82.507

Epoch 31: Validation loss decreased (0.400624 --> 0.399671).  Saving model ...
	 Train_Loss: 0.4408 Train_Acc: 79.337 Val_Loss: 0.3997  BEST VAL Loss: 0.3997  Val_Acc: 82.528

Epoch 32: Validation loss decreased (0.399671 --> 0.398701).  Saving model ...
	 Train_Loss: 0.4399 Train_Acc: 79.254 Val_Loss: 0.3987  BEST VAL Loss: 0.3987  Val_Acc: 82.600

Epoch 33: Validation loss decreased (0.398701 --> 0.397750).  Saving model ...
	 Train_Loss: 0.4389 Train_Acc: 79.374 Val_Loss: 0.3977  BEST VAL Loss: 0.3977  Val_Acc: 82.789

Epoch 34: Validation loss decreased (0.397750 --> 0.396866).  Saving model ...
	 Train_Loss: 0.4380 Train_Acc: 79.464 Val_Loss: 0.3969  BEST VAL Loss: 0.3969  Val_Acc: 82.677

Epoch 35: Validation loss decreased (0.396866 --> 0.396011).  Saving model ...
	 Train_Loss: 0.4371 Train_Acc: 79.444 Val_Loss: 0.3960  BEST VAL Loss: 0.3960  Val_Acc: 82.786

Epoch 36: Validation loss decreased (0.396011 --> 0.395180).  Saving model ...
	 Train_Loss: 0.4362 Train_Acc: 79.425 Val_Loss: 0.3952  BEST VAL Loss: 0.3952  Val_Acc: 82.799

Epoch 37: Validation loss decreased (0.395180 --> 0.394332).  Saving model ...
	 Train_Loss: 0.4355 Train_Acc: 79.475 Val_Loss: 0.3943  BEST VAL Loss: 0.3943  Val_Acc: 82.963

Epoch 38: Validation loss decreased (0.394332 --> 0.393499).  Saving model ...
	 Train_Loss: 0.4347 Train_Acc: 79.436 Val_Loss: 0.3935  BEST VAL Loss: 0.3935  Val_Acc: 83.072

Epoch 39: Validation loss decreased (0.393499 --> 0.392747).  Saving model ...
	 Train_Loss: 0.4339 Train_Acc: 79.496 Val_Loss: 0.3927  BEST VAL Loss: 0.3927  Val_Acc: 82.960

Epoch 40: Validation loss decreased (0.392747 --> 0.392048).  Saving model ...
	 Train_Loss: 0.4332 Train_Acc: 79.605 Val_Loss: 0.3920  BEST VAL Loss: 0.3920  Val_Acc: 82.948

Epoch 41: Validation loss decreased (0.392048 --> 0.391334).  Saving model ...
	 Train_Loss: 0.4325 Train_Acc: 79.558 Val_Loss: 0.3913  BEST VAL Loss: 0.3913  Val_Acc: 82.898

Epoch 42: Validation loss decreased (0.391334 --> 0.390672).  Saving model ...
	 Train_Loss: 0.4318 Train_Acc: 79.633 Val_Loss: 0.3907  BEST VAL Loss: 0.3907  Val_Acc: 82.889

Epoch 43: Validation loss decreased (0.390672 --> 0.389986).  Saving model ...
	 Train_Loss: 0.4311 Train_Acc: 79.612 Val_Loss: 0.3900  BEST VAL Loss: 0.3900  Val_Acc: 83.137

Epoch 44: Validation loss decreased (0.389986 --> 0.389330).  Saving model ...
	 Train_Loss: 0.4304 Train_Acc: 79.863 Val_Loss: 0.3893  BEST VAL Loss: 0.3893  Val_Acc: 82.985

Epoch 45: Validation loss decreased (0.389330 --> 0.388733).  Saving model ...
	 Train_Loss: 0.4298 Train_Acc: 79.841 Val_Loss: 0.3887  BEST VAL Loss: 0.3887  Val_Acc: 82.963

Epoch 46: Validation loss decreased (0.388733 --> 0.388151).  Saving model ...
	 Train_Loss: 0.4292 Train_Acc: 79.852 Val_Loss: 0.3882  BEST VAL Loss: 0.3882  Val_Acc: 82.910

Epoch 47: Validation loss decreased (0.388151 --> 0.387538).  Saving model ...
	 Train_Loss: 0.4286 Train_Acc: 79.836 Val_Loss: 0.3875  BEST VAL Loss: 0.3875  Val_Acc: 83.215

Epoch 48: Validation loss decreased (0.387538 --> 0.386984).  Saving model ...
	 Train_Loss: 0.4281 Train_Acc: 79.896 Val_Loss: 0.3870  BEST VAL Loss: 0.3870  Val_Acc: 83.134

Epoch 49: Validation loss decreased (0.386984 --> 0.386460).  Saving model ...
	 Train_Loss: 0.4275 Train_Acc: 79.921 Val_Loss: 0.3865  BEST VAL Loss: 0.3865  Val_Acc: 83.246

Epoch 50: Validation loss decreased (0.386460 --> 0.385927).  Saving model ...
	 Train_Loss: 0.4269 Train_Acc: 79.943 Val_Loss: 0.3859  BEST VAL Loss: 0.3859  Val_Acc: 83.109

Epoch 51: Validation loss decreased (0.385927 --> 0.385426).  Saving model ...
	 Train_Loss: 0.4264 Train_Acc: 79.921 Val_Loss: 0.3854  BEST VAL Loss: 0.3854  Val_Acc: 83.174

Epoch 52: Validation loss decreased (0.385426 --> 0.384940).  Saving model ...
	 Train_Loss: 0.4259 Train_Acc: 80.020 Val_Loss: 0.3849  BEST VAL Loss: 0.3849  Val_Acc: 83.193

Epoch 53: Validation loss decreased (0.384940 --> 0.384443).  Saving model ...
	 Train_Loss: 0.4254 Train_Acc: 80.111 Val_Loss: 0.3844  BEST VAL Loss: 0.3844  Val_Acc: 83.221

Epoch 54: Validation loss decreased (0.384443 --> 0.383929).  Saving model ...
	 Train_Loss: 0.4249 Train_Acc: 80.023 Val_Loss: 0.3839  BEST VAL Loss: 0.3839  Val_Acc: 83.497

Epoch 55: Validation loss decreased (0.383929 --> 0.383492).  Saving model ...
	 Train_Loss: 0.4244 Train_Acc: 79.949 Val_Loss: 0.3835  BEST VAL Loss: 0.3835  Val_Acc: 83.097

Epoch 56: Validation loss decreased (0.383492 --> 0.383050).  Saving model ...
	 Train_Loss: 0.4240 Train_Acc: 80.029 Val_Loss: 0.3830  BEST VAL Loss: 0.3830  Val_Acc: 83.140

Epoch 57: Validation loss decreased (0.383050 --> 0.382623).  Saving model ...
	 Train_Loss: 0.4235 Train_Acc: 80.155 Val_Loss: 0.3826  BEST VAL Loss: 0.3826  Val_Acc: 83.153

Epoch 58: Validation loss decreased (0.382623 --> 0.382170).  Saving model ...
	 Train_Loss: 0.4231 Train_Acc: 80.091 Val_Loss: 0.3822  BEST VAL Loss: 0.3822  Val_Acc: 83.320

Epoch 59: Validation loss decreased (0.382170 --> 0.381758).  Saving model ...
	 Train_Loss: 0.4226 Train_Acc: 80.107 Val_Loss: 0.3818  BEST VAL Loss: 0.3818  Val_Acc: 83.308

Epoch 60: Validation loss decreased (0.381758 --> 0.381350).  Saving model ...
	 Train_Loss: 0.4222 Train_Acc: 80.038 Val_Loss: 0.3814  BEST VAL Loss: 0.3814  Val_Acc: 83.296

Epoch 61: Validation loss decreased (0.381350 --> 0.381000).  Saving model ...
	 Train_Loss: 0.4218 Train_Acc: 80.105 Val_Loss: 0.3810  BEST VAL Loss: 0.3810  Val_Acc: 83.317

Epoch 62: Validation loss decreased (0.381000 --> 0.380642).  Saving model ...
	 Train_Loss: 0.4214 Train_Acc: 80.112 Val_Loss: 0.3806  BEST VAL Loss: 0.3806  Val_Acc: 83.202

Epoch 63: Validation loss decreased (0.380642 --> 0.380257).  Saving model ...
	 Train_Loss: 0.4210 Train_Acc: 80.222 Val_Loss: 0.3803  BEST VAL Loss: 0.3803  Val_Acc: 83.327

Epoch 64: Validation loss decreased (0.380257 --> 0.379931).  Saving model ...
	 Train_Loss: 0.4206 Train_Acc: 80.155 Val_Loss: 0.3799  BEST VAL Loss: 0.3799  Val_Acc: 83.168

Epoch 65: Validation loss decreased (0.379931 --> 0.379566).  Saving model ...
	 Train_Loss: 0.4202 Train_Acc: 80.231 Val_Loss: 0.3796  BEST VAL Loss: 0.3796  Val_Acc: 83.342

Epoch 66: Validation loss decreased (0.379566 --> 0.379213).  Saving model ...
	 Train_Loss: 0.4199 Train_Acc: 80.176 Val_Loss: 0.3792  BEST VAL Loss: 0.3792  Val_Acc: 83.342

Epoch 67: Validation loss decreased (0.379213 --> 0.378845).  Saving model ...
	 Train_Loss: 0.4195 Train_Acc: 80.227 Val_Loss: 0.3788  BEST VAL Loss: 0.3788  Val_Acc: 83.519

Epoch 68: Validation loss decreased (0.378845 --> 0.378498).  Saving model ...
	 Train_Loss: 0.4191 Train_Acc: 80.250 Val_Loss: 0.3785  BEST VAL Loss: 0.3785  Val_Acc: 83.376

Epoch 69: Validation loss decreased (0.378498 --> 0.378145).  Saving model ...
	 Train_Loss: 0.4188 Train_Acc: 80.250 Val_Loss: 0.3781  BEST VAL Loss: 0.3781  Val_Acc: 83.476

Epoch 70: Validation loss decreased (0.378145 --> 0.377841).  Saving model ...
	 Train_Loss: 0.4184 Train_Acc: 80.241 Val_Loss: 0.3778  BEST VAL Loss: 0.3778  Val_Acc: 83.454

Epoch 71: Validation loss decreased (0.377841 --> 0.377512).  Saving model ...
	 Train_Loss: 0.4181 Train_Acc: 80.192 Val_Loss: 0.3775  BEST VAL Loss: 0.3775  Val_Acc: 83.482

Epoch 72: Validation loss decreased (0.377512 --> 0.377193).  Saving model ...
	 Train_Loss: 0.4178 Train_Acc: 80.152 Val_Loss: 0.3772  BEST VAL Loss: 0.3772  Val_Acc: 83.445

Epoch 73: Validation loss decreased (0.377193 --> 0.376874).  Saving model ...
	 Train_Loss: 0.4175 Train_Acc: 80.252 Val_Loss: 0.3769  BEST VAL Loss: 0.3769  Val_Acc: 83.466

Epoch 74: Validation loss decreased (0.376874 --> 0.376627).  Saving model ...
	 Train_Loss: 0.4172 Train_Acc: 80.275 Val_Loss: 0.3766  BEST VAL Loss: 0.3766  Val_Acc: 83.187

Epoch 75: Validation loss decreased (0.376627 --> 0.376359).  Saving model ...
	 Train_Loss: 0.4169 Train_Acc: 80.288 Val_Loss: 0.3764  BEST VAL Loss: 0.3764  Val_Acc: 83.370

Epoch 76: Validation loss decreased (0.376359 --> 0.376088).  Saving model ...
	 Train_Loss: 0.4166 Train_Acc: 80.191 Val_Loss: 0.3761  BEST VAL Loss: 0.3761  Val_Acc: 83.323

Epoch 77: Validation loss decreased (0.376088 --> 0.375851).  Saving model ...
	 Train_Loss: 0.4163 Train_Acc: 80.305 Val_Loss: 0.3759  BEST VAL Loss: 0.3759  Val_Acc: 83.311

Epoch 78: Validation loss decreased (0.375851 --> 0.375552).  Saving model ...
	 Train_Loss: 0.4160 Train_Acc: 80.327 Val_Loss: 0.3756  BEST VAL Loss: 0.3756  Val_Acc: 83.544

Epoch 79: Validation loss decreased (0.375552 --> 0.375286).  Saving model ...
	 Train_Loss: 0.4157 Train_Acc: 80.272 Val_Loss: 0.3753  BEST VAL Loss: 0.3753  Val_Acc: 83.469

Epoch 80: Validation loss decreased (0.375286 --> 0.375044).  Saving model ...
	 Train_Loss: 0.4154 Train_Acc: 80.419 Val_Loss: 0.3750  BEST VAL Loss: 0.3750  Val_Acc: 83.348

Epoch 81: Validation loss decreased (0.375044 --> 0.374794).  Saving model ...
	 Train_Loss: 0.4151 Train_Acc: 80.371 Val_Loss: 0.3748  BEST VAL Loss: 0.3748  Val_Acc: 83.395

Epoch 82: Validation loss decreased (0.374794 --> 0.374544).  Saving model ...
	 Train_Loss: 0.4148 Train_Acc: 80.323 Val_Loss: 0.3745  BEST VAL Loss: 0.3745  Val_Acc: 83.479

Epoch 83: Validation loss decreased (0.374544 --> 0.374294).  Saving model ...
	 Train_Loss: 0.4146 Train_Acc: 80.357 Val_Loss: 0.3743  BEST VAL Loss: 0.3743  Val_Acc: 83.364

Epoch 84: Validation loss decreased (0.374294 --> 0.374029).  Saving model ...
	 Train_Loss: 0.4143 Train_Acc: 80.343 Val_Loss: 0.3740  BEST VAL Loss: 0.3740  Val_Acc: 83.637

Epoch 85: Validation loss decreased (0.374029 --> 0.373758).  Saving model ...
	 Train_Loss: 0.4141 Train_Acc: 80.402 Val_Loss: 0.3738  BEST VAL Loss: 0.3738  Val_Acc: 83.805

Epoch 86: Validation loss decreased (0.373758 --> 0.373545).  Saving model ...
	 Train_Loss: 0.4138 Train_Acc: 80.335 Val_Loss: 0.3735  BEST VAL Loss: 0.3735  Val_Acc: 83.395

Epoch 87: Validation loss decreased (0.373545 --> 0.373335).  Saving model ...
	 Train_Loss: 0.4136 Train_Acc: 80.418 Val_Loss: 0.3733  BEST VAL Loss: 0.3733  Val_Acc: 83.432

Epoch 88: Validation loss decreased (0.373335 --> 0.373142).  Saving model ...
	 Train_Loss: 0.4133 Train_Acc: 80.415 Val_Loss: 0.3731  BEST VAL Loss: 0.3731  Val_Acc: 83.264

Epoch 89: Validation loss decreased (0.373142 --> 0.372927).  Saving model ...
	 Train_Loss: 0.4131 Train_Acc: 80.406 Val_Loss: 0.3729  BEST VAL Loss: 0.3729  Val_Acc: 83.469

Epoch 90: Validation loss decreased (0.372927 --> 0.372700).  Saving model ...
	 Train_Loss: 0.4128 Train_Acc: 80.438 Val_Loss: 0.3727  BEST VAL Loss: 0.3727  Val_Acc: 83.609

Epoch 91: Validation loss decreased (0.372700 --> 0.372471).  Saving model ...
	 Train_Loss: 0.4126 Train_Acc: 80.451 Val_Loss: 0.3725  BEST VAL Loss: 0.3725  Val_Acc: 83.640

Epoch 92: Validation loss decreased (0.372471 --> 0.372250).  Saving model ...
	 Train_Loss: 0.4124 Train_Acc: 80.503 Val_Loss: 0.3723  BEST VAL Loss: 0.3723  Val_Acc: 83.581

Epoch 93: Validation loss decreased (0.372250 --> 0.372072).  Saving model ...
	 Train_Loss: 0.4121 Train_Acc: 80.389 Val_Loss: 0.3721  BEST VAL Loss: 0.3721  Val_Acc: 83.528

Epoch 94: Validation loss decreased (0.372072 --> 0.371913).  Saving model ...
	 Train_Loss: 0.4119 Train_Acc: 80.499 Val_Loss: 0.3719  BEST VAL Loss: 0.3719  Val_Acc: 83.383

Epoch 95: Validation loss decreased (0.371913 --> 0.371715).  Saving model ...
	 Train_Loss: 0.4117 Train_Acc: 80.530 Val_Loss: 0.3717  BEST VAL Loss: 0.3717  Val_Acc: 83.513

Epoch 96: Validation loss decreased (0.371715 --> 0.371541).  Saving model ...
	 Train_Loss: 0.4115 Train_Acc: 80.565 Val_Loss: 0.3715  BEST VAL Loss: 0.3715  Val_Acc: 83.628

Epoch 97: Validation loss decreased (0.371541 --> 0.371350).  Saving model ...
	 Train_Loss: 0.4113 Train_Acc: 80.533 Val_Loss: 0.3714  BEST VAL Loss: 0.3714  Val_Acc: 83.706

Epoch 98: Validation loss decreased (0.371350 --> 0.371161).  Saving model ...
	 Train_Loss: 0.4111 Train_Acc: 80.406 Val_Loss: 0.3712  BEST VAL Loss: 0.3712  Val_Acc: 83.588

Epoch 99: Validation loss decreased (0.371161 --> 0.371001).  Saving model ...
	 Train_Loss: 0.4109 Train_Acc: 80.534 Val_Loss: 0.3710  BEST VAL Loss: 0.3710  Val_Acc: 83.442

Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.85      0.93      0.89    169560
           1       0.84      0.69      0.76     87993

    accuracy                           0.85    257553
   macro avg       0.85      0.81      0.82    257553
weighted avg       0.85      0.85      0.85    257553

Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.84      0.92      0.88     21196
           1       0.82      0.67      0.73     10999

    accuracy                           0.83     32195
   macro avg       0.83      0.79      0.81     32195
weighted avg       0.83      0.83      0.83     32195

Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.84      0.92      0.88     21196
           1       0.82      0.67      0.74     10999

    accuracy                           0.84     32195
   macro avg       0.83      0.80      0.81     32195
weighted avg       0.84      0.84      0.83     32195

              precision    recall  f1-score   support

           0       0.84      0.92      0.88     21196
           1       0.82      0.67      0.74     10999

    accuracy                           0.84     32195
   macro avg       0.83      0.80      0.81     32195
weighted avg       0.84      0.84      0.83     32195

Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.65      0.90      0.75     28584
           1       0.89      0.64      0.74     38465

    accuracy                           0.75     67049
   macro avg       0.77      0.77      0.75     67049
weighted avg       0.79      0.75      0.75     67049

              precision    recall  f1-score   support

           0       0.65      0.90      0.75     28584
           1       0.89      0.64      0.74     38465

    accuracy                           0.75     67049
   macro avg       0.77      0.77      0.75     67049
weighted avg       0.79      0.75      0.75     67049

completed
