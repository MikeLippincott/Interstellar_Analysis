[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '573ac1ba'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '39090950'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '0333be4f'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c18948c5'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_0.010_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_0.010_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_0.010_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (31668, 1276)
Number of total missing values across all columns: 63336
Data Subset Is Off
Wells held out for testing: ['B16' 'M22']
Wells to use for training, validation, and testing ['B17' 'B20' 'B21' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.642301).  Saving model ...
	 Train_Loss: 1.0686 Train_Acc: 58.487 Val_Loss: 0.6423  BEST VAL Loss: 0.6423  Val_Acc: 62.553

Epoch 1: Validation loss decreased (0.642301 --> 0.636593).  Saving model ...
	 Train_Loss: 0.8521 Train_Acc: 63.693 Val_Loss: 0.6366  BEST VAL Loss: 0.6366  Val_Acc: 63.694

Epoch 2: Validation loss decreased (0.636593 --> 0.628135).  Saving model ...
	 Train_Loss: 0.7726 Train_Acc: 66.003 Val_Loss: 0.6281  BEST VAL Loss: 0.6281  Val_Acc: 65.511

Epoch 3: Validation loss decreased (0.628135 --> 0.621690).  Saving model ...
	 Train_Loss: 0.7268 Train_Acc: 67.889 Val_Loss: 0.6217  BEST VAL Loss: 0.6217  Val_Acc: 66.272

Epoch 4: Validation loss decreased (0.621690 --> 0.616739).  Saving model ...
	 Train_Loss: 0.6956 Train_Acc: 69.549 Val_Loss: 0.6167  BEST VAL Loss: 0.6167  Val_Acc: 67.878

Epoch 5: Validation loss decreased (0.616739 --> 0.611290).  Saving model ...
	 Train_Loss: 0.6735 Train_Acc: 70.299 Val_Loss: 0.6113  BEST VAL Loss: 0.6113  Val_Acc: 69.146

Epoch 6: Validation loss decreased (0.611290 --> 0.605834).  Saving model ...
	 Train_Loss: 0.6557 Train_Acc: 71.367 Val_Loss: 0.6058  BEST VAL Loss: 0.6058  Val_Acc: 69.315

Epoch 7: Validation loss decreased (0.605834 --> 0.600918).  Saving model ...
	 Train_Loss: 0.6418 Train_Acc: 71.900 Val_Loss: 0.6009  BEST VAL Loss: 0.6009  Val_Acc: 70.499

Epoch 8: Validation loss decreased (0.600918 --> 0.597919).  Saving model ...
	 Train_Loss: 0.6295 Train_Acc: 72.751 Val_Loss: 0.5979  BEST VAL Loss: 0.5979  Val_Acc: 69.104

Epoch 9: Validation loss decreased (0.597919 --> 0.597540).  Saving model ...
	 Train_Loss: 0.6183 Train_Acc: 73.967 Val_Loss: 0.5975  BEST VAL Loss: 0.5975  Val_Acc: 68.385

Epoch 10: Validation loss decreased (0.597540 --> 0.596083).  Saving model ...
	 Train_Loss: 0.6099 Train_Acc: 73.280 Val_Loss: 0.5961  BEST VAL Loss: 0.5961  Val_Acc: 69.653

Epoch 11: Validation loss decreased (0.596083 --> 0.595818).  Saving model ...
	 Train_Loss: 0.6019 Train_Acc: 74.226 Val_Loss: 0.5958  BEST VAL Loss: 0.5958  Val_Acc: 68.808

Epoch 12: Validation loss decreased (0.595818 --> 0.594130).  Saving model ...
	 Train_Loss: 0.5946 Train_Acc: 74.701 Val_Loss: 0.5941  BEST VAL Loss: 0.5941  Val_Acc: 69.484

Epoch 13: Validation loss decreased (0.594130 --> 0.592434).  Saving model ...
	 Train_Loss: 0.5880 Train_Acc: 75.029 Val_Loss: 0.5924  BEST VAL Loss: 0.5924  Val_Acc: 70.795

Epoch 14: Validation loss decreased (0.592434 --> 0.590985).  Saving model ...
	 Train_Loss: 0.5822 Train_Acc: 75.246 Val_Loss: 0.5910  BEST VAL Loss: 0.5910  Val_Acc: 70.626

Epoch 15: Validation loss decreased (0.590985 --> 0.589564).  Saving model ...
	 Train_Loss: 0.5768 Train_Acc: 75.542 Val_Loss: 0.5896  BEST VAL Loss: 0.5896  Val_Acc: 70.330

Epoch 16: Validation loss decreased (0.589564 --> 0.588537).  Saving model ...
	 Train_Loss: 0.5718 Train_Acc: 75.875 Val_Loss: 0.5885  BEST VAL Loss: 0.5885  Val_Acc: 70.245

Epoch 17: Validation loss decreased (0.588537 --> 0.587072).  Saving model ...
	 Train_Loss: 0.5670 Train_Acc: 76.541 Val_Loss: 0.5871  BEST VAL Loss: 0.5871  Val_Acc: 71.555

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.5628 Train_Acc: 76.408 Val_Loss: 0.5892  BEST VAL Loss: 0.5871  Val_Acc: 67.244

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.5586 Train_Acc: 76.673 Val_Loss: 0.5881  BEST VAL Loss: 0.5871  Val_Acc: 70.583

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.5543 Train_Acc: 77.154 Val_Loss: 0.5878  BEST VAL Loss: 0.5871  Val_Acc: 70.668

Epoch 21: Validation loss decreased (0.587072 --> 0.586385).  Saving model ...
	 Train_Loss: 0.5506 Train_Acc: 77.074 Val_Loss: 0.5864  BEST VAL Loss: 0.5864  Val_Acc: 71.302

Epoch 22: Validation loss decreased (0.586385 --> 0.586359).  Saving model ...
	 Train_Loss: 0.5469 Train_Acc: 77.571 Val_Loss: 0.5864  BEST VAL Loss: 0.5864  Val_Acc: 70.372

Epoch 23: Validation loss decreased (0.586359 --> 0.585670).  Saving model ...
	 Train_Loss: 0.5433 Train_Acc: 77.793 Val_Loss: 0.5857  BEST VAL Loss: 0.5857  Val_Acc: 71.936

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.5400 Train_Acc: 78.200 Val_Loss: 0.5862  BEST VAL Loss: 0.5857  Val_Acc: 70.626

Epoch 25: Validation loss decreased (0.585670 --> 0.585526).  Saving model ...
	 Train_Loss: 0.5369 Train_Acc: 78.068 Val_Loss: 0.5855  BEST VAL Loss: 0.5855  Val_Acc: 71.724

Epoch 26: Validation loss decreased (0.585526 --> 0.585299).  Saving model ...
	 Train_Loss: 0.5342 Train_Acc: 77.777 Val_Loss: 0.5853  BEST VAL Loss: 0.5853  Val_Acc: 71.006

Epoch 27: Validation loss decreased (0.585299 --> 0.584651).  Saving model ...
	 Train_Loss: 0.5312 Train_Acc: 78.765 Val_Loss: 0.5847  BEST VAL Loss: 0.5847  Val_Acc: 72.020

Epoch 28: Validation loss decreased (0.584651 --> 0.584332).  Saving model ...
	 Train_Loss: 0.5284 Train_Acc: 78.660 Val_Loss: 0.5843  BEST VAL Loss: 0.5843  Val_Acc: 71.006

Epoch 29: Validation loss decreased (0.584332 --> 0.583680).  Saving model ...
	 Train_Loss: 0.5257 Train_Acc: 79.104 Val_Loss: 0.5837  BEST VAL Loss: 0.5837  Val_Acc: 71.640

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.5231 Train_Acc: 78.924 Val_Loss: 0.5838  BEST VAL Loss: 0.5837  Val_Acc: 71.386

Epoch 31: Validation loss decreased (0.583680 --> 0.583378).  Saving model ...
	 Train_Loss: 0.5207 Train_Acc: 79.014 Val_Loss: 0.5834  BEST VAL Loss: 0.5834  Val_Acc: 72.189

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.5183 Train_Acc: 79.231 Val_Loss: 0.5834  BEST VAL Loss: 0.5834  Val_Acc: 70.710

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.5157 Train_Acc: 79.801 Val_Loss: 0.5835  BEST VAL Loss: 0.5834  Val_Acc: 71.978

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.5135 Train_Acc: 79.162 Val_Loss: 0.5838  BEST VAL Loss: 0.5834  Val_Acc: 71.090

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.5112 Train_Acc: 79.949 Val_Loss: 0.5838  BEST VAL Loss: 0.5834  Val_Acc: 71.893

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.5092 Train_Acc: 79.569 Val_Loss: 0.5842  BEST VAL Loss: 0.5834  Val_Acc: 70.626

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.5073 Train_Acc: 79.241 Val_Loss: 0.5843  BEST VAL Loss: 0.5834  Val_Acc: 70.879

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.5054 Train_Acc: 79.706 Val_Loss: 0.5847  BEST VAL Loss: 0.5834  Val_Acc: 71.133

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.5034 Train_Acc: 80.219 Val_Loss: 0.5847  BEST VAL Loss: 0.5834  Val_Acc: 71.048

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.5016 Train_Acc: 80.203 Val_Loss: 0.5850  BEST VAL Loss: 0.5834  Val_Acc: 71.175

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.4995 Train_Acc: 80.874 Val_Loss: 0.5855  BEST VAL Loss: 0.5834  Val_Acc: 71.936

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.4978 Train_Acc: 80.060 Val_Loss: 0.5861  BEST VAL Loss: 0.5834  Val_Acc: 70.456

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.4963 Train_Acc: 79.754 Val_Loss: 0.5861  BEST VAL Loss: 0.5834  Val_Acc: 71.640

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.4947 Train_Acc: 80.303 Val_Loss: 0.5865  BEST VAL Loss: 0.5834  Val_Acc: 70.541

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.4931 Train_Acc: 80.303 Val_Loss: 0.5866  BEST VAL Loss: 0.5834  Val_Acc: 70.752

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.4914 Train_Acc: 80.716 Val_Loss: 0.5871  BEST VAL Loss: 0.5834  Val_Acc: 71.090

Epoch 47: Validation loss did not decrease
Early stopped at epoch : 47
LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.81      0.86      0.84      9433
           1       0.86      0.80      0.83      9489

    accuracy                           0.83     18922
   macro avg       0.83      0.83      0.83     18922
weighted avg       0.83      0.83      0.83     18922

LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.71      0.74      0.73      1179
           1       0.73      0.70      0.72      1187

    accuracy                           0.72      2366
   macro avg       0.72      0.72      0.72      2366
weighted avg       0.72      0.72      0.72      2366

LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.71      0.74      0.73      1180
           1       0.73      0.70      0.72      1186

    accuracy                           0.72      2366
   macro avg       0.72      0.72      0.72      2366
weighted avg       0.72      0.72      0.72      2366

              precision    recall  f1-score   support

           0       0.71      0.74      0.73      1180
           1       0.73      0.70      0.72      1186

    accuracy                           0.72      2366
   macro avg       0.72      0.72      0.72      2366
weighted avg       0.72      0.72      0.72      2366

LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.010_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.64      0.51      0.57      4017
           1       0.59      0.72      0.65      3997

    accuracy                           0.61      8014
   macro avg       0.62      0.61      0.61      8014
weighted avg       0.62      0.61      0.61      8014

              precision    recall  f1-score   support

           0       0.64      0.51      0.57      4017
           1       0.59      0.72      0.65      3997

    accuracy                           0.61      8014
   macro avg       0.62      0.61      0.61      8014
weighted avg       0.62      0.61      0.61      8014

completed
