[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '5194ccce'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'b767012b'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '90c6c61d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '301a50e2'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (27614, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'M20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'M16' 'M17' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.145589).  Saving model ...
	 Train_Loss: 0.4081 Train_Acc: 82.206 Val_Loss: 0.1456  BEST VAL Loss: 0.1456  Val_Acc: 94.213

Epoch 1: Validation loss did not decrease
	 Train_Loss: 0.3175 Train_Acc: 92.302 Val_Loss: 0.1470  BEST VAL Loss: 0.1456  Val_Acc: 95.733

Epoch 2: Validation loss decreased (0.145589 --> 0.126157).  Saving model ...
	 Train_Loss: 0.2694 Train_Acc: 93.559 Val_Loss: 0.1262  BEST VAL Loss: 0.1262  Val_Acc: 96.273

Epoch 3: Validation loss decreased (0.126157 --> 0.111987).  Saving model ...
	 Train_Loss: 0.2397 Train_Acc: 93.977 Val_Loss: 0.1120  BEST VAL Loss: 0.1120  Val_Acc: 96.812

Epoch 4: Validation loss decreased (0.111987 --> 0.105815).  Saving model ...
	 Train_Loss: 0.2175 Train_Acc: 94.602 Val_Loss: 0.1058  BEST VAL Loss: 0.1058  Val_Acc: 97.303

Epoch 5: Validation loss decreased (0.105815 --> 0.099882).  Saving model ...
	 Train_Loss: 0.2014 Train_Acc: 95.075 Val_Loss: 0.0999  BEST VAL Loss: 0.0999  Val_Acc: 97.499

Epoch 6: Validation loss decreased (0.099882 --> 0.096925).  Saving model ...
	 Train_Loss: 0.1892 Train_Acc: 95.081 Val_Loss: 0.0969  BEST VAL Loss: 0.0969  Val_Acc: 96.714

Epoch 7: Validation loss decreased (0.096925 --> 0.092750).  Saving model ...
	 Train_Loss: 0.1795 Train_Acc: 95.032 Val_Loss: 0.0928  BEST VAL Loss: 0.0928  Val_Acc: 97.008

Epoch 8: Validation loss decreased (0.092750 --> 0.089473).  Saving model ...
	 Train_Loss: 0.1713 Train_Acc: 95.332 Val_Loss: 0.0895  BEST VAL Loss: 0.0895  Val_Acc: 97.303

Epoch 9: Validation loss did not decrease
	 Train_Loss: 0.1650 Train_Acc: 95.130 Val_Loss: 0.0960  BEST VAL Loss: 0.0895  Val_Acc: 97.646

Epoch 10: Validation loss did not decrease
	 Train_Loss: 0.1597 Train_Acc: 95.418 Val_Loss: 0.0950  BEST VAL Loss: 0.0895  Val_Acc: 97.744

Epoch 11: Validation loss did not decrease
	 Train_Loss: 0.1548 Train_Acc: 95.670 Val_Loss: 0.0936  BEST VAL Loss: 0.0895  Val_Acc: 97.891

Epoch 12: Validation loss did not decrease
	 Train_Loss: 0.1506 Train_Acc: 95.657 Val_Loss: 0.0901  BEST VAL Loss: 0.0895  Val_Acc: 97.793

Epoch 13: Validation loss decreased (0.089473 --> 0.087312).  Saving model ...
	 Train_Loss: 0.1466 Train_Acc: 95.835 Val_Loss: 0.0873  BEST VAL Loss: 0.0873  Val_Acc: 97.744

Epoch 14: Validation loss decreased (0.087312 --> 0.084812).  Saving model ...
	 Train_Loss: 0.1427 Train_Acc: 96.105 Val_Loss: 0.0848  BEST VAL Loss: 0.0848  Val_Acc: 97.695

Epoch 15: Validation loss decreased (0.084812 --> 0.083308).  Saving model ...
	 Train_Loss: 0.1393 Train_Acc: 95.964 Val_Loss: 0.0833  BEST VAL Loss: 0.0833  Val_Acc: 97.940

Epoch 16: Validation loss decreased (0.083308 --> 0.081602).  Saving model ...
	 Train_Loss: 0.1366 Train_Acc: 95.841 Val_Loss: 0.0816  BEST VAL Loss: 0.0816  Val_Acc: 98.038

Epoch 17: Validation loss decreased (0.081602 --> 0.081221).  Saving model ...
	 Train_Loss: 0.1341 Train_Acc: 95.982 Val_Loss: 0.0812  BEST VAL Loss: 0.0812  Val_Acc: 97.695

Epoch 18: Validation loss decreased (0.081221 --> 0.080651).  Saving model ...
	 Train_Loss: 0.1318 Train_Acc: 95.896 Val_Loss: 0.0807  BEST VAL Loss: 0.0807  Val_Acc: 97.597

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1296 Train_Acc: 95.547 Val_Loss: 0.0840  BEST VAL Loss: 0.0807  Val_Acc: 97.793

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1273 Train_Acc: 96.295 Val_Loss: 0.0828  BEST VAL Loss: 0.0807  Val_Acc: 97.793

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1253 Train_Acc: 96.099 Val_Loss: 0.0822  BEST VAL Loss: 0.0807  Val_Acc: 98.333

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1234 Train_Acc: 96.344 Val_Loss: 0.0813  BEST VAL Loss: 0.0807  Val_Acc: 97.793

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1217 Train_Acc: 96.123 Val_Loss: 0.0822  BEST VAL Loss: 0.0807  Val_Acc: 97.793

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1201 Train_Acc: 95.964 Val_Loss: 0.0812  BEST VAL Loss: 0.0807  Val_Acc: 98.038

Epoch 25: Validation loss decreased (0.080651 --> 0.080477).  Saving model ...
	 Train_Loss: 0.1187 Train_Acc: 95.890 Val_Loss: 0.0805  BEST VAL Loss: 0.0805  Val_Acc: 97.989

Epoch 26: Validation loss decreased (0.080477 --> 0.079942).  Saving model ...
	 Train_Loss: 0.1173 Train_Acc: 96.136 Val_Loss: 0.0799  BEST VAL Loss: 0.0799  Val_Acc: 98.087

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1160 Train_Acc: 96.516 Val_Loss: 0.0802  BEST VAL Loss: 0.0799  Val_Acc: 97.989

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1146 Train_Acc: 96.344 Val_Loss: 0.0810  BEST VAL Loss: 0.0799  Val_Acc: 97.891

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1134 Train_Acc: 96.277 Val_Loss: 0.0809  BEST VAL Loss: 0.0799  Val_Acc: 97.695

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1121 Train_Acc: 96.406 Val_Loss: 0.0856  BEST VAL Loss: 0.0799  Val_Acc: 98.087

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1109 Train_Acc: 96.430 Val_Loss: 0.0845  BEST VAL Loss: 0.0799  Val_Acc: 98.136

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1100 Train_Acc: 95.847 Val_Loss: 0.0848  BEST VAL Loss: 0.0799  Val_Acc: 97.940

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1091 Train_Acc: 95.976 Val_Loss: 0.0844  BEST VAL Loss: 0.0799  Val_Acc: 97.940

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1083 Train_Acc: 95.872 Val_Loss: 0.0839  BEST VAL Loss: 0.0799  Val_Acc: 97.989

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1074 Train_Acc: 96.289 Val_Loss: 0.0846  BEST VAL Loss: 0.0799  Val_Acc: 97.695

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1065 Train_Acc: 96.271 Val_Loss: 0.0870  BEST VAL Loss: 0.0799  Val_Acc: 97.940

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1057 Train_Acc: 96.320 Val_Loss: 0.0865  BEST VAL Loss: 0.0799  Val_Acc: 98.087

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1050 Train_Acc: 96.418 Val_Loss: 0.0863  BEST VAL Loss: 0.0799  Val_Acc: 97.695

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1043 Train_Acc: 96.246 Val_Loss: 0.0860  BEST VAL Loss: 0.0799  Val_Acc: 97.940

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1035 Train_Acc: 96.485 Val_Loss: 0.0858  BEST VAL Loss: 0.0799  Val_Acc: 97.842

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1028 Train_Acc: 96.185 Val_Loss: 0.0856  BEST VAL Loss: 0.0799  Val_Acc: 97.989

Epoch 42: Validation loss did not decrease
Early stopped at epoch : 42
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.51      0.52      8453
           1       0.49      0.50      0.49      7850

    accuracy                           0.51     16303
   macro avg       0.51      0.51      0.51     16303
weighted avg       0.51      0.51      0.51     16303

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.52      0.52      1057
           1       0.49      0.49      0.49       982

    accuracy                           0.51      2039
   macro avg       0.51      0.51      0.51      2039
weighted avg       0.51      0.51      0.51      2039

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.51      0.51      1057
           1       0.48      0.49      0.49       982

    accuracy                           0.50      2039
   macro avg       0.50      0.50      0.50      2039
weighted avg       0.50      0.50      0.50      2039

              precision    recall  f1-score   support

           0       0.52      0.51      0.51      1057
           1       0.48      0.49      0.49       982

    accuracy                           0.50      2039
   macro avg       0.50      0.50      0.50      2039
weighted avg       0.50      0.50      0.50      2039

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.52      0.52      3835
           1       0.46      0.47      0.47      3398

    accuracy                           0.49      7233
   macro avg       0.49      0.49      0.49      7233
weighted avg       0.49      0.49      0.49      7233

              precision    recall  f1-score   support

           0       0.52      0.52      0.52      3835
           1       0.46      0.47      0.47      3398

    accuracy                           0.49      7233
   macro avg       0.49      0.49      0.49      7233
weighted avg       0.49      0.49      0.49      7233

completed
