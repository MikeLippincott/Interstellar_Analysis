[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'ca65a1c0'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'ebcba743'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'f1d75d64'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'a1e3b354'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (29984, 1276)
Number of total missing values across all columns: 27532
Data Subset Is Off
Wells held out for testing: ['D14' 'K20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'K16' 'K17' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.190910).  Saving model ...
	 Train_Loss: 0.3164 Train_Acc: 86.218 Val_Loss: 0.1909  BEST VAL Loss: 0.1909  Val_Acc: 92.331

Epoch 1: Validation loss decreased (0.190910 --> 0.169879).  Saving model ...
	 Train_Loss: 0.2482 Train_Acc: 92.908 Val_Loss: 0.1699  BEST VAL Loss: 0.1699  Val_Acc: 94.315

Epoch 2: Validation loss decreased (0.169879 --> 0.152280).  Saving model ...
	 Train_Loss: 0.2110 Train_Acc: 94.578 Val_Loss: 0.1523  BEST VAL Loss: 0.1523  Val_Acc: 95.328

Epoch 3: Validation loss decreased (0.152280 --> 0.141735).  Saving model ...
	 Train_Loss: 0.1857 Train_Acc: 95.652 Val_Loss: 0.1417  BEST VAL Loss: 0.1417  Val_Acc: 95.416

Epoch 4: Validation loss decreased (0.141735 --> 0.135178).  Saving model ...
	 Train_Loss: 0.1675 Train_Acc: 96.159 Val_Loss: 0.1352  BEST VAL Loss: 0.1352  Val_Acc: 95.901

Epoch 5: Validation loss decreased (0.135178 --> 0.127039).  Saving model ...
	 Train_Loss: 0.1542 Train_Acc: 96.628 Val_Loss: 0.1270  BEST VAL Loss: 0.1270  Val_Acc: 96.739

Epoch 6: Validation loss decreased (0.127039 --> 0.123095).  Saving model ...
	 Train_Loss: 0.1433 Train_Acc: 97.024 Val_Loss: 0.1231  BEST VAL Loss: 0.1231  Val_Acc: 96.606

Epoch 7: Validation loss decreased (0.123095 --> 0.118193).  Saving model ...
	 Train_Loss: 0.1348 Train_Acc: 97.090 Val_Loss: 0.1182  BEST VAL Loss: 0.1182  Val_Acc: 96.695

Epoch 8: Validation loss decreased (0.118193 --> 0.115467).  Saving model ...
	 Train_Loss: 0.1281 Train_Acc: 97.217 Val_Loss: 0.1155  BEST VAL Loss: 0.1155  Val_Acc: 96.783

Epoch 9: Validation loss decreased (0.115467 --> 0.113414).  Saving model ...
	 Train_Loss: 0.1231 Train_Acc: 96.859 Val_Loss: 0.1134  BEST VAL Loss: 0.1134  Val_Acc: 96.298

Epoch 10: Validation loss decreased (0.113414 --> 0.112390).  Saving model ...
	 Train_Loss: 0.1175 Train_Acc: 97.421 Val_Loss: 0.1124  BEST VAL Loss: 0.1124  Val_Acc: 96.518

Epoch 11: Validation loss decreased (0.112390 --> 0.109702).  Saving model ...
	 Train_Loss: 0.1125 Train_Acc: 97.840 Val_Loss: 0.1097  BEST VAL Loss: 0.1097  Val_Acc: 96.915

Epoch 12: Validation loss decreased (0.109702 --> 0.108851).  Saving model ...
	 Train_Loss: 0.1084 Train_Acc: 97.669 Val_Loss: 0.1089  BEST VAL Loss: 0.1089  Val_Acc: 96.651

Epoch 13: Validation loss decreased (0.108851 --> 0.108274).  Saving model ...
	 Train_Loss: 0.1051 Train_Acc: 97.564 Val_Loss: 0.1083  BEST VAL Loss: 0.1083  Val_Acc: 96.827

Epoch 14: Validation loss decreased (0.108274 --> 0.105919).  Saving model ...
	 Train_Loss: 0.1015 Train_Acc: 97.906 Val_Loss: 0.1059  BEST VAL Loss: 0.1059  Val_Acc: 97.003

Epoch 15: Validation loss decreased (0.105919 --> 0.104772).  Saving model ...
	 Train_Loss: 0.0982 Train_Acc: 98.011 Val_Loss: 0.1048  BEST VAL Loss: 0.1048  Val_Acc: 96.915

Epoch 16: Validation loss decreased (0.104772 --> 0.103635).  Saving model ...
	 Train_Loss: 0.0952 Train_Acc: 98.038 Val_Loss: 0.1036  BEST VAL Loss: 0.1036  Val_Acc: 97.003

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.0930 Train_Acc: 97.790 Val_Loss: 0.1039  BEST VAL Loss: 0.1036  Val_Acc: 96.783

Epoch 18: Validation loss decreased (0.103635 --> 0.103295).  Saving model ...
	 Train_Loss: 0.0907 Train_Acc: 98.132 Val_Loss: 0.1033  BEST VAL Loss: 0.1033  Val_Acc: 96.739

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.0883 Train_Acc: 98.198 Val_Loss: 0.1038  BEST VAL Loss: 0.1033  Val_Acc: 96.606

Epoch 20: Validation loss decreased (0.103295 --> 0.102761).  Saving model ...
	 Train_Loss: 0.0865 Train_Acc: 98.055 Val_Loss: 0.1028  BEST VAL Loss: 0.1028  Val_Acc: 97.047

Epoch 21: Validation loss decreased (0.102761 --> 0.102087).  Saving model ...
	 Train_Loss: 0.0846 Train_Acc: 98.121 Val_Loss: 0.1021  BEST VAL Loss: 0.1021  Val_Acc: 96.915

Epoch 22: Validation loss decreased (0.102087 --> 0.101696).  Saving model ...
	 Train_Loss: 0.0829 Train_Acc: 98.115 Val_Loss: 0.1017  BEST VAL Loss: 0.1017  Val_Acc: 97.135

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0810 Train_Acc: 98.479 Val_Loss: 0.1020  BEST VAL Loss: 0.1017  Val_Acc: 97.223

Epoch 24: Validation loss decreased (0.101696 --> 0.101493).  Saving model ...
	 Train_Loss: 0.0793 Train_Acc: 98.496 Val_Loss: 0.1015  BEST VAL Loss: 0.1015  Val_Acc: 97.268

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.0778 Train_Acc: 98.396 Val_Loss: 0.1019  BEST VAL Loss: 0.1015  Val_Acc: 97.312

Epoch 26: Validation loss decreased (0.101493 --> 0.101297).  Saving model ...
	 Train_Loss: 0.0765 Train_Acc: 98.275 Val_Loss: 0.1013  BEST VAL Loss: 0.1013  Val_Acc: 97.400

Epoch 27: Validation loss decreased (0.101297 --> 0.100715).  Saving model ...
	 Train_Loss: 0.0752 Train_Acc: 98.336 Val_Loss: 0.1007  BEST VAL Loss: 0.1007  Val_Acc: 97.268

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.0739 Train_Acc: 98.518 Val_Loss: 0.1012  BEST VAL Loss: 0.1007  Val_Acc: 97.091

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.0725 Train_Acc: 98.854 Val_Loss: 0.1018  BEST VAL Loss: 0.1007  Val_Acc: 97.312

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0713 Train_Acc: 98.523 Val_Loss: 0.1022  BEST VAL Loss: 0.1007  Val_Acc: 97.135

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.0701 Train_Acc: 98.655 Val_Loss: 0.1026  BEST VAL Loss: 0.1007  Val_Acc: 97.488

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0690 Train_Acc: 98.650 Val_Loss: 0.1039  BEST VAL Loss: 0.1007  Val_Acc: 97.312

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0680 Train_Acc: 98.694 Val_Loss: 0.1046  BEST VAL Loss: 0.1007  Val_Acc: 97.135

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0670 Train_Acc: 98.716 Val_Loss: 0.1053  BEST VAL Loss: 0.1007  Val_Acc: 97.135

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.0660 Train_Acc: 98.755 Val_Loss: 0.1058  BEST VAL Loss: 0.1007  Val_Acc: 97.179

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.0650 Train_Acc: 98.986 Val_Loss: 0.1062  BEST VAL Loss: 0.1007  Val_Acc: 96.827

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.0641 Train_Acc: 98.909 Val_Loss: 0.1070  BEST VAL Loss: 0.1007  Val_Acc: 97.179

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0633 Train_Acc: 98.628 Val_Loss: 0.1081  BEST VAL Loss: 0.1007  Val_Acc: 96.871

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0630 Train_Acc: 97.934 Val_Loss: 0.1083  BEST VAL Loss: 0.1007  Val_Acc: 97.047

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0624 Train_Acc: 98.352 Val_Loss: 0.1095  BEST VAL Loss: 0.1007  Val_Acc: 96.474

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0620 Train_Acc: 98.308 Val_Loss: 0.1101  BEST VAL Loss: 0.1007  Val_Acc: 96.606

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0618 Train_Acc: 98.038 Val_Loss: 0.1108  BEST VAL Loss: 0.1007  Val_Acc: 97.135

Epoch 43: Validation loss did not decrease
Early stopped at epoch : 43
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.53      0.53      9777
           1       0.45      0.46      0.45      8370

    accuracy                           0.50     18147
   macro avg       0.49      0.49      0.49     18147
weighted avg       0.50      0.50      0.50     18147

Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.53      0.53      1222
           1       0.45      0.46      0.46      1047

    accuracy                           0.50      2269
   macro avg       0.49      0.49      0.49      2269
weighted avg       0.50      0.50      0.50      2269

Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.54      0.54      1223
           1       0.46      0.46      0.46      1046

    accuracy                           0.50      2269
   macro avg       0.50      0.50      0.50      2269
weighted avg       0.50      0.50      0.50      2269

              precision    recall  f1-score   support

           0       0.54      0.54      0.54      1223
           1       0.46      0.46      0.46      1046

    accuracy                           0.50      2269
   macro avg       0.50      0.50      0.50      2269
weighted avg       0.50      0.50      0.50      2269

Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.54      0.55      3996
           1       0.45      0.46      0.46      3303

    accuracy                           0.51      7299
   macro avg       0.50      0.50      0.50      7299
weighted avg       0.51      0.51      0.51      7299

              precision    recall  f1-score   support

           0       0.55      0.54      0.55      3996
           1       0.45      0.46      0.46      3303

    accuracy                           0.51      7299
   macro avg       0.50      0.50      0.50      7299
weighted avg       0.51      0.51      0.51      7299

completed
