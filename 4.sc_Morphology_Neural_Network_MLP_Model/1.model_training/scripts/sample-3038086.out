[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'b6339a97'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '5e2aa9d2'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '087519c9'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '4671cf75'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: H2O2_100.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: H2O2_100.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['H2O2_100.000_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (40552, 1276)
Number of total missing values across all columns: 53800
Data Subset Is Off
Wells held out for testing: ['H22' 'L16']
Wells to use for training, validation, and testing ['H18' 'H19' 'H23' 'L17' 'I18' 'I19' 'L20' 'L21' 'I22' 'I23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.206292).  Saving model ...
	 Train_Loss: 0.3832 Train_Acc: 81.212 Val_Loss: 0.2063  BEST VAL Loss: 0.2063  Val_Acc: 92.649

Epoch 1: Validation loss decreased (0.206292 --> 0.176370).  Saving model ...
	 Train_Loss: 0.3043 Train_Acc: 89.951 Val_Loss: 0.1764  BEST VAL Loss: 0.1764  Val_Acc: 94.334

Epoch 2: Validation loss decreased (0.176370 --> 0.160441).  Saving model ...
	 Train_Loss: 0.2630 Train_Acc: 92.023 Val_Loss: 0.1604  BEST VAL Loss: 0.1604  Val_Acc: 94.885

Epoch 3: Validation loss decreased (0.160441 --> 0.149548).  Saving model ...
	 Train_Loss: 0.2366 Train_Acc: 92.804 Val_Loss: 0.1495  BEST VAL Loss: 0.1495  Val_Acc: 95.835

Epoch 4: Validation loss decreased (0.149548 --> 0.143915).  Saving model ...
	 Train_Loss: 0.2174 Train_Acc: 93.279 Val_Loss: 0.1439  BEST VAL Loss: 0.1439  Val_Acc: 95.743

Epoch 5: Validation loss decreased (0.143915 --> 0.137072).  Saving model ...
	 Train_Loss: 0.2026 Train_Acc: 93.746 Val_Loss: 0.1371  BEST VAL Loss: 0.1371  Val_Acc: 96.018

Epoch 6: Validation loss decreased (0.137072 --> 0.131460).  Saving model ...
	 Train_Loss: 0.1900 Train_Acc: 94.248 Val_Loss: 0.1315  BEST VAL Loss: 0.1315  Val_Acc: 96.325

Epoch 7: Validation loss decreased (0.131460 --> 0.126688).  Saving model ...
	 Train_Loss: 0.1798 Train_Acc: 94.504 Val_Loss: 0.1267  BEST VAL Loss: 0.1267  Val_Acc: 96.570

Epoch 8: Validation loss decreased (0.126688 --> 0.122482).  Saving model ...
	 Train_Loss: 0.1718 Train_Acc: 94.696 Val_Loss: 0.1225  BEST VAL Loss: 0.1225  Val_Acc: 96.723

Epoch 9: Validation loss decreased (0.122482 --> 0.119108).  Saving model ...
	 Train_Loss: 0.1648 Train_Acc: 94.803 Val_Loss: 0.1191  BEST VAL Loss: 0.1191  Val_Acc: 97.060

Epoch 10: Validation loss decreased (0.119108 --> 0.116096).  Saving model ...
	 Train_Loss: 0.1583 Train_Acc: 95.071 Val_Loss: 0.1161  BEST VAL Loss: 0.1161  Val_Acc: 97.060

Epoch 11: Validation loss decreased (0.116096 --> 0.112890).  Saving model ...
	 Train_Loss: 0.1526 Train_Acc: 95.178 Val_Loss: 0.1129  BEST VAL Loss: 0.1129  Val_Acc: 96.937

Epoch 12: Validation loss decreased (0.112890 --> 0.110738).  Saving model ...
	 Train_Loss: 0.1475 Train_Acc: 95.393 Val_Loss: 0.1107  BEST VAL Loss: 0.1107  Val_Acc: 96.968

Epoch 13: Validation loss decreased (0.110738 --> 0.107847).  Saving model ...
	 Train_Loss: 0.1432 Train_Acc: 95.155 Val_Loss: 0.1078  BEST VAL Loss: 0.1078  Val_Acc: 96.998

Epoch 14: Validation loss decreased (0.107847 --> 0.106593).  Saving model ...
	 Train_Loss: 0.1389 Train_Acc: 95.623 Val_Loss: 0.1066  BEST VAL Loss: 0.1066  Val_Acc: 97.182

Epoch 15: Validation loss decreased (0.106593 --> 0.104940).  Saving model ...
	 Train_Loss: 0.1356 Train_Acc: 95.489 Val_Loss: 0.1049  BEST VAL Loss: 0.1049  Val_Acc: 96.998

Epoch 16: Validation loss decreased (0.104940 --> 0.103507).  Saving model ...
	 Train_Loss: 0.1323 Train_Acc: 95.481 Val_Loss: 0.1035  BEST VAL Loss: 0.1035  Val_Acc: 97.152

Epoch 17: Validation loss decreased (0.103507 --> 0.102285).  Saving model ...
	 Train_Loss: 0.1293 Train_Acc: 95.466 Val_Loss: 0.1023  BEST VAL Loss: 0.1023  Val_Acc: 97.152

Epoch 18: Validation loss decreased (0.102285 --> 0.101013).  Saving model ...
	 Train_Loss: 0.1266 Train_Acc: 95.680 Val_Loss: 0.1010  BEST VAL Loss: 0.1010  Val_Acc: 96.937

Epoch 19: Validation loss decreased (0.101013 --> 0.100141).  Saving model ...
	 Train_Loss: 0.1240 Train_Acc: 95.573 Val_Loss: 0.1001  BEST VAL Loss: 0.1001  Val_Acc: 96.968

Epoch 20: Validation loss decreased (0.100141 --> 0.098914).  Saving model ...
	 Train_Loss: 0.1216 Train_Acc: 95.730 Val_Loss: 0.0989  BEST VAL Loss: 0.0989  Val_Acc: 97.274

Epoch 21: Validation loss decreased (0.098914 --> 0.097959).  Saving model ...
	 Train_Loss: 0.1192 Train_Acc: 96.167 Val_Loss: 0.0980  BEST VAL Loss: 0.0980  Val_Acc: 96.845

Epoch 22: Validation loss decreased (0.097959 --> 0.096650).  Saving model ...
	 Train_Loss: 0.1169 Train_Acc: 96.955 Val_Loss: 0.0967  BEST VAL Loss: 0.0967  Val_Acc: 97.335

Epoch 23: Validation loss decreased (0.096650 --> 0.095504).  Saving model ...
	 Train_Loss: 0.1145 Train_Acc: 97.239 Val_Loss: 0.0955  BEST VAL Loss: 0.0955  Val_Acc: 97.243

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1127 Train_Acc: 96.821 Val_Loss: 0.0958  BEST VAL Loss: 0.0955  Val_Acc: 97.519

Epoch 25: Validation loss decreased (0.095504 --> 0.095466).  Saving model ...
	 Train_Loss: 0.1109 Train_Acc: 97.063 Val_Loss: 0.0955  BEST VAL Loss: 0.0955  Val_Acc: 97.121

Epoch 26: Validation loss decreased (0.095466 --> 0.095268).  Saving model ...
	 Train_Loss: 0.1093 Train_Acc: 96.902 Val_Loss: 0.0953  BEST VAL Loss: 0.0953  Val_Acc: 97.060

Epoch 27: Validation loss decreased (0.095268 --> 0.094665).  Saving model ...
	 Train_Loss: 0.1079 Train_Acc: 96.515 Val_Loss: 0.0947  BEST VAL Loss: 0.0947  Val_Acc: 97.182

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1062 Train_Acc: 97.040 Val_Loss: 0.0947  BEST VAL Loss: 0.0947  Val_Acc: 97.152

Epoch 29: Validation loss decreased (0.094665 --> 0.094618).  Saving model ...
	 Train_Loss: 0.1047 Train_Acc: 97.089 Val_Loss: 0.0946  BEST VAL Loss: 0.0946  Val_Acc: 97.152

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1035 Train_Acc: 97.078 Val_Loss: 0.0947  BEST VAL Loss: 0.0946  Val_Acc: 97.060

Epoch 31: Validation loss decreased (0.094618 --> 0.094083).  Saving model ...
	 Train_Loss: 0.1022 Train_Acc: 96.906 Val_Loss: 0.0941  BEST VAL Loss: 0.0941  Val_Acc: 96.968

Epoch 32: Validation loss decreased (0.094083 --> 0.093947).  Saving model ...
	 Train_Loss: 0.1010 Train_Acc: 96.913 Val_Loss: 0.0939  BEST VAL Loss: 0.0939  Val_Acc: 96.876

Epoch 33: Validation loss decreased (0.093947 --> 0.093758).  Saving model ...
	 Train_Loss: 0.0997 Train_Acc: 97.300 Val_Loss: 0.0938  BEST VAL Loss: 0.0938  Val_Acc: 97.182

Epoch 34: Validation loss decreased (0.093758 --> 0.093525).  Saving model ...
	 Train_Loss: 0.0984 Train_Acc: 97.338 Val_Loss: 0.0935  BEST VAL Loss: 0.0935  Val_Acc: 97.243

Epoch 35: Validation loss decreased (0.093525 --> 0.092912).  Saving model ...
	 Train_Loss: 0.0973 Train_Acc: 97.201 Val_Loss: 0.0929  BEST VAL Loss: 0.0929  Val_Acc: 97.397

Epoch 36: Validation loss decreased (0.092912 --> 0.092688).  Saving model ...
	 Train_Loss: 0.0962 Train_Acc: 97.354 Val_Loss: 0.0927  BEST VAL Loss: 0.0927  Val_Acc: 97.305

Epoch 37: Validation loss decreased (0.092688 --> 0.092305).  Saving model ...
	 Train_Loss: 0.0950 Train_Acc: 97.530 Val_Loss: 0.0923  BEST VAL Loss: 0.0923  Val_Acc: 97.550

Epoch 38: Validation loss decreased (0.092305 --> 0.092200).  Saving model ...
	 Train_Loss: 0.0939 Train_Acc: 97.606 Val_Loss: 0.0922  BEST VAL Loss: 0.0922  Val_Acc: 97.366

Epoch 39: Validation loss decreased (0.092200 --> 0.092029).  Saving model ...
	 Train_Loss: 0.0929 Train_Acc: 97.503 Val_Loss: 0.0920  BEST VAL Loss: 0.0920  Val_Acc: 97.672

Epoch 40: Validation loss decreased (0.092029 --> 0.091764).  Saving model ...
	 Train_Loss: 0.0918 Train_Acc: 97.664 Val_Loss: 0.0918  BEST VAL Loss: 0.0918  Val_Acc: 97.458

Epoch 41: Validation loss decreased (0.091764 --> 0.091551).  Saving model ...
	 Train_Loss: 0.0910 Train_Acc: 97.580 Val_Loss: 0.0916  BEST VAL Loss: 0.0916  Val_Acc: 97.458

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0901 Train_Acc: 97.419 Val_Loss: 0.0921  BEST VAL Loss: 0.0916  Val_Acc: 97.213

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0893 Train_Acc: 97.580 Val_Loss: 0.0925  BEST VAL Loss: 0.0916  Val_Acc: 97.090

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.0886 Train_Acc: 97.285 Val_Loss: 0.0928  BEST VAL Loss: 0.0916  Val_Acc: 97.458

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.0878 Train_Acc: 97.580 Val_Loss: 0.0929  BEST VAL Loss: 0.0916  Val_Acc: 97.243

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.0871 Train_Acc: 97.649 Val_Loss: 0.0932  BEST VAL Loss: 0.0916  Val_Acc: 97.182

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.0865 Train_Acc: 97.618 Val_Loss: 0.0934  BEST VAL Loss: 0.0916  Val_Acc: 97.305

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.0858 Train_Acc: 97.679 Val_Loss: 0.0941  BEST VAL Loss: 0.0916  Val_Acc: 97.121

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.0851 Train_Acc: 97.867 Val_Loss: 0.0941  BEST VAL Loss: 0.0916  Val_Acc: 97.335

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.0843 Train_Acc: 97.970 Val_Loss: 0.0943  BEST VAL Loss: 0.0916  Val_Acc: 97.152

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.0838 Train_Acc: 97.476 Val_Loss: 0.0943  BEST VAL Loss: 0.0916  Val_Acc: 96.998

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.0833 Train_Acc: 97.323 Val_Loss: 0.0948  BEST VAL Loss: 0.0916  Val_Acc: 97.182

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.0828 Train_Acc: 97.434 Val_Loss: 0.0949  BEST VAL Loss: 0.0916  Val_Acc: 97.550

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.0822 Train_Acc: 97.848 Val_Loss: 0.0953  BEST VAL Loss: 0.0916  Val_Acc: 97.305

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.0816 Train_Acc: 97.649 Val_Loss: 0.0955  BEST VAL Loss: 0.0916  Val_Acc: 97.366

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.0810 Train_Acc: 97.783 Val_Loss: 0.0958  BEST VAL Loss: 0.0916  Val_Acc: 97.427

Epoch 57: Validation loss did not decrease
Early stopped at epoch : 57
H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.70      0.69      0.69     18174
           1       0.30      0.31      0.30      7938

    accuracy                           0.58     26112
   macro avg       0.50      0.50      0.50     26112
weighted avg       0.58      0.58      0.58     26112

H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.71      0.72      0.71      2272
           1       0.33      0.32      0.33       993

    accuracy                           0.60      3265
   macro avg       0.52      0.52      0.52      3265
weighted avg       0.59      0.60      0.59      3265

H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.69      0.69      0.69      2272
           1       0.30      0.30      0.30       992

    accuracy                           0.57      3264
   macro avg       0.50      0.50      0.50      3264
weighted avg       0.57      0.57      0.57      3264

              precision    recall  f1-score   support

           0       0.69      0.69      0.69      2272
           1       0.30      0.30      0.30       992

    accuracy                           0.57      3264
   macro avg       0.50      0.50      0.50      3264
weighted avg       0.57      0.57      0.57      3264

H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
H2O2_100.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.57      0.55      4182
           1       0.47      0.43      0.45      3729

    accuracy                           0.50      7911
   macro avg       0.50      0.50      0.50      7911
weighted avg       0.50      0.50      0.50      7911

              precision    recall  f1-score   support

           0       0.53      0.57      0.55      4182
           1       0.47      0.43      0.45      3729

    accuracy                           0.50      7911
   macro avg       0.50      0.50      0.50      7911
weighted avg       0.50      0.50      0.50      7911

completed
