[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'c2b3d9b5'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '61871097'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '89bcfe99'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '3ed6cc6b'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (332997, 1270)
Number of total missing values across all columns: 333968
Data Subset Is Off
Wells held out for testing: ['K09' 'L06']
Wells to use for training, validation, and testing ['E06' 'E07' 'K02' 'K03' 'K08' 'L07']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.161074).  Saving model ...
	 Train_Loss: 0.3083 Train_Acc: 87.495 Val_Loss: 0.1611  BEST VAL Loss: 0.1611  Val_Acc: 94.070

Epoch 1: Validation loss decreased (0.161074 --> 0.149347).  Saving model ...
	 Train_Loss: 0.2486 Train_Acc: 92.286 Val_Loss: 0.1493  BEST VAL Loss: 0.1493  Val_Acc: 94.964

Epoch 2: Validation loss decreased (0.149347 --> 0.140206).  Saving model ...
	 Train_Loss: 0.2236 Train_Acc: 93.002 Val_Loss: 0.1402  BEST VAL Loss: 0.1402  Val_Acc: 95.560

Epoch 3: Validation loss decreased (0.140206 --> 0.136772).  Saving model ...
	 Train_Loss: 0.2082 Train_Acc: 93.470 Val_Loss: 0.1368  BEST VAL Loss: 0.1368  Val_Acc: 95.492

Epoch 4: Validation loss decreased (0.136772 --> 0.131949).  Saving model ...
	 Train_Loss: 0.1981 Train_Acc: 93.712 Val_Loss: 0.1319  BEST VAL Loss: 0.1319  Val_Acc: 95.979

Epoch 5: Validation loss decreased (0.131949 --> 0.128038).  Saving model ...
	 Train_Loss: 0.1903 Train_Acc: 93.927 Val_Loss: 0.1280  BEST VAL Loss: 0.1280  Val_Acc: 96.092

Epoch 6: Validation loss decreased (0.128038 --> 0.126427).  Saving model ...
	 Train_Loss: 0.1842 Train_Acc: 94.061 Val_Loss: 0.1264  BEST VAL Loss: 0.1264  Val_Acc: 95.713

Epoch 7: Validation loss decreased (0.126427 --> 0.123870).  Saving model ...
	 Train_Loss: 0.1795 Train_Acc: 94.140 Val_Loss: 0.1239  BEST VAL Loss: 0.1239  Val_Acc: 96.084

Epoch 8: Validation loss decreased (0.123870 --> 0.122219).  Saving model ...
	 Train_Loss: 0.1758 Train_Acc: 94.080 Val_Loss: 0.1222  BEST VAL Loss: 0.1222  Val_Acc: 96.185

Epoch 9: Validation loss decreased (0.122219 --> 0.119956).  Saving model ...
	 Train_Loss: 0.1722 Train_Acc: 94.279 Val_Loss: 0.1200  BEST VAL Loss: 0.1200  Val_Acc: 96.382

Epoch 10: Validation loss decreased (0.119956 --> 0.119306).  Saving model ...
	 Train_Loss: 0.1692 Train_Acc: 94.343 Val_Loss: 0.1193  BEST VAL Loss: 0.1193  Val_Acc: 95.661

Epoch 11: Validation loss decreased (0.119306 --> 0.117938).  Saving model ...
	 Train_Loss: 0.1666 Train_Acc: 94.447 Val_Loss: 0.1179  BEST VAL Loss: 0.1179  Val_Acc: 96.378

Epoch 12: Validation loss decreased (0.117938 --> 0.117098).  Saving model ...
	 Train_Loss: 0.1643 Train_Acc: 94.452 Val_Loss: 0.1171  BEST VAL Loss: 0.1171  Val_Acc: 96.165

Epoch 13: Validation loss decreased (0.117098 --> 0.116004).  Saving model ...
	 Train_Loss: 0.1624 Train_Acc: 94.393 Val_Loss: 0.1160  BEST VAL Loss: 0.1160  Val_Acc: 96.386

Epoch 14: Validation loss decreased (0.116004 --> 0.114509).  Saving model ...
	 Train_Loss: 0.1605 Train_Acc: 94.507 Val_Loss: 0.1145  BEST VAL Loss: 0.1145  Val_Acc: 96.741

Epoch 15: Validation loss decreased (0.114509 --> 0.113438).  Saving model ...
	 Train_Loss: 0.1589 Train_Acc: 94.526 Val_Loss: 0.1134  BEST VAL Loss: 0.1134  Val_Acc: 96.511

Epoch 16: Validation loss decreased (0.113438 --> 0.112408).  Saving model ...
	 Train_Loss: 0.1573 Train_Acc: 94.558 Val_Loss: 0.1124  BEST VAL Loss: 0.1124  Val_Acc: 96.624

Epoch 17: Validation loss decreased (0.112408 --> 0.111524).  Saving model ...
	 Train_Loss: 0.1558 Train_Acc: 94.664 Val_Loss: 0.1115  BEST VAL Loss: 0.1115  Val_Acc: 96.523

Epoch 18: Validation loss decreased (0.111524 --> 0.110936).  Saving model ...
	 Train_Loss: 0.1546 Train_Acc: 94.508 Val_Loss: 0.1109  BEST VAL Loss: 0.1109  Val_Acc: 96.539

Epoch 19: Validation loss decreased (0.110936 --> 0.109945).  Saving model ...
	 Train_Loss: 0.1535 Train_Acc: 94.549 Val_Loss: 0.1099  BEST VAL Loss: 0.1099  Val_Acc: 96.632

Epoch 20: Validation loss decreased (0.109945 --> 0.109526).  Saving model ...
	 Train_Loss: 0.1524 Train_Acc: 94.644 Val_Loss: 0.1095  BEST VAL Loss: 0.1095  Val_Acc: 96.414

Epoch 21: Validation loss decreased (0.109526 --> 0.108870).  Saving model ...
	 Train_Loss: 0.1514 Train_Acc: 94.628 Val_Loss: 0.1089  BEST VAL Loss: 0.1089  Val_Acc: 96.567

Epoch 22: Validation loss decreased (0.108870 --> 0.108271).  Saving model ...
	 Train_Loss: 0.1508 Train_Acc: 94.504 Val_Loss: 0.1083  BEST VAL Loss: 0.1083  Val_Acc: 96.652

Epoch 23: Validation loss decreased (0.108271 --> 0.107800).  Saving model ...
	 Train_Loss: 0.1499 Train_Acc: 94.639 Val_Loss: 0.1078  BEST VAL Loss: 0.1078  Val_Acc: 96.503

Epoch 24: Validation loss decreased (0.107800 --> 0.107150).  Saving model ...
	 Train_Loss: 0.1489 Train_Acc: 94.716 Val_Loss: 0.1071  BEST VAL Loss: 0.1071  Val_Acc: 96.841

Epoch 25: Validation loss decreased (0.107150 --> 0.106590).  Saving model ...
	 Train_Loss: 0.1480 Train_Acc: 94.805 Val_Loss: 0.1066  BEST VAL Loss: 0.1066  Val_Acc: 96.652

Epoch 26: Validation loss decreased (0.106590 --> 0.106099).  Saving model ...
	 Train_Loss: 0.1472 Train_Acc: 94.721 Val_Loss: 0.1061  BEST VAL Loss: 0.1061  Val_Acc: 96.531

Epoch 27: Validation loss decreased (0.106099 --> 0.105561).  Saving model ...
	 Train_Loss: 0.1465 Train_Acc: 94.731 Val_Loss: 0.1056  BEST VAL Loss: 0.1056  Val_Acc: 96.841

Epoch 28: Validation loss decreased (0.105561 --> 0.105441).  Saving model ...
	 Train_Loss: 0.1458 Train_Acc: 94.749 Val_Loss: 0.1054  BEST VAL Loss: 0.1054  Val_Acc: 96.813

Epoch 29: Validation loss decreased (0.105441 --> 0.105302).  Saving model ...
	 Train_Loss: 0.1451 Train_Acc: 94.701 Val_Loss: 0.1053  BEST VAL Loss: 0.1053  Val_Acc: 96.571

Epoch 30: Validation loss decreased (0.105302 --> 0.104975).  Saving model ...
	 Train_Loss: 0.1445 Train_Acc: 94.609 Val_Loss: 0.1050  BEST VAL Loss: 0.1050  Val_Acc: 96.575

Epoch 31: Validation loss decreased (0.104975 --> 0.104721).  Saving model ...
	 Train_Loss: 0.1440 Train_Acc: 94.648 Val_Loss: 0.1047  BEST VAL Loss: 0.1047  Val_Acc: 96.555

Epoch 32: Validation loss decreased (0.104721 --> 0.104328).  Saving model ...
	 Train_Loss: 0.1434 Train_Acc: 94.727 Val_Loss: 0.1043  BEST VAL Loss: 0.1043  Val_Acc: 96.725

Epoch 33: Validation loss decreased (0.104328 --> 0.104037).  Saving model ...
	 Train_Loss: 0.1429 Train_Acc: 94.724 Val_Loss: 0.1040  BEST VAL Loss: 0.1040  Val_Acc: 96.588

Epoch 34: Validation loss decreased (0.104037 --> 0.103708).  Saving model ...
	 Train_Loss: 0.1423 Train_Acc: 94.822 Val_Loss: 0.1037  BEST VAL Loss: 0.1037  Val_Acc: 96.781

Epoch 35: Validation loss decreased (0.103708 --> 0.103534).  Saving model ...
	 Train_Loss: 0.1418 Train_Acc: 94.808 Val_Loss: 0.1035  BEST VAL Loss: 0.1035  Val_Acc: 96.612

Epoch 36: Validation loss decreased (0.103534 --> 0.103367).  Saving model ...
	 Train_Loss: 0.1413 Train_Acc: 94.720 Val_Loss: 0.1034  BEST VAL Loss: 0.1034  Val_Acc: 96.624

Epoch 37: Validation loss decreased (0.103367 --> 0.103210).  Saving model ...
	 Train_Loss: 0.1409 Train_Acc: 94.759 Val_Loss: 0.1032  BEST VAL Loss: 0.1032  Val_Acc: 96.640

Epoch 38: Validation loss decreased (0.103210 --> 0.102965).  Saving model ...
	 Train_Loss: 0.1405 Train_Acc: 94.676 Val_Loss: 0.1030  BEST VAL Loss: 0.1030  Val_Acc: 96.845

Epoch 39: Validation loss decreased (0.102965 --> 0.102932).  Saving model ...
	 Train_Loss: 0.1401 Train_Acc: 94.779 Val_Loss: 0.1029  BEST VAL Loss: 0.1029  Val_Acc: 96.567

Epoch 40: Validation loss decreased (0.102932 --> 0.102644).  Saving model ...
	 Train_Loss: 0.1396 Train_Acc: 94.778 Val_Loss: 0.1026  BEST VAL Loss: 0.1026  Val_Acc: 96.696

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1392 Train_Acc: 94.784 Val_Loss: 0.1029  BEST VAL Loss: 0.1026  Val_Acc: 96.600

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1388 Train_Acc: 94.811 Val_Loss: 0.1028  BEST VAL Loss: 0.1026  Val_Acc: 96.793

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1384 Train_Acc: 94.808 Val_Loss: 0.1027  BEST VAL Loss: 0.1026  Val_Acc: 96.592

Epoch 44: Validation loss decreased (0.102644 --> 0.102571).  Saving model ...
	 Train_Loss: 0.1380 Train_Acc: 94.911 Val_Loss: 0.1026  BEST VAL Loss: 0.1026  Val_Acc: 96.765

Epoch 45: Validation loss decreased (0.102571 --> 0.102466).  Saving model ...
	 Train_Loss: 0.1377 Train_Acc: 94.853 Val_Loss: 0.1025  BEST VAL Loss: 0.1025  Val_Acc: 96.821

Epoch 46: Validation loss decreased (0.102466 --> 0.102383).  Saving model ...
	 Train_Loss: 0.1373 Train_Acc: 94.881 Val_Loss: 0.1024  BEST VAL Loss: 0.1024  Val_Acc: 96.825

Epoch 47: Validation loss decreased (0.102383 --> 0.102306).  Saving model ...
	 Train_Loss: 0.1370 Train_Acc: 94.882 Val_Loss: 0.1023  BEST VAL Loss: 0.1023  Val_Acc: 96.753

Epoch 48: Validation loss decreased (0.102306 --> 0.102168).  Saving model ...
	 Train_Loss: 0.1367 Train_Acc: 94.732 Val_Loss: 0.1022  BEST VAL Loss: 0.1022  Val_Acc: 96.813

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1363 Train_Acc: 94.856 Val_Loss: 0.1022  BEST VAL Loss: 0.1022  Val_Acc: 96.789

Epoch 50: Validation loss decreased (0.102168 --> 0.102006).  Saving model ...
	 Train_Loss: 0.1360 Train_Acc: 94.805 Val_Loss: 0.1020  BEST VAL Loss: 0.1020  Val_Acc: 96.954

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1357 Train_Acc: 94.972 Val_Loss: 0.1020  BEST VAL Loss: 0.1020  Val_Acc: 96.640

Epoch 52: Validation loss decreased (0.102006 --> 0.101831).  Saving model ...
	 Train_Loss: 0.1354 Train_Acc: 94.918 Val_Loss: 0.1018  BEST VAL Loss: 0.1018  Val_Acc: 96.741

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1351 Train_Acc: 94.679 Val_Loss: 0.1019  BEST VAL Loss: 0.1018  Val_Acc: 96.757

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1349 Train_Acc: 94.721 Val_Loss: 0.1019  BEST VAL Loss: 0.1018  Val_Acc: 96.765

Epoch 55: Validation loss decreased (0.101831 --> 0.101804).  Saving model ...
	 Train_Loss: 0.1346 Train_Acc: 94.677 Val_Loss: 0.1018  BEST VAL Loss: 0.1018  Val_Acc: 96.584

Epoch 56: Validation loss decreased (0.101804 --> 0.101683).  Saving model ...
	 Train_Loss: 0.1344 Train_Acc: 94.824 Val_Loss: 0.1017  BEST VAL Loss: 0.1017  Val_Acc: 96.640

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.1343 Train_Acc: 94.644 Val_Loss: 0.1018  BEST VAL Loss: 0.1017  Val_Acc: 96.491

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.1345 Train_Acc: 94.412 Val_Loss: 0.1017  BEST VAL Loss: 0.1017  Val_Acc: 96.692

Epoch 59: Validation loss decreased (0.101683 --> 0.101590).  Saving model ...
	 Train_Loss: 0.1343 Train_Acc: 94.817 Val_Loss: 0.1016  BEST VAL Loss: 0.1016  Val_Acc: 96.845

Epoch 60: Validation loss decreased (0.101590 --> 0.101452).  Saving model ...
	 Train_Loss: 0.1341 Train_Acc: 94.446 Val_Loss: 0.1015  BEST VAL Loss: 0.1015  Val_Acc: 96.833

Epoch 61: Validation loss decreased (0.101452 --> 0.101311).  Saving model ...
	 Train_Loss: 0.1339 Train_Acc: 94.659 Val_Loss: 0.1013  BEST VAL Loss: 0.1013  Val_Acc: 96.829

Epoch 62: Validation loss decreased (0.101311 --> 0.101241).  Saving model ...
	 Train_Loss: 0.1336 Train_Acc: 94.814 Val_Loss: 0.1012  BEST VAL Loss: 0.1012  Val_Acc: 96.942

Epoch 63: Validation loss decreased (0.101241 --> 0.101233).  Saving model ...
	 Train_Loss: 0.1334 Train_Acc: 94.837 Val_Loss: 0.1012  BEST VAL Loss: 0.1012  Val_Acc: 96.837

Epoch 64: Validation loss decreased (0.101233 --> 0.101177).  Saving model ...
	 Train_Loss: 0.1331 Train_Acc: 94.854 Val_Loss: 0.1012  BEST VAL Loss: 0.1012  Val_Acc: 96.797

Epoch 65: Validation loss decreased (0.101177 --> 0.101127).  Saving model ...
	 Train_Loss: 0.1329 Train_Acc: 94.742 Val_Loss: 0.1011  BEST VAL Loss: 0.1011  Val_Acc: 96.483

Epoch 66: Validation loss decreased (0.101127 --> 0.101123).  Saving model ...
	 Train_Loss: 0.1326 Train_Acc: 94.919 Val_Loss: 0.1011  BEST VAL Loss: 0.1011  Val_Acc: 96.853

Epoch 67: Validation loss decreased (0.101123 --> 0.100946).  Saving model ...
	 Train_Loss: 0.1323 Train_Acc: 95.092 Val_Loss: 0.1009  BEST VAL Loss: 0.1009  Val_Acc: 97.075

Epoch 68: Validation loss decreased (0.100946 --> 0.100833).  Saving model ...
	 Train_Loss: 0.1322 Train_Acc: 94.804 Val_Loss: 0.1008  BEST VAL Loss: 0.1008  Val_Acc: 96.837

Epoch 69: Validation loss decreased (0.100833 --> 0.100682).  Saving model ...
	 Train_Loss: 0.1320 Train_Acc: 94.874 Val_Loss: 0.1007  BEST VAL Loss: 0.1007  Val_Acc: 96.632

Epoch 70: Validation loss decreased (0.100682 --> 0.100659).  Saving model ...
	 Train_Loss: 0.1318 Train_Acc: 94.801 Val_Loss: 0.1007  BEST VAL Loss: 0.1007  Val_Acc: 97.003

Epoch 71: Validation loss decreased (0.100659 --> 0.100551).  Saving model ...
	 Train_Loss: 0.1316 Train_Acc: 94.848 Val_Loss: 0.1006  BEST VAL Loss: 0.1006  Val_Acc: 97.051

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.1314 Train_Acc: 94.871 Val_Loss: 0.1006  BEST VAL Loss: 0.1006  Val_Acc: 96.801

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.1315 Train_Acc: 94.676 Val_Loss: 0.1007  BEST VAL Loss: 0.1006  Val_Acc: 96.696

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.1314 Train_Acc: 94.569 Val_Loss: 0.1008  BEST VAL Loss: 0.1006  Val_Acc: 96.910

Epoch 75: Validation loss did not decrease
	 Train_Loss: 0.1314 Train_Acc: 94.284 Val_Loss: 0.1007  BEST VAL Loss: 0.1006  Val_Acc: 96.882

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.1312 Train_Acc: 94.920 Val_Loss: 0.1006  BEST VAL Loss: 0.1006  Val_Acc: 96.817

Epoch 77: Validation loss decreased (0.100551 --> 0.100493).  Saving model ...
	 Train_Loss: 0.1309 Train_Acc: 94.853 Val_Loss: 0.1005  BEST VAL Loss: 0.1005  Val_Acc: 97.067

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.1307 Train_Acc: 94.969 Val_Loss: 0.1006  BEST VAL Loss: 0.1005  Val_Acc: 96.853

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.1306 Train_Acc: 94.653 Val_Loss: 0.1006  BEST VAL Loss: 0.1005  Val_Acc: 96.857

Epoch 80: Validation loss did not decrease
	 Train_Loss: 0.1303 Train_Acc: 94.927 Val_Loss: 0.1005  BEST VAL Loss: 0.1005  Val_Acc: 96.821

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.1301 Train_Acc: 94.952 Val_Loss: 0.1006  BEST VAL Loss: 0.1005  Val_Acc: 96.451

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.1299 Train_Acc: 94.776 Val_Loss: 0.1007  BEST VAL Loss: 0.1005  Val_Acc: 96.781

Epoch 83: Validation loss did not decrease
	 Train_Loss: 0.1297 Train_Acc: 94.760 Val_Loss: 0.1008  BEST VAL Loss: 0.1005  Val_Acc: 96.765

Epoch 84: Validation loss did not decrease
	 Train_Loss: 0.1298 Train_Acc: 94.606 Val_Loss: 0.1009  BEST VAL Loss: 0.1005  Val_Acc: 96.604

Epoch 85: Validation loss did not decrease
	 Train_Loss: 0.1297 Train_Acc: 94.686 Val_Loss: 0.1010  BEST VAL Loss: 0.1005  Val_Acc: 96.853

Epoch 86: Validation loss did not decrease
	 Train_Loss: 0.1296 Train_Acc: 94.632 Val_Loss: 0.1010  BEST VAL Loss: 0.1005  Val_Acc: 96.749

Epoch 87: Validation loss did not decrease
	 Train_Loss: 0.1294 Train_Acc: 94.912 Val_Loss: 0.1011  BEST VAL Loss: 0.1005  Val_Acc: 96.914

Epoch 88: Validation loss did not decrease
	 Train_Loss: 0.1293 Train_Acc: 94.787 Val_Loss: 0.1010  BEST VAL Loss: 0.1005  Val_Acc: 96.890

Epoch 89: Validation loss did not decrease
	 Train_Loss: 0.1291 Train_Acc: 94.932 Val_Loss: 0.1012  BEST VAL Loss: 0.1005  Val_Acc: 96.785

Epoch 90: Validation loss did not decrease
	 Train_Loss: 0.1290 Train_Acc: 94.875 Val_Loss: 0.1012  BEST VAL Loss: 0.1005  Val_Acc: 96.862

Epoch 91: Validation loss did not decrease
	 Train_Loss: 0.1288 Train_Acc: 94.834 Val_Loss: 0.1013  BEST VAL Loss: 0.1005  Val_Acc: 96.761

Epoch 92: Validation loss did not decrease
	 Train_Loss: 0.1287 Train_Acc: 94.905 Val_Loss: 0.1013  BEST VAL Loss: 0.1005  Val_Acc: 96.700

Epoch 93: Validation loss did not decrease
Early stopped at epoch : 93
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.97      0.98    100908
           1       0.97      0.99      0.98     97655

    accuracy                           0.98    198563
   macro avg       0.98      0.98      0.98    198563
weighted avg       0.98      0.98      0.98    198563

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.96      0.97     12614
           1       0.96      0.98      0.97     12207

    accuracy                           0.97     24821
   macro avg       0.97      0.97      0.97     24821
weighted avg       0.97      0.97      0.97     24821

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.97      0.97     12614
           1       0.96      0.98      0.97     12207

    accuracy                           0.97     24821
   macro avg       0.97      0.97      0.97     24821
weighted avg       0.97      0.97      0.97     24821

              precision    recall  f1-score   support

           0       0.98      0.97      0.97     12614
           1       0.96      0.98      0.97     12207

    accuracy                           0.97     24821
   macro avg       0.97      0.97      0.97     24821
weighted avg       0.97      0.97      0.97     24821

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.91      0.93     39877
           1       0.92      0.95      0.94     44915

    accuracy                           0.93     84792
   macro avg       0.93      0.93      0.93     84792
weighted avg       0.93      0.93      0.93     84792

              precision    recall  f1-score   support

           0       0.94      0.91      0.93     39877
           1       0.92      0.95      0.94     44915

    accuracy                           0.93     84792
   macro avg       0.93      0.93      0.93     84792
weighted avg       0.93      0.93      0.93     84792

completed
