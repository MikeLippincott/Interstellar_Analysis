[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'dfe6c2b6'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2c420088'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '1da0d3ab'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '46c7b190'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (26864, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'L20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'L16' 'L17' 'L21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.090690).  Saving model ...
	 Train_Loss: 0.1971 Train_Acc: 91.072 Val_Loss: 0.0907  BEST VAL Loss: 0.0907  Val_Acc: 95.794

Epoch 1: Validation loss decreased (0.090690 --> 0.081199).  Saving model ...
	 Train_Loss: 0.1456 Train_Acc: 96.250 Val_Loss: 0.0812  BEST VAL Loss: 0.0812  Val_Acc: 96.338

Epoch 2: Validation loss decreased (0.081199 --> 0.071826).  Saving model ...
	 Train_Loss: 0.1196 Train_Acc: 97.216 Val_Loss: 0.0718  BEST VAL Loss: 0.0718  Val_Acc: 96.685

Epoch 3: Validation loss decreased (0.071826 --> 0.067030).  Saving model ...
	 Train_Loss: 0.1031 Train_Acc: 97.618 Val_Loss: 0.0670  BEST VAL Loss: 0.0670  Val_Acc: 96.883

Epoch 4: Validation loss decreased (0.067030 --> 0.065077).  Saving model ...
	 Train_Loss: 0.0916 Train_Acc: 97.971 Val_Loss: 0.0651  BEST VAL Loss: 0.0651  Val_Acc: 97.130

Epoch 5: Validation loss decreased (0.065077 --> 0.062968).  Saving model ...
	 Train_Loss: 0.0828 Train_Acc: 98.268 Val_Loss: 0.0630  BEST VAL Loss: 0.0630  Val_Acc: 97.081

Epoch 6: Validation loss decreased (0.062968 --> 0.060851).  Saving model ...
	 Train_Loss: 0.0759 Train_Acc: 98.181 Val_Loss: 0.0609  BEST VAL Loss: 0.0609  Val_Acc: 97.229

Epoch 7: Validation loss did not decrease
	 Train_Loss: 0.0703 Train_Acc: 98.546 Val_Loss: 0.0612  BEST VAL Loss: 0.0609  Val_Acc: 97.526

Epoch 8: Validation loss did not decrease
	 Train_Loss: 0.0660 Train_Acc: 98.534 Val_Loss: 0.0610  BEST VAL Loss: 0.0609  Val_Acc: 97.427

Epoch 9: Validation loss did not decrease
	 Train_Loss: 0.0625 Train_Acc: 98.490 Val_Loss: 0.0652  BEST VAL Loss: 0.0609  Val_Acc: 97.427

Epoch 10: Validation loss did not decrease
	 Train_Loss: 0.0593 Train_Acc: 98.620 Val_Loss: 0.0642  BEST VAL Loss: 0.0609  Val_Acc: 97.279

Epoch 11: Validation loss did not decrease
	 Train_Loss: 0.0566 Train_Acc: 98.707 Val_Loss: 0.0632  BEST VAL Loss: 0.0609  Val_Acc: 97.823

Epoch 12: Validation loss did not decrease
	 Train_Loss: 0.0542 Train_Acc: 98.620 Val_Loss: 0.0625  BEST VAL Loss: 0.0609  Val_Acc: 97.823

Epoch 13: Validation loss did not decrease
	 Train_Loss: 0.0520 Train_Acc: 98.948 Val_Loss: 0.0624  BEST VAL Loss: 0.0609  Val_Acc: 97.625

Epoch 14: Validation loss did not decrease
	 Train_Loss: 0.0501 Train_Acc: 98.998 Val_Loss: 0.0619  BEST VAL Loss: 0.0609  Val_Acc: 97.476

Epoch 15: Validation loss did not decrease
	 Train_Loss: 0.0483 Train_Acc: 98.961 Val_Loss: 0.0618  BEST VAL Loss: 0.0609  Val_Acc: 97.872

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.0465 Train_Acc: 99.022 Val_Loss: 0.0615  BEST VAL Loss: 0.0609  Val_Acc: 97.674

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.0450 Train_Acc: 99.060 Val_Loss: 0.0616  BEST VAL Loss: 0.0609  Val_Acc: 97.872

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.0435 Train_Acc: 99.053 Val_Loss: 0.0621  BEST VAL Loss: 0.0609  Val_Acc: 97.773

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.0423 Train_Acc: 99.060 Val_Loss: 0.0619  BEST VAL Loss: 0.0609  Val_Acc: 98.070

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.0414 Train_Acc: 98.973 Val_Loss: 0.0625  BEST VAL Loss: 0.0609  Val_Acc: 97.773

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.0404 Train_Acc: 99.010 Val_Loss: 0.0629  BEST VAL Loss: 0.0609  Val_Acc: 97.724

Epoch 22: Validation loss did not decrease
Early stopped at epoch : 22
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      8312
           1       1.00      0.99      0.99      7850

    accuracy                           0.99     16162
   macro avg       0.99      0.99      0.99     16162
weighted avg       0.99      0.99      0.99     16162

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1039
           1       0.98      0.96      0.97       982

    accuracy                           0.97      2021
   macro avg       0.97      0.97      0.97      2021
weighted avg       0.97      0.97      0.97      2021

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1039
           1       0.98      0.96      0.97       982

    accuracy                           0.97      2021
   macro avg       0.97      0.97      0.97      2021
weighted avg       0.97      0.97      0.97      2021

              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1039
           1       0.98      0.96      0.97       982

    accuracy                           0.97      2021
   macro avg       0.97      0.97      0.97      2021
weighted avg       0.97      0.97      0.97      2021

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.99      0.98      3262
           1       0.99      0.98      0.99      3398

    accuracy                           0.98      6660
   macro avg       0.98      0.99      0.98      6660
weighted avg       0.99      0.98      0.98      6660

              precision    recall  f1-score   support

           0       0.98      0.99      0.98      3262
           1       0.99      0.98      0.99      3398

    accuracy                           0.98      6660
   macro avg       0.98      0.99      0.98      6660
weighted avg       0.99      0.98      0.98      6660

completed
