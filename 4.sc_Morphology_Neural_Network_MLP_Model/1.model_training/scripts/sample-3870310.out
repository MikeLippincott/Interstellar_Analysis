[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18688 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Writing 40207 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:252: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:279: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:583: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:597: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:657: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:866: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:868: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:871: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:902: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:944: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1131: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1134: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1211: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1384: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1386: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1389: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1466: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
PBMC MultiClass_MLP False
[0.9436581681188537, 0.5245113046249099, 0.5318305272562364]
Data Subset Is Off
(1412200,) (353051,) (1877682,) 3642933     858323
3642934     858324
3642935     858325
3642936     858326
3642937     858327
            ...   
4061834    4689033
4061835    4689034
4061836    4689035
4061837    4689036
4061838    4689037
Name: labeled_data_index, Length: 418906, dtype: int64 (1536843,)
(1412200,) (353051,) (1877682,) 3642933     858323
3642934     858324
3642935     858325
3642936     858326
3642937     858327
            ...   
4061834    4689033
4061835    4689034
4061836    4689035
4061837    4689036
4061838    4689037
Name: labeled_data_index, Length: 418906, dtype: int64 (1536843,)
5598682
(51084,) (722888,) (638228,)
(12771,) (180720,) (159560,)
(63855,) (903610,) (910217,)
(112111,) (66074,) (240721,)
(75619,) (788818,) (672406,)
(1412200, 1245) (353051, 1245) (1877682, 1245) (418906, 1245) (1536843, 1245)
(1412200,) (353051,) (1877682,) (418906,) (1536843,)
Number of in features:  1245
Number of out features:  3
Multi_Class
Adam
Epoch 0: Validation loss decreased (inf --> 0.482789).  Saving model ...
	 Train_Loss: 0.5700 Train_Acc: 75.279 Val_Loss: 0.4828  BEST VAL Loss: 0.4828  Val_Acc: 78.864

Epoch 1: Validation loss decreased (0.482789 --> 0.465885).  Saving model ...
	 Train_Loss: 0.5359 Train_Acc: 78.418 Val_Loss: 0.4659  BEST VAL Loss: 0.4659  Val_Acc: 80.600

Epoch 2: Validation loss decreased (0.465885 --> 0.454011).  Saving model ...
	 Train_Loss: 0.5170 Train_Acc: 79.537 Val_Loss: 0.4540  BEST VAL Loss: 0.4540  Val_Acc: 81.381

Epoch 3: Validation loss decreased (0.454011 --> 0.445984).  Saving model ...
	 Train_Loss: 0.5043 Train_Acc: 80.146 Val_Loss: 0.4460  BEST VAL Loss: 0.4460  Val_Acc: 81.852

Epoch 4: Validation loss decreased (0.445984 --> 0.439425).  Saving model ...
	 Train_Loss: 0.4948 Train_Acc: 80.611 Val_Loss: 0.4394  BEST VAL Loss: 0.4394  Val_Acc: 82.354

Epoch 5: Validation loss decreased (0.439425 --> 0.434765).  Saving model ...
	 Train_Loss: 0.4873 Train_Acc: 80.942 Val_Loss: 0.4348  BEST VAL Loss: 0.4348  Val_Acc: 82.284

Epoch 6: Validation loss decreased (0.434765 --> 0.430124).  Saving model ...
	 Train_Loss: 0.4812 Train_Acc: 81.242 Val_Loss: 0.4301  BEST VAL Loss: 0.4301  Val_Acc: 82.851

Epoch 7: Validation loss decreased (0.430124 --> 0.426364).  Saving model ...
	 Train_Loss: 0.4760 Train_Acc: 81.420 Val_Loss: 0.4264  BEST VAL Loss: 0.4264  Val_Acc: 82.823

Epoch 8: Validation loss decreased (0.426364 --> 0.423413).  Saving model ...
	 Train_Loss: 0.4714 Train_Acc: 81.633 Val_Loss: 0.4234  BEST VAL Loss: 0.4234  Val_Acc: 82.912

Epoch 9: Validation loss decreased (0.423413 --> 0.420438).  Saving model ...
	 Train_Loss: 0.4674 Train_Acc: 81.840 Val_Loss: 0.4204  BEST VAL Loss: 0.4204  Val_Acc: 83.212

Epoch 10: Validation loss decreased (0.420438 --> 0.417768).  Saving model ...
	 Train_Loss: 0.4639 Train_Acc: 81.948 Val_Loss: 0.4178  BEST VAL Loss: 0.4178  Val_Acc: 83.409

Epoch 11: Validation loss decreased (0.417768 --> 0.415273).  Saving model ...
	 Train_Loss: 0.4607 Train_Acc: 82.108 Val_Loss: 0.4153  BEST VAL Loss: 0.4153  Val_Acc: 83.471

Epoch 12: Validation loss decreased (0.415273 --> 0.413243).  Saving model ...
	 Train_Loss: 0.4579 Train_Acc: 82.209 Val_Loss: 0.4132  BEST VAL Loss: 0.4132  Val_Acc: 83.425

Epoch 13: Validation loss decreased (0.413243 --> 0.411359).  Saving model ...
	 Train_Loss: 0.4553 Train_Acc: 82.310 Val_Loss: 0.4114  BEST VAL Loss: 0.4114  Val_Acc: 83.477

Epoch 14: Validation loss decreased (0.411359 --> 0.409651).  Saving model ...
	 Train_Loss: 0.4528 Train_Acc: 82.385 Val_Loss: 0.4097  BEST VAL Loss: 0.4097  Val_Acc: 83.676

Epoch 15: Validation loss decreased (0.409651 --> 0.408011).  Saving model ...
	 Train_Loss: 0.4506 Train_Acc: 82.487 Val_Loss: 0.4080  BEST VAL Loss: 0.4080  Val_Acc: 83.807

Epoch 16: Validation loss decreased (0.408011 --> 0.406493).  Saving model ...
	 Train_Loss: 0.4485 Train_Acc: 82.606 Val_Loss: 0.4065  BEST VAL Loss: 0.4065  Val_Acc: 83.651

Epoch 17: Validation loss decreased (0.406493 --> 0.405151).  Saving model ...
	 Train_Loss: 0.4466 Train_Acc: 82.603 Val_Loss: 0.4052  BEST VAL Loss: 0.4052  Val_Acc: 83.842

Epoch 18: Validation loss decreased (0.405151 --> 0.403786).  Saving model ...
	 Train_Loss: 0.4448 Train_Acc: 82.696 Val_Loss: 0.4038  BEST VAL Loss: 0.4038  Val_Acc: 83.824

Epoch 19: Validation loss decreased (0.403786 --> 0.402610).  Saving model ...
	 Train_Loss: 0.4431 Train_Acc: 82.744 Val_Loss: 0.4026  BEST VAL Loss: 0.4026  Val_Acc: 84.058

Epoch 20: Validation loss decreased (0.402610 --> 0.401328).  Saving model ...
	 Train_Loss: 0.4416 Train_Acc: 82.785 Val_Loss: 0.4013  BEST VAL Loss: 0.4013  Val_Acc: 84.094

Epoch 21: Validation loss decreased (0.401328 --> 0.400482).  Saving model ...
	 Train_Loss: 0.4401 Train_Acc: 82.895 Val_Loss: 0.4005  BEST VAL Loss: 0.4005  Val_Acc: 83.696

Epoch 22: Validation loss decreased (0.400482 --> 0.399322).  Saving model ...
	 Train_Loss: 0.4387 Train_Acc: 82.921 Val_Loss: 0.3993  BEST VAL Loss: 0.3993  Val_Acc: 84.163

Epoch 23: Validation loss decreased (0.399322 --> 0.398278).  Saving model ...
	 Train_Loss: 0.4374 Train_Acc: 82.947 Val_Loss: 0.3983  BEST VAL Loss: 0.3983  Val_Acc: 84.070

Epoch 24: Validation loss decreased (0.398278 --> 0.397336).  Saving model ...
	 Train_Loss: 0.4361 Train_Acc: 82.996 Val_Loss: 0.3973  BEST VAL Loss: 0.3973  Val_Acc: 84.131

Epoch 25: Validation loss decreased (0.397336 --> 0.396376).  Saving model ...
	 Train_Loss: 0.4349 Train_Acc: 83.069 Val_Loss: 0.3964  BEST VAL Loss: 0.3964  Val_Acc: 84.282

Epoch 26: Validation loss decreased (0.396376 --> 0.395452).  Saving model ...
	 Train_Loss: 0.4337 Train_Acc: 83.156 Val_Loss: 0.3955  BEST VAL Loss: 0.3955  Val_Acc: 84.261

Epoch 27: Validation loss decreased (0.395452 --> 0.394639).  Saving model ...
	 Train_Loss: 0.4326 Train_Acc: 83.127 Val_Loss: 0.3946  BEST VAL Loss: 0.3946  Val_Acc: 84.294

Epoch 28: Validation loss decreased (0.394639 --> 0.393872).  Saving model ...
	 Train_Loss: 0.4315 Train_Acc: 83.147 Val_Loss: 0.3939  BEST VAL Loss: 0.3939  Val_Acc: 84.260

Epoch 29: Validation loss decreased (0.393872 --> 0.393063).  Saving model ...
	 Train_Loss: 0.4305 Train_Acc: 83.212 Val_Loss: 0.3931  BEST VAL Loss: 0.3931  Val_Acc: 84.354

Epoch 30: Validation loss decreased (0.393063 --> 0.392271).  Saving model ...
	 Train_Loss: 0.4295 Train_Acc: 83.228 Val_Loss: 0.3923  BEST VAL Loss: 0.3923  Val_Acc: 84.389

Epoch 31: Validation loss decreased (0.392271 --> 0.391547).  Saving model ...
	 Train_Loss: 0.4286 Train_Acc: 83.285 Val_Loss: 0.3915  BEST VAL Loss: 0.3915  Val_Acc: 84.447

Epoch 32: Validation loss decreased (0.391547 --> 0.390747).  Saving model ...
	 Train_Loss: 0.4277 Train_Acc: 83.330 Val_Loss: 0.3907  BEST VAL Loss: 0.3907  Val_Acc: 84.555

Epoch 33: Validation loss decreased (0.390747 --> 0.390128).  Saving model ...
	 Train_Loss: 0.4268 Train_Acc: 83.391 Val_Loss: 0.3901  BEST VAL Loss: 0.3901  Val_Acc: 84.557

Epoch 34: Validation loss decreased (0.390128 --> 0.389483).  Saving model ...
	 Train_Loss: 0.4259 Train_Acc: 83.427 Val_Loss: 0.3895  BEST VAL Loss: 0.3895  Val_Acc: 84.470

Epoch 35: Validation loss decreased (0.389483 --> 0.388948).  Saving model ...
	 Train_Loss: 0.4251 Train_Acc: 83.423 Val_Loss: 0.3889  BEST VAL Loss: 0.3889  Val_Acc: 84.502

Epoch 36: Validation loss decreased (0.388948 --> 0.388287).  Saving model ...
	 Train_Loss: 0.4243 Train_Acc: 83.472 Val_Loss: 0.3883  BEST VAL Loss: 0.3883  Val_Acc: 84.556

Epoch 37: Validation loss decreased (0.388287 --> 0.387692).  Saving model ...
	 Train_Loss: 0.4236 Train_Acc: 83.456 Val_Loss: 0.3877  BEST VAL Loss: 0.3877  Val_Acc: 84.473

Epoch 38: Validation loss decreased (0.387692 --> 0.387111).  Saving model ...
	 Train_Loss: 0.4228 Train_Acc: 83.503 Val_Loss: 0.3871  BEST VAL Loss: 0.3871  Val_Acc: 84.617

Epoch 39: Validation loss decreased (0.387111 --> 0.386588).  Saving model ...
	 Train_Loss: 0.4221 Train_Acc: 83.519 Val_Loss: 0.3866  BEST VAL Loss: 0.3866  Val_Acc: 84.589

Epoch 40: Validation loss decreased (0.386588 --> 0.386064).  Saving model ...
	 Train_Loss: 0.4214 Train_Acc: 83.526 Val_Loss: 0.3861  BEST VAL Loss: 0.3861  Val_Acc: 84.609

Epoch 41: Validation loss decreased (0.386064 --> 0.385483).  Saving model ...
	 Train_Loss: 0.4207 Train_Acc: 83.574 Val_Loss: 0.3855  BEST VAL Loss: 0.3855  Val_Acc: 84.703

Epoch 42: Validation loss decreased (0.385483 --> 0.384990).  Saving model ...
	 Train_Loss: 0.4201 Train_Acc: 83.590 Val_Loss: 0.3850  BEST VAL Loss: 0.3850  Val_Acc: 84.692

Epoch 43: Validation loss decreased (0.384990 --> 0.384390).  Saving model ...
	 Train_Loss: 0.4195 Train_Acc: 83.587 Val_Loss: 0.3844  BEST VAL Loss: 0.3844  Val_Acc: 84.789

Epoch 44: Validation loss decreased (0.384390 --> 0.383979).  Saving model ...
	 Train_Loss: 0.4188 Train_Acc: 83.597 Val_Loss: 0.3840  BEST VAL Loss: 0.3840  Val_Acc: 84.734

Epoch 45: Validation loss decreased (0.383979 --> 0.383434).  Saving model ...
	 Train_Loss: 0.4182 Train_Acc: 83.656 Val_Loss: 0.3834  BEST VAL Loss: 0.3834  Val_Acc: 84.857

Epoch 46: Validation loss decreased (0.383434 --> 0.383024).  Saving model ...
	 Train_Loss: 0.4177 Train_Acc: 83.640 Val_Loss: 0.3830  BEST VAL Loss: 0.3830  Val_Acc: 84.735

Epoch 47: Validation loss decreased (0.383024 --> 0.382569).  Saving model ...
	 Train_Loss: 0.4171 Train_Acc: 83.703 Val_Loss: 0.3826  BEST VAL Loss: 0.3826  Val_Acc: 84.751

Epoch 48: Validation loss decreased (0.382569 --> 0.382135).  Saving model ...
	 Train_Loss: 0.4166 Train_Acc: 83.704 Val_Loss: 0.3821  BEST VAL Loss: 0.3821  Val_Acc: 84.802

Epoch 49: Validation loss decreased (0.382135 --> 0.381694).  Saving model ...
	 Train_Loss: 0.4161 Train_Acc: 83.664 Val_Loss: 0.3817  BEST VAL Loss: 0.3817  Val_Acc: 84.804

Epoch 50: Validation loss decreased (0.381694 --> 0.381319).  Saving model ...
	 Train_Loss: 0.4155 Train_Acc: 83.745 Val_Loss: 0.3813  BEST VAL Loss: 0.3813  Val_Acc: 84.876

Epoch 51: Validation loss decreased (0.381319 --> 0.380918).  Saving model ...
	 Train_Loss: 0.4150 Train_Acc: 83.810 Val_Loss: 0.3809  BEST VAL Loss: 0.3809  Val_Acc: 84.879

Epoch 52: Validation loss decreased (0.380918 --> 0.380535).  Saving model ...
	 Train_Loss: 0.4145 Train_Acc: 83.744 Val_Loss: 0.3805  BEST VAL Loss: 0.3805  Val_Acc: 84.839

Epoch 53: Validation loss decreased (0.380535 --> 0.380172).  Saving model ...
	 Train_Loss: 0.4140 Train_Acc: 83.771 Val_Loss: 0.3802  BEST VAL Loss: 0.3802  Val_Acc: 84.793

Epoch 54: Validation loss decreased (0.380172 --> 0.379820).  Saving model ...
	 Train_Loss: 0.4135 Train_Acc: 83.805 Val_Loss: 0.3798  BEST VAL Loss: 0.3798  Val_Acc: 84.823

Epoch 55: Validation loss decreased (0.379820 --> 0.379460).  Saving model ...
	 Train_Loss: 0.4131 Train_Acc: 83.835 Val_Loss: 0.3795  BEST VAL Loss: 0.3795  Val_Acc: 84.943

Epoch 56: Validation loss decreased (0.379460 --> 0.379163).  Saving model ...
	 Train_Loss: 0.4126 Train_Acc: 83.890 Val_Loss: 0.3792  BEST VAL Loss: 0.3792  Val_Acc: 84.837

Epoch 57: Validation loss decreased (0.379163 --> 0.378784).  Saving model ...
	 Train_Loss: 0.4122 Train_Acc: 83.877 Val_Loss: 0.3788  BEST VAL Loss: 0.3788  Val_Acc: 84.959

Epoch 58: Validation loss decreased (0.378784 --> 0.378534).  Saving model ...
	 Train_Loss: 0.4117 Train_Acc: 83.895 Val_Loss: 0.3785  BEST VAL Loss: 0.3785  Val_Acc: 84.751

Epoch 59: Validation loss decreased (0.378534 --> 0.378213).  Saving model ...
	 Train_Loss: 0.4113 Train_Acc: 83.904 Val_Loss: 0.3782  BEST VAL Loss: 0.3782  Val_Acc: 84.823

Epoch 60: Validation loss decreased (0.378213 --> 0.377902).  Saving model ...
	 Train_Loss: 0.4109 Train_Acc: 83.868 Val_Loss: 0.3779  BEST VAL Loss: 0.3779  Val_Acc: 84.965

Epoch 61: Validation loss decreased (0.377902 --> 0.377564).  Saving model ...
	 Train_Loss: 0.4105 Train_Acc: 83.919 Val_Loss: 0.3776  BEST VAL Loss: 0.3776  Val_Acc: 85.076

Epoch 62: Validation loss decreased (0.377564 --> 0.377251).  Saving model ...
	 Train_Loss: 0.4101 Train_Acc: 83.904 Val_Loss: 0.3773  BEST VAL Loss: 0.3773  Val_Acc: 84.988

Epoch 63: Validation loss decreased (0.377251 --> 0.376966).  Saving model ...
	 Train_Loss: 0.4097 Train_Acc: 83.946 Val_Loss: 0.3770  BEST VAL Loss: 0.3770  Val_Acc: 84.944

Epoch 64: Validation loss decreased (0.376966 --> 0.376672).  Saving model ...
	 Train_Loss: 0.4093 Train_Acc: 83.975 Val_Loss: 0.3767  BEST VAL Loss: 0.3767  Val_Acc: 84.983

Epoch 65: Validation loss decreased (0.376672 --> 0.376358).  Saving model ...
	 Train_Loss: 0.4089 Train_Acc: 84.016 Val_Loss: 0.3764  BEST VAL Loss: 0.3764  Val_Acc: 85.076

Epoch 66: Validation loss decreased (0.376358 --> 0.376061).  Saving model ...
	 Train_Loss: 0.4086 Train_Acc: 84.031 Val_Loss: 0.3761  BEST VAL Loss: 0.3761  Val_Acc: 85.059

Epoch 67: Validation loss decreased (0.376061 --> 0.375810).  Saving model ...
	 Train_Loss: 0.4082 Train_Acc: 83.989 Val_Loss: 0.3758  BEST VAL Loss: 0.3758  Val_Acc: 84.867

Epoch 68: Validation loss decreased (0.375810 --> 0.375548).  Saving model ...
	 Train_Loss: 0.4078 Train_Acc: 84.023 Val_Loss: 0.3755  BEST VAL Loss: 0.3755  Val_Acc: 85.019

Epoch 69: Validation loss decreased (0.375548 --> 0.375248).  Saving model ...
	 Train_Loss: 0.4075 Train_Acc: 84.053 Val_Loss: 0.3752  BEST VAL Loss: 0.3752  Val_Acc: 84.934

Epoch 70: Validation loss decreased (0.375248 --> 0.374991).  Saving model ...
	 Train_Loss: 0.4071 Train_Acc: 84.026 Val_Loss: 0.3750  BEST VAL Loss: 0.3750  Val_Acc: 84.910

Epoch 71: Validation loss decreased (0.374991 --> 0.374696).  Saving model ...
	 Train_Loss: 0.4068 Train_Acc: 84.025 Val_Loss: 0.3747  BEST VAL Loss: 0.3747  Val_Acc: 85.089

Epoch 72: Validation loss decreased (0.374696 --> 0.374418).  Saving model ...
	 Train_Loss: 0.4065 Train_Acc: 83.988 Val_Loss: 0.3744  BEST VAL Loss: 0.3744  Val_Acc: 85.042

Epoch 73: Validation loss decreased (0.374418 --> 0.374174).  Saving model ...
	 Train_Loss: 0.4061 Train_Acc: 84.070 Val_Loss: 0.3742  BEST VAL Loss: 0.3742  Val_Acc: 84.904

Epoch 74: Validation loss decreased (0.374174 --> 0.373930).  Saving model ...
	 Train_Loss: 0.4058 Train_Acc: 84.078 Val_Loss: 0.3739  BEST VAL Loss: 0.3739  Val_Acc: 85.083

Epoch 75: Validation loss decreased (0.373930 --> 0.373688).  Saving model ...
	 Train_Loss: 0.4055 Train_Acc: 84.108 Val_Loss: 0.3737  BEST VAL Loss: 0.3737  Val_Acc: 85.166

Epoch 76: Validation loss decreased (0.373688 --> 0.373486).  Saving model ...
	 Train_Loss: 0.4052 Train_Acc: 84.070 Val_Loss: 0.3735  BEST VAL Loss: 0.3735  Val_Acc: 85.037

Epoch 77: Validation loss decreased (0.373486 --> 0.373246).  Saving model ...
	 Train_Loss: 0.4049 Train_Acc: 84.100 Val_Loss: 0.3732  BEST VAL Loss: 0.3732  Val_Acc: 85.222

Epoch 78: Validation loss decreased (0.373246 --> 0.373016).  Saving model ...
	 Train_Loss: 0.4046 Train_Acc: 84.138 Val_Loss: 0.3730  BEST VAL Loss: 0.3730  Val_Acc: 85.029

Epoch 79: Validation loss decreased (0.373016 --> 0.372779).  Saving model ...
	 Train_Loss: 0.4043 Train_Acc: 84.143 Val_Loss: 0.3728  BEST VAL Loss: 0.3728  Val_Acc: 85.243

Epoch 80: Validation loss decreased (0.372779 --> 0.372545).  Saving model ...
	 Train_Loss: 0.4040 Train_Acc: 84.141 Val_Loss: 0.3725  BEST VAL Loss: 0.3725  Val_Acc: 85.164

Epoch 81: Validation loss decreased (0.372545 --> 0.372328).  Saving model ...
	 Train_Loss: 0.4037 Train_Acc: 84.163 Val_Loss: 0.3723  BEST VAL Loss: 0.3723  Val_Acc: 85.041

Epoch 82: Validation loss decreased (0.372328 --> 0.372092).  Saving model ...
	 Train_Loss: 0.4034 Train_Acc: 84.169 Val_Loss: 0.3721  BEST VAL Loss: 0.3721  Val_Acc: 85.183

Epoch 83: Validation loss decreased (0.372092 --> 0.371873).  Saving model ...
	 Train_Loss: 0.4032 Train_Acc: 84.141 Val_Loss: 0.3719  BEST VAL Loss: 0.3719  Val_Acc: 84.943

Epoch 84: Validation loss decreased (0.371873 --> 0.371627).  Saving model ...
	 Train_Loss: 0.4029 Train_Acc: 84.142 Val_Loss: 0.3716  BEST VAL Loss: 0.3716  Val_Acc: 85.229

Epoch 85: Validation loss decreased (0.371627 --> 0.371395).  Saving model ...
	 Train_Loss: 0.4026 Train_Acc: 84.196 Val_Loss: 0.3714  BEST VAL Loss: 0.3714  Val_Acc: 85.301

Epoch 86: Validation loss decreased (0.371395 --> 0.371220).  Saving model ...
	 Train_Loss: 0.4023 Train_Acc: 84.223 Val_Loss: 0.3712  BEST VAL Loss: 0.3712  Val_Acc: 85.062

Epoch 87: Validation loss decreased (0.371220 --> 0.371020).  Saving model ...
	 Train_Loss: 0.4021 Train_Acc: 84.242 Val_Loss: 0.3710  BEST VAL Loss: 0.3710  Val_Acc: 85.121

Epoch 88: Validation loss decreased (0.371020 --> 0.370856).  Saving model ...
	 Train_Loss: 0.4018 Train_Acc: 84.204 Val_Loss: 0.3709  BEST VAL Loss: 0.3709  Val_Acc: 85.149

Epoch 89: Validation loss decreased (0.370856 --> 0.370656).  Saving model ...
	 Train_Loss: 0.4016 Train_Acc: 84.217 Val_Loss: 0.3707  BEST VAL Loss: 0.3707  Val_Acc: 85.234

Epoch 90: Validation loss decreased (0.370656 --> 0.370456).  Saving model ...
	 Train_Loss: 0.4013 Train_Acc: 84.245 Val_Loss: 0.3705  BEST VAL Loss: 0.3705  Val_Acc: 85.170

Epoch 91: Validation loss decreased (0.370456 --> 0.370287).  Saving model ...
	 Train_Loss: 0.4011 Train_Acc: 84.216 Val_Loss: 0.3703  BEST VAL Loss: 0.3703  Val_Acc: 85.277

Epoch 92: Validation loss decreased (0.370287 --> 0.370067).  Saving model ...
	 Train_Loss: 0.4008 Train_Acc: 84.227 Val_Loss: 0.3701  BEST VAL Loss: 0.3701  Val_Acc: 85.262

Epoch 93: Validation loss decreased (0.370067 --> 0.369899).  Saving model ...
	 Train_Loss: 0.4006 Train_Acc: 84.261 Val_Loss: 0.3699  BEST VAL Loss: 0.3699  Val_Acc: 85.209

Epoch 94: Validation loss decreased (0.369899 --> 0.369725).  Saving model ...
	 Train_Loss: 0.4003 Train_Acc: 84.262 Val_Loss: 0.3697  BEST VAL Loss: 0.3697  Val_Acc: 85.314

Epoch 95: Validation loss decreased (0.369725 --> 0.369535).  Saving model ...
	 Train_Loss: 0.4001 Train_Acc: 84.291 Val_Loss: 0.3695  BEST VAL Loss: 0.3695  Val_Acc: 85.228

Epoch 96: Validation loss decreased (0.369535 --> 0.369332).  Saving model ...
	 Train_Loss: 0.3998 Train_Acc: 84.279 Val_Loss: 0.3693  BEST VAL Loss: 0.3693  Val_Acc: 85.369

Epoch 97: Validation loss decreased (0.369332 --> 0.369147).  Saving model ...
	 Train_Loss: 0.3996 Train_Acc: 84.293 Val_Loss: 0.3691  BEST VAL Loss: 0.3691  Val_Acc: 85.305

Epoch 98: Validation loss decreased (0.369147 --> 0.368983).  Saving model ...
	 Train_Loss: 0.3994 Train_Acc: 84.309 Val_Loss: 0.3690  BEST VAL Loss: 0.3690  Val_Acc: 85.206

Epoch 99: Validation loss decreased (0.368983 --> 0.368820).  Saving model ...
	 Train_Loss: 0.3992 Train_Acc: 84.276 Val_Loss: 0.3688  BEST VAL Loss: 0.3688  Val_Acc: 85.211

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.04      0.04      0.04     51084
           1       0.51      0.52      0.52    722888
           2       0.45      0.43      0.44    638228

    accuracy                           0.47   1412200
   macro avg       0.33      0.33      0.33   1412200
weighted avg       0.47      0.47      0.47   1412200

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.72      0.75      0.74     12771
           1       0.86      0.88      0.87    180720
           2       0.85      0.82      0.84    159560

    accuracy                           0.85    353051
   macro avg       0.81      0.82      0.82    353051
weighted avg       0.85      0.85      0.85    353051

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.67      0.75      0.71     63855
           1       0.84      0.88      0.86    903610
           2       0.87      0.81      0.84    910217

    accuracy                           0.84   1877682
   macro avg       0.79      0.82      0.80   1877682
weighted avg       0.85      0.84      0.84   1877682

Precision for class 0: 0.6729420346781887
Recall for class 0: 0.7512332628611699
Precision for class 1: 0.8367393126571668
Recall for class 1: 0.8837706532685561
Precision for class 2: 0.8683846675696423
Recall for class 2: 0.8128413334402675
3
              precision    recall  f1-score   support

           0       0.67      0.75      0.71     63855
           1       0.84      0.88      0.86    903610
           2       0.87      0.81      0.84    910217

    accuracy                           0.84   1877682
   macro avg       0.79      0.82      0.80   1877682
weighted avg       0.85      0.84      0.84   1877682

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.65      0.08      0.14    112111
           1       0.28      0.60      0.38     66074
           2       0.79      0.86      0.82    240721

    accuracy                           0.61    418906
   macro avg       0.57      0.51      0.45    418906
weighted avg       0.67      0.61      0.57    418906

Precision for class 0: 0.6486227721313891
Recall for class 0: 0.0785560738910544
Precision for class 1: 0.27831450133714175
Recall for class 1: 0.6016738808003148
Precision for class 2: 0.7876267686657574
Recall for class 2: 0.8588407326323836
3
              precision    recall  f1-score   support

           0       0.65      0.08      0.14    112111
           1       0.28      0.60      0.38     66074
           2       0.79      0.86      0.82    240721

    accuracy                           0.61    418906
   macro avg       0.57      0.51      0.45    418906
weighted avg       0.67      0.61      0.57    418906

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.50      0.31      0.38     75619
           1       0.81      0.75      0.78    788818
           2       0.71      0.81      0.76    672406

    accuracy                           0.75   1536843
   macro avg       0.68      0.62      0.64   1536843
weighted avg       0.75      0.75      0.75   1536843

Precision for class 0: 0.50252024814751
Recall for class 0: 0.3085071212261469
Precision for class 1: 0.8142001603208606
Recall for class 1: 0.754556817922512
Precision for class 2: 0.7131349710621094
Recall for class 2: 0.8053824623813589
3
              precision    recall  f1-score   support

           0       0.50      0.31      0.38     75619
           1       0.81      0.75      0.78    788818
           2       0.71      0.81      0.76    672406

    accuracy                           0.75   1536843
   macro avg       0.68      0.62      0.64   1536843
weighted avg       0.75      0.75      0.75   1536843

Done
