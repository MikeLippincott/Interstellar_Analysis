[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '858ae97a'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '3c484321'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'b8cdf6c0'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '739a3e66'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_1.0_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (29870, 1276)
Number of total missing values across all columns: 0
Data Subset Is Off
Wells held out for testing: ['L16' 'K20']
Wells to use for training, validation, and testing ['K16' 'K17' 'L17' 'L20' 'K21' 'L21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.343615).  Saving model ...
	 Train_Loss: 0.4721 Train_Acc: 77.612 Val_Loss: 0.3436  BEST VAL Loss: 0.3436  Val_Acc: 85.914

Epoch 1: Validation loss decreased (0.343615 --> 0.302053).  Saving model ...
	 Train_Loss: 0.3956 Train_Acc: 85.735 Val_Loss: 0.3021  BEST VAL Loss: 0.3021  Val_Acc: 89.165

Epoch 2: Validation loss decreased (0.302053 --> 0.275469).  Saving model ...
	 Train_Loss: 0.3503 Train_Acc: 88.146 Val_Loss: 0.2755  BEST VAL Loss: 0.2755  Val_Acc: 91.061

Epoch 3: Validation loss decreased (0.275469 --> 0.256959).  Saving model ...
	 Train_Loss: 0.3173 Train_Acc: 90.161 Val_Loss: 0.2570  BEST VAL Loss: 0.2570  Val_Acc: 91.648

Epoch 4: Validation loss decreased (0.256959 --> 0.242601).  Saving model ...
	 Train_Loss: 0.2926 Train_Acc: 91.391 Val_Loss: 0.2426  BEST VAL Loss: 0.2426  Val_Acc: 93.228

Epoch 5: Validation loss decreased (0.242601 --> 0.233843).  Saving model ...
	 Train_Loss: 0.2738 Train_Acc: 92.001 Val_Loss: 0.2338  BEST VAL Loss: 0.2338  Val_Acc: 93.047

Epoch 6: Validation loss decreased (0.233843 --> 0.226970).  Saving model ...
	 Train_Loss: 0.2578 Train_Acc: 92.306 Val_Loss: 0.2270  BEST VAL Loss: 0.2270  Val_Acc: 93.138

Epoch 7: Validation loss decreased (0.226970 --> 0.222186).  Saving model ...
	 Train_Loss: 0.2450 Train_Acc: 92.662 Val_Loss: 0.2222  BEST VAL Loss: 0.2222  Val_Acc: 93.047

Epoch 8: Validation loss decreased (0.222186 --> 0.216623).  Saving model ...
	 Train_Loss: 0.2348 Train_Acc: 92.814 Val_Loss: 0.2166  BEST VAL Loss: 0.2166  Val_Acc: 93.860

Epoch 9: Validation loss decreased (0.216623 --> 0.214302).  Saving model ...
	 Train_Loss: 0.2255 Train_Acc: 93.141 Val_Loss: 0.2143  BEST VAL Loss: 0.2143  Val_Acc: 93.138

Epoch 10: Validation loss decreased (0.214302 --> 0.210731).  Saving model ...
	 Train_Loss: 0.2182 Train_Acc: 93.029 Val_Loss: 0.2107  BEST VAL Loss: 0.2107  Val_Acc: 92.822

Epoch 11: Validation loss decreased (0.210731 --> 0.207321).  Saving model ...
	 Train_Loss: 0.2119 Train_Acc: 93.294 Val_Loss: 0.2073  BEST VAL Loss: 0.2073  Val_Acc: 93.815

Epoch 12: Validation loss decreased (0.207321 --> 0.203764).  Saving model ...
	 Train_Loss: 0.2065 Train_Acc: 93.271 Val_Loss: 0.2038  BEST VAL Loss: 0.2038  Val_Acc: 93.995

Epoch 13: Validation loss decreased (0.203764 --> 0.201690).  Saving model ...
	 Train_Loss: 0.2018 Train_Acc: 93.187 Val_Loss: 0.2017  BEST VAL Loss: 0.2017  Val_Acc: 93.815

Epoch 14: Validation loss decreased (0.201690 --> 0.199996).  Saving model ...
	 Train_Loss: 0.1974 Train_Acc: 93.305 Val_Loss: 0.2000  BEST VAL Loss: 0.2000  Val_Acc: 93.815

Epoch 15: Validation loss decreased (0.199996 --> 0.199255).  Saving model ...
	 Train_Loss: 0.1927 Train_Acc: 93.898 Val_Loss: 0.1993  BEST VAL Loss: 0.1993  Val_Acc: 93.860

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.1885 Train_Acc: 94.062 Val_Loss: 0.1994  BEST VAL Loss: 0.1993  Val_Acc: 93.679

Epoch 17: Validation loss decreased (0.199255 --> 0.199110).  Saving model ...
	 Train_Loss: 0.1845 Train_Acc: 93.994 Val_Loss: 0.1991  BEST VAL Loss: 0.1991  Val_Acc: 94.131

Epoch 18: Validation loss decreased (0.199110 --> 0.199080).  Saving model ...
	 Train_Loss: 0.1811 Train_Acc: 94.107 Val_Loss: 0.1991  BEST VAL Loss: 0.1991  Val_Acc: 94.041

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1781 Train_Acc: 94.338 Val_Loss: 0.1994  BEST VAL Loss: 0.1991  Val_Acc: 93.634

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1755 Train_Acc: 94.016 Val_Loss: 0.1995  BEST VAL Loss: 0.1991  Val_Acc: 93.860

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1730 Train_Acc: 93.892 Val_Loss: 0.2011  BEST VAL Loss: 0.1991  Val_Acc: 93.544

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1711 Train_Acc: 93.740 Val_Loss: 0.2008  BEST VAL Loss: 0.1991  Val_Acc: 94.221

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1689 Train_Acc: 94.203 Val_Loss: 0.1994  BEST VAL Loss: 0.1991  Val_Acc: 94.312

Epoch 24: Validation loss decreased (0.199080 --> 0.198849).  Saving model ...
	 Train_Loss: 0.1666 Train_Acc: 94.400 Val_Loss: 0.1988  BEST VAL Loss: 0.1988  Val_Acc: 94.402

Epoch 25: Validation loss decreased (0.198849 --> 0.197477).  Saving model ...
	 Train_Loss: 0.1643 Train_Acc: 94.423 Val_Loss: 0.1975  BEST VAL Loss: 0.1975  Val_Acc: 94.492

Epoch 26: Validation loss decreased (0.197477 --> 0.196522).  Saving model ...
	 Train_Loss: 0.1620 Train_Acc: 94.976 Val_Loss: 0.1965  BEST VAL Loss: 0.1965  Val_Acc: 94.447

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1596 Train_Acc: 95.100 Val_Loss: 0.1966  BEST VAL Loss: 0.1965  Val_Acc: 94.944

Epoch 28: Validation loss decreased (0.196522 --> 0.195987).  Saving model ...
	 Train_Loss: 0.1575 Train_Acc: 95.061 Val_Loss: 0.1960  BEST VAL Loss: 0.1960  Val_Acc: 94.763

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1554 Train_Acc: 94.965 Val_Loss: 0.1966  BEST VAL Loss: 0.1960  Val_Acc: 94.944

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1534 Train_Acc: 95.303 Val_Loss: 0.1968  BEST VAL Loss: 0.1960  Val_Acc: 94.537

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1515 Train_Acc: 95.196 Val_Loss: 0.1974  BEST VAL Loss: 0.1960  Val_Acc: 94.673

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1497 Train_Acc: 95.247 Val_Loss: 0.1977  BEST VAL Loss: 0.1960  Val_Acc: 94.898

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1481 Train_Acc: 95.270 Val_Loss: 0.1976  BEST VAL Loss: 0.1960  Val_Acc: 94.492

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1464 Train_Acc: 95.332 Val_Loss: 0.1981  BEST VAL Loss: 0.1960  Val_Acc: 94.402

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1450 Train_Acc: 95.207 Val_Loss: 0.1994  BEST VAL Loss: 0.1960  Val_Acc: 94.266

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1438 Train_Acc: 94.897 Val_Loss: 0.1998  BEST VAL Loss: 0.1960  Val_Acc: 94.402

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1427 Train_Acc: 95.061 Val_Loss: 0.2000  BEST VAL Loss: 0.1960  Val_Acc: 94.131

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1415 Train_Acc: 95.021 Val_Loss: 0.2000  BEST VAL Loss: 0.1960  Val_Acc: 94.312

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1402 Train_Acc: 95.128 Val_Loss: 0.2004  BEST VAL Loss: 0.1960  Val_Acc: 93.995

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1390 Train_Acc: 95.196 Val_Loss: 0.2007  BEST VAL Loss: 0.1960  Val_Acc: 93.860

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1377 Train_Acc: 95.270 Val_Loss: 0.2013  BEST VAL Loss: 0.1960  Val_Acc: 94.492

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1366 Train_Acc: 95.117 Val_Loss: 0.2017  BEST VAL Loss: 0.1960  Val_Acc: 94.628

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1355 Train_Acc: 95.095 Val_Loss: 0.2022  BEST VAL Loss: 0.1960  Val_Acc: 94.673

Epoch 44: Validation loss did not decrease
Early stopped at epoch : 44
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.99      0.98      9777
           1       0.99      0.96      0.97      7938

    accuracy                           0.98     17715
   macro avg       0.98      0.98      0.98     17715
weighted avg       0.98      0.98      0.98     17715

LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.97      0.95      1222
           1       0.96      0.92      0.94       993

    accuracy                           0.95      2215
   macro avg       0.95      0.95      0.95      2215
weighted avg       0.95      0.95      0.95      2215

LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.96      0.95      1223
           1       0.95      0.92      0.94       992

    accuracy                           0.94      2215
   macro avg       0.94      0.94      0.94      2215
weighted avg       0.94      0.94      0.94      2215

              precision    recall  f1-score   support

           0       0.94      0.96      0.95      1223
           1       0.95      0.92      0.94       992

    accuracy                           0.94      2215
   macro avg       0.94      0.94      0.94      2215
weighted avg       0.94      0.94      0.94      2215

LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_3.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.79      0.94      0.86      3996
           1       0.92      0.73      0.81      3729

    accuracy                           0.84      7725
   macro avg       0.85      0.83      0.83      7725
weighted avg       0.85      0.84      0.83      7725

              precision    recall  f1-score   support

           0       0.79      0.94      0.86      3996
           1       0.92      0.73      0.81      3729

    accuracy                           0.84      7725
   macro avg       0.85      0.83      0.83      7725
weighted avg       0.85      0.84      0.83      7725

completed
