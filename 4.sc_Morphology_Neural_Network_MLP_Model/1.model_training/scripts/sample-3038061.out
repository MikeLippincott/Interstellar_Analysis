[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '692901f4'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '7e229e17'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '1c653ef0'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '3d8dd988'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_0.100_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_0.100_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_0.100_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (31162, 1276)
Number of total missing values across all columns: 35020
Data Subset Is Off
Wells held out for testing: ['C20' 'L16']
Wells to use for training, validation, and testing ['C16' 'C17' 'C21' 'L17' 'L20' 'L21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.218290).  Saving model ...
	 Train_Loss: 0.4604 Train_Acc: 73.545 Val_Loss: 0.2183  BEST VAL Loss: 0.2183  Val_Acc: 92.127

Epoch 1: Validation loss decreased (0.218290 --> 0.195751).  Saving model ...
	 Train_Loss: 0.3514 Train_Acc: 88.951 Val_Loss: 0.1958  BEST VAL Loss: 0.1958  Val_Acc: 93.345

Epoch 2: Validation loss decreased (0.195751 --> 0.175170).  Saving model ...
	 Train_Loss: 0.2957 Train_Acc: 92.768 Val_Loss: 0.1752  BEST VAL Loss: 0.1752  Val_Acc: 93.084

Epoch 3: Validation loss decreased (0.175170 --> 0.161200).  Saving model ...
	 Train_Loss: 0.2603 Train_Acc: 93.910 Val_Loss: 0.1612  BEST VAL Loss: 0.1612  Val_Acc: 94.867

Epoch 4: Validation loss decreased (0.161200 --> 0.151506).  Saving model ...
	 Train_Loss: 0.2341 Train_Acc: 95.144 Val_Loss: 0.1515  BEST VAL Loss: 0.1515  Val_Acc: 94.345

Epoch 5: Validation loss decreased (0.151506 --> 0.143914).  Saving model ...
	 Train_Loss: 0.2152 Train_Acc: 95.519 Val_Loss: 0.1439  BEST VAL Loss: 0.1439  Val_Acc: 95.520

Epoch 6: Validation loss decreased (0.143914 --> 0.139666).  Saving model ...
	 Train_Loss: 0.1993 Train_Acc: 95.927 Val_Loss: 0.1397  BEST VAL Loss: 0.1397  Val_Acc: 95.824

Epoch 7: Validation loss decreased (0.139666 --> 0.134486).  Saving model ...
	 Train_Loss: 0.1876 Train_Acc: 95.862 Val_Loss: 0.1345  BEST VAL Loss: 0.1345  Val_Acc: 96.085

Epoch 8: Validation loss decreased (0.134486 --> 0.130638).  Saving model ...
	 Train_Loss: 0.1778 Train_Acc: 96.194 Val_Loss: 0.1306  BEST VAL Loss: 0.1306  Val_Acc: 96.129

Epoch 9: Validation loss decreased (0.130638 --> 0.127956).  Saving model ...
	 Train_Loss: 0.1690 Train_Acc: 96.542 Val_Loss: 0.1280  BEST VAL Loss: 0.1280  Val_Acc: 96.912

Epoch 10: Validation loss decreased (0.127956 --> 0.125181).  Saving model ...
	 Train_Loss: 0.1610 Train_Acc: 97.042 Val_Loss: 0.1252  BEST VAL Loss: 0.1252  Val_Acc: 96.607

Epoch 11: Validation loss decreased (0.125181 --> 0.123030).  Saving model ...
	 Train_Loss: 0.1539 Train_Acc: 97.194 Val_Loss: 0.1230  BEST VAL Loss: 0.1230  Val_Acc: 96.607

Epoch 12: Validation loss decreased (0.123030 --> 0.122507).  Saving model ...
	 Train_Loss: 0.1484 Train_Acc: 96.862 Val_Loss: 0.1225  BEST VAL Loss: 0.1225  Val_Acc: 96.346

Epoch 13: Validation loss decreased (0.122507 --> 0.122113).  Saving model ...
	 Train_Loss: 0.1443 Train_Acc: 96.525 Val_Loss: 0.1221  BEST VAL Loss: 0.1221  Val_Acc: 96.694

Epoch 14: Validation loss decreased (0.122113 --> 0.120212).  Saving model ...
	 Train_Loss: 0.1398 Train_Acc: 97.145 Val_Loss: 0.1202  BEST VAL Loss: 0.1202  Val_Acc: 96.738

Epoch 15: Validation loss did not decrease
	 Train_Loss: 0.1355 Train_Acc: 97.537 Val_Loss: 0.1217  BEST VAL Loss: 0.1202  Val_Acc: 96.433

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.1323 Train_Acc: 97.308 Val_Loss: 0.1212  BEST VAL Loss: 0.1202  Val_Acc: 96.390

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.1292 Train_Acc: 96.998 Val_Loss: 0.1205  BEST VAL Loss: 0.1202  Val_Acc: 96.694

Epoch 18: Validation loss decreased (0.120212 --> 0.119252).  Saving model ...
	 Train_Loss: 0.1261 Train_Acc: 97.379 Val_Loss: 0.1193  BEST VAL Loss: 0.1193  Val_Acc: 96.781

Epoch 19: Validation loss decreased (0.119252 --> 0.118357).  Saving model ...
	 Train_Loss: 0.1233 Train_Acc: 97.482 Val_Loss: 0.1184  BEST VAL Loss: 0.1184  Val_Acc: 96.390

Epoch 20: Validation loss decreased (0.118357 --> 0.117578).  Saving model ...
	 Train_Loss: 0.1205 Train_Acc: 97.607 Val_Loss: 0.1176  BEST VAL Loss: 0.1176  Val_Acc: 96.825

Epoch 21: Validation loss decreased (0.117578 --> 0.116355).  Saving model ...
	 Train_Loss: 0.1182 Train_Acc: 97.439 Val_Loss: 0.1164  BEST VAL Loss: 0.1164  Val_Acc: 96.825

Epoch 22: Validation loss decreased (0.116355 --> 0.115015).  Saving model ...
	 Train_Loss: 0.1161 Train_Acc: 97.488 Val_Loss: 0.1150  BEST VAL Loss: 0.1150  Val_Acc: 96.694

Epoch 23: Validation loss decreased (0.115015 --> 0.114251).  Saving model ...
	 Train_Loss: 0.1139 Train_Acc: 97.667 Val_Loss: 0.1143  BEST VAL Loss: 0.1143  Val_Acc: 96.738

Epoch 24: Validation loss decreased (0.114251 --> 0.113338).  Saving model ...
	 Train_Loss: 0.1118 Train_Acc: 97.836 Val_Loss: 0.1133  BEST VAL Loss: 0.1133  Val_Acc: 96.955

Epoch 25: Validation loss decreased (0.113338 --> 0.112967).  Saving model ...
	 Train_Loss: 0.1100 Train_Acc: 97.591 Val_Loss: 0.1130  BEST VAL Loss: 0.1130  Val_Acc: 97.042

Epoch 26: Validation loss decreased (0.112967 --> 0.112779).  Saving model ...
	 Train_Loss: 0.1083 Train_Acc: 97.711 Val_Loss: 0.1128  BEST VAL Loss: 0.1128  Val_Acc: 96.738

Epoch 27: Validation loss decreased (0.112779 --> 0.112313).  Saving model ...
	 Train_Loss: 0.1068 Train_Acc: 97.803 Val_Loss: 0.1123  BEST VAL Loss: 0.1123  Val_Acc: 96.868

Epoch 28: Validation loss decreased (0.112313 --> 0.111866).  Saving model ...
	 Train_Loss: 0.1056 Train_Acc: 97.482 Val_Loss: 0.1119  BEST VAL Loss: 0.1119  Val_Acc: 96.651

Epoch 29: Validation loss decreased (0.111866 --> 0.111130).  Saving model ...
	 Train_Loss: 0.1041 Train_Acc: 97.809 Val_Loss: 0.1111  BEST VAL Loss: 0.1111  Val_Acc: 96.738

Epoch 30: Validation loss decreased (0.111130 --> 0.110807).  Saving model ...
	 Train_Loss: 0.1029 Train_Acc: 97.874 Val_Loss: 0.1108  BEST VAL Loss: 0.1108  Val_Acc: 96.738

Epoch 31: Validation loss decreased (0.110807 --> 0.110162).  Saving model ...
	 Train_Loss: 0.1017 Train_Acc: 97.732 Val_Loss: 0.1102  BEST VAL Loss: 0.1102  Val_Acc: 97.086

Epoch 32: Validation loss decreased (0.110162 --> 0.108881).  Saving model ...
	 Train_Loss: 0.1005 Train_Acc: 97.868 Val_Loss: 0.1089  BEST VAL Loss: 0.1089  Val_Acc: 97.129

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0992 Train_Acc: 98.091 Val_Loss: 0.1095  BEST VAL Loss: 0.1089  Val_Acc: 96.825

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0980 Train_Acc: 97.966 Val_Loss: 0.1102  BEST VAL Loss: 0.1089  Val_Acc: 96.781

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.0969 Train_Acc: 98.086 Val_Loss: 0.1095  BEST VAL Loss: 0.1089  Val_Acc: 97.216

Epoch 36: Validation loss decreased (0.108881 --> 0.108843).  Saving model ...
	 Train_Loss: 0.0958 Train_Acc: 98.135 Val_Loss: 0.1088  BEST VAL Loss: 0.1088  Val_Acc: 97.303

Epoch 37: Validation loss decreased (0.108843 --> 0.108652).  Saving model ...
	 Train_Loss: 0.0948 Train_Acc: 98.097 Val_Loss: 0.1087  BEST VAL Loss: 0.1087  Val_Acc: 97.129

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0938 Train_Acc: 98.184 Val_Loss: 0.1091  BEST VAL Loss: 0.1087  Val_Acc: 96.912

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0927 Train_Acc: 98.075 Val_Loss: 0.1092  BEST VAL Loss: 0.1087  Val_Acc: 97.390

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0919 Train_Acc: 97.993 Val_Loss: 0.1095  BEST VAL Loss: 0.1087  Val_Acc: 96.999

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0909 Train_Acc: 98.227 Val_Loss: 0.1096  BEST VAL Loss: 0.1087  Val_Acc: 97.260

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0899 Train_Acc: 98.341 Val_Loss: 0.1088  BEST VAL Loss: 0.1087  Val_Acc: 97.260

Epoch 43: Validation loss decreased (0.108652 --> 0.108445).  Saving model ...
	 Train_Loss: 0.0890 Train_Acc: 98.260 Val_Loss: 0.1084  BEST VAL Loss: 0.1084  Val_Acc: 97.303

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.0880 Train_Acc: 98.401 Val_Loss: 0.1091  BEST VAL Loss: 0.1084  Val_Acc: 96.694

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.0871 Train_Acc: 98.467 Val_Loss: 0.1087  BEST VAL Loss: 0.1084  Val_Acc: 97.086

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.0863 Train_Acc: 98.331 Val_Loss: 0.1085  BEST VAL Loss: 0.1084  Val_Acc: 97.303

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.0855 Train_Acc: 98.390 Val_Loss: 0.1085  BEST VAL Loss: 0.1084  Val_Acc: 96.955

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.0846 Train_Acc: 98.603 Val_Loss: 0.1094  BEST VAL Loss: 0.1084  Val_Acc: 96.955

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.0837 Train_Acc: 98.700 Val_Loss: 0.1098  BEST VAL Loss: 0.1084  Val_Acc: 97.216

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.0830 Train_Acc: 98.521 Val_Loss: 0.1106  BEST VAL Loss: 0.1084  Val_Acc: 97.086

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.0823 Train_Acc: 98.380 Val_Loss: 0.1104  BEST VAL Loss: 0.1084  Val_Acc: 96.999

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.0818 Train_Acc: 98.108 Val_Loss: 0.1095  BEST VAL Loss: 0.1084  Val_Acc: 96.999

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.0812 Train_Acc: 98.412 Val_Loss: 0.1094  BEST VAL Loss: 0.1084  Val_Acc: 96.651

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.0808 Train_Acc: 98.037 Val_Loss: 0.1092  BEST VAL Loss: 0.1084  Val_Acc: 97.042

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.0807 Train_Acc: 97.450 Val_Loss: 0.1091  BEST VAL Loss: 0.1084  Val_Acc: 97.129

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.0803 Train_Acc: 97.890 Val_Loss: 0.1091  BEST VAL Loss: 0.1084  Val_Acc: 97.173

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.0799 Train_Acc: 97.901 Val_Loss: 0.1090  BEST VAL Loss: 0.1084  Val_Acc: 97.521

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.0794 Train_Acc: 98.222 Val_Loss: 0.1098  BEST VAL Loss: 0.1084  Val_Acc: 97.129

Epoch 59: Validation loss did not decrease
Early stopped at epoch : 59
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.57      0.56      0.57     10451
           1       0.43      0.44      0.43      7939

    accuracy                           0.51     18390
   macro avg       0.50      0.50      0.50     18390
weighted avg       0.51      0.51      0.51     18390

LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.55      0.56      1307
           1       0.42      0.43      0.43       992

    accuracy                           0.50      2299
   macro avg       0.49      0.49      0.49      2299
weighted avg       0.50      0.50      0.50      2299

LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.54      0.54      1307
           1       0.40      0.40      0.40       992

    accuracy                           0.48      2299
   macro avg       0.47      0.47      0.47      2299
weighted avg       0.48      0.48      0.48      2299

              precision    recall  f1-score   support

           0       0.54      0.54      0.54      1307
           1       0.40      0.40      0.40       992

    accuracy                           0.48      2299
   macro avg       0.47      0.47      0.47      2299
weighted avg       0.48      0.48      0.48      2299

LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.57      0.56      4445
           1       0.46      0.44      0.45      3729

    accuracy                           0.51      8174
   macro avg       0.51      0.51      0.51      8174
weighted avg       0.51      0.51      0.51      8174

              precision    recall  f1-score   support

           0       0.55      0.57      0.56      4445
           1       0.46      0.44      0.45      3729

    accuracy                           0.51      8174
   macro avg       0.51      0.51      0.51      8174
weighted avg       0.51      0.51      0.51      8174

completed
