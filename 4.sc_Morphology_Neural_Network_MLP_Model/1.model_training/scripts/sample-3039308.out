[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '8427b136'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '3fbcfcb0'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '034961c7'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '7a019171'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_10.000_DMSO_0.025 treatment_name: H2O2_100.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_10.000_DMSO_0.025
TREATMENT_NAME: H2O2_100.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_10.000_DMSO_0.025' 'H2O2_100.000_DMSO_0.025']
The dimensions of the data are: (43710, 1276)
Number of total missing values across all columns: 87420
Data Subset Is Off
Wells held out for testing: ['E21' 'H22']
Wells to use for training, validation, and testing ['E16' 'E17' 'H18' 'H19' 'E20' 'H23' 'I18' 'I19' 'I22' 'I23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.606923).  Saving model ...
	 Train_Loss: 0.8070 Train_Acc: 61.947 Val_Loss: 0.6069  BEST VAL Loss: 0.6069  Val_Acc: 65.611

Epoch 1: Validation loss decreased (0.606923 --> 0.570583).  Saving model ...
	 Train_Loss: 0.6945 Train_Acc: 68.802 Val_Loss: 0.5706  BEST VAL Loss: 0.5706  Val_Acc: 72.681

Epoch 2: Validation loss decreased (0.570583 --> 0.546508).  Saving model ...
	 Train_Loss: 0.6398 Train_Acc: 72.330 Val_Loss: 0.5465  BEST VAL Loss: 0.5465  Val_Acc: 74.350

Epoch 3: Validation loss decreased (0.546508 --> 0.528414).  Saving model ...
	 Train_Loss: 0.6043 Train_Acc: 74.430 Val_Loss: 0.5284  BEST VAL Loss: 0.5284  Val_Acc: 76.697

Epoch 4: Validation loss decreased (0.528414 --> 0.515244).  Saving model ...
	 Train_Loss: 0.5803 Train_Acc: 75.384 Val_Loss: 0.5152  BEST VAL Loss: 0.5152  Val_Acc: 77.008

Epoch 5: Validation loss decreased (0.515244 --> 0.503620).  Saving model ...
	 Train_Loss: 0.5620 Train_Acc: 76.159 Val_Loss: 0.5036  BEST VAL Loss: 0.5036  Val_Acc: 77.913

Epoch 6: Validation loss decreased (0.503620 --> 0.495759).  Saving model ...
	 Train_Loss: 0.5467 Train_Acc: 76.834 Val_Loss: 0.4958  BEST VAL Loss: 0.4958  Val_Acc: 77.856

Epoch 7: Validation loss decreased (0.495759 --> 0.490360).  Saving model ...
	 Train_Loss: 0.5345 Train_Acc: 77.124 Val_Loss: 0.4904  BEST VAL Loss: 0.4904  Val_Acc: 77.687

Epoch 8: Validation loss decreased (0.490360 --> 0.484372).  Saving model ...
	 Train_Loss: 0.5247 Train_Acc: 77.481 Val_Loss: 0.4844  BEST VAL Loss: 0.4844  Val_Acc: 78.479

Epoch 9: Validation loss decreased (0.484372 --> 0.478888).  Saving model ...
	 Train_Loss: 0.5160 Train_Acc: 78.057 Val_Loss: 0.4789  BEST VAL Loss: 0.4789  Val_Acc: 79.186

Epoch 10: Validation loss decreased (0.478888 --> 0.474661).  Saving model ...
	 Train_Loss: 0.5092 Train_Acc: 77.619 Val_Loss: 0.4747  BEST VAL Loss: 0.4747  Val_Acc: 79.242

Epoch 11: Validation loss decreased (0.474661 --> 0.470749).  Saving model ...
	 Train_Loss: 0.5029 Train_Acc: 78.202 Val_Loss: 0.4707  BEST VAL Loss: 0.4707  Val_Acc: 78.676

Epoch 12: Validation loss decreased (0.470749 --> 0.467074).  Saving model ...
	 Train_Loss: 0.4966 Train_Acc: 78.538 Val_Loss: 0.4671  BEST VAL Loss: 0.4671  Val_Acc: 79.751

Epoch 13: Validation loss decreased (0.467074 --> 0.464485).  Saving model ...
	 Train_Loss: 0.4916 Train_Acc: 78.181 Val_Loss: 0.4645  BEST VAL Loss: 0.4645  Val_Acc: 79.327

Epoch 14: Validation loss decreased (0.464485 --> 0.462053).  Saving model ...
	 Train_Loss: 0.4870 Train_Acc: 78.531 Val_Loss: 0.4621  BEST VAL Loss: 0.4621  Val_Acc: 79.355

Epoch 15: Validation loss decreased (0.462053 --> 0.459734).  Saving model ...
	 Train_Loss: 0.4828 Train_Acc: 78.640 Val_Loss: 0.4597  BEST VAL Loss: 0.4597  Val_Acc: 78.903

Epoch 16: Validation loss decreased (0.459734 --> 0.458115).  Saving model ...
	 Train_Loss: 0.4788 Train_Acc: 78.817 Val_Loss: 0.4581  BEST VAL Loss: 0.4581  Val_Acc: 78.620

Epoch 17: Validation loss decreased (0.458115 --> 0.456129).  Saving model ...
	 Train_Loss: 0.4749 Train_Acc: 79.309 Val_Loss: 0.4561  BEST VAL Loss: 0.4561  Val_Acc: 79.695

Epoch 18: Validation loss decreased (0.456129 --> 0.454568).  Saving model ...
	 Train_Loss: 0.4715 Train_Acc: 79.185 Val_Loss: 0.4546  BEST VAL Loss: 0.4546  Val_Acc: 79.638

Epoch 19: Validation loss decreased (0.454568 --> 0.452451).  Saving model ...
	 Train_Loss: 0.4683 Train_Acc: 79.195 Val_Loss: 0.4525  BEST VAL Loss: 0.4525  Val_Acc: 80.119

Epoch 20: Validation loss decreased (0.452451 --> 0.451024).  Saving model ...
	 Train_Loss: 0.4654 Train_Acc: 79.832 Val_Loss: 0.4510  BEST VAL Loss: 0.4510  Val_Acc: 79.695

Epoch 21: Validation loss decreased (0.451024 --> 0.449919).  Saving model ...
	 Train_Loss: 0.4628 Train_Acc: 79.185 Val_Loss: 0.4499  BEST VAL Loss: 0.4499  Val_Acc: 79.666

Epoch 22: Validation loss decreased (0.449919 --> 0.448166).  Saving model ...
	 Train_Loss: 0.4602 Train_Acc: 79.506 Val_Loss: 0.4482  BEST VAL Loss: 0.4482  Val_Acc: 80.317

Epoch 23: Validation loss decreased (0.448166 --> 0.446961).  Saving model ...
	 Train_Loss: 0.4575 Train_Acc: 80.115 Val_Loss: 0.4470  BEST VAL Loss: 0.4470  Val_Acc: 79.553

Epoch 24: Validation loss decreased (0.446961 --> 0.446095).  Saving model ...
	 Train_Loss: 0.4551 Train_Acc: 79.627 Val_Loss: 0.4461  BEST VAL Loss: 0.4461  Val_Acc: 79.864

Epoch 25: Validation loss decreased (0.446095 --> 0.445189).  Saving model ...
	 Train_Loss: 0.4530 Train_Acc: 79.340 Val_Loss: 0.4452  BEST VAL Loss: 0.4452  Val_Acc: 79.921

Epoch 26: Validation loss decreased (0.445189 --> 0.444611).  Saving model ...
	 Train_Loss: 0.4508 Train_Acc: 79.849 Val_Loss: 0.4446  BEST VAL Loss: 0.4446  Val_Acc: 79.101

Epoch 27: Validation loss decreased (0.444611 --> 0.443308).  Saving model ...
	 Train_Loss: 0.4488 Train_Acc: 79.970 Val_Loss: 0.4433  BEST VAL Loss: 0.4433  Val_Acc: 80.543

Epoch 28: Validation loss decreased (0.443308 --> 0.442509).  Saving model ...
	 Train_Loss: 0.4468 Train_Acc: 80.079 Val_Loss: 0.4425  BEST VAL Loss: 0.4425  Val_Acc: 79.581

Epoch 29: Validation loss decreased (0.442509 --> 0.441952).  Saving model ...
	 Train_Loss: 0.4449 Train_Acc: 80.465 Val_Loss: 0.4420  BEST VAL Loss: 0.4420  Val_Acc: 79.497

Epoch 30: Validation loss decreased (0.441952 --> 0.441271).  Saving model ...
	 Train_Loss: 0.4431 Train_Acc: 79.998 Val_Loss: 0.4413  BEST VAL Loss: 0.4413  Val_Acc: 79.893

Epoch 31: Validation loss decreased (0.441271 --> 0.440434).  Saving model ...
	 Train_Loss: 0.4414 Train_Acc: 80.054 Val_Loss: 0.4404  BEST VAL Loss: 0.4404  Val_Acc: 80.288

Epoch 32: Validation loss decreased (0.440434 --> 0.439487).  Saving model ...
	 Train_Loss: 0.4397 Train_Acc: 80.079 Val_Loss: 0.4395  BEST VAL Loss: 0.4395  Val_Acc: 80.119

Epoch 33: Validation loss decreased (0.439487 --> 0.438871).  Saving model ...
	 Train_Loss: 0.4380 Train_Acc: 80.366 Val_Loss: 0.4389  BEST VAL Loss: 0.4389  Val_Acc: 80.232

Epoch 34: Validation loss decreased (0.438871 --> 0.438179).  Saving model ...
	 Train_Loss: 0.4365 Train_Acc: 80.217 Val_Loss: 0.4382  BEST VAL Loss: 0.4382  Val_Acc: 79.949

Epoch 35: Validation loss decreased (0.438179 --> 0.437750).  Saving model ...
	 Train_Loss: 0.4350 Train_Acc: 80.553 Val_Loss: 0.4377  BEST VAL Loss: 0.4377  Val_Acc: 80.345

Epoch 36: Validation loss decreased (0.437750 --> 0.436953).  Saving model ...
	 Train_Loss: 0.4336 Train_Acc: 80.938 Val_Loss: 0.4370  BEST VAL Loss: 0.4370  Val_Acc: 79.808

Epoch 37: Validation loss decreased (0.436953 --> 0.436292).  Saving model ...
	 Train_Loss: 0.4322 Train_Acc: 80.369 Val_Loss: 0.4363  BEST VAL Loss: 0.4363  Val_Acc: 80.288

Epoch 38: Validation loss decreased (0.436292 --> 0.435618).  Saving model ...
	 Train_Loss: 0.4309 Train_Acc: 80.334 Val_Loss: 0.4356  BEST VAL Loss: 0.4356  Val_Acc: 80.571

Epoch 39: Validation loss decreased (0.435618 --> 0.434854).  Saving model ...
	 Train_Loss: 0.4296 Train_Acc: 80.369 Val_Loss: 0.4349  BEST VAL Loss: 0.4349  Val_Acc: 80.402

Epoch 40: Validation loss decreased (0.434854 --> 0.434324).  Saving model ...
	 Train_Loss: 0.4285 Train_Acc: 80.510 Val_Loss: 0.4343  BEST VAL Loss: 0.4343  Val_Acc: 80.628

Epoch 41: Validation loss decreased (0.434324 --> 0.433620).  Saving model ...
	 Train_Loss: 0.4273 Train_Acc: 80.829 Val_Loss: 0.4336  BEST VAL Loss: 0.4336  Val_Acc: 80.430

Epoch 42: Validation loss decreased (0.433620 --> 0.433590).  Saving model ...
	 Train_Loss: 0.4261 Train_Acc: 80.503 Val_Loss: 0.4336  BEST VAL Loss: 0.4336  Val_Acc: 80.515

Epoch 43: Validation loss decreased (0.433590 --> 0.433350).  Saving model ...
	 Train_Loss: 0.4250 Train_Acc: 80.507 Val_Loss: 0.4334  BEST VAL Loss: 0.4334  Val_Acc: 80.458

Epoch 44: Validation loss decreased (0.433350 --> 0.432970).  Saving model ...
	 Train_Loss: 0.4240 Train_Acc: 80.518 Val_Loss: 0.4330  BEST VAL Loss: 0.4330  Val_Acc: 80.034

Epoch 45: Validation loss decreased (0.432970 --> 0.432650).  Saving model ...
	 Train_Loss: 0.4229 Train_Acc: 80.896 Val_Loss: 0.4326  BEST VAL Loss: 0.4326  Val_Acc: 80.430

Epoch 46: Validation loss decreased (0.432650 --> 0.432463).  Saving model ...
	 Train_Loss: 0.4217 Train_Acc: 80.613 Val_Loss: 0.4325  BEST VAL Loss: 0.4325  Val_Acc: 80.571

Epoch 47: Validation loss decreased (0.432463 --> 0.432215).  Saving model ...
	 Train_Loss: 0.4207 Train_Acc: 80.807 Val_Loss: 0.4322  BEST VAL Loss: 0.4322  Val_Acc: 80.458

Epoch 48: Validation loss decreased (0.432215 --> 0.431712).  Saving model ...
	 Train_Loss: 0.4196 Train_Acc: 81.324 Val_Loss: 0.4317  BEST VAL Loss: 0.4317  Val_Acc: 80.062

Epoch 49: Validation loss decreased (0.431712 --> 0.431288).  Saving model ...
	 Train_Loss: 0.4186 Train_Acc: 81.055 Val_Loss: 0.4313  BEST VAL Loss: 0.4313  Val_Acc: 80.402

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.4175 Train_Acc: 80.860 Val_Loss: 0.4313  BEST VAL Loss: 0.4313  Val_Acc: 80.854

Epoch 51: Validation loss decreased (0.431288 --> 0.430919).  Saving model ...
	 Train_Loss: 0.4164 Train_Acc: 80.885 Val_Loss: 0.4309  BEST VAL Loss: 0.4309  Val_Acc: 80.373

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.4153 Train_Acc: 81.080 Val_Loss: 0.4309  BEST VAL Loss: 0.4309  Val_Acc: 80.373

Epoch 53: Validation loss decreased (0.430919 --> 0.430626).  Saving model ...
	 Train_Loss: 0.4143 Train_Acc: 81.408 Val_Loss: 0.4306  BEST VAL Loss: 0.4306  Val_Acc: 79.921

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.4135 Train_Acc: 80.691 Val_Loss: 0.4309  BEST VAL Loss: 0.4306  Val_Acc: 79.723

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.4126 Train_Acc: 80.818 Val_Loss: 0.4313  BEST VAL Loss: 0.4306  Val_Acc: 79.949

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.4117 Train_Acc: 81.150 Val_Loss: 0.4313  BEST VAL Loss: 0.4306  Val_Acc: 80.317

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.4109 Train_Acc: 80.832 Val_Loss: 0.4311  BEST VAL Loss: 0.4306  Val_Acc: 81.165

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.4101 Train_Acc: 80.938 Val_Loss: 0.4308  BEST VAL Loss: 0.4306  Val_Acc: 80.656

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.4091 Train_Acc: 81.412 Val_Loss: 0.4307  BEST VAL Loss: 0.4306  Val_Acc: 80.515

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.4082 Train_Acc: 81.744 Val_Loss: 0.4309  BEST VAL Loss: 0.4306  Val_Acc: 80.543

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.4074 Train_Acc: 81.550 Val_Loss: 0.4309  BEST VAL Loss: 0.4306  Val_Acc: 80.345

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.4066 Train_Acc: 81.182 Val_Loss: 0.4308  BEST VAL Loss: 0.4306  Val_Acc: 80.430

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.4058 Train_Acc: 81.500 Val_Loss: 0.4308  BEST VAL Loss: 0.4306  Val_Acc: 80.600

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.4049 Train_Acc: 81.334 Val_Loss: 0.4311  BEST VAL Loss: 0.4306  Val_Acc: 80.882

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.4043 Train_Acc: 80.931 Val_Loss: 0.4310  BEST VAL Loss: 0.4306  Val_Acc: 80.062

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.4035 Train_Acc: 81.511 Val_Loss: 0.4311  BEST VAL Loss: 0.4306  Val_Acc: 79.779

Epoch 67: Validation loss did not decrease
	 Train_Loss: 0.4027 Train_Acc: 81.260 Val_Loss: 0.4311  BEST VAL Loss: 0.4306  Val_Acc: 80.402

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.4019 Train_Acc: 81.514 Val_Loss: 0.4313  BEST VAL Loss: 0.4306  Val_Acc: 79.751

Epoch 69: Validation loss did not decrease
Early stopped at epoch : 69
LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.64      0.69      0.66     18174
           1       0.36      0.31      0.33     10113

    accuracy                           0.55     28287
   macro avg       0.50      0.50      0.50     28287
weighted avg       0.54      0.55      0.54     28287

LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.64      0.70      0.67      2272
           1       0.35      0.29      0.32      1264

    accuracy                           0.55      3536
   macro avg       0.50      0.50      0.49      3536
weighted avg       0.54      0.55      0.54      3536

LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.64      0.70      0.67      2272
           1       0.35      0.29      0.32      1265

    accuracy                           0.55      3537
   macro avg       0.50      0.50      0.49      3537
weighted avg       0.54      0.55      0.54      3537

              precision    recall  f1-score   support

           0       0.64      0.70      0.67      2272
           1       0.35      0.29      0.32      1265

    accuracy                           0.55      3537
   macro avg       0.50      0.50      0.49      3537
weighted avg       0.54      0.55      0.54      3537

LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.61      0.55      4182
           1       0.51      0.41      0.46      4168

    accuracy                           0.51      8350
   macro avg       0.51      0.51      0.50      8350
weighted avg       0.51      0.51      0.50      8350

              precision    recall  f1-score   support

           0       0.51      0.61      0.55      4182
           1       0.51      0.41      0.46      4168

    accuracy                           0.51      8350
   macro avg       0.51      0.51      0.50      8350
weighted avg       0.51      0.51      0.50      8350

completed
