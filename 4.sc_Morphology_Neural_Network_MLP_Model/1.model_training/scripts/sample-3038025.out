[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '18ac2acd'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'af3b2e48'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '9e2bfec3'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'af525db1'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_Nigericin_1.000_3.0_DMSO_0.025'
 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (355427, 1270)
Number of total missing values across all columns: 0
Data Subset Is Off
Wells held out for testing: ['M08' 'L09']
Wells to use for training, validation, and testing ['L02' 'M02' 'L03' 'M03' 'L08' 'M09']
Number of in features:  1245
Number of out features:  2
Binary_Classification
SGD
Epoch 0: Validation loss decreased (inf --> 0.684123).  Saving model ...
	 Train_Loss: 0.6894 Train_Acc: 53.678 Val_Loss: 0.6841  BEST VAL Loss: 0.6841  Val_Acc: 55.498

Epoch 1: Validation loss decreased (0.684123 --> 0.680718).  Saving model ...
	 Train_Loss: 0.6859 Train_Acc: 56.066 Val_Loss: 0.6807  BEST VAL Loss: 0.6807  Val_Acc: 57.458

Epoch 2: Validation loss decreased (0.680718 --> 0.676639).  Saving model ...
	 Train_Loss: 0.6821 Train_Acc: 57.226 Val_Loss: 0.6766  BEST VAL Loss: 0.6766  Val_Acc: 59.205

Epoch 3: Validation loss decreased (0.676639 --> 0.672562).  Saving model ...
	 Train_Loss: 0.6785 Train_Acc: 58.196 Val_Loss: 0.6726  BEST VAL Loss: 0.6726  Val_Acc: 60.423

Epoch 4: Validation loss decreased (0.672562 --> 0.668645).  Saving model ...
	 Train_Loss: 0.6751 Train_Acc: 59.080 Val_Loss: 0.6686  BEST VAL Loss: 0.6686  Val_Acc: 61.296

Epoch 5: Validation loss decreased (0.668645 --> 0.665267).  Saving model ...
	 Train_Loss: 0.6719 Train_Acc: 59.557 Val_Loss: 0.6653  BEST VAL Loss: 0.6653  Val_Acc: 62.667

Epoch 6: Validation loss decreased (0.665267 --> 0.662160).  Saving model ...
	 Train_Loss: 0.6692 Train_Acc: 60.116 Val_Loss: 0.6622  BEST VAL Loss: 0.6622  Val_Acc: 63.629

Epoch 7: Validation loss decreased (0.662160 --> 0.659241).  Saving model ...
	 Train_Loss: 0.6666 Train_Acc: 60.647 Val_Loss: 0.6592  BEST VAL Loss: 0.6592  Val_Acc: 63.899

Epoch 8: Validation loss decreased (0.659241 --> 0.656572).  Saving model ...
	 Train_Loss: 0.6643 Train_Acc: 60.936 Val_Loss: 0.6566  BEST VAL Loss: 0.6566  Val_Acc: 64.817

Epoch 9: Validation loss decreased (0.656572 --> 0.654288).  Saving model ...
	 Train_Loss: 0.6621 Train_Acc: 61.359 Val_Loss: 0.6543  BEST VAL Loss: 0.6543  Val_Acc: 64.437

Epoch 10: Validation loss decreased (0.654288 --> 0.652105).  Saving model ...
	 Train_Loss: 0.6602 Train_Acc: 61.585 Val_Loss: 0.6521  BEST VAL Loss: 0.6521  Val_Acc: 65.146

Epoch 11: Validation loss decreased (0.652105 --> 0.649911).  Saving model ...
	 Train_Loss: 0.6584 Train_Acc: 61.819 Val_Loss: 0.6499  BEST VAL Loss: 0.6499  Val_Acc: 65.336

Epoch 12: Validation loss decreased (0.649911 --> 0.647883).  Saving model ...
	 Train_Loss: 0.6567 Train_Acc: 61.879 Val_Loss: 0.6479  BEST VAL Loss: 0.6479  Val_Acc: 65.859

Epoch 13: Validation loss decreased (0.647883 --> 0.646005).  Saving model ...
	 Train_Loss: 0.6552 Train_Acc: 62.037 Val_Loss: 0.6460  BEST VAL Loss: 0.6460  Val_Acc: 66.027

Epoch 14: Validation loss decreased (0.646005 --> 0.644188).  Saving model ...
	 Train_Loss: 0.6537 Train_Acc: 62.381 Val_Loss: 0.6442  BEST VAL Loss: 0.6442  Val_Acc: 66.100

Epoch 15: Validation loss decreased (0.644188 --> 0.642648).  Saving model ...
	 Train_Loss: 0.6524 Train_Acc: 62.563 Val_Loss: 0.6426  BEST VAL Loss: 0.6426  Val_Acc: 65.928

Epoch 16: Validation loss decreased (0.642648 --> 0.641031).  Saving model ...
	 Train_Loss: 0.6510 Train_Acc: 62.847 Val_Loss: 0.6410  BEST VAL Loss: 0.6410  Val_Acc: 66.257

Epoch 17: Validation loss decreased (0.641031 --> 0.639671).  Saving model ...
	 Train_Loss: 0.6498 Train_Acc: 62.945 Val_Loss: 0.6397  BEST VAL Loss: 0.6397  Val_Acc: 65.903

Epoch 18: Validation loss decreased (0.639671 --> 0.638217).  Saving model ...
	 Train_Loss: 0.6486 Train_Acc: 63.132 Val_Loss: 0.6382  BEST VAL Loss: 0.6382  Val_Acc: 66.605

Epoch 19: Validation loss decreased (0.638217 --> 0.636907).  Saving model ...
	 Train_Loss: 0.6475 Train_Acc: 62.993 Val_Loss: 0.6369  BEST VAL Loss: 0.6369  Val_Acc: 66.553

Epoch 20: Validation loss decreased (0.636907 --> 0.635758).  Saving model ...
	 Train_Loss: 0.6464 Train_Acc: 63.366 Val_Loss: 0.6358  BEST VAL Loss: 0.6358  Val_Acc: 66.334

Epoch 21: Validation loss decreased (0.635758 --> 0.634538).  Saving model ...
	 Train_Loss: 0.6454 Train_Acc: 63.318 Val_Loss: 0.6345  BEST VAL Loss: 0.6345  Val_Acc: 66.736

Epoch 22: Validation loss decreased (0.634538 --> 0.633584).  Saving model ...
	 Train_Loss: 0.6445 Train_Acc: 63.460 Val_Loss: 0.6336  BEST VAL Loss: 0.6336  Val_Acc: 66.469

Epoch 23: Validation loss decreased (0.633584 --> 0.632462).  Saving model ...
	 Train_Loss: 0.6436 Train_Acc: 63.568 Val_Loss: 0.6325  BEST VAL Loss: 0.6325  Val_Acc: 66.813

Epoch 24: Validation loss decreased (0.632462 --> 0.631365).  Saving model ...
	 Train_Loss: 0.6427 Train_Acc: 63.802 Val_Loss: 0.6314  BEST VAL Loss: 0.6314  Val_Acc: 67.036

Epoch 25: Validation loss decreased (0.631365 --> 0.630423).  Saving model ...
	 Train_Loss: 0.6418 Train_Acc: 63.763 Val_Loss: 0.6304  BEST VAL Loss: 0.6304  Val_Acc: 66.886

Epoch 26: Validation loss decreased (0.630423 --> 0.629520).  Saving model ...
	 Train_Loss: 0.6410 Train_Acc: 64.043 Val_Loss: 0.6295  BEST VAL Loss: 0.6295  Val_Acc: 67.007

Epoch 27: Validation loss decreased (0.629520 --> 0.628662).  Saving model ...
	 Train_Loss: 0.6402 Train_Acc: 64.193 Val_Loss: 0.6287  BEST VAL Loss: 0.6287  Val_Acc: 67.230

Epoch 28: Validation loss decreased (0.628662 --> 0.627773).  Saving model ...
	 Train_Loss: 0.6394 Train_Acc: 64.035 Val_Loss: 0.6278  BEST VAL Loss: 0.6278  Val_Acc: 67.186

Epoch 29: Validation loss decreased (0.627773 --> 0.626957).  Saving model ...
	 Train_Loss: 0.6387 Train_Acc: 64.244 Val_Loss: 0.6270  BEST VAL Loss: 0.6270  Val_Acc: 67.372

Epoch 30: Validation loss decreased (0.626957 --> 0.626163).  Saving model ...
	 Train_Loss: 0.6380 Train_Acc: 64.361 Val_Loss: 0.6262  BEST VAL Loss: 0.6262  Val_Acc: 67.581

Epoch 31: Validation loss decreased (0.626163 --> 0.625457).  Saving model ...
	 Train_Loss: 0.6373 Train_Acc: 64.336 Val_Loss: 0.6255  BEST VAL Loss: 0.6255  Val_Acc: 67.127

Epoch 32: Validation loss decreased (0.625457 --> 0.624701).  Saving model ...
	 Train_Loss: 0.6366 Train_Acc: 64.462 Val_Loss: 0.6247  BEST VAL Loss: 0.6247  Val_Acc: 67.369

Epoch 33: Validation loss decreased (0.624701 --> 0.623982).  Saving model ...
	 Train_Loss: 0.6359 Train_Acc: 64.486 Val_Loss: 0.6240  BEST VAL Loss: 0.6240  Val_Acc: 67.160

Epoch 34: Validation loss decreased (0.623982 --> 0.623276).  Saving model ...
	 Train_Loss: 0.6353 Train_Acc: 64.541 Val_Loss: 0.6233  BEST VAL Loss: 0.6233  Val_Acc: 67.533

Epoch 35: Validation loss decreased (0.623276 --> 0.622668).  Saving model ...
	 Train_Loss: 0.6346 Train_Acc: 64.612 Val_Loss: 0.6227  BEST VAL Loss: 0.6227  Val_Acc: 67.270

Epoch 36: Validation loss decreased (0.622668 --> 0.622044).  Saving model ...
	 Train_Loss: 0.6340 Train_Acc: 64.789 Val_Loss: 0.6220  BEST VAL Loss: 0.6220  Val_Acc: 67.065

Epoch 37: Validation loss decreased (0.622044 --> 0.621319).  Saving model ...
	 Train_Loss: 0.6334 Train_Acc: 64.781 Val_Loss: 0.6213  BEST VAL Loss: 0.6213  Val_Acc: 67.902

Epoch 38: Validation loss decreased (0.621319 --> 0.620690).  Saving model ...
	 Train_Loss: 0.6328 Train_Acc: 64.741 Val_Loss: 0.6207  BEST VAL Loss: 0.6207  Val_Acc: 67.690

Epoch 39: Validation loss decreased (0.620690 --> 0.620011).  Saving model ...
	 Train_Loss: 0.6322 Train_Acc: 64.827 Val_Loss: 0.6200  BEST VAL Loss: 0.6200  Val_Acc: 68.114

Epoch 40: Validation loss decreased (0.620011 --> 0.619357).  Saving model ...
	 Train_Loss: 0.6317 Train_Acc: 64.810 Val_Loss: 0.6194  BEST VAL Loss: 0.6194  Val_Acc: 67.990

Epoch 41: Validation loss decreased (0.619357 --> 0.618738).  Saving model ...
	 Train_Loss: 0.6311 Train_Acc: 64.794 Val_Loss: 0.6187  BEST VAL Loss: 0.6187  Val_Acc: 68.231

Epoch 42: Validation loss decreased (0.618738 --> 0.618211).  Saving model ...
	 Train_Loss: 0.6306 Train_Acc: 65.048 Val_Loss: 0.6182  BEST VAL Loss: 0.6182  Val_Acc: 67.654

Epoch 43: Validation loss decreased (0.618211 --> 0.617633).  Saving model ...
	 Train_Loss: 0.6301 Train_Acc: 64.943 Val_Loss: 0.6176  BEST VAL Loss: 0.6176  Val_Acc: 67.720

Epoch 44: Validation loss decreased (0.617633 --> 0.617132).  Saving model ...
	 Train_Loss: 0.6295 Train_Acc: 65.067 Val_Loss: 0.6171  BEST VAL Loss: 0.6171  Val_Acc: 67.120

Epoch 45: Validation loss decreased (0.617132 --> 0.616557).  Saving model ...
	 Train_Loss: 0.6290 Train_Acc: 65.140 Val_Loss: 0.6166  BEST VAL Loss: 0.6166  Val_Acc: 67.994

Epoch 46: Validation loss decreased (0.616557 --> 0.615965).  Saving model ...
	 Train_Loss: 0.6285 Train_Acc: 65.103 Val_Loss: 0.6160  BEST VAL Loss: 0.6160  Val_Acc: 68.060

Epoch 47: Validation loss decreased (0.615965 --> 0.615440).  Saving model ...
	 Train_Loss: 0.6281 Train_Acc: 65.123 Val_Loss: 0.6154  BEST VAL Loss: 0.6154  Val_Acc: 68.001

Epoch 48: Validation loss decreased (0.615440 --> 0.614914).  Saving model ...
	 Train_Loss: 0.6276 Train_Acc: 64.985 Val_Loss: 0.6149  BEST VAL Loss: 0.6149  Val_Acc: 67.687

Epoch 49: Validation loss decreased (0.614914 --> 0.614390).  Saving model ...
	 Train_Loss: 0.6271 Train_Acc: 65.212 Val_Loss: 0.6144  BEST VAL Loss: 0.6144  Val_Acc: 68.177

Epoch 50: Validation loss decreased (0.614390 --> 0.613844).  Saving model ...
	 Train_Loss: 0.6267 Train_Acc: 65.281 Val_Loss: 0.6138  BEST VAL Loss: 0.6138  Val_Acc: 68.166

Epoch 51: Validation loss decreased (0.613844 --> 0.613307).  Saving model ...
	 Train_Loss: 0.6262 Train_Acc: 65.285 Val_Loss: 0.6133  BEST VAL Loss: 0.6133  Val_Acc: 68.480

Epoch 52: Validation loss decreased (0.613307 --> 0.612817).  Saving model ...
	 Train_Loss: 0.6258 Train_Acc: 65.249 Val_Loss: 0.6128  BEST VAL Loss: 0.6128  Val_Acc: 68.228

Epoch 53: Validation loss decreased (0.612817 --> 0.612318).  Saving model ...
	 Train_Loss: 0.6254 Train_Acc: 65.148 Val_Loss: 0.6123  BEST VAL Loss: 0.6123  Val_Acc: 68.337

Epoch 54: Validation loss decreased (0.612318 --> 0.611821).  Saving model ...
	 Train_Loss: 0.6249 Train_Acc: 65.155 Val_Loss: 0.6118  BEST VAL Loss: 0.6118  Val_Acc: 68.169

Epoch 55: Validation loss decreased (0.611821 --> 0.611403).  Saving model ...
	 Train_Loss: 0.6245 Train_Acc: 65.298 Val_Loss: 0.6114  BEST VAL Loss: 0.6114  Val_Acc: 67.811

Epoch 56: Validation loss decreased (0.611403 --> 0.611016).  Saving model ...
	 Train_Loss: 0.6241 Train_Acc: 65.153 Val_Loss: 0.6110  BEST VAL Loss: 0.6110  Val_Acc: 68.008

Epoch 57: Validation loss decreased (0.611016 --> 0.610566).  Saving model ...
	 Train_Loss: 0.6238 Train_Acc: 65.384 Val_Loss: 0.6106  BEST VAL Loss: 0.6106  Val_Acc: 67.928

Epoch 58: Validation loss decreased (0.610566 --> 0.610154).  Saving model ...
	 Train_Loss: 0.6234 Train_Acc: 65.351 Val_Loss: 0.6102  BEST VAL Loss: 0.6102  Val_Acc: 67.734

Epoch 59: Validation loss decreased (0.610154 --> 0.609711).  Saving model ...
	 Train_Loss: 0.6230 Train_Acc: 65.339 Val_Loss: 0.6097  BEST VAL Loss: 0.6097  Val_Acc: 68.495

Epoch 60: Validation loss decreased (0.609711 --> 0.609302).  Saving model ...
	 Train_Loss: 0.6226 Train_Acc: 65.480 Val_Loss: 0.6093  BEST VAL Loss: 0.6093  Val_Acc: 68.506

Epoch 61: Validation loss decreased (0.609302 --> 0.608851).  Saving model ...
	 Train_Loss: 0.6223 Train_Acc: 65.398 Val_Loss: 0.6089  BEST VAL Loss: 0.6089  Val_Acc: 68.436

Epoch 62: Validation loss decreased (0.608851 --> 0.608654).  Saving model ...
	 Train_Loss: 0.6219 Train_Acc: 65.425 Val_Loss: 0.6087  BEST VAL Loss: 0.6087  Val_Acc: 66.857

Epoch 63: Validation loss decreased (0.608654 --> 0.608251).  Saving model ...
	 Train_Loss: 0.6215 Train_Acc: 65.394 Val_Loss: 0.6083  BEST VAL Loss: 0.6083  Val_Acc: 68.407

Epoch 64: Validation loss decreased (0.608251 --> 0.607826).  Saving model ...
	 Train_Loss: 0.6212 Train_Acc: 65.451 Val_Loss: 0.6078  BEST VAL Loss: 0.6078  Val_Acc: 68.513

Epoch 65: Validation loss decreased (0.607826 --> 0.607449).  Saving model ...
	 Train_Loss: 0.6208 Train_Acc: 65.563 Val_Loss: 0.6074  BEST VAL Loss: 0.6074  Val_Acc: 68.279

Epoch 66: Validation loss decreased (0.607449 --> 0.607087).  Saving model ...
	 Train_Loss: 0.6205 Train_Acc: 65.440 Val_Loss: 0.6071  BEST VAL Loss: 0.6071  Val_Acc: 68.487

Epoch 67: Validation loss decreased (0.607087 --> 0.606742).  Saving model ...
	 Train_Loss: 0.6202 Train_Acc: 65.338 Val_Loss: 0.6067  BEST VAL Loss: 0.6067  Val_Acc: 68.513

Epoch 68: Validation loss decreased (0.606742 --> 0.606430).  Saving model ...
	 Train_Loss: 0.6198 Train_Acc: 65.585 Val_Loss: 0.6064  BEST VAL Loss: 0.6064  Val_Acc: 67.961

Epoch 69: Validation loss decreased (0.606430 --> 0.606049).  Saving model ...
	 Train_Loss: 0.6195 Train_Acc: 65.539 Val_Loss: 0.6060  BEST VAL Loss: 0.6060  Val_Acc: 68.253

Epoch 70: Validation loss decreased (0.606049 --> 0.605779).  Saving model ...
	 Train_Loss: 0.6192 Train_Acc: 65.548 Val_Loss: 0.6058  BEST VAL Loss: 0.6058  Val_Acc: 67.811

Epoch 71: Validation loss decreased (0.605779 --> 0.605569).  Saving model ...
	 Train_Loss: 0.6189 Train_Acc: 65.479 Val_Loss: 0.6056  BEST VAL Loss: 0.6056  Val_Acc: 67.135

Epoch 72: Validation loss decreased (0.605569 --> 0.605307).  Saving model ...
	 Train_Loss: 0.6186 Train_Acc: 65.617 Val_Loss: 0.6053  BEST VAL Loss: 0.6053  Val_Acc: 68.257

Epoch 73: Validation loss decreased (0.605307 --> 0.604996).  Saving model ...
	 Train_Loss: 0.6183 Train_Acc: 65.429 Val_Loss: 0.6050  BEST VAL Loss: 0.6050  Val_Acc: 68.674

Epoch 74: Validation loss decreased (0.604996 --> 0.604665).  Saving model ...
	 Train_Loss: 0.6180 Train_Acc: 65.569 Val_Loss: 0.6047  BEST VAL Loss: 0.6047  Val_Acc: 68.546

Epoch 75: Validation loss decreased (0.604665 --> 0.604361).  Saving model ...
	 Train_Loss: 0.6177 Train_Acc: 65.594 Val_Loss: 0.6044  BEST VAL Loss: 0.6044  Val_Acc: 68.348

Epoch 76: Validation loss decreased (0.604361 --> 0.603998).  Saving model ...
	 Train_Loss: 0.6174 Train_Acc: 65.360 Val_Loss: 0.6040  BEST VAL Loss: 0.6040  Val_Acc: 68.824

Epoch 77: Validation loss decreased (0.603998 --> 0.603732).  Saving model ...
	 Train_Loss: 0.6172 Train_Acc: 65.574 Val_Loss: 0.6037  BEST VAL Loss: 0.6037  Val_Acc: 68.107

Epoch 78: Validation loss decreased (0.603732 --> 0.603429).  Saving model ...
	 Train_Loss: 0.6169 Train_Acc: 65.535 Val_Loss: 0.6034  BEST VAL Loss: 0.6034  Val_Acc: 68.337

Epoch 79: Validation loss decreased (0.603429 --> 0.603105).  Saving model ...
	 Train_Loss: 0.6166 Train_Acc: 65.582 Val_Loss: 0.6031  BEST VAL Loss: 0.6031  Val_Acc: 68.798

Epoch 80: Validation loss decreased (0.603105 --> 0.602817).  Saving model ...
	 Train_Loss: 0.6164 Train_Acc: 65.627 Val_Loss: 0.6028  BEST VAL Loss: 0.6028  Val_Acc: 68.794

Epoch 81: Validation loss decreased (0.602817 --> 0.602533).  Saving model ...
	 Train_Loss: 0.6161 Train_Acc: 65.654 Val_Loss: 0.6025  BEST VAL Loss: 0.6025  Val_Acc: 68.465

Epoch 82: Validation loss decreased (0.602533 --> 0.602289).  Saving model ...
	 Train_Loss: 0.6158 Train_Acc: 65.772 Val_Loss: 0.6023  BEST VAL Loss: 0.6023  Val_Acc: 68.688

Epoch 83: Validation loss decreased (0.602289 --> 0.601978).  Saving model ...
	 Train_Loss: 0.6156 Train_Acc: 65.664 Val_Loss: 0.6020  BEST VAL Loss: 0.6020  Val_Acc: 68.871

Epoch 84: Validation loss decreased (0.601978 --> 0.601688).  Saving model ...
	 Train_Loss: 0.6153 Train_Acc: 65.611 Val_Loss: 0.6017  BEST VAL Loss: 0.6017  Val_Acc: 68.681

Epoch 85: Validation loss decreased (0.601688 --> 0.601418).  Saving model ...
	 Train_Loss: 0.6151 Train_Acc: 65.563 Val_Loss: 0.6014  BEST VAL Loss: 0.6014  Val_Acc: 68.553

Epoch 86: Validation loss decreased (0.601418 --> 0.601150).  Saving model ...
	 Train_Loss: 0.6148 Train_Acc: 65.676 Val_Loss: 0.6012  BEST VAL Loss: 0.6012  Val_Acc: 68.301

Epoch 87: Validation loss decreased (0.601150 --> 0.600855).  Saving model ...
	 Train_Loss: 0.6146 Train_Acc: 65.600 Val_Loss: 0.6009  BEST VAL Loss: 0.6009  Val_Acc: 68.860

Epoch 88: Validation loss decreased (0.600855 --> 0.600580).  Saving model ...
	 Train_Loss: 0.6143 Train_Acc: 65.709 Val_Loss: 0.6006  BEST VAL Loss: 0.6006  Val_Acc: 68.553

Epoch 89: Validation loss decreased (0.600580 --> 0.600307).  Saving model ...
	 Train_Loss: 0.6141 Train_Acc: 65.617 Val_Loss: 0.6003  BEST VAL Loss: 0.6003  Val_Acc: 68.875

Epoch 90: Validation loss decreased (0.600307 --> 0.600025).  Saving model ...
	 Train_Loss: 0.6139 Train_Acc: 65.599 Val_Loss: 0.6000  BEST VAL Loss: 0.6000  Val_Acc: 68.652

Epoch 91: Validation loss decreased (0.600025 --> 0.599758).  Saving model ...
	 Train_Loss: 0.6137 Train_Acc: 65.808 Val_Loss: 0.5998  BEST VAL Loss: 0.5998  Val_Acc: 68.370

Epoch 92: Validation loss decreased (0.599758 --> 0.599530).  Saving model ...
	 Train_Loss: 0.6134 Train_Acc: 65.668 Val_Loss: 0.5995  BEST VAL Loss: 0.5995  Val_Acc: 68.348

Epoch 93: Validation loss decreased (0.599530 --> 0.599263).  Saving model ...
	 Train_Loss: 0.6132 Train_Acc: 65.696 Val_Loss: 0.5993  BEST VAL Loss: 0.5993  Val_Acc: 68.900

Epoch 94: Validation loss decreased (0.599263 --> 0.599004).  Saving model ...
	 Train_Loss: 0.6130 Train_Acc: 65.738 Val_Loss: 0.5990  BEST VAL Loss: 0.5990  Val_Acc: 68.941

Epoch 95: Validation loss decreased (0.599004 --> 0.598753).  Saving model ...
	 Train_Loss: 0.6127 Train_Acc: 65.738 Val_Loss: 0.5988  BEST VAL Loss: 0.5988  Val_Acc: 68.878

Epoch 96: Validation loss decreased (0.598753 --> 0.598515).  Saving model ...
	 Train_Loss: 0.6125 Train_Acc: 65.676 Val_Loss: 0.5985  BEST VAL Loss: 0.5985  Val_Acc: 68.663

Epoch 97: Validation loss decreased (0.598515 --> 0.598304).  Saving model ...
	 Train_Loss: 0.6123 Train_Acc: 65.655 Val_Loss: 0.5983  BEST VAL Loss: 0.5983  Val_Acc: 68.875

Epoch 98: Validation loss decreased (0.598304 --> 0.598082).  Saving model ...
	 Train_Loss: 0.6121 Train_Acc: 65.723 Val_Loss: 0.5981  BEST VAL Loss: 0.5981  Val_Acc: 69.043

Epoch 99: Validation loss decreased (0.598082 --> 0.597856).  Saving model ...
	 Train_Loss: 0.6119 Train_Acc: 65.765 Val_Loss: 0.5979  BEST VAL Loss: 0.5979  Val_Acc: 68.696

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.50      0.62      0.56    109598
           1       0.50      0.38      0.43    109228

    accuracy                           0.50    218826
   macro avg       0.50      0.50      0.49    218826
weighted avg       0.50      0.50      0.49    218826

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.50      0.62      0.55     13700
           1       0.50      0.38      0.43     13654

    accuracy                           0.50     27354
   macro avg       0.50      0.50      0.49     27354
weighted avg       0.50      0.50      0.49     27354

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.50      0.62      0.55     13700
           1       0.50      0.38      0.43     13654

    accuracy                           0.50     27354
   macro avg       0.50      0.50      0.49     27354
weighted avg       0.50      0.50      0.49     27354

              precision    recall  f1-score   support

           0       0.50      0.62      0.55     13700
           1       0.50      0.38      0.43     13654

    accuracy                           0.50     27354
   macro avg       0.50      0.50      0.49     27354
weighted avg       0.50      0.50      0.49     27354

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.69      0.61     44168
           1       0.46      0.31      0.37     37725

    accuracy                           0.52     81893
   macro avg       0.50      0.50      0.49     81893
weighted avg       0.50      0.52      0.50     81893

              precision    recall  f1-score   support

           0       0.54      0.69      0.61     44168
           1       0.46      0.31      0.37     37725

    accuracy                           0.52     81893
   macro avg       0.50      0.50      0.49     81893
weighted avg       0.50      0.52      0.50     81893

completed
