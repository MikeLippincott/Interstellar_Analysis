[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd5a0ea8d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e296bcb9'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '2230e1eb'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '902aa8b9'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Flagellin_1.000_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (29021, 1276)
Number of total missing values across all columns: 58042
Data Subset Is Off
Wells held out for testing: ['E14' 'M22']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.411574).  Saving model ...
	 Train_Loss: 0.5756 Train_Acc: 66.374 Val_Loss: 0.4116  BEST VAL Loss: 0.4116  Val_Acc: 85.192

Epoch 1: Validation loss decreased (0.411574 --> 0.393287).  Saving model ...
	 Train_Loss: 0.5074 Train_Acc: 78.350 Val_Loss: 0.3933  BEST VAL Loss: 0.3933  Val_Acc: 88.015

Epoch 2: Validation loss decreased (0.393287 --> 0.363328).  Saving model ...
	 Train_Loss: 0.4693 Train_Acc: 81.758 Val_Loss: 0.3633  BEST VAL Loss: 0.3633  Val_Acc: 88.061

Epoch 3: Validation loss decreased (0.363328 --> 0.345192).  Saving model ...
	 Train_Loss: 0.4475 Train_Acc: 82.863 Val_Loss: 0.3452  BEST VAL Loss: 0.3452  Val_Acc: 88.987

Epoch 4: Validation loss decreased (0.345192 --> 0.342134).  Saving model ...
	 Train_Loss: 0.4326 Train_Acc: 83.569 Val_Loss: 0.3421  BEST VAL Loss: 0.3421  Val_Acc: 88.616

Epoch 5: Validation loss decreased (0.342134 --> 0.337657).  Saving model ...
	 Train_Loss: 0.4232 Train_Acc: 83.412 Val_Loss: 0.3377  BEST VAL Loss: 0.3377  Val_Acc: 88.663

Epoch 6: Validation loss decreased (0.337657 --> 0.331289).  Saving model ...
	 Train_Loss: 0.4142 Train_Acc: 83.997 Val_Loss: 0.3313  BEST VAL Loss: 0.3313  Val_Acc: 88.940

Epoch 7: Validation loss decreased (0.331289 --> 0.326265).  Saving model ...
	 Train_Loss: 0.4075 Train_Acc: 84.228 Val_Loss: 0.3263  BEST VAL Loss: 0.3263  Val_Acc: 89.496

Epoch 8: Validation loss decreased (0.326265 --> 0.320029).  Saving model ...
	 Train_Loss: 0.4001 Train_Acc: 84.616 Val_Loss: 0.3200  BEST VAL Loss: 0.3200  Val_Acc: 88.663

Epoch 9: Validation loss decreased (0.320029 --> 0.317969).  Saving model ...
	 Train_Loss: 0.3938 Train_Acc: 84.899 Val_Loss: 0.3180  BEST VAL Loss: 0.3180  Val_Acc: 89.958

Epoch 10: Validation loss decreased (0.317969 --> 0.311633).  Saving model ...
	 Train_Loss: 0.3885 Train_Acc: 84.801 Val_Loss: 0.3116  BEST VAL Loss: 0.3116  Val_Acc: 90.421

Epoch 11: Validation loss decreased (0.311633 --> 0.304819).  Saving model ...
	 Train_Loss: 0.3840 Train_Acc: 85.686 Val_Loss: 0.3048  BEST VAL Loss: 0.3048  Val_Acc: 90.236

Epoch 12: Validation loss decreased (0.304819 --> 0.302973).  Saving model ...
	 Train_Loss: 0.3795 Train_Acc: 85.680 Val_Loss: 0.3030  BEST VAL Loss: 0.3030  Val_Acc: 89.033

Epoch 13: Validation loss decreased (0.302973 --> 0.301099).  Saving model ...
	 Train_Loss: 0.3756 Train_Acc: 85.246 Val_Loss: 0.3011  BEST VAL Loss: 0.3011  Val_Acc: 89.912

Epoch 14: Validation loss decreased (0.301099 --> 0.296900).  Saving model ...
	 Train_Loss: 0.3719 Train_Acc: 85.837 Val_Loss: 0.2969  BEST VAL Loss: 0.2969  Val_Acc: 90.606

Epoch 15: Validation loss decreased (0.296900 --> 0.293780).  Saving model ...
	 Train_Loss: 0.3683 Train_Acc: 86.577 Val_Loss: 0.2938  BEST VAL Loss: 0.2938  Val_Acc: 91.161

Epoch 16: Validation loss decreased (0.293780 --> 0.291630).  Saving model ...
	 Train_Loss: 0.3662 Train_Acc: 86.068 Val_Loss: 0.2916  BEST VAL Loss: 0.2916  Val_Acc: 89.634

Epoch 17: Validation loss decreased (0.291630 --> 0.290940).  Saving model ...
	 Train_Loss: 0.3638 Train_Acc: 86.317 Val_Loss: 0.2909  BEST VAL Loss: 0.2909  Val_Acc: 90.467

Epoch 18: Validation loss decreased (0.290940 --> 0.288296).  Saving model ...
	 Train_Loss: 0.3614 Train_Acc: 86.832 Val_Loss: 0.2883  BEST VAL Loss: 0.2883  Val_Acc: 90.467

Epoch 19: Validation loss decreased (0.288296 --> 0.287167).  Saving model ...
	 Train_Loss: 0.3599 Train_Acc: 86.259 Val_Loss: 0.2872  BEST VAL Loss: 0.2872  Val_Acc: 91.254

Epoch 20: Validation loss decreased (0.287167 --> 0.283540).  Saving model ...
	 Train_Loss: 0.3574 Train_Acc: 86.907 Val_Loss: 0.2835  BEST VAL Loss: 0.2835  Val_Acc: 91.994

Epoch 21: Validation loss decreased (0.283540 --> 0.280693).  Saving model ...
	 Train_Loss: 0.3551 Train_Acc: 87.057 Val_Loss: 0.2807  BEST VAL Loss: 0.2807  Val_Acc: 91.254

Epoch 22: Validation loss decreased (0.280693 --> 0.278601).  Saving model ...
	 Train_Loss: 0.3528 Train_Acc: 87.491 Val_Loss: 0.2786  BEST VAL Loss: 0.2786  Val_Acc: 91.300

Epoch 23: Validation loss decreased (0.278601 --> 0.276255).  Saving model ...
	 Train_Loss: 0.3509 Train_Acc: 87.115 Val_Loss: 0.2763  BEST VAL Loss: 0.2763  Val_Acc: 91.578

Epoch 24: Validation loss decreased (0.276255 --> 0.275346).  Saving model ...
	 Train_Loss: 0.3495 Train_Acc: 87.173 Val_Loss: 0.2753  BEST VAL Loss: 0.2753  Val_Acc: 91.161

Epoch 25: Validation loss decreased (0.275346 --> 0.272469).  Saving model ...
	 Train_Loss: 0.3484 Train_Acc: 86.531 Val_Loss: 0.2725  BEST VAL Loss: 0.2725  Val_Acc: 91.532

Epoch 26: Validation loss decreased (0.272469 --> 0.271657).  Saving model ...
	 Train_Loss: 0.3474 Train_Acc: 85.264 Val_Loss: 0.2717  BEST VAL Loss: 0.2717  Val_Acc: 91.023

Epoch 27: Validation loss decreased (0.271657 --> 0.269824).  Saving model ...
	 Train_Loss: 0.3466 Train_Acc: 85.651 Val_Loss: 0.2698  BEST VAL Loss: 0.2698  Val_Acc: 91.254

Epoch 28: Validation loss decreased (0.269824 --> 0.267713).  Saving model ...
	 Train_Loss: 0.3457 Train_Acc: 85.455 Val_Loss: 0.2677  BEST VAL Loss: 0.2677  Val_Acc: 91.069

Epoch 29: Validation loss decreased (0.267713 --> 0.266283).  Saving model ...
	 Train_Loss: 0.3446 Train_Acc: 85.542 Val_Loss: 0.2663  BEST VAL Loss: 0.2663  Val_Acc: 90.421

Epoch 30: Validation loss decreased (0.266283 --> 0.264339).  Saving model ...
	 Train_Loss: 0.3436 Train_Acc: 86.444 Val_Loss: 0.2643  BEST VAL Loss: 0.2643  Val_Acc: 91.393

Epoch 31: Validation loss decreased (0.264339 --> 0.263098).  Saving model ...
	 Train_Loss: 0.3429 Train_Acc: 86.149 Val_Loss: 0.2631  BEST VAL Loss: 0.2631  Val_Acc: 90.976

Epoch 32: Validation loss decreased (0.263098 --> 0.261983).  Saving model ...
	 Train_Loss: 0.3422 Train_Acc: 85.686 Val_Loss: 0.2620  BEST VAL Loss: 0.2620  Val_Acc: 91.763

Epoch 33: Validation loss decreased (0.261983 --> 0.260808).  Saving model ...
	 Train_Loss: 0.3416 Train_Acc: 85.530 Val_Loss: 0.2608  BEST VAL Loss: 0.2608  Val_Acc: 91.161

Epoch 34: Validation loss decreased (0.260808 --> 0.260065).  Saving model ...
	 Train_Loss: 0.3408 Train_Acc: 85.761 Val_Loss: 0.2601  BEST VAL Loss: 0.2601  Val_Acc: 90.699

Epoch 35: Validation loss decreased (0.260065 --> 0.258915).  Saving model ...
	 Train_Loss: 0.3401 Train_Acc: 85.449 Val_Loss: 0.2589  BEST VAL Loss: 0.2589  Val_Acc: 89.079

Epoch 36: Validation loss decreased (0.258915 --> 0.257627).  Saving model ...
	 Train_Loss: 0.3395 Train_Acc: 85.293 Val_Loss: 0.2576  BEST VAL Loss: 0.2576  Val_Acc: 90.421

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.3390 Train_Acc: 85.594 Val_Loss: 0.2577  BEST VAL Loss: 0.2576  Val_Acc: 90.745

Epoch 38: Validation loss decreased (0.257627 --> 0.256994).  Saving model ...
	 Train_Loss: 0.3383 Train_Acc: 85.941 Val_Loss: 0.2570  BEST VAL Loss: 0.2570  Val_Acc: 90.606

Epoch 39: Validation loss decreased (0.256994 --> 0.256206).  Saving model ...
	 Train_Loss: 0.3378 Train_Acc: 85.553 Val_Loss: 0.2562  BEST VAL Loss: 0.2562  Val_Acc: 91.161

Epoch 40: Validation loss decreased (0.256206 --> 0.255849).  Saving model ...
	 Train_Loss: 0.3373 Train_Acc: 85.640 Val_Loss: 0.2558  BEST VAL Loss: 0.2558  Val_Acc: 91.532

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.3367 Train_Acc: 86.213 Val_Loss: 0.2563  BEST VAL Loss: 0.2558  Val_Acc: 90.097

Epoch 42: Validation loss decreased (0.255849 --> 0.255233).  Saving model ...
	 Train_Loss: 0.3369 Train_Acc: 85.275 Val_Loss: 0.2552  BEST VAL Loss: 0.2552  Val_Acc: 91.578

Epoch 43: Validation loss decreased (0.255233 --> 0.254317).  Saving model ...
	 Train_Loss: 0.3364 Train_Acc: 85.663 Val_Loss: 0.2543  BEST VAL Loss: 0.2543  Val_Acc: 91.485

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.3363 Train_Acc: 85.680 Val_Loss: 0.2552  BEST VAL Loss: 0.2543  Val_Acc: 89.820

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.3367 Train_Acc: 85.194 Val_Loss: 0.2554  BEST VAL Loss: 0.2543  Val_Acc: 90.467

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.3367 Train_Acc: 85.258 Val_Loss: 0.2550  BEST VAL Loss: 0.2543  Val_Acc: 91.347

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.3365 Train_Acc: 85.837 Val_Loss: 0.2548  BEST VAL Loss: 0.2543  Val_Acc: 90.930

Epoch 48: Validation loss decreased (0.254317 --> 0.253561).  Saving model ...
	 Train_Loss: 0.3361 Train_Acc: 86.137 Val_Loss: 0.2536  BEST VAL Loss: 0.2536  Val_Acc: 91.485

Epoch 49: Validation loss decreased (0.253561 --> 0.252847).  Saving model ...
	 Train_Loss: 0.3356 Train_Acc: 86.514 Val_Loss: 0.2528  BEST VAL Loss: 0.2528  Val_Acc: 90.884

Epoch 50: Validation loss decreased (0.252847 --> 0.252086).  Saving model ...
	 Train_Loss: 0.3352 Train_Acc: 85.952 Val_Loss: 0.2521  BEST VAL Loss: 0.2521  Val_Acc: 91.069

Epoch 51: Validation loss decreased (0.252086 --> 0.251156).  Saving model ...
	 Train_Loss: 0.3349 Train_Acc: 85.623 Val_Loss: 0.2512  BEST VAL Loss: 0.2512  Val_Acc: 91.347

Epoch 52: Validation loss decreased (0.251156 --> 0.250168).  Saving model ...
	 Train_Loss: 0.3345 Train_Acc: 85.657 Val_Loss: 0.2502  BEST VAL Loss: 0.2502  Val_Acc: 91.809

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.3340 Train_Acc: 85.918 Val_Loss: 0.2502  BEST VAL Loss: 0.2502  Val_Acc: 91.069

Epoch 54: Validation loss decreased (0.250168 --> 0.249795).  Saving model ...
	 Train_Loss: 0.3336 Train_Acc: 85.518 Val_Loss: 0.2498  BEST VAL Loss: 0.2498  Val_Acc: 91.208

Epoch 55: Validation loss decreased (0.249795 --> 0.249377).  Saving model ...
	 Train_Loss: 0.3332 Train_Acc: 85.362 Val_Loss: 0.2494  BEST VAL Loss: 0.2494  Val_Acc: 90.421

Epoch 56: Validation loss decreased (0.249377 --> 0.249063).  Saving model ...
	 Train_Loss: 0.3327 Train_Acc: 85.866 Val_Loss: 0.2491  BEST VAL Loss: 0.2491  Val_Acc: 90.976

Epoch 57: Validation loss decreased (0.249063 --> 0.248704).  Saving model ...
	 Train_Loss: 0.3321 Train_Acc: 85.929 Val_Loss: 0.2487  BEST VAL Loss: 0.2487  Val_Acc: 91.347

Epoch 58: Validation loss decreased (0.248704 --> 0.248552).  Saving model ...
	 Train_Loss: 0.3318 Train_Acc: 86.386 Val_Loss: 0.2486  BEST VAL Loss: 0.2486  Val_Acc: 91.254

Epoch 59: Validation loss decreased (0.248552 --> 0.248190).  Saving model ...
	 Train_Loss: 0.3315 Train_Acc: 86.901 Val_Loss: 0.2482  BEST VAL Loss: 0.2482  Val_Acc: 91.208

Epoch 60: Validation loss decreased (0.248190 --> 0.248109).  Saving model ...
	 Train_Loss: 0.3310 Train_Acc: 86.461 Val_Loss: 0.2481  BEST VAL Loss: 0.2481  Val_Acc: 90.930

Epoch 61: Validation loss decreased (0.248109 --> 0.247617).  Saving model ...
	 Train_Loss: 0.3307 Train_Acc: 86.224 Val_Loss: 0.2476  BEST VAL Loss: 0.2476  Val_Acc: 91.161

Epoch 62: Validation loss decreased (0.247617 --> 0.247440).  Saving model ...
	 Train_Loss: 0.3309 Train_Acc: 85.461 Val_Loss: 0.2474  BEST VAL Loss: 0.2474  Val_Acc: 89.079

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.3332 Train_Acc: 79.663 Val_Loss: 0.2476  BEST VAL Loss: 0.2474  Val_Acc: 90.467

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.3349 Train_Acc: 76.510 Val_Loss: 0.2480  BEST VAL Loss: 0.2474  Val_Acc: 87.783

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.3357 Train_Acc: 78.685 Val_Loss: 0.2479  BEST VAL Loss: 0.2474  Val_Acc: 90.976

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.3364 Train_Acc: 78.911 Val_Loss: 0.2478  BEST VAL Loss: 0.2474  Val_Acc: 91.254

Epoch 67: Validation loss did not decrease
	 Train_Loss: 0.3372 Train_Acc: 79.177 Val_Loss: 0.2481  BEST VAL Loss: 0.2474  Val_Acc: 89.866

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.3382 Train_Acc: 79.299 Val_Loss: 0.2483  BEST VAL Loss: 0.2474  Val_Acc: 90.005

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.3392 Train_Acc: 80.178 Val_Loss: 0.2492  BEST VAL Loss: 0.2474  Val_Acc: 88.154

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.3404 Train_Acc: 79.403 Val_Loss: 0.2500  BEST VAL Loss: 0.2474  Val_Acc: 88.987

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.3416 Train_Acc: 79.073 Val_Loss: 0.2504  BEST VAL Loss: 0.2474  Val_Acc: 89.542

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.3426 Train_Acc: 80.097 Val_Loss: 0.2511  BEST VAL Loss: 0.2474  Val_Acc: 88.894

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.3437 Train_Acc: 80.005 Val_Loss: 0.2513  BEST VAL Loss: 0.2474  Val_Acc: 88.755

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.3447 Train_Acc: 80.358 Val_Loss: 0.2517  BEST VAL Loss: 0.2474  Val_Acc: 89.033

Epoch 75: Validation loss did not decrease
	 Train_Loss: 0.3454 Train_Acc: 80.369 Val_Loss: 0.2520  BEST VAL Loss: 0.2474  Val_Acc: 88.154

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.3461 Train_Acc: 80.520 Val_Loss: 0.2522  BEST VAL Loss: 0.2474  Val_Acc: 88.940

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.3467 Train_Acc: 80.439 Val_Loss: 0.2523  BEST VAL Loss: 0.2474  Val_Acc: 87.830

Epoch 78: Validation loss did not decrease
Early stopped at epoch : 78
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.92      0.91      0.92      9434
           1       0.90      0.90      0.90      7850

    accuracy                           0.91     17284
   macro avg       0.91      0.91      0.91     17284
weighted avg       0.91      0.91      0.91     17284

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.91      0.89      0.90      1179
           1       0.87      0.89      0.88       982

    accuracy                           0.89      2161
   macro avg       0.89      0.89      0.89      2161
weighted avg       0.89      0.89      0.89      2161

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.91      0.90      0.91      1179
           1       0.89      0.90      0.89       982

    accuracy                           0.90      2161
   macro avg       0.90      0.90      0.90      2161
weighted avg       0.90      0.90      0.90      2161

              precision    recall  f1-score   support

           0       0.91      0.90      0.91      1179
           1       0.89      0.90      0.89       982

    accuracy                           0.90      2161
   macro avg       0.90      0.90      0.90      2161
weighted avg       0.90      0.90      0.90      2161

Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.87      0.90      4017
           1       0.86      0.92      0.89      3398

    accuracy                           0.89      7415
   macro avg       0.89      0.90      0.89      7415
weighted avg       0.90      0.89      0.89      7415

              precision    recall  f1-score   support

           0       0.93      0.87      0.90      4017
           1       0.86      0.92      0.89      3398

    accuracy                           0.89      7415
   macro avg       0.89      0.90      0.89      7415
weighted avg       0.90      0.89      0.89      7415

completed
