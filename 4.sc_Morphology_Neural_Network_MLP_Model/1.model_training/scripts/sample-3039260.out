[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'f9885cf7'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '91128c2f'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'de1311e6'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '01e3fefc'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_0.100_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_0.100_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_0.100_DMSO_0.025']
The dimensions of the data are: (31276, 1276)
Number of total missing values across all columns: 62552
Data Subset Is Off
Wells held out for testing: ['D14' 'C20']
Wells to use for training, validation, and testing ['D15' 'C16' 'C17' 'C21' 'K14' 'K15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.483372).  Saving model ...
	 Train_Loss: 0.6959 Train_Acc: 64.733 Val_Loss: 0.4834  BEST VAL Loss: 0.4834  Val_Acc: 79.983

Epoch 1: Validation loss decreased (0.483372 --> 0.455448).  Saving model ...
	 Train_Loss: 0.5924 Train_Acc: 74.036 Val_Loss: 0.4554  BEST VAL Loss: 0.4554  Val_Acc: 81.513

Epoch 2: Validation loss decreased (0.455448 --> 0.435603).  Saving model ...
	 Train_Loss: 0.5432 Train_Acc: 79.099 Val_Loss: 0.4356  BEST VAL Loss: 0.4356  Val_Acc: 82.915

Epoch 3: Validation loss decreased (0.435603 --> 0.421002).  Saving model ...
	 Train_Loss: 0.5080 Train_Acc: 81.569 Val_Loss: 0.4210  BEST VAL Loss: 0.4210  Val_Acc: 83.553

Epoch 4: Validation loss decreased (0.421002 --> 0.409094).  Saving model ...
	 Train_Loss: 0.4843 Train_Acc: 82.393 Val_Loss: 0.4091  BEST VAL Loss: 0.4091  Val_Acc: 84.190

Epoch 5: Validation loss decreased (0.409094 --> 0.400880).  Saving model ...
	 Train_Loss: 0.4656 Train_Acc: 83.429 Val_Loss: 0.4009  BEST VAL Loss: 0.4009  Val_Acc: 84.955

Epoch 6: Validation loss decreased (0.400880 --> 0.393341).  Saving model ...
	 Train_Loss: 0.4496 Train_Acc: 84.274 Val_Loss: 0.3933  BEST VAL Loss: 0.3933  Val_Acc: 85.593

Epoch 7: Validation loss decreased (0.393341 --> 0.387961).  Saving model ...
	 Train_Loss: 0.4362 Train_Acc: 85.012 Val_Loss: 0.3880  BEST VAL Loss: 0.3880  Val_Acc: 85.465

Epoch 8: Validation loss decreased (0.387961 --> 0.381160).  Saving model ...
	 Train_Loss: 0.4250 Train_Acc: 85.464 Val_Loss: 0.3812  BEST VAL Loss: 0.3812  Val_Acc: 86.783

Epoch 9: Validation loss decreased (0.381160 --> 0.376274).  Saving model ...
	 Train_Loss: 0.4150 Train_Acc: 85.559 Val_Loss: 0.3763  BEST VAL Loss: 0.3763  Val_Acc: 86.400

Epoch 10: Validation loss decreased (0.376274 --> 0.372834).  Saving model ...
	 Train_Loss: 0.4058 Train_Acc: 86.250 Val_Loss: 0.3728  BEST VAL Loss: 0.3728  Val_Acc: 86.103

Epoch 11: Validation loss decreased (0.372834 --> 0.369581).  Saving model ...
	 Train_Loss: 0.3976 Train_Acc: 86.580 Val_Loss: 0.3696  BEST VAL Loss: 0.3696  Val_Acc: 86.910

Epoch 12: Validation loss decreased (0.369581 --> 0.367010).  Saving model ...
	 Train_Loss: 0.3906 Train_Acc: 86.760 Val_Loss: 0.3670  BEST VAL Loss: 0.3670  Val_Acc: 86.740

Epoch 13: Validation loss decreased (0.367010 --> 0.364559).  Saving model ...
	 Train_Loss: 0.3842 Train_Acc: 86.861 Val_Loss: 0.3646  BEST VAL Loss: 0.3646  Val_Acc: 87.080

Epoch 14: Validation loss decreased (0.364559 --> 0.362104).  Saving model ...
	 Train_Loss: 0.3789 Train_Acc: 86.872 Val_Loss: 0.3621  BEST VAL Loss: 0.3621  Val_Acc: 86.655

Epoch 15: Validation loss decreased (0.362104 --> 0.360560).  Saving model ...
	 Train_Loss: 0.3730 Train_Acc: 87.653 Val_Loss: 0.3606  BEST VAL Loss: 0.3606  Val_Acc: 86.230

Epoch 16: Validation loss decreased (0.360560 --> 0.357784).  Saving model ...
	 Train_Loss: 0.3679 Train_Acc: 87.578 Val_Loss: 0.3578  BEST VAL Loss: 0.3578  Val_Acc: 87.080

Epoch 17: Validation loss decreased (0.357784 --> 0.355174).  Saving model ...
	 Train_Loss: 0.3630 Train_Acc: 87.887 Val_Loss: 0.3552  BEST VAL Loss: 0.3552  Val_Acc: 87.973

Epoch 18: Validation loss decreased (0.355174 --> 0.353177).  Saving model ...
	 Train_Loss: 0.3583 Train_Acc: 88.620 Val_Loss: 0.3532  BEST VAL Loss: 0.3532  Val_Acc: 87.930

Epoch 19: Validation loss decreased (0.353177 --> 0.352171).  Saving model ...
	 Train_Loss: 0.3540 Train_Acc: 88.668 Val_Loss: 0.3522  BEST VAL Loss: 0.3522  Val_Acc: 87.590

Epoch 20: Validation loss decreased (0.352171 --> 0.350634).  Saving model ...
	 Train_Loss: 0.3500 Train_Acc: 88.758 Val_Loss: 0.3506  BEST VAL Loss: 0.3506  Val_Acc: 88.228

Epoch 21: Validation loss decreased (0.350634 --> 0.350464).  Saving model ...
	 Train_Loss: 0.3460 Train_Acc: 89.321 Val_Loss: 0.3505  BEST VAL Loss: 0.3505  Val_Acc: 87.250

Epoch 22: Validation loss decreased (0.350464 --> 0.349834).  Saving model ...
	 Train_Loss: 0.3424 Train_Acc: 89.204 Val_Loss: 0.3498  BEST VAL Loss: 0.3498  Val_Acc: 87.293

Epoch 23: Validation loss decreased (0.349834 --> 0.349107).  Saving model ...
	 Train_Loss: 0.3391 Train_Acc: 89.342 Val_Loss: 0.3491  BEST VAL Loss: 0.3491  Val_Acc: 87.718

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.3358 Train_Acc: 89.624 Val_Loss: 0.3493  BEST VAL Loss: 0.3491  Val_Acc: 86.655

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.3327 Train_Acc: 89.704 Val_Loss: 0.3493  BEST VAL Loss: 0.3491  Val_Acc: 87.420

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.3298 Train_Acc: 89.576 Val_Loss: 0.3493  BEST VAL Loss: 0.3491  Val_Acc: 86.910

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.3270 Train_Acc: 89.932 Val_Loss: 0.3493  BEST VAL Loss: 0.3491  Val_Acc: 85.933

Epoch 28: Validation loss decreased (0.349107 --> 0.348414).  Saving model ...
	 Train_Loss: 0.3241 Train_Acc: 90.166 Val_Loss: 0.3484  BEST VAL Loss: 0.3484  Val_Acc: 86.740

Epoch 29: Validation loss decreased (0.348414 --> 0.347506).  Saving model ...
	 Train_Loss: 0.3215 Train_Acc: 90.006 Val_Loss: 0.3475  BEST VAL Loss: 0.3475  Val_Acc: 88.185

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.3191 Train_Acc: 90.166 Val_Loss: 0.3476  BEST VAL Loss: 0.3475  Val_Acc: 86.655

Epoch 31: Validation loss decreased (0.347506 --> 0.346953).  Saving model ...
	 Train_Loss: 0.3165 Train_Acc: 90.585 Val_Loss: 0.3470  BEST VAL Loss: 0.3470  Val_Acc: 87.845

Epoch 32: Validation loss decreased (0.346953 --> 0.346808).  Saving model ...
	 Train_Loss: 0.3139 Train_Acc: 90.771 Val_Loss: 0.3468  BEST VAL Loss: 0.3468  Val_Acc: 87.335

Epoch 33: Validation loss decreased (0.346808 --> 0.346185).  Saving model ...
	 Train_Loss: 0.3114 Train_Acc: 90.793 Val_Loss: 0.3462  BEST VAL Loss: 0.3462  Val_Acc: 87.590

Epoch 34: Validation loss decreased (0.346185 --> 0.346096).  Saving model ...
	 Train_Loss: 0.3091 Train_Acc: 90.835 Val_Loss: 0.3461  BEST VAL Loss: 0.3461  Val_Acc: 87.930

Epoch 35: Validation loss decreased (0.346096 --> 0.345792).  Saving model ...
	 Train_Loss: 0.3069 Train_Acc: 91.021 Val_Loss: 0.3458  BEST VAL Loss: 0.3458  Val_Acc: 88.483

Epoch 36: Validation loss decreased (0.345792 --> 0.345028).  Saving model ...
	 Train_Loss: 0.3050 Train_Acc: 90.532 Val_Loss: 0.3450  BEST VAL Loss: 0.3450  Val_Acc: 88.058

Epoch 37: Validation loss decreased (0.345028 --> 0.344865).  Saving model ...
	 Train_Loss: 0.3029 Train_Acc: 91.058 Val_Loss: 0.3449  BEST VAL Loss: 0.3449  Val_Acc: 87.888

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.3010 Train_Acc: 90.904 Val_Loss: 0.3451  BEST VAL Loss: 0.3449  Val_Acc: 87.505

Epoch 39: Validation loss decreased (0.344865 --> 0.344446).  Saving model ...
	 Train_Loss: 0.2990 Train_Acc: 91.175 Val_Loss: 0.3444  BEST VAL Loss: 0.3444  Val_Acc: 87.803

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2974 Train_Acc: 90.931 Val_Loss: 0.3445  BEST VAL Loss: 0.3444  Val_Acc: 87.930

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2955 Train_Acc: 91.196 Val_Loss: 0.3458  BEST VAL Loss: 0.3444  Val_Acc: 87.038

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2940 Train_Acc: 91.127 Val_Loss: 0.3450  BEST VAL Loss: 0.3444  Val_Acc: 87.675

Epoch 43: Validation loss decreased (0.344446 --> 0.344400).  Saving model ...
	 Train_Loss: 0.2923 Train_Acc: 90.979 Val_Loss: 0.3444  BEST VAL Loss: 0.3444  Val_Acc: 87.675

Epoch 44: Validation loss decreased (0.344400 --> 0.344200).  Saving model ...
	 Train_Loss: 0.2908 Train_Acc: 91.207 Val_Loss: 0.3442  BEST VAL Loss: 0.3442  Val_Acc: 87.293

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2892 Train_Acc: 91.138 Val_Loss: 0.3447  BEST VAL Loss: 0.3442  Val_Acc: 87.335

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.2876 Train_Acc: 91.797 Val_Loss: 0.3453  BEST VAL Loss: 0.3442  Val_Acc: 86.953

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.2862 Train_Acc: 91.335 Val_Loss: 0.3451  BEST VAL Loss: 0.3442  Val_Acc: 88.058

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.2848 Train_Acc: 91.499 Val_Loss: 0.3447  BEST VAL Loss: 0.3442  Val_Acc: 88.355

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2834 Train_Acc: 91.547 Val_Loss: 0.3456  BEST VAL Loss: 0.3442  Val_Acc: 87.080

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.2819 Train_Acc: 91.877 Val_Loss: 0.3465  BEST VAL Loss: 0.3442  Val_Acc: 87.378

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.2805 Train_Acc: 91.866 Val_Loss: 0.3466  BEST VAL Loss: 0.3442  Val_Acc: 87.803

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.2791 Train_Acc: 91.738 Val_Loss: 0.3468  BEST VAL Loss: 0.3442  Val_Acc: 87.463

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.2777 Train_Acc: 91.946 Val_Loss: 0.3475  BEST VAL Loss: 0.3442  Val_Acc: 87.760

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.2764 Train_Acc: 92.073 Val_Loss: 0.3478  BEST VAL Loss: 0.3442  Val_Acc: 88.100

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.2751 Train_Acc: 91.935 Val_Loss: 0.3489  BEST VAL Loss: 0.3442  Val_Acc: 88.270

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2738 Train_Acc: 92.328 Val_Loss: 0.3499  BEST VAL Loss: 0.3442  Val_Acc: 87.038

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.2726 Train_Acc: 92.397 Val_Loss: 0.3503  BEST VAL Loss: 0.3442  Val_Acc: 87.633

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.2713 Train_Acc: 92.376 Val_Loss: 0.3517  BEST VAL Loss: 0.3442  Val_Acc: 86.995

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.2701 Train_Acc: 91.977 Val_Loss: 0.3518  BEST VAL Loss: 0.3442  Val_Acc: 87.803

Epoch 60: Validation loss did not decrease
Early stopped at epoch : 60
LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.96      0.95     10451
           1       0.95      0.92      0.93      8371

    accuracy                           0.94     18822
   macro avg       0.94      0.94      0.94     18822
weighted avg       0.94      0.94      0.94     18822

LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.88      0.90      0.89      1307
           1       0.87      0.84      0.85      1046

    accuracy                           0.87      2353
   macro avg       0.87      0.87      0.87      2353
weighted avg       0.87      0.87      0.87      2353

LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.88      0.92      0.90      1307
           1       0.89      0.84      0.87      1046

    accuracy                           0.88      2353
   macro avg       0.89      0.88      0.88      2353
weighted avg       0.88      0.88      0.88      2353

              precision    recall  f1-score   support

           0       0.88      0.92      0.90      1307
           1       0.89      0.84      0.87      1046

    accuracy                           0.88      2353
   macro avg       0.89      0.88      0.88      2353
weighted avg       0.88      0.88      0.88      2353

LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.90      0.92      0.91      4445
           1       0.89      0.87      0.88      3303

    accuracy                           0.90      7748
   macro avg       0.90      0.89      0.90      7748
weighted avg       0.90      0.90      0.90      7748

              precision    recall  f1-score   support

           0       0.90      0.92      0.91      4445
           1       0.89      0.87      0.88      3303

    accuracy                           0.90      7748
   macro avg       0.90      0.89      0.90      7748
weighted avg       0.90      0.90      0.90      7748

completed
