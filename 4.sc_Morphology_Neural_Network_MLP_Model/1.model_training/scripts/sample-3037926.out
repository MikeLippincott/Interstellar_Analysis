[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '2db749d3'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '47961e67'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd73ce76e'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '21427bdc'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (253003, 1270)
Number of total missing values across all columns: 542622
Data Subset Is Off
Wells held out for testing: ['K07' 'M10']
Wells to use for training, validation, and testing ['D06' 'D07' 'K06' 'M05' 'M11']
Number of in features:  1245
Number of out features:  2
Binary_Classification
SGD
Epoch 0: Validation loss decreased (inf --> 0.571501).  Saving model ...
	 Train_Loss: 0.6340 Train_Acc: 64.146 Val_Loss: 0.5715  BEST VAL Loss: 0.5715  Val_Acc: 71.057

Epoch 1: Validation loss decreased (0.571501 --> 0.555189).  Saving model ...
	 Train_Loss: 0.5905 Train_Acc: 72.241 Val_Loss: 0.5552  BEST VAL Loss: 0.5552  Val_Acc: 71.996

Epoch 2: Validation loss decreased (0.555189 --> 0.538065).  Saving model ...
	 Train_Loss: 0.5641 Train_Acc: 74.252 Val_Loss: 0.5381  BEST VAL Loss: 0.5381  Val_Acc: 74.210

Epoch 3: Validation loss decreased (0.538065 --> 0.525275).  Saving model ...
	 Train_Loss: 0.5454 Train_Acc: 75.267 Val_Loss: 0.5253  BEST VAL Loss: 0.5253  Val_Acc: 75.998

Epoch 4: Validation loss did not decrease
	 Train_Loss: 0.5316 Train_Acc: 75.719 Val_Loss: 0.5285  BEST VAL Loss: 0.5253  Val_Acc: 73.733

Epoch 5: Validation loss decreased (0.525275 --> 0.520582).  Saving model ...
	 Train_Loss: 0.5209 Train_Acc: 76.163 Val_Loss: 0.5206  BEST VAL Loss: 0.5206  Val_Acc: 76.536

Epoch 6: Validation loss decreased (0.520582 --> 0.515120).  Saving model ...
	 Train_Loss: 0.5118 Train_Acc: 76.359 Val_Loss: 0.5151  BEST VAL Loss: 0.5151  Val_Acc: 76.420

Epoch 7: Validation loss decreased (0.515120 --> 0.510879).  Saving model ...
	 Train_Loss: 0.5044 Train_Acc: 76.795 Val_Loss: 0.5109  BEST VAL Loss: 0.5109  Val_Acc: 75.376

Epoch 8: Validation loss decreased (0.510879 --> 0.504394).  Saving model ...
	 Train_Loss: 0.4980 Train_Acc: 76.764 Val_Loss: 0.5044  BEST VAL Loss: 0.5044  Val_Acc: 77.447

Epoch 9: Validation loss decreased (0.504394 --> 0.498419).  Saving model ...
	 Train_Loss: 0.4925 Train_Acc: 76.977 Val_Loss: 0.4984  BEST VAL Loss: 0.4984  Val_Acc: 77.247

Epoch 10: Validation loss decreased (0.498419 --> 0.498343).  Saving model ...
	 Train_Loss: 0.4877 Train_Acc: 77.869 Val_Loss: 0.4983  BEST VAL Loss: 0.4983  Val_Acc: 74.888

Epoch 11: Validation loss decreased (0.498343 --> 0.495068).  Saving model ...
	 Train_Loss: 0.4833 Train_Acc: 79.040 Val_Loss: 0.4951  BEST VAL Loss: 0.4951  Val_Acc: 76.536

Epoch 12: Validation loss decreased (0.495068 --> 0.490802).  Saving model ...
	 Train_Loss: 0.4794 Train_Acc: 79.151 Val_Loss: 0.4908  BEST VAL Loss: 0.4908  Val_Acc: 78.068

Epoch 13: Validation loss decreased (0.490802 --> 0.486106).  Saving model ...
	 Train_Loss: 0.4757 Train_Acc: 79.559 Val_Loss: 0.4861  BEST VAL Loss: 0.4861  Val_Acc: 79.556

Epoch 14: Validation loss decreased (0.486106 --> 0.481218).  Saving model ...
	 Train_Loss: 0.4724 Train_Acc: 79.577 Val_Loss: 0.4812  BEST VAL Loss: 0.4812  Val_Acc: 80.161

Epoch 15: Validation loss decreased (0.481218 --> 0.477775).  Saving model ...
	 Train_Loss: 0.4693 Train_Acc: 79.656 Val_Loss: 0.4778  BEST VAL Loss: 0.4778  Val_Acc: 79.234

Epoch 16: Validation loss decreased (0.477775 --> 0.473054).  Saving model ...
	 Train_Loss: 0.4666 Train_Acc: 79.755 Val_Loss: 0.4731  BEST VAL Loss: 0.4731  Val_Acc: 81.266

Epoch 17: Validation loss decreased (0.473054 --> 0.470298).  Saving model ...
	 Train_Loss: 0.4639 Train_Acc: 79.941 Val_Loss: 0.4703  BEST VAL Loss: 0.4703  Val_Acc: 79.944

Epoch 18: Validation loss decreased (0.470298 --> 0.467503).  Saving model ...
	 Train_Loss: 0.4615 Train_Acc: 79.937 Val_Loss: 0.4675  BEST VAL Loss: 0.4675  Val_Acc: 79.706

Epoch 19: Validation loss decreased (0.467503 --> 0.463907).  Saving model ...
	 Train_Loss: 0.4592 Train_Acc: 80.107 Val_Loss: 0.4639  BEST VAL Loss: 0.4639  Val_Acc: 81.549

Epoch 20: Validation loss decreased (0.463907 --> 0.463475).  Saving model ...
	 Train_Loss: 0.4570 Train_Acc: 80.033 Val_Loss: 0.4635  BEST VAL Loss: 0.4635  Val_Acc: 77.263

Epoch 21: Validation loss decreased (0.463475 --> 0.461443).  Saving model ...
	 Train_Loss: 0.4550 Train_Acc: 80.152 Val_Loss: 0.4614  BEST VAL Loss: 0.4614  Val_Acc: 79.789

Epoch 22: Validation loss decreased (0.461443 --> 0.458379).  Saving model ...
	 Train_Loss: 0.4531 Train_Acc: 80.293 Val_Loss: 0.4584  BEST VAL Loss: 0.4584  Val_Acc: 81.699

Epoch 23: Validation loss decreased (0.458379 --> 0.456250).  Saving model ...
	 Train_Loss: 0.4513 Train_Acc: 80.334 Val_Loss: 0.4563  BEST VAL Loss: 0.4563  Val_Acc: 80.466

Epoch 24: Validation loss decreased (0.456250 --> 0.455663).  Saving model ...
	 Train_Loss: 0.4496 Train_Acc: 80.452 Val_Loss: 0.4557  BEST VAL Loss: 0.4557  Val_Acc: 77.913

Epoch 25: Validation loss decreased (0.455663 --> 0.453194).  Saving model ...
	 Train_Loss: 0.4480 Train_Acc: 80.456 Val_Loss: 0.4532  BEST VAL Loss: 0.4532  Val_Acc: 81.549

Epoch 26: Validation loss decreased (0.453194 --> 0.450694).  Saving model ...
	 Train_Loss: 0.4465 Train_Acc: 80.427 Val_Loss: 0.4507  BEST VAL Loss: 0.4507  Val_Acc: 81.959

Epoch 27: Validation loss decreased (0.450694 --> 0.448832).  Saving model ...
	 Train_Loss: 0.4450 Train_Acc: 80.440 Val_Loss: 0.4488  BEST VAL Loss: 0.4488  Val_Acc: 81.132

Epoch 28: Validation loss decreased (0.448832 --> 0.446740).  Saving model ...
	 Train_Loss: 0.4436 Train_Acc: 80.526 Val_Loss: 0.4467  BEST VAL Loss: 0.4467  Val_Acc: 81.354

Epoch 29: Validation loss decreased (0.446740 --> 0.445987).  Saving model ...
	 Train_Loss: 0.4423 Train_Acc: 80.643 Val_Loss: 0.4460  BEST VAL Loss: 0.4460  Val_Acc: 79.400

Epoch 30: Validation loss decreased (0.445987 --> 0.444610).  Saving model ...
	 Train_Loss: 0.4410 Train_Acc: 80.487 Val_Loss: 0.4446  BEST VAL Loss: 0.4446  Val_Acc: 81.321

Epoch 31: Validation loss decreased (0.444610 --> 0.442703).  Saving model ...
	 Train_Loss: 0.4398 Train_Acc: 80.881 Val_Loss: 0.4427  BEST VAL Loss: 0.4427  Val_Acc: 82.104

Epoch 32: Validation loss decreased (0.442703 --> 0.441302).  Saving model ...
	 Train_Loss: 0.4386 Train_Acc: 80.649 Val_Loss: 0.4413  BEST VAL Loss: 0.4413  Val_Acc: 81.527

Epoch 33: Validation loss decreased (0.441302 --> 0.439957).  Saving model ...
	 Train_Loss: 0.4375 Train_Acc: 80.826 Val_Loss: 0.4400  BEST VAL Loss: 0.4400  Val_Acc: 81.687

Epoch 34: Validation loss decreased (0.439957 --> 0.438498).  Saving model ...
	 Train_Loss: 0.4364 Train_Acc: 80.808 Val_Loss: 0.4385  BEST VAL Loss: 0.4385  Val_Acc: 81.871

Epoch 35: Validation loss decreased (0.438498 --> 0.436793).  Saving model ...
	 Train_Loss: 0.4354 Train_Acc: 80.692 Val_Loss: 0.4368  BEST VAL Loss: 0.4368  Val_Acc: 82.392

Epoch 36: Validation loss decreased (0.436793 --> 0.435502).  Saving model ...
	 Train_Loss: 0.4345 Train_Acc: 80.888 Val_Loss: 0.4355  BEST VAL Loss: 0.4355  Val_Acc: 81.876

Epoch 37: Validation loss decreased (0.435502 --> 0.434318).  Saving model ...
	 Train_Loss: 0.4335 Train_Acc: 80.999 Val_Loss: 0.4343  BEST VAL Loss: 0.4343  Val_Acc: 81.743

Epoch 38: Validation loss decreased (0.434318 --> 0.433400).  Saving model ...
	 Train_Loss: 0.4326 Train_Acc: 80.862 Val_Loss: 0.4334  BEST VAL Loss: 0.4334  Val_Acc: 81.044

Epoch 39: Validation loss decreased (0.433400 --> 0.432705).  Saving model ...
	 Train_Loss: 0.4317 Train_Acc: 80.901 Val_Loss: 0.4327  BEST VAL Loss: 0.4327  Val_Acc: 80.849

Epoch 40: Validation loss decreased (0.432705 --> 0.431460).  Saving model ...
	 Train_Loss: 0.4308 Train_Acc: 81.016 Val_Loss: 0.4315  BEST VAL Loss: 0.4315  Val_Acc: 82.059

Epoch 41: Validation loss decreased (0.431460 --> 0.430682).  Saving model ...
	 Train_Loss: 0.4300 Train_Acc: 81.009 Val_Loss: 0.4307  BEST VAL Loss: 0.4307  Val_Acc: 81.243

Epoch 42: Validation loss decreased (0.430682 --> 0.429729).  Saving model ...
	 Train_Loss: 0.4292 Train_Acc: 81.138 Val_Loss: 0.4297  BEST VAL Loss: 0.4297  Val_Acc: 81.665

Epoch 43: Validation loss decreased (0.429729 --> 0.428492).  Saving model ...
	 Train_Loss: 0.4284 Train_Acc: 81.076 Val_Loss: 0.4285  BEST VAL Loss: 0.4285  Val_Acc: 82.459

Epoch 44: Validation loss decreased (0.428492 --> 0.427202).  Saving model ...
	 Train_Loss: 0.4277 Train_Acc: 81.091 Val_Loss: 0.4272  BEST VAL Loss: 0.4272  Val_Acc: 82.859

Epoch 45: Validation loss decreased (0.427202 --> 0.426335).  Saving model ...
	 Train_Loss: 0.4270 Train_Acc: 81.218 Val_Loss: 0.4263  BEST VAL Loss: 0.4263  Val_Acc: 82.182

Epoch 46: Validation loss decreased (0.426335 --> 0.425733).  Saving model ...
	 Train_Loss: 0.4262 Train_Acc: 81.249 Val_Loss: 0.4257  BEST VAL Loss: 0.4257  Val_Acc: 81.327

Epoch 47: Validation loss decreased (0.425733 --> 0.424778).  Saving model ...
	 Train_Loss: 0.4256 Train_Acc: 81.106 Val_Loss: 0.4248  BEST VAL Loss: 0.4248  Val_Acc: 82.531

Epoch 48: Validation loss decreased (0.424778 --> 0.423829).  Saving model ...
	 Train_Loss: 0.4249 Train_Acc: 81.342 Val_Loss: 0.4238  BEST VAL Loss: 0.4238  Val_Acc: 82.392

Epoch 49: Validation loss decreased (0.423829 --> 0.422844).  Saving model ...
	 Train_Loss: 0.4242 Train_Acc: 81.301 Val_Loss: 0.4228  BEST VAL Loss: 0.4228  Val_Acc: 82.415

Epoch 50: Validation loss decreased (0.422844 --> 0.422038).  Saving model ...
	 Train_Loss: 0.4236 Train_Acc: 81.306 Val_Loss: 0.4220  BEST VAL Loss: 0.4220  Val_Acc: 82.420

Epoch 51: Validation loss decreased (0.422038 --> 0.421198).  Saving model ...
	 Train_Loss: 0.4230 Train_Acc: 81.318 Val_Loss: 0.4212  BEST VAL Loss: 0.4212  Val_Acc: 82.309

Epoch 52: Validation loss decreased (0.421198 --> 0.420453).  Saving model ...
	 Train_Loss: 0.4224 Train_Acc: 81.156 Val_Loss: 0.4205  BEST VAL Loss: 0.4205  Val_Acc: 82.243

Epoch 53: Validation loss decreased (0.420453 --> 0.419735).  Saving model ...
	 Train_Loss: 0.4218 Train_Acc: 81.432 Val_Loss: 0.4197  BEST VAL Loss: 0.4197  Val_Acc: 81.882

Epoch 54: Validation loss decreased (0.419735 --> 0.419254).  Saving model ...
	 Train_Loss: 0.4212 Train_Acc: 81.416 Val_Loss: 0.4193  BEST VAL Loss: 0.4193  Val_Acc: 81.710

Epoch 55: Validation loss decreased (0.419254 --> 0.418589).  Saving model ...
	 Train_Loss: 0.4207 Train_Acc: 81.442 Val_Loss: 0.4186  BEST VAL Loss: 0.4186  Val_Acc: 82.248

Epoch 56: Validation loss decreased (0.418589 --> 0.417823).  Saving model ...
	 Train_Loss: 0.4201 Train_Acc: 81.499 Val_Loss: 0.4178  BEST VAL Loss: 0.4178  Val_Acc: 82.487

Epoch 57: Validation loss decreased (0.417823 --> 0.417267).  Saving model ...
	 Train_Loss: 0.4196 Train_Acc: 81.587 Val_Loss: 0.4173  BEST VAL Loss: 0.4173  Val_Acc: 82.126

Epoch 58: Validation loss decreased (0.417267 --> 0.416581).  Saving model ...
	 Train_Loss: 0.4191 Train_Acc: 81.558 Val_Loss: 0.4166  BEST VAL Loss: 0.4166  Val_Acc: 82.542

Epoch 59: Validation loss decreased (0.416581 --> 0.415865).  Saving model ...
	 Train_Loss: 0.4186 Train_Acc: 81.356 Val_Loss: 0.4159  BEST VAL Loss: 0.4159  Val_Acc: 82.542

Epoch 60: Validation loss decreased (0.415865 --> 0.415476).  Saving model ...
	 Train_Loss: 0.4181 Train_Acc: 81.532 Val_Loss: 0.4155  BEST VAL Loss: 0.4155  Val_Acc: 81.388

Epoch 61: Validation loss decreased (0.415476 --> 0.414781).  Saving model ...
	 Train_Loss: 0.4176 Train_Acc: 81.539 Val_Loss: 0.4148  BEST VAL Loss: 0.4148  Val_Acc: 82.648

Epoch 62: Validation loss decreased (0.414781 --> 0.414208).  Saving model ...
	 Train_Loss: 0.4171 Train_Acc: 81.471 Val_Loss: 0.4142  BEST VAL Loss: 0.4142  Val_Acc: 82.426

Epoch 63: Validation loss decreased (0.414208 --> 0.413581).  Saving model ...
	 Train_Loss: 0.4167 Train_Acc: 81.654 Val_Loss: 0.4136  BEST VAL Loss: 0.4136  Val_Acc: 82.548

Epoch 64: Validation loss decreased (0.413581 --> 0.412883).  Saving model ...
	 Train_Loss: 0.4162 Train_Acc: 81.637 Val_Loss: 0.4129  BEST VAL Loss: 0.4129  Val_Acc: 82.859

Epoch 65: Validation loss decreased (0.412883 --> 0.412326).  Saving model ...
	 Train_Loss: 0.4158 Train_Acc: 81.541 Val_Loss: 0.4123  BEST VAL Loss: 0.4123  Val_Acc: 82.598

Epoch 66: Validation loss decreased (0.412326 --> 0.411671).  Saving model ...
	 Train_Loss: 0.4154 Train_Acc: 81.628 Val_Loss: 0.4117  BEST VAL Loss: 0.4117  Val_Acc: 82.920

Epoch 67: Validation loss decreased (0.411671 --> 0.411327).  Saving model ...
	 Train_Loss: 0.4150 Train_Acc: 81.520 Val_Loss: 0.4113  BEST VAL Loss: 0.4113  Val_Acc: 82.154

Epoch 68: Validation loss decreased (0.411327 --> 0.410859).  Saving model ...
	 Train_Loss: 0.4146 Train_Acc: 81.393 Val_Loss: 0.4109  BEST VAL Loss: 0.4109  Val_Acc: 82.376

Epoch 69: Validation loss decreased (0.410859 --> 0.410368).  Saving model ...
	 Train_Loss: 0.4142 Train_Acc: 81.617 Val_Loss: 0.4104  BEST VAL Loss: 0.4104  Val_Acc: 82.298

Epoch 70: Validation loss decreased (0.410368 --> 0.409807).  Saving model ...
	 Train_Loss: 0.4138 Train_Acc: 81.570 Val_Loss: 0.4098  BEST VAL Loss: 0.4098  Val_Acc: 83.081

Epoch 71: Validation loss decreased (0.409807 --> 0.409285).  Saving model ...
	 Train_Loss: 0.4134 Train_Acc: 81.618 Val_Loss: 0.4093  BEST VAL Loss: 0.4093  Val_Acc: 82.676

Epoch 72: Validation loss decreased (0.409285 --> 0.408756).  Saving model ...
	 Train_Loss: 0.4131 Train_Acc: 81.699 Val_Loss: 0.4088  BEST VAL Loss: 0.4088  Val_Acc: 82.936

Epoch 73: Validation loss decreased (0.408756 --> 0.408157).  Saving model ...
	 Train_Loss: 0.4127 Train_Acc: 81.635 Val_Loss: 0.4082  BEST VAL Loss: 0.4082  Val_Acc: 83.170

Epoch 74: Validation loss decreased (0.408157 --> 0.407614).  Saving model ...
	 Train_Loss: 0.4123 Train_Acc: 81.798 Val_Loss: 0.4076  BEST VAL Loss: 0.4076  Val_Acc: 83.047

Epoch 75: Validation loss decreased (0.407614 --> 0.407458).  Saving model ...
	 Train_Loss: 0.4120 Train_Acc: 81.668 Val_Loss: 0.4075  BEST VAL Loss: 0.4075  Val_Acc: 81.044

Epoch 76: Validation loss decreased (0.407458 --> 0.407187).  Saving model ...
	 Train_Loss: 0.4116 Train_Acc: 81.665 Val_Loss: 0.4072  BEST VAL Loss: 0.4072  Val_Acc: 81.804

Epoch 77: Validation loss decreased (0.407187 --> 0.406741).  Saving model ...
	 Train_Loss: 0.4113 Train_Acc: 81.818 Val_Loss: 0.4067  BEST VAL Loss: 0.4067  Val_Acc: 82.381

Epoch 78: Validation loss decreased (0.406741 --> 0.406440).  Saving model ...
	 Train_Loss: 0.4110 Train_Acc: 81.651 Val_Loss: 0.4064  BEST VAL Loss: 0.4064  Val_Acc: 81.921

Epoch 79: Validation loss decreased (0.406440 --> 0.405968).  Saving model ...
	 Train_Loss: 0.4106 Train_Acc: 81.742 Val_Loss: 0.4060  BEST VAL Loss: 0.4060  Val_Acc: 82.936

Epoch 80: Validation loss decreased (0.405968 --> 0.405478).  Saving model ...
	 Train_Loss: 0.4103 Train_Acc: 81.872 Val_Loss: 0.4055  BEST VAL Loss: 0.4055  Val_Acc: 82.992

Epoch 81: Validation loss decreased (0.405478 --> 0.405061).  Saving model ...
	 Train_Loss: 0.4100 Train_Acc: 81.754 Val_Loss: 0.4051  BEST VAL Loss: 0.4051  Val_Acc: 82.664

Epoch 82: Validation loss decreased (0.405061 --> 0.404673).  Saving model ...
	 Train_Loss: 0.4097 Train_Acc: 81.873 Val_Loss: 0.4047  BEST VAL Loss: 0.4047  Val_Acc: 82.565

Epoch 83: Validation loss decreased (0.404673 --> 0.404418).  Saving model ...
	 Train_Loss: 0.4094 Train_Acc: 81.811 Val_Loss: 0.4044  BEST VAL Loss: 0.4044  Val_Acc: 82.087

Epoch 84: Validation loss decreased (0.404418 --> 0.403950).  Saving model ...
	 Train_Loss: 0.4091 Train_Acc: 81.805 Val_Loss: 0.4039  BEST VAL Loss: 0.4039  Val_Acc: 83.081

Epoch 85: Validation loss decreased (0.403950 --> 0.403572).  Saving model ...
	 Train_Loss: 0.4088 Train_Acc: 81.807 Val_Loss: 0.4036  BEST VAL Loss: 0.4036  Val_Acc: 82.659

Epoch 86: Validation loss decreased (0.403572 --> 0.403118).  Saving model ...
	 Train_Loss: 0.4085 Train_Acc: 81.925 Val_Loss: 0.4031  BEST VAL Loss: 0.4031  Val_Acc: 82.959

Epoch 87: Validation loss decreased (0.403118 --> 0.402700).  Saving model ...
	 Train_Loss: 0.4082 Train_Acc: 82.000 Val_Loss: 0.4027  BEST VAL Loss: 0.4027  Val_Acc: 82.964

Epoch 88: Validation loss decreased (0.402700 --> 0.402356).  Saving model ...
	 Train_Loss: 0.4079 Train_Acc: 81.871 Val_Loss: 0.4024  BEST VAL Loss: 0.4024  Val_Acc: 82.520

Epoch 89: Validation loss decreased (0.402356 --> 0.401997).  Saving model ...
	 Train_Loss: 0.4076 Train_Acc: 81.825 Val_Loss: 0.4020  BEST VAL Loss: 0.4020  Val_Acc: 82.998

Epoch 90: Validation loss decreased (0.401997 --> 0.401648).  Saving model ...
	 Train_Loss: 0.4073 Train_Acc: 82.019 Val_Loss: 0.4016  BEST VAL Loss: 0.4016  Val_Acc: 82.631

Epoch 91: Validation loss decreased (0.401648 --> 0.401410).  Saving model ...
	 Train_Loss: 0.4071 Train_Acc: 81.955 Val_Loss: 0.4014  BEST VAL Loss: 0.4014  Val_Acc: 82.614

Epoch 92: Validation loss decreased (0.401410 --> 0.401003).  Saving model ...
	 Train_Loss: 0.4068 Train_Acc: 81.829 Val_Loss: 0.4010  BEST VAL Loss: 0.4010  Val_Acc: 83.147

Epoch 93: Validation loss decreased (0.401003 --> 0.400695).  Saving model ...
	 Train_Loss: 0.4065 Train_Acc: 82.060 Val_Loss: 0.4007  BEST VAL Loss: 0.4007  Val_Acc: 82.676

Epoch 94: Validation loss decreased (0.400695 --> 0.400360).  Saving model ...
	 Train_Loss: 0.4063 Train_Acc: 82.017 Val_Loss: 0.4004  BEST VAL Loss: 0.4004  Val_Acc: 82.614

Epoch 95: Validation loss decreased (0.400360 --> 0.400069).  Saving model ...
	 Train_Loss: 0.4060 Train_Acc: 81.891 Val_Loss: 0.4001  BEST VAL Loss: 0.4001  Val_Acc: 82.770

Epoch 96: Validation loss decreased (0.400069 --> 0.399714).  Saving model ...
	 Train_Loss: 0.4058 Train_Acc: 81.975 Val_Loss: 0.3997  BEST VAL Loss: 0.3997  Val_Acc: 83.025

Epoch 97: Validation loss decreased (0.399714 --> 0.399478).  Saving model ...
	 Train_Loss: 0.4055 Train_Acc: 82.025 Val_Loss: 0.3995  BEST VAL Loss: 0.3995  Val_Acc: 82.248

Epoch 98: Validation loss decreased (0.399478 --> 0.399114).  Saving model ...
	 Train_Loss: 0.4053 Train_Acc: 81.984 Val_Loss: 0.3991  BEST VAL Loss: 0.3991  Val_Acc: 83.158

Epoch 99: Validation loss decreased (0.399114 --> 0.398752).  Saving model ...
	 Train_Loss: 0.4050 Train_Acc: 82.055 Val_Loss: 0.3988  BEST VAL Loss: 0.3988  Val_Acc: 83.053

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.83      0.80      0.81     56122
           1       0.88      0.89      0.88     87992

    accuracy                           0.86    144114
   macro avg       0.85      0.85      0.85    144114
weighted avg       0.86      0.86      0.86    144114

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.79      0.77      0.78      7016
           1       0.85      0.87      0.86     10999

    accuracy                           0.83     18015
   macro avg       0.82      0.82      0.82     18015
weighted avg       0.83      0.83      0.83     18015

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.80      0.78      0.79      7015
           1       0.86      0.87      0.87     11000

    accuracy                           0.84     18015
   macro avg       0.83      0.83      0.83     18015
weighted avg       0.84      0.84      0.84     18015

              precision    recall  f1-score   support

           0       0.80      0.78      0.79      7015
           1       0.86      0.87      0.87     11000

    accuracy                           0.84     18015
   macro avg       0.83      0.83      0.83     18015
weighted avg       0.84      0.84      0.84     18015

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.45      0.46      0.45     34394
           1       0.50      0.49      0.50     38465

    accuracy                           0.48     72859
   macro avg       0.48      0.48      0.48     72859
weighted avg       0.48      0.48      0.48     72859

              precision    recall  f1-score   support

           0       0.45      0.46      0.45     34394
           1       0.50      0.49      0.50     38465

    accuracy                           0.48     72859
   macro avg       0.48      0.48      0.48     72859
weighted avg       0.48      0.48      0.48     72859

completed
