[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '514077f8'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c84df852'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '04f776b8'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c952c1bf'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Flagellin_0.100_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Flagellin_0.100_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Flagellin_0.100_DMSO_0.025']
The dimensions of the data are: (244371, 1270)
Number of total missing values across all columns: 488742
Data Subset Is Off
Wells held out for testing: ['K07' 'L10']
Wells to use for training, validation, and testing ['D06' 'D07' 'K06' 'L05' 'L11']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.512964).  Saving model ...
	 Train_Loss: 0.5743 Train_Acc: 68.012 Val_Loss: 0.5130  BEST VAL Loss: 0.5130  Val_Acc: 74.309

Epoch 1: Validation loss decreased (0.512964 --> 0.502597).  Saving model ...
	 Train_Loss: 0.5530 Train_Acc: 70.958 Val_Loss: 0.5026  BEST VAL Loss: 0.5026  Val_Acc: 74.962

Epoch 2: Validation loss decreased (0.502597 --> 0.489102).  Saving model ...
	 Train_Loss: 0.5395 Train_Acc: 72.947 Val_Loss: 0.4891  BEST VAL Loss: 0.4891  Val_Acc: 77.505

Epoch 3: Validation loss decreased (0.489102 --> 0.481204).  Saving model ...
	 Train_Loss: 0.5294 Train_Acc: 73.475 Val_Loss: 0.4812  BEST VAL Loss: 0.4812  Val_Acc: 79.153

Epoch 4: Validation loss decreased (0.481204 --> 0.473914).  Saving model ...
	 Train_Loss: 0.5207 Train_Acc: 74.146 Val_Loss: 0.4739  BEST VAL Loss: 0.4739  Val_Acc: 79.713

Epoch 5: Validation loss decreased (0.473914 --> 0.466199).  Saving model ...
	 Train_Loss: 0.5132 Train_Acc: 74.709 Val_Loss: 0.4662  BEST VAL Loss: 0.4662  Val_Acc: 80.528

Epoch 6: Validation loss decreased (0.466199 --> 0.460087).  Saving model ...
	 Train_Loss: 0.5070 Train_Acc: 75.216 Val_Loss: 0.4601  BEST VAL Loss: 0.4601  Val_Acc: 80.291

Epoch 7: Validation loss decreased (0.460087 --> 0.455087).  Saving model ...
	 Train_Loss: 0.5017 Train_Acc: 75.584 Val_Loss: 0.4551  BEST VAL Loss: 0.4551  Val_Acc: 79.973

Epoch 8: Validation loss decreased (0.455087 --> 0.450396).  Saving model ...
	 Train_Loss: 0.4972 Train_Acc: 75.990 Val_Loss: 0.4504  BEST VAL Loss: 0.4504  Val_Acc: 80.829

Epoch 9: Validation loss decreased (0.450396 --> 0.446510).  Saving model ...
	 Train_Loss: 0.4932 Train_Acc: 76.346 Val_Loss: 0.4465  BEST VAL Loss: 0.4465  Val_Acc: 81.147

Epoch 10: Validation loss decreased (0.446510 --> 0.442868).  Saving model ...
	 Train_Loss: 0.4899 Train_Acc: 76.190 Val_Loss: 0.4429  BEST VAL Loss: 0.4429  Val_Acc: 80.950

Epoch 11: Validation loss decreased (0.442868 --> 0.439807).  Saving model ...
	 Train_Loss: 0.4867 Train_Acc: 76.483 Val_Loss: 0.4398  BEST VAL Loss: 0.4398  Val_Acc: 81.320

Epoch 12: Validation loss decreased (0.439807 --> 0.436873).  Saving model ...
	 Train_Loss: 0.4838 Train_Acc: 76.667 Val_Loss: 0.4369  BEST VAL Loss: 0.4369  Val_Acc: 80.644

Epoch 13: Validation loss decreased (0.436873 --> 0.433695).  Saving model ...
	 Train_Loss: 0.4813 Train_Acc: 76.527 Val_Loss: 0.4337  BEST VAL Loss: 0.4337  Val_Acc: 81.499

Epoch 14: Validation loss decreased (0.433695 --> 0.431698).  Saving model ...
	 Train_Loss: 0.4791 Train_Acc: 76.654 Val_Loss: 0.4317  BEST VAL Loss: 0.4317  Val_Acc: 81.823

Epoch 15: Validation loss decreased (0.431698 --> 0.429909).  Saving model ...
	 Train_Loss: 0.4769 Train_Acc: 76.798 Val_Loss: 0.4299  BEST VAL Loss: 0.4299  Val_Acc: 81.297

Epoch 16: Validation loss decreased (0.429909 --> 0.428421).  Saving model ...
	 Train_Loss: 0.4750 Train_Acc: 76.654 Val_Loss: 0.4284  BEST VAL Loss: 0.4284  Val_Acc: 81.176

Epoch 17: Validation loss decreased (0.428421 --> 0.426210).  Saving model ...
	 Train_Loss: 0.4733 Train_Acc: 76.586 Val_Loss: 0.4262  BEST VAL Loss: 0.4262  Val_Acc: 82.314

Epoch 18: Validation loss decreased (0.426210 --> 0.424379).  Saving model ...
	 Train_Loss: 0.4715 Train_Acc: 77.138 Val_Loss: 0.4244  BEST VAL Loss: 0.4244  Val_Acc: 81.239

Epoch 19: Validation loss decreased (0.424379 --> 0.422814).  Saving model ...
	 Train_Loss: 0.4700 Train_Acc: 76.825 Val_Loss: 0.4228  BEST VAL Loss: 0.4228  Val_Acc: 81.939

Epoch 20: Validation loss decreased (0.422814 --> 0.421135).  Saving model ...
	 Train_Loss: 0.4686 Train_Acc: 77.003 Val_Loss: 0.4211  BEST VAL Loss: 0.4211  Val_Acc: 82.551

Epoch 21: Validation loss decreased (0.421135 --> 0.419599).  Saving model ...
	 Train_Loss: 0.4673 Train_Acc: 76.936 Val_Loss: 0.4196  BEST VAL Loss: 0.4196  Val_Acc: 81.991

Epoch 22: Validation loss decreased (0.419599 --> 0.418400).  Saving model ...
	 Train_Loss: 0.4660 Train_Acc: 77.137 Val_Loss: 0.4184  BEST VAL Loss: 0.4184  Val_Acc: 81.661

Epoch 23: Validation loss decreased (0.418400 --> 0.417010).  Saving model ...
	 Train_Loss: 0.4649 Train_Acc: 77.028 Val_Loss: 0.4170  BEST VAL Loss: 0.4170  Val_Acc: 81.632

Epoch 24: Validation loss decreased (0.417010 --> 0.415878).  Saving model ...
	 Train_Loss: 0.4638 Train_Acc: 76.903 Val_Loss: 0.4159  BEST VAL Loss: 0.4159  Val_Acc: 81.349

Epoch 25: Validation loss decreased (0.415878 --> 0.414522).  Saving model ...
	 Train_Loss: 0.4628 Train_Acc: 76.969 Val_Loss: 0.4145  BEST VAL Loss: 0.4145  Val_Acc: 82.447

Epoch 26: Validation loss decreased (0.414522 --> 0.413481).  Saving model ...
	 Train_Loss: 0.4618 Train_Acc: 77.051 Val_Loss: 0.4135  BEST VAL Loss: 0.4135  Val_Acc: 81.361

Epoch 27: Validation loss decreased (0.413481 --> 0.412251).  Saving model ...
	 Train_Loss: 0.4608 Train_Acc: 77.057 Val_Loss: 0.4123  BEST VAL Loss: 0.4123  Val_Acc: 82.603

Epoch 28: Validation loss decreased (0.412251 --> 0.411134).  Saving model ...
	 Train_Loss: 0.4599 Train_Acc: 77.133 Val_Loss: 0.4111  BEST VAL Loss: 0.4111  Val_Acc: 81.921

Epoch 29: Validation loss decreased (0.411134 --> 0.410395).  Saving model ...
	 Train_Loss: 0.4591 Train_Acc: 76.896 Val_Loss: 0.4104  BEST VAL Loss: 0.4104  Val_Acc: 81.482

Epoch 30: Validation loss decreased (0.410395 --> 0.409694).  Saving model ...
	 Train_Loss: 0.4583 Train_Acc: 77.037 Val_Loss: 0.4097  BEST VAL Loss: 0.4097  Val_Acc: 81.563

Epoch 31: Validation loss decreased (0.409694 --> 0.409159).  Saving model ...
	 Train_Loss: 0.4575 Train_Acc: 77.218 Val_Loss: 0.4092  BEST VAL Loss: 0.4092  Val_Acc: 81.187

Epoch 32: Validation loss decreased (0.409159 --> 0.408529).  Saving model ...
	 Train_Loss: 0.4568 Train_Acc: 76.995 Val_Loss: 0.4085  BEST VAL Loss: 0.4085  Val_Acc: 82.690

Epoch 33: Validation loss decreased (0.408529 --> 0.407693).  Saving model ...
	 Train_Loss: 0.4561 Train_Acc: 77.099 Val_Loss: 0.4077  BEST VAL Loss: 0.4077  Val_Acc: 81.800

Epoch 34: Validation loss decreased (0.407693 --> 0.406923).  Saving model ...
	 Train_Loss: 0.4553 Train_Acc: 77.236 Val_Loss: 0.4069  BEST VAL Loss: 0.4069  Val_Acc: 81.673

Epoch 35: Validation loss decreased (0.406923 --> 0.406066).  Saving model ...
	 Train_Loss: 0.4547 Train_Acc: 77.070 Val_Loss: 0.4061  BEST VAL Loss: 0.4061  Val_Acc: 82.707

Epoch 36: Validation loss decreased (0.406066 --> 0.405215).  Saving model ...
	 Train_Loss: 0.4541 Train_Acc: 76.986 Val_Loss: 0.4052  BEST VAL Loss: 0.4052  Val_Acc: 82.892

Epoch 37: Validation loss decreased (0.405215 --> 0.404489).  Saving model ...
	 Train_Loss: 0.4534 Train_Acc: 77.259 Val_Loss: 0.4045  BEST VAL Loss: 0.4045  Val_Acc: 81.892

Epoch 38: Validation loss decreased (0.404489 --> 0.403902).  Saving model ...
	 Train_Loss: 0.4528 Train_Acc: 77.248 Val_Loss: 0.4039  BEST VAL Loss: 0.4039  Val_Acc: 81.644

Epoch 39: Validation loss decreased (0.403902 --> 0.403388).  Saving model ...
	 Train_Loss: 0.4522 Train_Acc: 77.015 Val_Loss: 0.4034  BEST VAL Loss: 0.4034  Val_Acc: 81.603

Epoch 40: Validation loss decreased (0.403388 --> 0.402819).  Saving model ...
	 Train_Loss: 0.4517 Train_Acc: 77.094 Val_Loss: 0.4028  BEST VAL Loss: 0.4028  Val_Acc: 81.228

Epoch 41: Validation loss decreased (0.402819 --> 0.402355).  Saving model ...
	 Train_Loss: 0.4511 Train_Acc: 77.109 Val_Loss: 0.4024  BEST VAL Loss: 0.4024  Val_Acc: 81.355

Epoch 42: Validation loss decreased (0.402355 --> 0.401600).  Saving model ...
	 Train_Loss: 0.4506 Train_Acc: 77.170 Val_Loss: 0.4016  BEST VAL Loss: 0.4016  Val_Acc: 82.701

Epoch 43: Validation loss decreased (0.401600 --> 0.400973).  Saving model ...
	 Train_Loss: 0.4501 Train_Acc: 77.233 Val_Loss: 0.4010  BEST VAL Loss: 0.4010  Val_Acc: 82.667

Epoch 44: Validation loss decreased (0.400973 --> 0.400415).  Saving model ...
	 Train_Loss: 0.4496 Train_Acc: 77.334 Val_Loss: 0.4004  BEST VAL Loss: 0.4004  Val_Acc: 82.800

Epoch 45: Validation loss decreased (0.400415 --> 0.399943).  Saving model ...
	 Train_Loss: 0.4491 Train_Acc: 77.302 Val_Loss: 0.3999  BEST VAL Loss: 0.3999  Val_Acc: 81.863

Epoch 46: Validation loss decreased (0.399943 --> 0.399490).  Saving model ...
	 Train_Loss: 0.4486 Train_Acc: 77.232 Val_Loss: 0.3995  BEST VAL Loss: 0.3995  Val_Acc: 81.251

Epoch 47: Validation loss decreased (0.399490 --> 0.398912).  Saving model ...
	 Train_Loss: 0.4482 Train_Acc: 77.177 Val_Loss: 0.3989  BEST VAL Loss: 0.3989  Val_Acc: 82.673

Epoch 48: Validation loss decreased (0.398912 --> 0.398460).  Saving model ...
	 Train_Loss: 0.4477 Train_Acc: 77.246 Val_Loss: 0.3985  BEST VAL Loss: 0.3985  Val_Acc: 81.586

Epoch 49: Validation loss decreased (0.398460 --> 0.398048).  Saving model ...
	 Train_Loss: 0.4473 Train_Acc: 77.271 Val_Loss: 0.3980  BEST VAL Loss: 0.3980  Val_Acc: 81.742

Epoch 50: Validation loss decreased (0.398048 --> 0.397613).  Saving model ...
	 Train_Loss: 0.4469 Train_Acc: 77.317 Val_Loss: 0.3976  BEST VAL Loss: 0.3976  Val_Acc: 81.638

Epoch 51: Validation loss decreased (0.397613 --> 0.397183).  Saving model ...
	 Train_Loss: 0.4465 Train_Acc: 77.303 Val_Loss: 0.3972  BEST VAL Loss: 0.3972  Val_Acc: 82.638

Epoch 52: Validation loss decreased (0.397183 --> 0.396736).  Saving model ...
	 Train_Loss: 0.4461 Train_Acc: 77.341 Val_Loss: 0.3967  BEST VAL Loss: 0.3967  Val_Acc: 81.603

Epoch 53: Validation loss decreased (0.396736 --> 0.396281).  Saving model ...
	 Train_Loss: 0.4456 Train_Acc: 77.416 Val_Loss: 0.3963  BEST VAL Loss: 0.3963  Val_Acc: 81.655

Epoch 54: Validation loss decreased (0.396281 --> 0.395833).  Saving model ...
	 Train_Loss: 0.4452 Train_Acc: 77.294 Val_Loss: 0.3958  BEST VAL Loss: 0.3958  Val_Acc: 82.713

Epoch 55: Validation loss decreased (0.395833 --> 0.395417).  Saving model ...
	 Train_Loss: 0.4448 Train_Acc: 77.333 Val_Loss: 0.3954  BEST VAL Loss: 0.3954  Val_Acc: 81.441

Epoch 56: Validation loss decreased (0.395417 --> 0.395109).  Saving model ...
	 Train_Loss: 0.4445 Train_Acc: 77.450 Val_Loss: 0.3951  BEST VAL Loss: 0.3951  Val_Acc: 81.661

Epoch 57: Validation loss decreased (0.395109 --> 0.394715).  Saving model ...
	 Train_Loss: 0.4441 Train_Acc: 77.457 Val_Loss: 0.3947  BEST VAL Loss: 0.3947  Val_Acc: 81.863

Epoch 58: Validation loss decreased (0.394715 --> 0.394262).  Saving model ...
	 Train_Loss: 0.4438 Train_Acc: 77.385 Val_Loss: 0.3943  BEST VAL Loss: 0.3943  Val_Acc: 82.158

Epoch 59: Validation loss decreased (0.394262 --> 0.393893).  Saving model ...
	 Train_Loss: 0.4434 Train_Acc: 77.278 Val_Loss: 0.3939  BEST VAL Loss: 0.3939  Val_Acc: 82.777

Epoch 60: Validation loss decreased (0.393893 --> 0.393523).  Saving model ...
	 Train_Loss: 0.4431 Train_Acc: 77.459 Val_Loss: 0.3935  BEST VAL Loss: 0.3935  Val_Acc: 81.661

Epoch 61: Validation loss decreased (0.393523 --> 0.393152).  Saving model ...
	 Train_Loss: 0.4428 Train_Acc: 77.223 Val_Loss: 0.3932  BEST VAL Loss: 0.3932  Val_Acc: 82.014

Epoch 62: Validation loss decreased (0.393152 --> 0.392712).  Saving model ...
	 Train_Loss: 0.4425 Train_Acc: 77.190 Val_Loss: 0.3927  BEST VAL Loss: 0.3927  Val_Acc: 81.869

Epoch 63: Validation loss decreased (0.392712 --> 0.392358).  Saving model ...
	 Train_Loss: 0.4422 Train_Acc: 77.324 Val_Loss: 0.3924  BEST VAL Loss: 0.3924  Val_Acc: 82.915

Epoch 64: Validation loss decreased (0.392358 --> 0.392067).  Saving model ...
	 Train_Loss: 0.4418 Train_Acc: 77.192 Val_Loss: 0.3921  BEST VAL Loss: 0.3921  Val_Acc: 82.118

Epoch 65: Validation loss decreased (0.392067 --> 0.391745).  Saving model ...
	 Train_Loss: 0.4415 Train_Acc: 77.445 Val_Loss: 0.3917  BEST VAL Loss: 0.3917  Val_Acc: 82.649

Epoch 66: Validation loss decreased (0.391745 --> 0.391437).  Saving model ...
	 Train_Loss: 0.4412 Train_Acc: 77.479 Val_Loss: 0.3914  BEST VAL Loss: 0.3914  Val_Acc: 81.788

Epoch 67: Validation loss decreased (0.391437 --> 0.391188).  Saving model ...
	 Train_Loss: 0.4409 Train_Acc: 77.433 Val_Loss: 0.3912  BEST VAL Loss: 0.3912  Val_Acc: 81.488

Epoch 68: Validation loss decreased (0.391188 --> 0.390798).  Saving model ...
	 Train_Loss: 0.4407 Train_Acc: 77.312 Val_Loss: 0.3908  BEST VAL Loss: 0.3908  Val_Acc: 82.534

Epoch 69: Validation loss decreased (0.390798 --> 0.390471).  Saving model ...
	 Train_Loss: 0.4404 Train_Acc: 77.504 Val_Loss: 0.3905  BEST VAL Loss: 0.3905  Val_Acc: 82.262

Epoch 70: Validation loss decreased (0.390471 --> 0.390130).  Saving model ...
	 Train_Loss: 0.4401 Train_Acc: 77.469 Val_Loss: 0.3901  BEST VAL Loss: 0.3901  Val_Acc: 82.719

Epoch 71: Validation loss decreased (0.390130 --> 0.389790).  Saving model ...
	 Train_Loss: 0.4398 Train_Acc: 77.492 Val_Loss: 0.3898  BEST VAL Loss: 0.3898  Val_Acc: 82.002

Epoch 72: Validation loss decreased (0.389790 --> 0.389491).  Saving model ...
	 Train_Loss: 0.4395 Train_Acc: 77.247 Val_Loss: 0.3895  BEST VAL Loss: 0.3895  Val_Acc: 82.071

Epoch 73: Validation loss decreased (0.389491 --> 0.389385).  Saving model ...
	 Train_Loss: 0.4393 Train_Acc: 77.450 Val_Loss: 0.3894  BEST VAL Loss: 0.3894  Val_Acc: 81.274

Epoch 74: Validation loss decreased (0.389385 --> 0.389136).  Saving model ...
	 Train_Loss: 0.4390 Train_Acc: 77.518 Val_Loss: 0.3891  BEST VAL Loss: 0.3891  Val_Acc: 82.066

Epoch 75: Validation loss decreased (0.389136 --> 0.388857).  Saving model ...
	 Train_Loss: 0.4388 Train_Acc: 77.448 Val_Loss: 0.3889  BEST VAL Loss: 0.3889  Val_Acc: 82.649

Epoch 76: Validation loss decreased (0.388857 --> 0.388554).  Saving model ...
	 Train_Loss: 0.4385 Train_Acc: 77.342 Val_Loss: 0.3886  BEST VAL Loss: 0.3886  Val_Acc: 81.921

Epoch 77: Validation loss decreased (0.388554 --> 0.388291).  Saving model ...
	 Train_Loss: 0.4383 Train_Acc: 77.578 Val_Loss: 0.3883  BEST VAL Loss: 0.3883  Val_Acc: 81.644

Epoch 78: Validation loss decreased (0.388291 --> 0.388052).  Saving model ...
	 Train_Loss: 0.4380 Train_Acc: 77.367 Val_Loss: 0.3881  BEST VAL Loss: 0.3881  Val_Acc: 81.465

Epoch 79: Validation loss decreased (0.388052 --> 0.387767).  Saving model ...
	 Train_Loss: 0.4378 Train_Acc: 77.491 Val_Loss: 0.3878  BEST VAL Loss: 0.3878  Val_Acc: 81.811

Epoch 80: Validation loss decreased (0.387767 --> 0.387526).  Saving model ...
	 Train_Loss: 0.4376 Train_Acc: 77.454 Val_Loss: 0.3875  BEST VAL Loss: 0.3875  Val_Acc: 81.829

Epoch 81: Validation loss decreased (0.387526 --> 0.387231).  Saving model ...
	 Train_Loss: 0.4373 Train_Acc: 77.455 Val_Loss: 0.3872  BEST VAL Loss: 0.3872  Val_Acc: 83.077

Epoch 82: Validation loss decreased (0.387231 --> 0.387067).  Saving model ...
	 Train_Loss: 0.4371 Train_Acc: 77.570 Val_Loss: 0.3871  BEST VAL Loss: 0.3871  Val_Acc: 82.037

Epoch 83: Validation loss decreased (0.387067 --> 0.386865).  Saving model ...
	 Train_Loss: 0.4369 Train_Acc: 77.628 Val_Loss: 0.3869  BEST VAL Loss: 0.3869  Val_Acc: 81.782

Epoch 84: Validation loss decreased (0.386865 --> 0.386695).  Saving model ...
	 Train_Loss: 0.4367 Train_Acc: 77.325 Val_Loss: 0.3867  BEST VAL Loss: 0.3867  Val_Acc: 81.551

Epoch 85: Validation loss decreased (0.386695 --> 0.386458).  Saving model ...
	 Train_Loss: 0.4365 Train_Acc: 77.576 Val_Loss: 0.3865  BEST VAL Loss: 0.3865  Val_Acc: 81.869

Epoch 86: Validation loss decreased (0.386458 --> 0.386270).  Saving model ...
	 Train_Loss: 0.4363 Train_Acc: 77.547 Val_Loss: 0.3863  BEST VAL Loss: 0.3863  Val_Acc: 82.170

Epoch 87: Validation loss decreased (0.386270 --> 0.386120).  Saving model ...
	 Train_Loss: 0.4361 Train_Acc: 77.567 Val_Loss: 0.3861  BEST VAL Loss: 0.3861  Val_Acc: 81.690

Epoch 88: Validation loss decreased (0.386120 --> 0.385861).  Saving model ...
	 Train_Loss: 0.4358 Train_Acc: 77.458 Val_Loss: 0.3859  BEST VAL Loss: 0.3859  Val_Acc: 81.944

Epoch 89: Validation loss decreased (0.385861 --> 0.385617).  Saving model ...
	 Train_Loss: 0.4357 Train_Acc: 77.310 Val_Loss: 0.3856  BEST VAL Loss: 0.3856  Val_Acc: 82.973

Epoch 90: Validation loss decreased (0.385617 --> 0.385357).  Saving model ...
	 Train_Loss: 0.4355 Train_Acc: 77.484 Val_Loss: 0.3854  BEST VAL Loss: 0.3854  Val_Acc: 82.123

Epoch 91: Validation loss decreased (0.385357 --> 0.385189).  Saving model ...
	 Train_Loss: 0.4353 Train_Acc: 77.523 Val_Loss: 0.3852  BEST VAL Loss: 0.3852  Val_Acc: 82.054

Epoch 92: Validation loss decreased (0.385189 --> 0.385093).  Saving model ...
	 Train_Loss: 0.4351 Train_Acc: 77.390 Val_Loss: 0.3851  BEST VAL Loss: 0.3851  Val_Acc: 81.563

Epoch 93: Validation loss decreased (0.385093 --> 0.384918).  Saving model ...
	 Train_Loss: 0.4349 Train_Acc: 77.440 Val_Loss: 0.3849  BEST VAL Loss: 0.3849  Val_Acc: 81.754

Epoch 94: Validation loss decreased (0.384918 --> 0.384694).  Saving model ...
	 Train_Loss: 0.4347 Train_Acc: 77.426 Val_Loss: 0.3847  BEST VAL Loss: 0.3847  Val_Acc: 82.678

Epoch 95: Validation loss decreased (0.384694 --> 0.384534).  Saving model ...
	 Train_Loss: 0.4345 Train_Acc: 77.517 Val_Loss: 0.3845  BEST VAL Loss: 0.3845  Val_Acc: 81.834

Epoch 96: Validation loss decreased (0.384534 --> 0.384336).  Saving model ...
	 Train_Loss: 0.4344 Train_Acc: 77.575 Val_Loss: 0.3843  BEST VAL Loss: 0.3843  Val_Acc: 81.806

Epoch 97: Validation loss decreased (0.384336 --> 0.384160).  Saving model ...
	 Train_Loss: 0.4342 Train_Acc: 77.599 Val_Loss: 0.3842  BEST VAL Loss: 0.3842  Val_Acc: 82.095

Epoch 98: Validation loss decreased (0.384160 --> 0.383968).  Saving model ...
	 Train_Loss: 0.4340 Train_Acc: 77.507 Val_Loss: 0.3840  BEST VAL Loss: 0.3840  Val_Acc: 82.962

Epoch 99: Validation loss decreased (0.383968 --> 0.383783).  Saving model ...
	 Train_Loss: 0.4338 Train_Acc: 77.549 Val_Loss: 0.3838  BEST VAL Loss: 0.3838  Val_Acc: 81.811

Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.75      0.83      0.79     50422
           1       0.90      0.84      0.87     87993

    accuracy                           0.84    138415
   macro avg       0.82      0.84      0.83    138415
weighted avg       0.84      0.84      0.84    138415

Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.72      0.81      0.77      6303
           1       0.88      0.82      0.85     10999

    accuracy                           0.82     17302
   macro avg       0.80      0.82      0.81     17302
weighted avg       0.83      0.82      0.82     17302

Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.73      0.81      0.77      6303
           1       0.88      0.83      0.85     10999

    accuracy                           0.82     17302
   macro avg       0.81      0.82      0.81     17302
weighted avg       0.83      0.82      0.82     17302

              precision    recall  f1-score   support

           0       0.73      0.81      0.77      6303
           1       0.88      0.83      0.85     10999

    accuracy                           0.82     17302
   macro avg       0.81      0.82      0.81     17302
weighted avg       0.83      0.82      0.82     17302

Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_0.100_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.63      0.69      0.66     32887
           1       0.71      0.65      0.68     38465

    accuracy                           0.67     71352
   macro avg       0.67      0.67      0.67     71352
weighted avg       0.67      0.67      0.67     71352

              precision    recall  f1-score   support

           0       0.63      0.69      0.66     32887
           1       0.71      0.65      0.68     38465

    accuracy                           0.67     71352
   macro avg       0.67      0.67      0.67     71352
weighted avg       0.67      0.67      0.67     71352

completed
