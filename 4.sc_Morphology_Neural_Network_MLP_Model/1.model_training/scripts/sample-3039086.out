[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'be42404b'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'cfd440c0'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '9eb1fd69'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '48eb1549'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_0.100_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_0.100_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_0.100_DMSO_0.025']
The dimensions of the data are: (30722, 1276)
Number of total missing values across all columns: 61444
Data Subset Is Off
Wells held out for testing: ['E14' 'C20']
Wells to use for training, validation, and testing ['E15' 'C16' 'C17' 'C21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.403859).  Saving model ...
	 Train_Loss: 0.7615 Train_Acc: 71.644 Val_Loss: 0.4039  BEST VAL Loss: 0.4039  Val_Acc: 83.260

Epoch 1: Validation loss decreased (0.403859 --> 0.362286).  Saving model ...
	 Train_Loss: 0.5868 Train_Acc: 81.872 Val_Loss: 0.3623  BEST VAL Loss: 0.3623  Val_Acc: 87.937

Epoch 2: Validation loss decreased (0.362286 --> 0.329552).  Saving model ...
	 Train_Loss: 0.4975 Train_Acc: 86.489 Val_Loss: 0.3296  BEST VAL Loss: 0.3296  Val_Acc: 89.467

Epoch 3: Validation loss decreased (0.329552 --> 0.307221).  Saving model ...
	 Train_Loss: 0.4411 Train_Acc: 88.526 Val_Loss: 0.3072  BEST VAL Loss: 0.3072  Val_Acc: 90.079

Epoch 4: Validation loss decreased (0.307221 --> 0.292446).  Saving model ...
	 Train_Loss: 0.4028 Train_Acc: 89.619 Val_Loss: 0.2924  BEST VAL Loss: 0.2924  Val_Acc: 90.385

Epoch 5: Validation loss decreased (0.292446 --> 0.279595).  Saving model ...
	 Train_Loss: 0.3756 Train_Acc: 89.996 Val_Loss: 0.2796  BEST VAL Loss: 0.2796  Val_Acc: 91.084

Epoch 6: Validation loss decreased (0.279595 --> 0.270837).  Saving model ...
	 Train_Loss: 0.3537 Train_Acc: 90.624 Val_Loss: 0.2708  BEST VAL Loss: 0.2708  Val_Acc: 90.778

Epoch 7: Validation loss decreased (0.270837 --> 0.267979).  Saving model ...
	 Train_Loss: 0.3351 Train_Acc: 91.761 Val_Loss: 0.2680  BEST VAL Loss: 0.2680  Val_Acc: 90.647

Epoch 8: Validation loss decreased (0.267979 --> 0.261487).  Saving model ...
	 Train_Loss: 0.3204 Train_Acc: 91.673 Val_Loss: 0.2615  BEST VAL Loss: 0.2615  Val_Acc: 91.477

Epoch 9: Validation loss decreased (0.261487 --> 0.259340).  Saving model ...
	 Train_Loss: 0.3076 Train_Acc: 92.072 Val_Loss: 0.2593  BEST VAL Loss: 0.2593  Val_Acc: 91.084

Epoch 10: Validation loss decreased (0.259340 --> 0.256367).  Saving model ...
	 Train_Loss: 0.2968 Train_Acc: 92.187 Val_Loss: 0.2564  BEST VAL Loss: 0.2564  Val_Acc: 92.308

Epoch 11: Validation loss decreased (0.256367 --> 0.251896).  Saving model ...
	 Train_Loss: 0.2872 Train_Acc: 92.460 Val_Loss: 0.2519  BEST VAL Loss: 0.2519  Val_Acc: 93.444

Epoch 12: Validation loss decreased (0.251896 --> 0.249070).  Saving model ...
	 Train_Loss: 0.2776 Train_Acc: 93.258 Val_Loss: 0.2491  BEST VAL Loss: 0.2491  Val_Acc: 92.876

Epoch 13: Validation loss decreased (0.249070 --> 0.245094).  Saving model ...
	 Train_Loss: 0.2692 Train_Acc: 93.367 Val_Loss: 0.2451  BEST VAL Loss: 0.2451  Val_Acc: 92.920

Epoch 14: Validation loss decreased (0.245094 --> 0.243425).  Saving model ...
	 Train_Loss: 0.2618 Train_Acc: 93.362 Val_Loss: 0.2434  BEST VAL Loss: 0.2434  Val_Acc: 92.614

Epoch 15: Validation loss decreased (0.243425 --> 0.241919).  Saving model ...
	 Train_Loss: 0.2557 Train_Acc: 93.094 Val_Loss: 0.2419  BEST VAL Loss: 0.2419  Val_Acc: 92.963

Epoch 16: Validation loss decreased (0.241919 --> 0.239338).  Saving model ...
	 Train_Loss: 0.2498 Train_Acc: 93.515 Val_Loss: 0.2393  BEST VAL Loss: 0.2393  Val_Acc: 93.138

Epoch 17: Validation loss decreased (0.239338 --> 0.237510).  Saving model ...
	 Train_Loss: 0.2440 Train_Acc: 93.924 Val_Loss: 0.2375  BEST VAL Loss: 0.2375  Val_Acc: 93.313

Epoch 18: Validation loss decreased (0.237510 --> 0.235585).  Saving model ...
	 Train_Loss: 0.2390 Train_Acc: 93.744 Val_Loss: 0.2356  BEST VAL Loss: 0.2356  Val_Acc: 94.100

Epoch 19: Validation loss decreased (0.235585 --> 0.233519).  Saving model ...
	 Train_Loss: 0.2340 Train_Acc: 94.203 Val_Loss: 0.2335  BEST VAL Loss: 0.2335  Val_Acc: 94.143

Epoch 20: Validation loss decreased (0.233519 --> 0.233113).  Saving model ...
	 Train_Loss: 0.2292 Train_Acc: 94.520 Val_Loss: 0.2331  BEST VAL Loss: 0.2331  Val_Acc: 93.794

Epoch 21: Validation loss decreased (0.233113 --> 0.232945).  Saving model ...
	 Train_Loss: 0.2252 Train_Acc: 94.198 Val_Loss: 0.2329  BEST VAL Loss: 0.2329  Val_Acc: 93.357

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.2212 Train_Acc: 94.312 Val_Loss: 0.2335  BEST VAL Loss: 0.2329  Val_Acc: 93.969

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.2173 Train_Acc: 94.607 Val_Loss: 0.2337  BEST VAL Loss: 0.2329  Val_Acc: 93.663

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.2139 Train_Acc: 94.542 Val_Loss: 0.2339  BEST VAL Loss: 0.2329  Val_Acc: 93.313

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.2107 Train_Acc: 94.662 Val_Loss: 0.2332  BEST VAL Loss: 0.2329  Val_Acc: 93.969

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.2076 Train_Acc: 94.624 Val_Loss: 0.2332  BEST VAL Loss: 0.2329  Val_Acc: 93.837

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.2048 Train_Acc: 94.405 Val_Loss: 0.2333  BEST VAL Loss: 0.2329  Val_Acc: 93.750

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.2022 Train_Acc: 94.383 Val_Loss: 0.2341  BEST VAL Loss: 0.2329  Val_Acc: 93.531

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1997 Train_Acc: 94.569 Val_Loss: 0.2334  BEST VAL Loss: 0.2329  Val_Acc: 93.226

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1972 Train_Acc: 94.793 Val_Loss: 0.2330  BEST VAL Loss: 0.2329  Val_Acc: 93.881

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1948 Train_Acc: 94.924 Val_Loss: 0.2335  BEST VAL Loss: 0.2329  Val_Acc: 94.187

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1926 Train_Acc: 94.968 Val_Loss: 0.2353  BEST VAL Loss: 0.2329  Val_Acc: 94.100

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1904 Train_Acc: 94.979 Val_Loss: 0.2348  BEST VAL Loss: 0.2329  Val_Acc: 94.493

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1883 Train_Acc: 95.012 Val_Loss: 0.2348  BEST VAL Loss: 0.2329  Val_Acc: 93.969

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1862 Train_Acc: 95.083 Val_Loss: 0.2353  BEST VAL Loss: 0.2329  Val_Acc: 93.531

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1845 Train_Acc: 94.897 Val_Loss: 0.2361  BEST VAL Loss: 0.2329  Val_Acc: 93.750

Epoch 37: Validation loss did not decrease
Early stopped at epoch : 37
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.97      0.97     10451
           1       0.97      0.97      0.97      7852

    accuracy                           0.97     18303
   macro avg       0.97      0.97      0.97     18303
weighted avg       0.97      0.97      0.97     18303

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.94      0.94      1307
           1       0.92      0.93      0.92       981

    accuracy                           0.93      2288
   macro avg       0.93      0.93      0.93      2288
weighted avg       0.93      0.93      0.93      2288

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.94      0.94      1307
           1       0.92      0.93      0.93       981

    accuracy                           0.94      2288
   macro avg       0.93      0.94      0.94      2288
weighted avg       0.94      0.94      0.94      2288

              precision    recall  f1-score   support

           0       0.95      0.94      0.94      1307
           1       0.92      0.93      0.93       981

    accuracy                           0.94      2288
   macro avg       0.93      0.94      0.94      2288
weighted avg       0.94      0.94      0.94      2288

Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.96      0.95      4445
           1       0.94      0.94      0.94      3398

    accuracy                           0.95      7843
   macro avg       0.95      0.95      0.95      7843
weighted avg       0.95      0.95      0.95      7843

              precision    recall  f1-score   support

           0       0.95      0.96      0.95      4445
           1       0.94      0.94      0.94      3398

    accuracy                           0.95      7843
   macro avg       0.95      0.95      0.95      7843
weighted avg       0.95      0.95      0.95      7843

completed
