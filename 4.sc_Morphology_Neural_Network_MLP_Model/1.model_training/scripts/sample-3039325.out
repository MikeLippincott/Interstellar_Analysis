[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'e8a70a03'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'aa3ec6a3'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a090ff35'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'efdc6d56'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: H2O2_100.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: H2O2_100.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'H2O2_100.000_DMSO_0.025']
The dimensions of the data are: (40112, 1276)
Number of total missing values across all columns: 80224
Data Subset Is Off
Wells held out for testing: ['E14' 'H22']
Wells to use for training, validation, and testing ['E15' 'H18' 'H19' 'H23' 'L14' 'L15' 'I18' 'I19' 'I22' 'I23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.395188).  Saving model ...
	 Train_Loss: 0.5511 Train_Acc: 78.289 Val_Loss: 0.3952  BEST VAL Loss: 0.3952  Val_Acc: 82.790

Epoch 1: Validation loss decreased (0.395188 --> 0.316660).  Saving model ...
	 Train_Loss: 0.4419 Train_Acc: 86.643 Val_Loss: 0.3167  BEST VAL Loss: 0.3167  Val_Acc: 91.426

Epoch 2: Validation loss decreased (0.316660 --> 0.306268).  Saving model ...
	 Train_Loss: 0.3916 Train_Acc: 89.237 Val_Loss: 0.3063  BEST VAL Loss: 0.3063  Val_Acc: 88.476

Epoch 3: Validation loss decreased (0.306268 --> 0.288130).  Saving model ...
	 Train_Loss: 0.3680 Train_Acc: 87.573 Val_Loss: 0.2881  BEST VAL Loss: 0.2881  Val_Acc: 90.811

Epoch 4: Validation loss decreased (0.288130 --> 0.272090).  Saving model ...
	 Train_Loss: 0.3443 Train_Acc: 89.360 Val_Loss: 0.2721  BEST VAL Loss: 0.2721  Val_Acc: 92.409

Epoch 5: Validation loss decreased (0.272090 --> 0.259689).  Saving model ...
	 Train_Loss: 0.3251 Train_Acc: 90.586 Val_Loss: 0.2597  BEST VAL Loss: 0.2597  Val_Acc: 92.963

Epoch 6: Validation loss decreased (0.259689 --> 0.249677).  Saving model ...
	 Train_Loss: 0.3097 Train_Acc: 90.858 Val_Loss: 0.2497  BEST VAL Loss: 0.2497  Val_Acc: 93.423

Epoch 7: Validation loss decreased (0.249677 --> 0.242183).  Saving model ...
	 Train_Loss: 0.2966 Train_Acc: 91.177 Val_Loss: 0.2422  BEST VAL Loss: 0.2422  Val_Acc: 93.669

Epoch 8: Validation loss decreased (0.242183 --> 0.235144).  Saving model ...
	 Train_Loss: 0.2859 Train_Acc: 91.615 Val_Loss: 0.2351  BEST VAL Loss: 0.2351  Val_Acc: 93.762

Epoch 9: Validation loss decreased (0.235144 --> 0.228823).  Saving model ...
	 Train_Loss: 0.2768 Train_Acc: 91.662 Val_Loss: 0.2288  BEST VAL Loss: 0.2288  Val_Acc: 94.161

Epoch 10: Validation loss decreased (0.228823 --> 0.222399).  Saving model ...
	 Train_Loss: 0.2703 Train_Acc: 91.808 Val_Loss: 0.2224  BEST VAL Loss: 0.2224  Val_Acc: 94.530

Epoch 11: Validation loss decreased (0.222399 --> 0.217793).  Saving model ...
	 Train_Loss: 0.2643 Train_Acc: 92.150 Val_Loss: 0.2178  BEST VAL Loss: 0.2178  Val_Acc: 94.038

Epoch 12: Validation loss decreased (0.217793 --> 0.215439).  Saving model ...
	 Train_Loss: 0.2587 Train_Acc: 92.046 Val_Loss: 0.2154  BEST VAL Loss: 0.2154  Val_Acc: 93.024

Epoch 13: Validation loss decreased (0.215439 --> 0.212432).  Saving model ...
	 Train_Loss: 0.2541 Train_Acc: 91.665 Val_Loss: 0.2124  BEST VAL Loss: 0.2124  Val_Acc: 94.561

Epoch 14: Validation loss decreased (0.212432 --> 0.210320).  Saving model ...
	 Train_Loss: 0.2492 Train_Acc: 91.950 Val_Loss: 0.2103  BEST VAL Loss: 0.2103  Val_Acc: 94.222

Epoch 15: Validation loss decreased (0.210320 --> 0.208706).  Saving model ...
	 Train_Loss: 0.2442 Train_Acc: 92.457 Val_Loss: 0.2087  BEST VAL Loss: 0.2087  Val_Acc: 94.192

Epoch 16: Validation loss decreased (0.208706 --> 0.206760).  Saving model ...
	 Train_Loss: 0.2396 Train_Acc: 92.292 Val_Loss: 0.2068  BEST VAL Loss: 0.2068  Val_Acc: 94.929

Epoch 17: Validation loss decreased (0.206760 --> 0.206182).  Saving model ...
	 Train_Loss: 0.2354 Train_Acc: 92.542 Val_Loss: 0.2062  BEST VAL Loss: 0.2062  Val_Acc: 94.499

Epoch 18: Validation loss decreased (0.206182 --> 0.204311).  Saving model ...
	 Train_Loss: 0.2314 Train_Acc: 92.937 Val_Loss: 0.2043  BEST VAL Loss: 0.2043  Val_Acc: 94.806

Epoch 19: Validation loss decreased (0.204311 --> 0.203192).  Saving model ...
	 Train_Loss: 0.2278 Train_Acc: 92.680 Val_Loss: 0.2032  BEST VAL Loss: 0.2032  Val_Acc: 93.977

Epoch 20: Validation loss decreased (0.203192 --> 0.201634).  Saving model ...
	 Train_Loss: 0.2247 Train_Acc: 92.618 Val_Loss: 0.2016  BEST VAL Loss: 0.2016  Val_Acc: 94.468

Epoch 21: Validation loss decreased (0.201634 --> 0.200674).  Saving model ...
	 Train_Loss: 0.2214 Train_Acc: 92.910 Val_Loss: 0.2007  BEST VAL Loss: 0.2007  Val_Acc: 94.960

Epoch 22: Validation loss decreased (0.200674 --> 0.199454).  Saving model ...
	 Train_Loss: 0.2183 Train_Acc: 92.980 Val_Loss: 0.1995  BEST VAL Loss: 0.1995  Val_Acc: 94.745

Epoch 23: Validation loss decreased (0.199454 --> 0.198191).  Saving model ...
	 Train_Loss: 0.2153 Train_Acc: 92.864 Val_Loss: 0.1982  BEST VAL Loss: 0.1982  Val_Acc: 94.714

Epoch 24: Validation loss decreased (0.198191 --> 0.196935).  Saving model ...
	 Train_Loss: 0.2128 Train_Acc: 92.907 Val_Loss: 0.1969  BEST VAL Loss: 0.1969  Val_Acc: 94.806

Epoch 25: Validation loss decreased (0.196935 --> 0.195854).  Saving model ...
	 Train_Loss: 0.2105 Train_Acc: 92.807 Val_Loss: 0.1959  BEST VAL Loss: 0.1959  Val_Acc: 94.561

Epoch 26: Validation loss decreased (0.195854 --> 0.194417).  Saving model ...
	 Train_Loss: 0.2083 Train_Acc: 92.880 Val_Loss: 0.1944  BEST VAL Loss: 0.1944  Val_Acc: 94.868

Epoch 27: Validation loss decreased (0.194417 --> 0.193636).  Saving model ...
	 Train_Loss: 0.2062 Train_Acc: 92.914 Val_Loss: 0.1936  BEST VAL Loss: 0.1936  Val_Acc: 94.683

Epoch 28: Validation loss decreased (0.193636 --> 0.192167).  Saving model ...
	 Train_Loss: 0.2044 Train_Acc: 92.503 Val_Loss: 0.1922  BEST VAL Loss: 0.1922  Val_Acc: 94.745

Epoch 29: Validation loss decreased (0.192167 --> 0.191193).  Saving model ...
	 Train_Loss: 0.2024 Train_Acc: 92.949 Val_Loss: 0.1912  BEST VAL Loss: 0.1912  Val_Acc: 95.175

Epoch 30: Validation loss decreased (0.191193 --> 0.190650).  Saving model ...
	 Train_Loss: 0.2004 Train_Acc: 93.110 Val_Loss: 0.1907  BEST VAL Loss: 0.1907  Val_Acc: 95.267

Epoch 31: Validation loss decreased (0.190650 --> 0.190485).  Saving model ...
	 Train_Loss: 0.1984 Train_Acc: 93.256 Val_Loss: 0.1905  BEST VAL Loss: 0.1905  Val_Acc: 94.714

Epoch 32: Validation loss decreased (0.190485 --> 0.190101).  Saving model ...
	 Train_Loss: 0.1966 Train_Acc: 93.110 Val_Loss: 0.1901  BEST VAL Loss: 0.1901  Val_Acc: 94.899

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1948 Train_Acc: 93.202 Val_Loss: 0.1903  BEST VAL Loss: 0.1901  Val_Acc: 94.407

Epoch 34: Validation loss decreased (0.190101 --> 0.189416).  Saving model ...
	 Train_Loss: 0.1933 Train_Acc: 93.060 Val_Loss: 0.1894  BEST VAL Loss: 0.1894  Val_Acc: 94.929

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1917 Train_Acc: 93.237 Val_Loss: 0.1898  BEST VAL Loss: 0.1894  Val_Acc: 94.499

Epoch 36: Validation loss decreased (0.189416 --> 0.189394).  Saving model ...
	 Train_Loss: 0.1903 Train_Acc: 93.275 Val_Loss: 0.1894  BEST VAL Loss: 0.1894  Val_Acc: 94.899

Epoch 37: Validation loss decreased (0.189394 --> 0.189059).  Saving model ...
	 Train_Loss: 0.1888 Train_Acc: 93.337 Val_Loss: 0.1891  BEST VAL Loss: 0.1891  Val_Acc: 94.622

Epoch 38: Validation loss decreased (0.189059 --> 0.188774).  Saving model ...
	 Train_Loss: 0.1874 Train_Acc: 93.218 Val_Loss: 0.1888  BEST VAL Loss: 0.1888  Val_Acc: 94.745

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1872 Train_Acc: 92.622 Val_Loss: 0.1889  BEST VAL Loss: 0.1888  Val_Acc: 93.577

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1898 Train_Acc: 91.235 Val_Loss: 0.1942  BEST VAL Loss: 0.1888  Val_Acc: 86.878

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1918 Train_Acc: 89.794 Val_Loss: 0.1952  BEST VAL Loss: 0.1888  Val_Acc: 93.085

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1917 Train_Acc: 91.815 Val_Loss: 0.1951  BEST VAL Loss: 0.1888  Val_Acc: 94.561

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1913 Train_Acc: 92.645 Val_Loss: 0.1951  BEST VAL Loss: 0.1888  Val_Acc: 94.530

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1905 Train_Acc: 92.876 Val_Loss: 0.1951  BEST VAL Loss: 0.1888  Val_Acc: 94.991

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.1896 Train_Acc: 93.160 Val_Loss: 0.1951  BEST VAL Loss: 0.1888  Val_Acc: 94.806

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1887 Train_Acc: 92.964 Val_Loss: 0.1951  BEST VAL Loss: 0.1888  Val_Acc: 94.714

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1877 Train_Acc: 93.337 Val_Loss: 0.1956  BEST VAL Loss: 0.1888  Val_Acc: 94.468

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1868 Train_Acc: 93.471 Val_Loss: 0.1956  BEST VAL Loss: 0.1888  Val_Acc: 94.561

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1858 Train_Acc: 93.333 Val_Loss: 0.1959  BEST VAL Loss: 0.1888  Val_Acc: 94.499

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1849 Train_Acc: 93.310 Val_Loss: 0.1960  BEST VAL Loss: 0.1888  Val_Acc: 94.837

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1849 Train_Acc: 92.695 Val_Loss: 0.1965  BEST VAL Loss: 0.1888  Val_Acc: 94.960

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1847 Train_Acc: 93.010 Val_Loss: 0.1968  BEST VAL Loss: 0.1888  Val_Acc: 94.745

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1843 Train_Acc: 93.018 Val_Loss: 0.1968  BEST VAL Loss: 0.1888  Val_Acc: 95.206

Epoch 54: Validation loss did not decrease
Early stopped at epoch : 54
Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.99      0.98     18174
           1       0.97      0.92      0.95      7850

    accuracy                           0.97     26024
   macro avg       0.97      0.96      0.96     26024
weighted avg       0.97      0.97      0.97     26024

Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.97      0.96      2272
           1       0.93      0.89      0.91       982

    accuracy                           0.95      3254
   macro avg       0.94      0.93      0.94      3254
weighted avg       0.95      0.95      0.95      3254

Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.98      0.97      2272
           1       0.94      0.89      0.92       982

    accuracy                           0.95      3254
   macro avg       0.95      0.93      0.94      3254
weighted avg       0.95      0.95      0.95      3254

              precision    recall  f1-score   support

           0       0.96      0.98      0.97      2272
           1       0.94      0.89      0.92       982

    accuracy                           0.95      3254
   macro avg       0.95      0.93      0.94      3254
weighted avg       0.95      0.95      0.95      3254

Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_H2O2_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.97      0.95      4182
           1       0.96      0.91      0.94      3398

    accuracy                           0.94      7580
   macro avg       0.95      0.94      0.94      7580
weighted avg       0.95      0.94      0.94      7580

              precision    recall  f1-score   support

           0       0.93      0.97      0.95      4182
           1       0.96      0.91      0.94      3398

    accuracy                           0.94      7580
   macro avg       0.95      0.94      0.94      7580
weighted avg       0.95      0.94      0.94      7580

completed
