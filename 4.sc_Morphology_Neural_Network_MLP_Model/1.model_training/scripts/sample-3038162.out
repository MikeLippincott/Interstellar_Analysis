[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '89f073ac'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '3c8ae73f'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '9320bc37'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '97ee631c'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Flagellin_1.000_DMSO_0.025 treatment_name: LPS_100.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_100.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_100.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (31796, 1276)
Number of total missing values across all columns: 63592
Data Subset Is Off
Wells held out for testing: ['J16' 'M22']
Wells to use for training, validation, and testing ['J17' 'M18' 'M19' 'J20' 'J21' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.539145).  Saving model ...
	 Train_Loss: 0.7139 Train_Acc: 53.981 Val_Loss: 0.5391  BEST VAL Loss: 0.5391  Val_Acc: 74.704

Epoch 1: Validation loss decreased (0.539145 --> 0.462951).  Saving model ...
	 Train_Loss: 0.6366 Train_Acc: 69.071 Val_Loss: 0.4630  BEST VAL Loss: 0.4630  Val_Acc: 85.262

Epoch 2: Validation loss decreased (0.462951 --> 0.414101).  Saving model ...
	 Train_Loss: 0.5779 Train_Acc: 75.998 Val_Loss: 0.4141  BEST VAL Loss: 0.4141  Val_Acc: 88.387

Epoch 3: Validation loss decreased (0.414101 --> 0.378768).  Saving model ...
	 Train_Loss: 0.5332 Train_Acc: 79.952 Val_Loss: 0.3788  BEST VAL Loss: 0.3788  Val_Acc: 88.936

Epoch 4: Validation loss decreased (0.378768 --> 0.355542).  Saving model ...
	 Train_Loss: 0.5030 Train_Acc: 80.539 Val_Loss: 0.3555  BEST VAL Loss: 0.3555  Val_Acc: 89.907

Epoch 5: Validation loss decreased (0.355542 --> 0.337846).  Saving model ...
	 Train_Loss: 0.4781 Train_Acc: 82.202 Val_Loss: 0.3378  BEST VAL Loss: 0.3378  Val_Acc: 90.160

Epoch 6: Validation loss decreased (0.337846 --> 0.323580).  Saving model ...
	 Train_Loss: 0.4589 Train_Acc: 82.661 Val_Loss: 0.3236  BEST VAL Loss: 0.3236  Val_Acc: 90.245

Epoch 7: Validation loss decreased (0.323580 --> 0.311884).  Saving model ...
	 Train_Loss: 0.4425 Train_Acc: 83.400 Val_Loss: 0.3119  BEST VAL Loss: 0.3119  Val_Acc: 90.794

Epoch 8: Validation loss decreased (0.311884 --> 0.302405).  Saving model ...
	 Train_Loss: 0.4289 Train_Acc: 83.886 Val_Loss: 0.3024  BEST VAL Loss: 0.3024  Val_Acc: 90.963

Epoch 9: Validation loss decreased (0.302405 --> 0.294700).  Saving model ...
	 Train_Loss: 0.4176 Train_Acc: 83.949 Val_Loss: 0.2947  BEST VAL Loss: 0.2947  Val_Acc: 91.090

Epoch 10: Validation loss decreased (0.294700 --> 0.287221).  Saving model ...
	 Train_Loss: 0.4079 Train_Acc: 84.430 Val_Loss: 0.2872  BEST VAL Loss: 0.2872  Val_Acc: 91.976

Epoch 11: Validation loss decreased (0.287221 --> 0.281286).  Saving model ...
	 Train_Loss: 0.3993 Train_Acc: 84.414 Val_Loss: 0.2813  BEST VAL Loss: 0.2813  Val_Acc: 91.850

Epoch 12: Validation loss decreased (0.281286 --> 0.275080).  Saving model ...
	 Train_Loss: 0.3912 Train_Acc: 84.725 Val_Loss: 0.2751  BEST VAL Loss: 0.2751  Val_Acc: 92.230

Epoch 13: Validation loss decreased (0.275080 --> 0.270402).  Saving model ...
	 Train_Loss: 0.3841 Train_Acc: 85.053 Val_Loss: 0.2704  BEST VAL Loss: 0.2704  Val_Acc: 92.061

Epoch 14: Validation loss decreased (0.270402 --> 0.265913).  Saving model ...
	 Train_Loss: 0.3777 Train_Acc: 85.623 Val_Loss: 0.2659  BEST VAL Loss: 0.2659  Val_Acc: 91.765

Epoch 15: Validation loss decreased (0.265913 --> 0.261450).  Saving model ...
	 Train_Loss: 0.3717 Train_Acc: 85.639 Val_Loss: 0.2615  BEST VAL Loss: 0.2615  Val_Acc: 91.470

Epoch 16: Validation loss decreased (0.261450 --> 0.258004).  Saving model ...
	 Train_Loss: 0.3667 Train_Acc: 85.444 Val_Loss: 0.2580  BEST VAL Loss: 0.2580  Val_Acc: 92.483

Epoch 17: Validation loss decreased (0.258004 --> 0.254837).  Saving model ...
	 Train_Loss: 0.3619 Train_Acc: 85.628 Val_Loss: 0.2548  BEST VAL Loss: 0.2548  Val_Acc: 92.061

Epoch 18: Validation loss decreased (0.254837 --> 0.252252).  Saving model ...
	 Train_Loss: 0.3575 Train_Acc: 85.755 Val_Loss: 0.2523  BEST VAL Loss: 0.2523  Val_Acc: 92.356

Epoch 19: Validation loss decreased (0.252252 --> 0.249420).  Saving model ...
	 Train_Loss: 0.3532 Train_Acc: 85.771 Val_Loss: 0.2494  BEST VAL Loss: 0.2494  Val_Acc: 92.399

Epoch 20: Validation loss decreased (0.249420 --> 0.247425).  Saving model ...
	 Train_Loss: 0.3492 Train_Acc: 85.813 Val_Loss: 0.2474  BEST VAL Loss: 0.2474  Val_Acc: 92.779

Epoch 21: Validation loss decreased (0.247425 --> 0.245129).  Saving model ...
	 Train_Loss: 0.3457 Train_Acc: 86.151 Val_Loss: 0.2451  BEST VAL Loss: 0.2451  Val_Acc: 92.230

Epoch 22: Validation loss decreased (0.245129 --> 0.243194).  Saving model ...
	 Train_Loss: 0.3424 Train_Acc: 85.998 Val_Loss: 0.2432  BEST VAL Loss: 0.2432  Val_Acc: 92.272

Epoch 23: Validation loss decreased (0.243194 --> 0.241627).  Saving model ...
	 Train_Loss: 0.3394 Train_Acc: 85.993 Val_Loss: 0.2416  BEST VAL Loss: 0.2416  Val_Acc: 91.639

Epoch 24: Validation loss decreased (0.241627 --> 0.240157).  Saving model ...
	 Train_Loss: 0.3364 Train_Acc: 86.431 Val_Loss: 0.2402  BEST VAL Loss: 0.2402  Val_Acc: 92.525

Epoch 25: Validation loss decreased (0.240157 --> 0.238954).  Saving model ...
	 Train_Loss: 0.3334 Train_Acc: 86.795 Val_Loss: 0.2390  BEST VAL Loss: 0.2390  Val_Acc: 92.779

Epoch 26: Validation loss decreased (0.238954 --> 0.237597).  Saving model ...
	 Train_Loss: 0.3306 Train_Acc: 86.320 Val_Loss: 0.2376  BEST VAL Loss: 0.2376  Val_Acc: 92.652

Epoch 27: Validation loss decreased (0.237597 --> 0.236912).  Saving model ...
	 Train_Loss: 0.3279 Train_Acc: 86.589 Val_Loss: 0.2369  BEST VAL Loss: 0.2369  Val_Acc: 92.314

Epoch 28: Validation loss decreased (0.236912 --> 0.235929).  Saving model ...
	 Train_Loss: 0.3255 Train_Acc: 86.800 Val_Loss: 0.2359  BEST VAL Loss: 0.2359  Val_Acc: 92.399

Epoch 29: Validation loss decreased (0.235929 --> 0.234839).  Saving model ...
	 Train_Loss: 0.3230 Train_Acc: 86.595 Val_Loss: 0.2348  BEST VAL Loss: 0.2348  Val_Acc: 92.736

Epoch 30: Validation loss decreased (0.234839 --> 0.233905).  Saving model ...
	 Train_Loss: 0.3208 Train_Acc: 86.832 Val_Loss: 0.2339  BEST VAL Loss: 0.2339  Val_Acc: 92.399

Epoch 31: Validation loss decreased (0.233905 --> 0.232786).  Saving model ...
	 Train_Loss: 0.3187 Train_Acc: 86.869 Val_Loss: 0.2328  BEST VAL Loss: 0.2328  Val_Acc: 92.483

Epoch 32: Validation loss decreased (0.232786 --> 0.232050).  Saving model ...
	 Train_Loss: 0.3167 Train_Acc: 86.932 Val_Loss: 0.2321  BEST VAL Loss: 0.2321  Val_Acc: 92.399

Epoch 33: Validation loss decreased (0.232050 --> 0.231275).  Saving model ...
	 Train_Loss: 0.3146 Train_Acc: 87.117 Val_Loss: 0.2313  BEST VAL Loss: 0.2313  Val_Acc: 92.399

Epoch 34: Validation loss decreased (0.231275 --> 0.230943).  Saving model ...
	 Train_Loss: 0.3127 Train_Acc: 86.816 Val_Loss: 0.2309  BEST VAL Loss: 0.2309  Val_Acc: 92.568

Epoch 35: Validation loss decreased (0.230943 --> 0.230189).  Saving model ...
	 Train_Loss: 0.3106 Train_Acc: 87.677 Val_Loss: 0.2302  BEST VAL Loss: 0.2302  Val_Acc: 92.821

Epoch 36: Validation loss decreased (0.230189 --> 0.229724).  Saving model ...
	 Train_Loss: 0.3088 Train_Acc: 87.186 Val_Loss: 0.2297  BEST VAL Loss: 0.2297  Val_Acc: 92.821

Epoch 37: Validation loss decreased (0.229724 --> 0.229422).  Saving model ...
	 Train_Loss: 0.3072 Train_Acc: 87.191 Val_Loss: 0.2294  BEST VAL Loss: 0.2294  Val_Acc: 92.103

Epoch 38: Validation loss decreased (0.229422 --> 0.228969).  Saving model ...
	 Train_Loss: 0.3056 Train_Acc: 87.228 Val_Loss: 0.2290  BEST VAL Loss: 0.2290  Val_Acc: 92.736

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.3040 Train_Acc: 87.186 Val_Loss: 0.2292  BEST VAL Loss: 0.2290  Val_Acc: 92.652

Epoch 40: Validation loss decreased (0.228969 --> 0.228885).  Saving model ...
	 Train_Loss: 0.3025 Train_Acc: 87.328 Val_Loss: 0.2289  BEST VAL Loss: 0.2289  Val_Acc: 93.074

Epoch 41: Validation loss decreased (0.228885 --> 0.228829).  Saving model ...
	 Train_Loss: 0.3011 Train_Acc: 87.381 Val_Loss: 0.2288  BEST VAL Loss: 0.2288  Val_Acc: 92.779

Epoch 42: Validation loss decreased (0.228829 --> 0.228342).  Saving model ...
	 Train_Loss: 0.2997 Train_Acc: 87.270 Val_Loss: 0.2283  BEST VAL Loss: 0.2283  Val_Acc: 92.736

Epoch 43: Validation loss decreased (0.228342 --> 0.228329).  Saving model ...
	 Train_Loss: 0.2983 Train_Acc: 87.629 Val_Loss: 0.2283  BEST VAL Loss: 0.2283  Val_Acc: 92.652

Epoch 44: Validation loss decreased (0.228329 --> 0.227970).  Saving model ...
	 Train_Loss: 0.2970 Train_Acc: 87.381 Val_Loss: 0.2280  BEST VAL Loss: 0.2280  Val_Acc: 92.694

Epoch 45: Validation loss decreased (0.227970 --> 0.227736).  Saving model ...
	 Train_Loss: 0.2957 Train_Acc: 87.297 Val_Loss: 0.2277  BEST VAL Loss: 0.2277  Val_Acc: 92.399

Epoch 46: Validation loss decreased (0.227736 --> 0.227381).  Saving model ...
	 Train_Loss: 0.2946 Train_Acc: 87.291 Val_Loss: 0.2274  BEST VAL Loss: 0.2274  Val_Acc: 92.188

Epoch 47: Validation loss decreased (0.227381 --> 0.227106).  Saving model ...
	 Train_Loss: 0.2933 Train_Acc: 87.582 Val_Loss: 0.2271  BEST VAL Loss: 0.2271  Val_Acc: 92.990

Epoch 48: Validation loss decreased (0.227106 --> 0.226744).  Saving model ...
	 Train_Loss: 0.2921 Train_Acc: 87.899 Val_Loss: 0.2267  BEST VAL Loss: 0.2267  Val_Acc: 93.328

Epoch 49: Validation loss decreased (0.226744 --> 0.226268).  Saving model ...
	 Train_Loss: 0.2910 Train_Acc: 87.455 Val_Loss: 0.2263  BEST VAL Loss: 0.2263  Val_Acc: 92.948

Epoch 50: Validation loss decreased (0.226268 --> 0.225767).  Saving model ...
	 Train_Loss: 0.2900 Train_Acc: 87.297 Val_Loss: 0.2258  BEST VAL Loss: 0.2258  Val_Acc: 93.159

Epoch 51: Validation loss decreased (0.225767 --> 0.225499).  Saving model ...
	 Train_Loss: 0.2889 Train_Acc: 87.355 Val_Loss: 0.2255  BEST VAL Loss: 0.2255  Val_Acc: 93.412

Epoch 52: Validation loss decreased (0.225499 --> 0.225108).  Saving model ...
	 Train_Loss: 0.2879 Train_Acc: 87.508 Val_Loss: 0.2251  BEST VAL Loss: 0.2251  Val_Acc: 92.948

Epoch 53: Validation loss decreased (0.225108 --> 0.224846).  Saving model ...
	 Train_Loss: 0.2868 Train_Acc: 87.835 Val_Loss: 0.2248  BEST VAL Loss: 0.2248  Val_Acc: 93.454

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.2858 Train_Acc: 87.746 Val_Loss: 0.2249  BEST VAL Loss: 0.2248  Val_Acc: 93.074

Epoch 55: Validation loss decreased (0.224846 --> 0.224601).  Saving model ...
	 Train_Loss: 0.2848 Train_Acc: 87.524 Val_Loss: 0.2246  BEST VAL Loss: 0.2246  Val_Acc: 92.736

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2838 Train_Acc: 88.126 Val_Loss: 0.2249  BEST VAL Loss: 0.2246  Val_Acc: 92.821

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.2828 Train_Acc: 87.841 Val_Loss: 0.2252  BEST VAL Loss: 0.2246  Val_Acc: 92.821

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.2819 Train_Acc: 87.751 Val_Loss: 0.2254  BEST VAL Loss: 0.2246  Val_Acc: 93.201

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.2811 Train_Acc: 87.941 Val_Loss: 0.2254  BEST VAL Loss: 0.2246  Val_Acc: 92.736

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.2802 Train_Acc: 87.941 Val_Loss: 0.2254  BEST VAL Loss: 0.2246  Val_Acc: 92.694

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.2793 Train_Acc: 88.083 Val_Loss: 0.2255  BEST VAL Loss: 0.2246  Val_Acc: 92.483

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.2784 Train_Acc: 87.751 Val_Loss: 0.2261  BEST VAL Loss: 0.2246  Val_Acc: 92.483

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.2778 Train_Acc: 87.555 Val_Loss: 0.2261  BEST VAL Loss: 0.2246  Val_Acc: 92.694

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.2770 Train_Acc: 88.041 Val_Loss: 0.2261  BEST VAL Loss: 0.2246  Val_Acc: 92.610

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.2763 Train_Acc: 87.878 Val_Loss: 0.2263  BEST VAL Loss: 0.2246  Val_Acc: 92.736

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.2756 Train_Acc: 87.661 Val_Loss: 0.2265  BEST VAL Loss: 0.2246  Val_Acc: 92.736

Epoch 67: Validation loss did not decrease
	 Train_Loss: 0.2749 Train_Acc: 87.777 Val_Loss: 0.2265  BEST VAL Loss: 0.2246  Val_Acc: 92.441

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.2742 Train_Acc: 87.661 Val_Loss: 0.2268  BEST VAL Loss: 0.2246  Val_Acc: 92.863

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.2735 Train_Acc: 87.878 Val_Loss: 0.2272  BEST VAL Loss: 0.2246  Val_Acc: 93.243

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.2728 Train_Acc: 88.163 Val_Loss: 0.2275  BEST VAL Loss: 0.2246  Val_Acc: 92.863

Epoch 71: Validation loss did not decrease
Early stopped at epoch : 71
Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      9434
           1       0.97      0.98      0.97      9506

    accuracy                           0.97     18940
   macro avg       0.97      0.97      0.97     18940
weighted avg       0.97      0.97      0.97     18940

Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.92      0.93      1179
           1       0.92      0.93      0.93      1189

    accuracy                           0.93      2368
   macro avg       0.93      0.93      0.93      2368
weighted avg       0.93      0.93      0.93      2368

Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.93      0.90      0.92      1179
           1       0.91      0.93      0.92      1189

    accuracy                           0.92      2368
   macro avg       0.92      0.92      0.92      2368
weighted avg       0.92      0.92      0.92      2368

              precision    recall  f1-score   support

           0       0.93      0.90      0.92      1179
           1       0.91      0.93      0.92      1189

    accuracy                           0.92      2368
   macro avg       0.92      0.92      0.92      2368
weighted avg       0.92      0.92      0.92      2368

Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Flagellin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.99      0.98      4017
           1       0.99      0.97      0.98      4103

    accuracy                           0.98      8120
   macro avg       0.98      0.98      0.98      8120
weighted avg       0.98      0.98      0.98      8120

              precision    recall  f1-score   support

           0       0.97      0.99      0.98      4017
           1       0.99      0.97      0.98      4103

    accuracy                           0.98      8120
   macro avg       0.98      0.98      0.98      8120
weighted avg       0.98      0.98      0.98      8120

completed
