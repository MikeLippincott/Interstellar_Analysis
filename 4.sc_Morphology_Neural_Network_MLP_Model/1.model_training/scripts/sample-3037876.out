[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'cbcabb03'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f57318c9'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'dac30788'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '9ba924fd'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_1.0_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (32027, 1276)
Number of total missing values across all columns: 31618
Data Subset Is Off
Wells held out for testing: ['K16' 'M22']
Wells to use for training, validation, and testing ['K17' 'M18' 'M19' 'K20' 'K21' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.489596).  Saving model ...
	 Train_Loss: 0.6402 Train_Acc: 62.664 Val_Loss: 0.4896  BEST VAL Loss: 0.4896  Val_Acc: 75.966

Epoch 1: Validation loss decreased (0.489596 --> 0.424788).  Saving model ...
	 Train_Loss: 0.5664 Train_Acc: 72.208 Val_Loss: 0.4248  BEST VAL Loss: 0.4248  Val_Acc: 84.664

Epoch 2: Validation loss decreased (0.424788 --> 0.388855).  Saving model ...
	 Train_Loss: 0.5198 Train_Acc: 78.023 Val_Loss: 0.3889  BEST VAL Loss: 0.3889  Val_Acc: 86.975

Epoch 3: Validation loss decreased (0.388855 --> 0.353880).  Saving model ...
	 Train_Loss: 0.4804 Train_Acc: 83.107 Val_Loss: 0.3539  BEST VAL Loss: 0.3539  Val_Acc: 89.202

Epoch 4: Validation loss decreased (0.353880 --> 0.326447).  Saving model ...
	 Train_Loss: 0.4465 Train_Acc: 85.602 Val_Loss: 0.3264  BEST VAL Loss: 0.3264  Val_Acc: 90.630

Epoch 5: Validation loss decreased (0.326447 --> 0.305124).  Saving model ...
	 Train_Loss: 0.4184 Train_Acc: 87.462 Val_Loss: 0.3051  BEST VAL Loss: 0.3051  Val_Acc: 91.849

Epoch 6: Validation loss decreased (0.305124 --> 0.288133).  Saving model ...
	 Train_Loss: 0.3964 Train_Acc: 88.145 Val_Loss: 0.2881  BEST VAL Loss: 0.2881  Val_Acc: 92.437

Epoch 7: Validation loss decreased (0.288133 --> 0.274936).  Saving model ...
	 Train_Loss: 0.3783 Train_Acc: 88.728 Val_Loss: 0.2749  BEST VAL Loss: 0.2749  Val_Acc: 91.807

Epoch 8: Validation loss decreased (0.274936 --> 0.263907).  Saving model ...
	 Train_Loss: 0.3632 Train_Acc: 89.206 Val_Loss: 0.2639  BEST VAL Loss: 0.2639  Val_Acc: 93.109

Epoch 9: Validation loss decreased (0.263907 --> 0.254074).  Saving model ...
	 Train_Loss: 0.3502 Train_Acc: 89.479 Val_Loss: 0.2541  BEST VAL Loss: 0.2541  Val_Acc: 92.437

Epoch 10: Validation loss decreased (0.254074 --> 0.245356).  Saving model ...
	 Train_Loss: 0.3396 Train_Acc: 89.637 Val_Loss: 0.2454  BEST VAL Loss: 0.2454  Val_Acc: 93.697

Epoch 11: Validation loss decreased (0.245356 --> 0.240717).  Saving model ...
	 Train_Loss: 0.3298 Train_Acc: 90.372 Val_Loss: 0.2407  BEST VAL Loss: 0.2407  Val_Acc: 92.815

Epoch 12: Validation loss decreased (0.240717 --> 0.236596).  Saving model ...
	 Train_Loss: 0.3242 Train_Acc: 89.038 Val_Loss: 0.2366  BEST VAL Loss: 0.2366  Val_Acc: 92.185

Epoch 13: Validation loss decreased (0.236596 --> 0.236265).  Saving model ...
	 Train_Loss: 0.3185 Train_Acc: 89.216 Val_Loss: 0.2363  BEST VAL Loss: 0.2363  Val_Acc: 92.563

Epoch 14: Validation loss decreased (0.236265 --> 0.232968).  Saving model ...
	 Train_Loss: 0.3144 Train_Acc: 89.311 Val_Loss: 0.2330  BEST VAL Loss: 0.2330  Val_Acc: 93.193

Epoch 15: Validation loss decreased (0.232968 --> 0.229066).  Saving model ...
	 Train_Loss: 0.3089 Train_Acc: 90.524 Val_Loss: 0.2291  BEST VAL Loss: 0.2291  Val_Acc: 93.824

Epoch 16: Validation loss decreased (0.229066 --> 0.224445).  Saving model ...
	 Train_Loss: 0.3036 Train_Acc: 90.981 Val_Loss: 0.2244  BEST VAL Loss: 0.2244  Val_Acc: 93.739

Epoch 17: Validation loss decreased (0.224445 --> 0.220917).  Saving model ...
	 Train_Loss: 0.3004 Train_Acc: 89.637 Val_Loss: 0.2209  BEST VAL Loss: 0.2209  Val_Acc: 93.277

Epoch 18: Validation loss decreased (0.220917 --> 0.217101).  Saving model ...
	 Train_Loss: 0.2960 Train_Acc: 90.582 Val_Loss: 0.2171  BEST VAL Loss: 0.2171  Val_Acc: 94.118

Epoch 19: Validation loss decreased (0.217101 --> 0.213479).  Saving model ...
	 Train_Loss: 0.2913 Train_Acc: 91.375 Val_Loss: 0.2135  BEST VAL Loss: 0.2135  Val_Acc: 94.076

Epoch 20: Validation loss decreased (0.213479 --> 0.210885).  Saving model ...
	 Train_Loss: 0.2877 Train_Acc: 90.824 Val_Loss: 0.2109  BEST VAL Loss: 0.2109  Val_Acc: 92.941

Epoch 21: Validation loss decreased (0.210885 --> 0.208208).  Saving model ...
	 Train_Loss: 0.2842 Train_Acc: 91.160 Val_Loss: 0.2082  BEST VAL Loss: 0.2082  Val_Acc: 93.992

Epoch 22: Validation loss decreased (0.208208 --> 0.205563).  Saving model ...
	 Train_Loss: 0.2810 Train_Acc: 91.323 Val_Loss: 0.2056  BEST VAL Loss: 0.2056  Val_Acc: 94.118

Epoch 23: Validation loss decreased (0.205563 --> 0.203385).  Saving model ...
	 Train_Loss: 0.2771 Train_Acc: 92.273 Val_Loss: 0.2034  BEST VAL Loss: 0.2034  Val_Acc: 94.244

Epoch 24: Validation loss decreased (0.203385 --> 0.201103).  Saving model ...
	 Train_Loss: 0.2737 Train_Acc: 91.806 Val_Loss: 0.2011  BEST VAL Loss: 0.2011  Val_Acc: 93.908

Epoch 25: Validation loss decreased (0.201103 --> 0.198626).  Saving model ...
	 Train_Loss: 0.2708 Train_Acc: 91.559 Val_Loss: 0.1986  BEST VAL Loss: 0.1986  Val_Acc: 94.538

Epoch 26: Validation loss decreased (0.198626 --> 0.196998).  Saving model ...
	 Train_Loss: 0.2682 Train_Acc: 91.249 Val_Loss: 0.1970  BEST VAL Loss: 0.1970  Val_Acc: 93.992

Epoch 27: Validation loss decreased (0.196998 --> 0.195602).  Saving model ...
	 Train_Loss: 0.2653 Train_Acc: 91.906 Val_Loss: 0.1956  BEST VAL Loss: 0.1956  Val_Acc: 93.950

Epoch 28: Validation loss decreased (0.195602 --> 0.193987).  Saving model ...
	 Train_Loss: 0.2630 Train_Acc: 91.764 Val_Loss: 0.1940  BEST VAL Loss: 0.1940  Val_Acc: 94.664

Epoch 29: Validation loss decreased (0.193987 --> 0.192983).  Saving model ...
	 Train_Loss: 0.2600 Train_Acc: 92.284 Val_Loss: 0.1930  BEST VAL Loss: 0.1930  Val_Acc: 94.790

Epoch 30: Validation loss decreased (0.192983 --> 0.191516).  Saving model ...
	 Train_Loss: 0.2576 Train_Acc: 91.832 Val_Loss: 0.1915  BEST VAL Loss: 0.1915  Val_Acc: 94.412

Epoch 31: Validation loss decreased (0.191516 --> 0.189884).  Saving model ...
	 Train_Loss: 0.2552 Train_Acc: 92.336 Val_Loss: 0.1899  BEST VAL Loss: 0.1899  Val_Acc: 94.412

Epoch 32: Validation loss decreased (0.189884 --> 0.189028).  Saving model ...
	 Train_Loss: 0.2529 Train_Acc: 92.273 Val_Loss: 0.1890  BEST VAL Loss: 0.1890  Val_Acc: 94.874

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.2506 Train_Acc: 92.573 Val_Loss: 0.1893  BEST VAL Loss: 0.1890  Val_Acc: 94.118

Epoch 34: Validation loss decreased (0.189028 --> 0.188412).  Saving model ...
	 Train_Loss: 0.2487 Train_Acc: 92.216 Val_Loss: 0.1884  BEST VAL Loss: 0.1884  Val_Acc: 93.866

Epoch 35: Validation loss decreased (0.188412 --> 0.187797).  Saving model ...
	 Train_Loss: 0.2470 Train_Acc: 92.205 Val_Loss: 0.1878  BEST VAL Loss: 0.1878  Val_Acc: 94.412

Epoch 36: Validation loss decreased (0.187797 --> 0.186756).  Saving model ...
	 Train_Loss: 0.2455 Train_Acc: 91.743 Val_Loss: 0.1868  BEST VAL Loss: 0.1868  Val_Acc: 94.706

Epoch 37: Validation loss decreased (0.186756 --> 0.186062).  Saving model ...
	 Train_Loss: 0.2438 Train_Acc: 92.573 Val_Loss: 0.1861  BEST VAL Loss: 0.1861  Val_Acc: 94.874

Epoch 38: Validation loss decreased (0.186062 --> 0.184926).  Saving model ...
	 Train_Loss: 0.2422 Train_Acc: 92.289 Val_Loss: 0.1849  BEST VAL Loss: 0.1849  Val_Acc: 94.706

Epoch 39: Validation loss decreased (0.184926 --> 0.184393).  Saving model ...
	 Train_Loss: 0.2419 Train_Acc: 88.660 Val_Loss: 0.1844  BEST VAL Loss: 0.1844  Val_Acc: 93.109

Epoch 40: Validation loss decreased (0.184393 --> 0.184220).  Saving model ...
	 Train_Loss: 0.2417 Train_Acc: 89.679 Val_Loss: 0.1842  BEST VAL Loss: 0.1842  Val_Acc: 92.773

Epoch 41: Validation loss decreased (0.184220 --> 0.183561).  Saving model ...
	 Train_Loss: 0.2415 Train_Acc: 89.342 Val_Loss: 0.1836  BEST VAL Loss: 0.1836  Val_Acc: 93.529

Epoch 42: Validation loss decreased (0.183561 --> 0.183112).  Saving model ...
	 Train_Loss: 0.2414 Train_Acc: 90.083 Val_Loss: 0.1831  BEST VAL Loss: 0.1831  Val_Acc: 92.857

Epoch 43: Validation loss decreased (0.183112 --> 0.182443).  Saving model ...
	 Train_Loss: 0.2410 Train_Acc: 90.172 Val_Loss: 0.1824  BEST VAL Loss: 0.1824  Val_Acc: 93.613

Epoch 44: Validation loss decreased (0.182443 --> 0.181895).  Saving model ...
	 Train_Loss: 0.2406 Train_Acc: 89.668 Val_Loss: 0.1819  BEST VAL Loss: 0.1819  Val_Acc: 93.319

Epoch 45: Validation loss decreased (0.181895 --> 0.181440).  Saving model ...
	 Train_Loss: 0.2402 Train_Acc: 89.337 Val_Loss: 0.1814  BEST VAL Loss: 0.1814  Val_Acc: 92.017

Epoch 46: Validation loss decreased (0.181440 --> 0.181047).  Saving model ...
	 Train_Loss: 0.2400 Train_Acc: 88.702 Val_Loss: 0.1810  BEST VAL Loss: 0.1810  Val_Acc: 92.983

Epoch 47: Validation loss decreased (0.181047 --> 0.180768).  Saving model ...
	 Train_Loss: 0.2396 Train_Acc: 89.820 Val_Loss: 0.1808  BEST VAL Loss: 0.1808  Val_Acc: 92.521

Epoch 48: Validation loss decreased (0.180768 --> 0.180410).  Saving model ...
	 Train_Loss: 0.2398 Train_Acc: 88.066 Val_Loss: 0.1804  BEST VAL Loss: 0.1804  Val_Acc: 94.202

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2399 Train_Acc: 87.966 Val_Loss: 0.1807  BEST VAL Loss: 0.1804  Val_Acc: 92.647

Epoch 50: Validation loss decreased (0.180410 --> 0.180304).  Saving model ...
	 Train_Loss: 0.2398 Train_Acc: 88.365 Val_Loss: 0.1803  BEST VAL Loss: 0.1803  Val_Acc: 92.353

Epoch 51: Validation loss decreased (0.180304 --> 0.179742).  Saving model ...
	 Train_Loss: 0.2395 Train_Acc: 88.355 Val_Loss: 0.1797  BEST VAL Loss: 0.1797  Val_Acc: 92.395

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.2395 Train_Acc: 88.376 Val_Loss: 0.1800  BEST VAL Loss: 0.1797  Val_Acc: 92.185

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.2399 Train_Acc: 87.924 Val_Loss: 0.1802  BEST VAL Loss: 0.1797  Val_Acc: 92.227

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.2431 Train_Acc: 80.660 Val_Loss: 0.1829  BEST VAL Loss: 0.1797  Val_Acc: 89.790

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.2455 Train_Acc: 82.398 Val_Loss: 0.1838  BEST VAL Loss: 0.1797  Val_Acc: 91.513

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2472 Train_Acc: 84.825 Val_Loss: 0.1845  BEST VAL Loss: 0.1797  Val_Acc: 90.966

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.2484 Train_Acc: 85.881 Val_Loss: 0.1852  BEST VAL Loss: 0.1797  Val_Acc: 91.092

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.2494 Train_Acc: 86.679 Val_Loss: 0.1857  BEST VAL Loss: 0.1797  Val_Acc: 91.176

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.2506 Train_Acc: 86.285 Val_Loss: 0.1868  BEST VAL Loss: 0.1797  Val_Acc: 90.798

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.2515 Train_Acc: 86.495 Val_Loss: 0.1870  BEST VAL Loss: 0.1797  Val_Acc: 93.824

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.2521 Train_Acc: 86.380 Val_Loss: 0.1872  BEST VAL Loss: 0.1797  Val_Acc: 93.193

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.2532 Train_Acc: 86.674 Val_Loss: 0.1879  BEST VAL Loss: 0.1797  Val_Acc: 91.092

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.2543 Train_Acc: 86.264 Val_Loss: 0.1883  BEST VAL Loss: 0.1797  Val_Acc: 93.025

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.2552 Train_Acc: 85.802 Val_Loss: 0.1885  BEST VAL Loss: 0.1797  Val_Acc: 94.202

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.2557 Train_Acc: 86.979 Val_Loss: 0.1888  BEST VAL Loss: 0.1797  Val_Acc: 91.891

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.2563 Train_Acc: 87.073 Val_Loss: 0.1895  BEST VAL Loss: 0.1797  Val_Acc: 91.975

Epoch 67: Validation loss did not decrease
Early stopped at epoch : 67
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.90      0.94      9434
           1       0.91      0.99      0.95      9604

    accuracy                           0.94     19038
   macro avg       0.95      0.94      0.94     19038
weighted avg       0.95      0.94      0.94     19038

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.88      0.92      1179
           1       0.89      0.97      0.93      1201

    accuracy                           0.92      2380
   macro avg       0.93      0.92      0.92      2380
weighted avg       0.93      0.92      0.92      2380

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.89      0.93      1179
           1       0.90      0.97      0.93      1201

    accuracy                           0.93      2380
   macro avg       0.93      0.93      0.93      2380
weighted avg       0.93      0.93      0.93      2380

              precision    recall  f1-score   support

           0       0.97      0.89      0.93      1179
           1       0.90      0.97      0.93      1201

    accuracy                           0.93      2380
   macro avg       0.93      0.93      0.93      2380
weighted avg       0.93      0.93      0.93      2380

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.88      0.93      4017
           1       0.89      0.99      0.94      4212

    accuracy                           0.93      8229
   macro avg       0.94      0.93      0.93      8229
weighted avg       0.94      0.93      0.93      8229

              precision    recall  f1-score   support

           0       0.99      0.88      0.93      4017
           1       0.89      0.99      0.94      4212

    accuracy                           0.93      8229
   macro avg       0.94      0.93      0.93      8229
weighted avg       0.94      0.93      0.93      8229

completed
