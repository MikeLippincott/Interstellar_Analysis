[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'e5fc2c2a'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '71b9e020'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '93aae5f5'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'ccbd26d3'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: Flagellin_0.100_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: Flagellin_0.100_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_10.0_DMSO_0.025' 'Flagellin_0.100_DMSO_0.025']
The dimensions of the data are: (29330, 1276)
Number of total missing values across all columns: 29856
Data Subset Is Off
Wells held out for testing: ['M16' 'L22']
Wells to use for training, validation, and testing ['M17' 'L18' 'L19' 'M20' 'M21' 'L23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.393958).  Saving model ...
	 Train_Loss: 0.5352 Train_Acc: 73.868 Val_Loss: 0.3940  BEST VAL Loss: 0.3940  Val_Acc: 85.052

Epoch 1: Validation loss decreased (0.393958 --> 0.363277).  Saving model ...
	 Train_Loss: 0.4882 Train_Acc: 80.646 Val_Loss: 0.3633  BEST VAL Loss: 0.3633  Val_Acc: 89.035

Epoch 2: Validation loss decreased (0.363277 --> 0.354138).  Saving model ...
	 Train_Loss: 0.4610 Train_Acc: 83.311 Val_Loss: 0.3541  BEST VAL Loss: 0.3541  Val_Acc: 88.097

Epoch 3: Validation loss decreased (0.354138 --> 0.346626).  Saving model ...
	 Train_Loss: 0.4458 Train_Acc: 83.088 Val_Loss: 0.3466  BEST VAL Loss: 0.3466  Val_Acc: 88.613

Epoch 4: Validation loss decreased (0.346626 --> 0.336391).  Saving model ...
	 Train_Loss: 0.4351 Train_Acc: 83.703 Val_Loss: 0.3364  BEST VAL Loss: 0.3364  Val_Acc: 90.722

Epoch 5: Validation loss decreased (0.336391 --> 0.329622).  Saving model ...
	 Train_Loss: 0.4235 Train_Acc: 85.203 Val_Loss: 0.3296  BEST VAL Loss: 0.3296  Val_Acc: 90.253

Epoch 6: Validation loss decreased (0.329622 --> 0.325423).  Saving model ...
	 Train_Loss: 0.4164 Train_Acc: 84.594 Val_Loss: 0.3254  BEST VAL Loss: 0.3254  Val_Acc: 89.597

Epoch 7: Validation loss decreased (0.325423 --> 0.319681).  Saving model ...
	 Train_Loss: 0.4107 Train_Acc: 84.758 Val_Loss: 0.3197  BEST VAL Loss: 0.3197  Val_Acc: 90.534

Epoch 8: Validation loss decreased (0.319681 --> 0.317560).  Saving model ...
	 Train_Loss: 0.4060 Train_Acc: 84.887 Val_Loss: 0.3176  BEST VAL Loss: 0.3176  Val_Acc: 90.581

Epoch 9: Validation loss decreased (0.317560 --> 0.312643).  Saving model ...
	 Train_Loss: 0.4024 Train_Acc: 85.004 Val_Loss: 0.3126  BEST VAL Loss: 0.3126  Val_Acc: 90.581

Epoch 10: Validation loss decreased (0.312643 --> 0.308236).  Saving model ...
	 Train_Loss: 0.3988 Train_Acc: 85.308 Val_Loss: 0.3082  BEST VAL Loss: 0.3082  Val_Acc: 91.190

Epoch 11: Validation loss decreased (0.308236 --> 0.305979).  Saving model ...
	 Train_Loss: 0.3940 Train_Acc: 86.117 Val_Loss: 0.3060  BEST VAL Loss: 0.3060  Val_Acc: 91.612

Epoch 12: Validation loss decreased (0.305979 --> 0.302765).  Saving model ...
	 Train_Loss: 0.3901 Train_Acc: 86.047 Val_Loss: 0.3028  BEST VAL Loss: 0.3028  Val_Acc: 91.425

Epoch 13: Validation loss decreased (0.302765 --> 0.300744).  Saving model ...
	 Train_Loss: 0.3869 Train_Acc: 86.082 Val_Loss: 0.3007  BEST VAL Loss: 0.3007  Val_Acc: 91.425

Epoch 14: Validation loss decreased (0.300744 --> 0.297105).  Saving model ...
	 Train_Loss: 0.3832 Train_Acc: 86.439 Val_Loss: 0.2971  BEST VAL Loss: 0.2971  Val_Acc: 91.378

Epoch 15: Validation loss decreased (0.297105 --> 0.294041).  Saving model ...
	 Train_Loss: 0.3802 Train_Acc: 86.673 Val_Loss: 0.2940  BEST VAL Loss: 0.2940  Val_Acc: 91.659

Epoch 16: Validation loss decreased (0.294041 --> 0.291956).  Saving model ...
	 Train_Loss: 0.3775 Train_Acc: 86.990 Val_Loss: 0.2920  BEST VAL Loss: 0.2920  Val_Acc: 92.409

Epoch 17: Validation loss decreased (0.291956 --> 0.289702).  Saving model ...
	 Train_Loss: 0.3746 Train_Acc: 86.990 Val_Loss: 0.2897  BEST VAL Loss: 0.2897  Val_Acc: 91.659

Epoch 18: Validation loss decreased (0.289702 --> 0.288024).  Saving model ...
	 Train_Loss: 0.3723 Train_Acc: 87.001 Val_Loss: 0.2880  BEST VAL Loss: 0.2880  Val_Acc: 90.300

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.3698 Train_Acc: 86.878 Val_Loss: 0.2894  BEST VAL Loss: 0.2880  Val_Acc: 92.268

Epoch 20: Validation loss decreased (0.288024 --> 0.287972).  Saving model ...
	 Train_Loss: 0.3677 Train_Acc: 87.048 Val_Loss: 0.2880  BEST VAL Loss: 0.2880  Val_Acc: 91.940

Epoch 21: Validation loss decreased (0.287972 --> 0.286964).  Saving model ...
	 Train_Loss: 0.3649 Train_Acc: 87.786 Val_Loss: 0.2870  BEST VAL Loss: 0.2870  Val_Acc: 92.221

Epoch 22: Validation loss decreased (0.286964 --> 0.284815).  Saving model ...
	 Train_Loss: 0.3625 Train_Acc: 87.376 Val_Loss: 0.2848  BEST VAL Loss: 0.2848  Val_Acc: 93.158

Epoch 23: Validation loss decreased (0.284815 --> 0.283315).  Saving model ...
	 Train_Loss: 0.3598 Train_Acc: 88.208 Val_Loss: 0.2833  BEST VAL Loss: 0.2833  Val_Acc: 92.830

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.3574 Train_Acc: 88.173 Val_Loss: 0.2840  BEST VAL Loss: 0.2833  Val_Acc: 93.018

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.3554 Train_Acc: 87.663 Val_Loss: 0.2857  BEST VAL Loss: 0.2833  Val_Acc: 93.486

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.3535 Train_Acc: 88.003 Val_Loss: 0.2850  BEST VAL Loss: 0.2833  Val_Acc: 93.346

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.3517 Train_Acc: 88.149 Val_Loss: 0.2851  BEST VAL Loss: 0.2833  Val_Acc: 93.252

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.3505 Train_Acc: 87.347 Val_Loss: 0.2861  BEST VAL Loss: 0.2833  Val_Acc: 93.580

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.3492 Train_Acc: 87.675 Val_Loss: 0.2862  BEST VAL Loss: 0.2833  Val_Acc: 92.877

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.3480 Train_Acc: 87.657 Val_Loss: 0.2873  BEST VAL Loss: 0.2833  Val_Acc: 93.533

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.3468 Train_Acc: 87.669 Val_Loss: 0.2885  BEST VAL Loss: 0.2833  Val_Acc: 93.299

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.3455 Train_Acc: 88.026 Val_Loss: 0.2894  BEST VAL Loss: 0.2833  Val_Acc: 94.049

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.3442 Train_Acc: 88.372 Val_Loss: 0.2894  BEST VAL Loss: 0.2833  Val_Acc: 93.252

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.3432 Train_Acc: 88.296 Val_Loss: 0.2899  BEST VAL Loss: 0.2833  Val_Acc: 93.627

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.3418 Train_Acc: 88.478 Val_Loss: 0.2894  BEST VAL Loss: 0.2833  Val_Acc: 93.768

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.3403 Train_Acc: 88.765 Val_Loss: 0.2910  BEST VAL Loss: 0.2833  Val_Acc: 94.002

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.3389 Train_Acc: 88.753 Val_Loss: 0.2919  BEST VAL Loss: 0.2833  Val_Acc: 93.861

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.3378 Train_Acc: 88.577 Val_Loss: 0.2924  BEST VAL Loss: 0.2833  Val_Acc: 93.768

Epoch 39: Validation loss did not decrease
Early stopped at epoch : 39
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       1.00      0.93      0.96      8635
           1       0.93      1.00      0.96      8436

    accuracy                           0.96     17071
   macro avg       0.96      0.96      0.96     17071
weighted avg       0.96      0.96      0.96     17071

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.89      0.93      1079
           1       0.90      0.96      0.93      1055

    accuracy                           0.93      2134
   macro avg       0.93      0.93      0.93      2134
weighted avg       0.93      0.93      0.93      2134

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.90      0.94      1079
           1       0.91      0.97      0.94      1055

    accuracy                           0.94      2134
   macro avg       0.94      0.94      0.94      2134
weighted avg       0.94      0.94      0.94      2134

              precision    recall  f1-score   support

           0       0.97      0.90      0.94      1079
           1       0.91      0.97      0.94      1055

    accuracy                           0.94      2134
   macro avg       0.94      0.94      0.94      2134
weighted avg       0.94      0.94      0.94      2134

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Flagellin_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.89      0.92      4135
           1       0.89      0.95      0.92      3856

    accuracy                           0.92      7991
   macro avg       0.92      0.92      0.92      7991
weighted avg       0.92      0.92      0.92      7991

              precision    recall  f1-score   support

           0       0.95      0.89      0.92      4135
           1       0.89      0.95      0.92      3856

    accuracy                           0.92      7991
   macro avg       0.92      0.92      0.92      7991
weighted avg       0.92      0.92      0.92      7991

completed
