[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'b7f38fd5'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '0c87ac4e'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a204f820'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '27d204e6'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (329622, 1270)
Number of total missing values across all columns: 296912
Data Subset Is Off
Wells held out for testing: ['K06' 'M09']
Wells to use for training, validation, and testing ['D06' 'D07' 'K07' 'M02' 'M03' 'M08']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.162271).  Saving model ...
	 Train_Loss: 0.2331 Train_Acc: 90.237 Val_Loss: 0.1623  BEST VAL Loss: 0.1623  Val_Acc: 93.500

Epoch 1: Validation loss decreased (0.162271 --> 0.146925).  Saving model ...
	 Train_Loss: 0.2027 Train_Acc: 93.208 Val_Loss: 0.1469  BEST VAL Loss: 0.1469  Val_Acc: 94.911

Epoch 2: Validation loss decreased (0.146925 --> 0.140684).  Saving model ...
	 Train_Loss: 0.1873 Train_Acc: 93.906 Val_Loss: 0.1407  BEST VAL Loss: 0.1407  Val_Acc: 95.047

Epoch 3: Validation loss decreased (0.140684 --> 0.135302).  Saving model ...
	 Train_Loss: 0.1774 Train_Acc: 94.254 Val_Loss: 0.1353  BEST VAL Loss: 0.1353  Val_Acc: 95.477

Epoch 4: Validation loss decreased (0.135302 --> 0.132030).  Saving model ...
	 Train_Loss: 0.1700 Train_Acc: 94.494 Val_Loss: 0.1320  BEST VAL Loss: 0.1320  Val_Acc: 95.602

Epoch 5: Validation loss decreased (0.132030 --> 0.128879).  Saving model ...
	 Train_Loss: 0.1649 Train_Acc: 94.567 Val_Loss: 0.1289  BEST VAL Loss: 0.1289  Val_Acc: 95.846

Epoch 6: Validation loss decreased (0.128879 --> 0.126207).  Saving model ...
	 Train_Loss: 0.1605 Train_Acc: 94.751 Val_Loss: 0.1262  BEST VAL Loss: 0.1262  Val_Acc: 95.796

Epoch 7: Validation loss decreased (0.126207 --> 0.124169).  Saving model ...
	 Train_Loss: 0.1567 Train_Acc: 94.846 Val_Loss: 0.1242  BEST VAL Loss: 0.1242  Val_Acc: 95.904

Epoch 8: Validation loss decreased (0.124169 --> 0.122609).  Saving model ...
	 Train_Loss: 0.1535 Train_Acc: 94.852 Val_Loss: 0.1226  BEST VAL Loss: 0.1226  Val_Acc: 95.982

Epoch 9: Validation loss decreased (0.122609 --> 0.121845).  Saving model ...
	 Train_Loss: 0.1509 Train_Acc: 94.875 Val_Loss: 0.1218  BEST VAL Loss: 0.1218  Val_Acc: 95.829

Epoch 10: Validation loss decreased (0.121845 --> 0.120565).  Saving model ...
	 Train_Loss: 0.1485 Train_Acc: 94.943 Val_Loss: 0.1206  BEST VAL Loss: 0.1206  Val_Acc: 96.131

Epoch 11: Validation loss decreased (0.120565 --> 0.119573).  Saving model ...
	 Train_Loss: 0.1465 Train_Acc: 95.007 Val_Loss: 0.1196  BEST VAL Loss: 0.1196  Val_Acc: 96.020

Epoch 12: Validation loss decreased (0.119573 --> 0.118880).  Saving model ...
	 Train_Loss: 0.1446 Train_Acc: 95.113 Val_Loss: 0.1189  BEST VAL Loss: 0.1189  Val_Acc: 95.937

Epoch 13: Validation loss decreased (0.118880 --> 0.118027).  Saving model ...
	 Train_Loss: 0.1429 Train_Acc: 95.138 Val_Loss: 0.1180  BEST VAL Loss: 0.1180  Val_Acc: 96.061

Epoch 14: Validation loss decreased (0.118027 --> 0.117318).  Saving model ...
	 Train_Loss: 0.1414 Train_Acc: 95.221 Val_Loss: 0.1173  BEST VAL Loss: 0.1173  Val_Acc: 96.202

Epoch 15: Validation loss decreased (0.117318 --> 0.116485).  Saving model ...
	 Train_Loss: 0.1399 Train_Acc: 95.238 Val_Loss: 0.1165  BEST VAL Loss: 0.1165  Val_Acc: 96.218

Epoch 16: Validation loss decreased (0.116485 --> 0.115790).  Saving model ...
	 Train_Loss: 0.1385 Train_Acc: 95.283 Val_Loss: 0.1158  BEST VAL Loss: 0.1158  Val_Acc: 96.334

Epoch 17: Validation loss decreased (0.115790 --> 0.115172).  Saving model ...
	 Train_Loss: 0.1373 Train_Acc: 95.365 Val_Loss: 0.1152  BEST VAL Loss: 0.1152  Val_Acc: 96.222

Epoch 18: Validation loss decreased (0.115172 --> 0.114620).  Saving model ...
	 Train_Loss: 0.1362 Train_Acc: 95.278 Val_Loss: 0.1146  BEST VAL Loss: 0.1146  Val_Acc: 96.222

Epoch 19: Validation loss decreased (0.114620 --> 0.113978).  Saving model ...
	 Train_Loss: 0.1352 Train_Acc: 95.370 Val_Loss: 0.1140  BEST VAL Loss: 0.1140  Val_Acc: 96.491

Epoch 20: Validation loss decreased (0.113978 --> 0.113649).  Saving model ...
	 Train_Loss: 0.1342 Train_Acc: 95.374 Val_Loss: 0.1136  BEST VAL Loss: 0.1136  Val_Acc: 96.135

Epoch 21: Validation loss decreased (0.113649 --> 0.112986).  Saving model ...
	 Train_Loss: 0.1332 Train_Acc: 95.423 Val_Loss: 0.1130  BEST VAL Loss: 0.1130  Val_Acc: 96.491

Epoch 22: Validation loss decreased (0.112986 --> 0.112887).  Saving model ...
	 Train_Loss: 0.1324 Train_Acc: 95.393 Val_Loss: 0.1129  BEST VAL Loss: 0.1129  Val_Acc: 96.392

Epoch 23: Validation loss decreased (0.112887 --> 0.112532).  Saving model ...
	 Train_Loss: 0.1316 Train_Acc: 95.376 Val_Loss: 0.1125  BEST VAL Loss: 0.1125  Val_Acc: 96.396

Epoch 24: Validation loss decreased (0.112532 --> 0.112342).  Saving model ...
	 Train_Loss: 0.1309 Train_Acc: 95.436 Val_Loss: 0.1123  BEST VAL Loss: 0.1123  Val_Acc: 96.202

Epoch 25: Validation loss decreased (0.112342 --> 0.112032).  Saving model ...
	 Train_Loss: 0.1302 Train_Acc: 95.432 Val_Loss: 0.1120  BEST VAL Loss: 0.1120  Val_Acc: 96.504

Epoch 26: Validation loss decreased (0.112032 --> 0.111635).  Saving model ...
	 Train_Loss: 0.1294 Train_Acc: 95.556 Val_Loss: 0.1116  BEST VAL Loss: 0.1116  Val_Acc: 96.363

Epoch 27: Validation loss decreased (0.111635 --> 0.111344).  Saving model ...
	 Train_Loss: 0.1288 Train_Acc: 95.542 Val_Loss: 0.1113  BEST VAL Loss: 0.1113  Val_Acc: 96.309

Epoch 28: Validation loss decreased (0.111344 --> 0.111177).  Saving model ...
	 Train_Loss: 0.1282 Train_Acc: 95.515 Val_Loss: 0.1112  BEST VAL Loss: 0.1112  Val_Acc: 96.297

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1276 Train_Acc: 95.438 Val_Loss: 0.1112  BEST VAL Loss: 0.1112  Val_Acc: 96.264

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1271 Train_Acc: 95.493 Val_Loss: 0.1112  BEST VAL Loss: 0.1112  Val_Acc: 96.235

Epoch 31: Validation loss decreased (0.111177 --> 0.111065).  Saving model ...
	 Train_Loss: 0.1265 Train_Acc: 95.564 Val_Loss: 0.1111  BEST VAL Loss: 0.1111  Val_Acc: 96.313

Epoch 32: Validation loss decreased (0.111065 --> 0.110916).  Saving model ...
	 Train_Loss: 0.1260 Train_Acc: 95.540 Val_Loss: 0.1109  BEST VAL Loss: 0.1109  Val_Acc: 96.367

Epoch 33: Validation loss decreased (0.110916 --> 0.110818).  Saving model ...
	 Train_Loss: 0.1255 Train_Acc: 95.518 Val_Loss: 0.1108  BEST VAL Loss: 0.1108  Val_Acc: 96.338

Epoch 34: Validation loss decreased (0.110818 --> 0.110602).  Saving model ...
	 Train_Loss: 0.1251 Train_Acc: 95.622 Val_Loss: 0.1106  BEST VAL Loss: 0.1106  Val_Acc: 96.462

Epoch 35: Validation loss decreased (0.110602 --> 0.110478).  Saving model ...
	 Train_Loss: 0.1247 Train_Acc: 95.500 Val_Loss: 0.1105  BEST VAL Loss: 0.1105  Val_Acc: 96.326

Epoch 36: Validation loss decreased (0.110478 --> 0.110416).  Saving model ...
	 Train_Loss: 0.1242 Train_Acc: 95.611 Val_Loss: 0.1104  BEST VAL Loss: 0.1104  Val_Acc: 96.276

Epoch 37: Validation loss decreased (0.110416 --> 0.110384).  Saving model ...
	 Train_Loss: 0.1238 Train_Acc: 95.538 Val_Loss: 0.1104  BEST VAL Loss: 0.1104  Val_Acc: 96.371

Epoch 38: Validation loss decreased (0.110384 --> 0.110326).  Saving model ...
	 Train_Loss: 0.1235 Train_Acc: 95.489 Val_Loss: 0.1103  BEST VAL Loss: 0.1103  Val_Acc: 96.346

Epoch 39: Validation loss decreased (0.110326 --> 0.110315).  Saving model ...
	 Train_Loss: 0.1231 Train_Acc: 95.678 Val_Loss: 0.1103  BEST VAL Loss: 0.1103  Val_Acc: 96.471

Epoch 40: Validation loss decreased (0.110315 --> 0.110144).  Saving model ...
	 Train_Loss: 0.1227 Train_Acc: 95.628 Val_Loss: 0.1101  BEST VAL Loss: 0.1101  Val_Acc: 96.483

Epoch 41: Validation loss decreased (0.110144 --> 0.109980).  Saving model ...
	 Train_Loss: 0.1223 Train_Acc: 95.694 Val_Loss: 0.1100  BEST VAL Loss: 0.1100  Val_Acc: 96.566

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1219 Train_Acc: 95.654 Val_Loss: 0.1100  BEST VAL Loss: 0.1100  Val_Acc: 96.450

Epoch 43: Validation loss decreased (0.109980 --> 0.109950).  Saving model ...
	 Train_Loss: 0.1216 Train_Acc: 95.544 Val_Loss: 0.1100  BEST VAL Loss: 0.1100  Val_Acc: 96.582

Epoch 44: Validation loss decreased (0.109950 --> 0.109890).  Saving model ...
	 Train_Loss: 0.1212 Train_Acc: 95.647 Val_Loss: 0.1099  BEST VAL Loss: 0.1099  Val_Acc: 96.400

Epoch 45: Validation loss decreased (0.109890 --> 0.109835).  Saving model ...
	 Train_Loss: 0.1209 Train_Acc: 95.655 Val_Loss: 0.1098  BEST VAL Loss: 0.1098  Val_Acc: 96.284

Epoch 46: Validation loss decreased (0.109835 --> 0.109832).  Saving model ...
	 Train_Loss: 0.1206 Train_Acc: 95.670 Val_Loss: 0.1098  BEST VAL Loss: 0.1098  Val_Acc: 96.388

Epoch 47: Validation loss decreased (0.109832 --> 0.109784).  Saving model ...
	 Train_Loss: 0.1203 Train_Acc: 95.627 Val_Loss: 0.1098  BEST VAL Loss: 0.1098  Val_Acc: 96.471

Epoch 48: Validation loss decreased (0.109784 --> 0.109647).  Saving model ...
	 Train_Loss: 0.1200 Train_Acc: 95.655 Val_Loss: 0.1096  BEST VAL Loss: 0.1096  Val_Acc: 96.603

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1197 Train_Acc: 95.618 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.317

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1194 Train_Acc: 95.581 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.380

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1192 Train_Acc: 95.586 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.326

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1189 Train_Acc: 95.623 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.380

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1187 Train_Acc: 95.673 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.202

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1184 Train_Acc: 95.709 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.334

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.1182 Train_Acc: 95.617 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.388

Epoch 56: Validation loss decreased (0.109647 --> 0.109622).  Saving model ...
	 Train_Loss: 0.1180 Train_Acc: 95.702 Val_Loss: 0.1096  BEST VAL Loss: 0.1096  Val_Acc: 96.466

Epoch 57: Validation loss decreased (0.109622 --> 0.109576).  Saving model ...
	 Train_Loss: 0.1177 Train_Acc: 95.711 Val_Loss: 0.1096  BEST VAL Loss: 0.1096  Val_Acc: 96.425

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.1175 Train_Acc: 95.765 Val_Loss: 0.1096  BEST VAL Loss: 0.1096  Val_Acc: 96.375

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.1172 Train_Acc: 95.728 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.164

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.1170 Train_Acc: 95.746 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.549

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.1167 Train_Acc: 95.700 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.520

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.1166 Train_Acc: 95.663 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.367

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.1164 Train_Acc: 95.701 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.611

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.1161 Train_Acc: 95.710 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.413

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.1159 Train_Acc: 95.731 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.160

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.1157 Train_Acc: 95.742 Val_Loss: 0.1097  BEST VAL Loss: 0.1096  Val_Acc: 96.512

Epoch 67: Validation loss did not decrease
	 Train_Loss: 0.1155 Train_Acc: 95.720 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.495

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.1153 Train_Acc: 95.660 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.346

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.1152 Train_Acc: 95.643 Val_Loss: 0.1098  BEST VAL Loss: 0.1096  Val_Acc: 96.644

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.1150 Train_Acc: 95.697 Val_Loss: 0.1100  BEST VAL Loss: 0.1096  Val_Acc: 96.284

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.1148 Train_Acc: 95.708 Val_Loss: 0.1101  BEST VAL Loss: 0.1096  Val_Acc: 96.260

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.1146 Train_Acc: 95.755 Val_Loss: 0.1101  BEST VAL Loss: 0.1096  Val_Acc: 96.396

Epoch 73: Validation loss did not decrease
Early stopped at epoch : 73
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.98      0.97    105242
           1       0.98      0.96      0.97     88098

    accuracy                           0.97    193340
   macro avg       0.97      0.97      0.97    193340
weighted avg       0.97      0.97      0.97    193340

Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.98      0.97     13155
           1       0.97      0.95      0.96     11013

    accuracy                           0.96     24168
   macro avg       0.96      0.96      0.96     24168
weighted avg       0.96      0.96      0.96     24168

Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.97      0.97     13155
           1       0.97      0.95      0.96     11013

    accuracy                           0.96     24168
   macro avg       0.96      0.96      0.96     24168
weighted avg       0.96      0.96      0.96     24168

              precision    recall  f1-score   support

           0       0.96      0.97      0.97     13155
           1       0.97      0.95      0.96     11013

    accuracy                           0.96     24168
   macro avg       0.96      0.96      0.96     24168
weighted avg       0.96      0.96      0.96     24168

Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.96      0.95     49614
           1       0.94      0.92      0.93     38332

    accuracy                           0.94     87946
   macro avg       0.94      0.94      0.94     87946
weighted avg       0.94      0.94      0.94     87946

              precision    recall  f1-score   support

           0       0.94      0.96      0.95     49614
           1       0.94      0.92      0.93     38332

    accuracy                           0.94     87946
   macro avg       0.94      0.94      0.94     87946
weighted avg       0.94      0.94      0.94     87946

completed
