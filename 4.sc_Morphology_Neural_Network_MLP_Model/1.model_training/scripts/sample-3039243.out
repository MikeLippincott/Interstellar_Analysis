[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '00328bb9'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '960b79de'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'e0534717'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2731fb4c'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (28168, 1276)
Number of total missing values across all columns: 27532
Data Subset Is Off
Wells held out for testing: ['D14' 'M20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'M16' 'M17' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.160325).  Saving model ...
	 Train_Loss: 0.3481 Train_Acc: 86.044 Val_Loss: 0.1603  BEST VAL Loss: 0.1603  Val_Acc: 93.343

Epoch 1: Validation loss decreased (0.160325 --> 0.132976).  Saving model ...
	 Train_Loss: 0.2713 Train_Acc: 92.939 Val_Loss: 0.1330  BEST VAL Loss: 0.1330  Val_Acc: 95.673

Epoch 2: Validation loss decreased (0.132976 --> 0.113410).  Saving model ...
	 Train_Loss: 0.2294 Train_Acc: 94.650 Val_Loss: 0.1134  BEST VAL Loss: 0.1134  Val_Acc: 97.194

Epoch 3: Validation loss decreased (0.113410 --> 0.101182).  Saving model ...
	 Train_Loss: 0.2016 Train_Acc: 95.566 Val_Loss: 0.1012  BEST VAL Loss: 0.1012  Val_Acc: 97.480

Epoch 4: Validation loss decreased (0.101182 --> 0.092420).  Saving model ...
	 Train_Loss: 0.1822 Train_Acc: 96.041 Val_Loss: 0.0924  BEST VAL Loss: 0.0924  Val_Acc: 97.622

Epoch 5: Validation loss decreased (0.092420 --> 0.086132).  Saving model ...
	 Train_Loss: 0.1675 Train_Acc: 96.505 Val_Loss: 0.0861  BEST VAL Loss: 0.0861  Val_Acc: 97.908

Epoch 6: Validation loss decreased (0.086132 --> 0.082793).  Saving model ...
	 Train_Loss: 0.1569 Train_Acc: 96.475 Val_Loss: 0.0828  BEST VAL Loss: 0.0828  Val_Acc: 97.575

Epoch 7: Validation loss decreased (0.082793 --> 0.079183).  Saving model ...
	 Train_Loss: 0.1488 Train_Acc: 96.517 Val_Loss: 0.0792  BEST VAL Loss: 0.0792  Val_Acc: 97.860

Epoch 8: Validation loss decreased (0.079183 --> 0.075756).  Saving model ...
	 Train_Loss: 0.1426 Train_Acc: 96.547 Val_Loss: 0.0758  BEST VAL Loss: 0.0758  Val_Acc: 97.908

Epoch 9: Validation loss decreased (0.075756 --> 0.073458).  Saving model ...
	 Train_Loss: 0.1374 Train_Acc: 96.642 Val_Loss: 0.0735  BEST VAL Loss: 0.0735  Val_Acc: 97.813

Epoch 10: Validation loss decreased (0.073458 --> 0.070800).  Saving model ...
	 Train_Loss: 0.1322 Train_Acc: 97.052 Val_Loss: 0.0708  BEST VAL Loss: 0.0708  Val_Acc: 98.193

Epoch 11: Validation loss decreased (0.070800 --> 0.069492).  Saving model ...
	 Train_Loss: 0.1274 Train_Acc: 97.058 Val_Loss: 0.0695  BEST VAL Loss: 0.0695  Val_Acc: 97.860

Epoch 12: Validation loss decreased (0.069492 --> 0.068383).  Saving model ...
	 Train_Loss: 0.1234 Train_Acc: 97.093 Val_Loss: 0.0684  BEST VAL Loss: 0.0684  Val_Acc: 98.146

Epoch 13: Validation loss decreased (0.068383 --> 0.066795).  Saving model ...
	 Train_Loss: 0.1203 Train_Acc: 97.070 Val_Loss: 0.0668  BEST VAL Loss: 0.0668  Val_Acc: 98.003

Epoch 14: Validation loss decreased (0.066795 --> 0.065588).  Saving model ...
	 Train_Loss: 0.1174 Train_Acc: 97.153 Val_Loss: 0.0656  BEST VAL Loss: 0.0656  Val_Acc: 98.003

Epoch 15: Validation loss decreased (0.065588 --> 0.064278).  Saving model ...
	 Train_Loss: 0.1146 Train_Acc: 97.319 Val_Loss: 0.0643  BEST VAL Loss: 0.0643  Val_Acc: 98.288

Epoch 16: Validation loss decreased (0.064278 --> 0.063026).  Saving model ...
	 Train_Loss: 0.1122 Train_Acc: 97.254 Val_Loss: 0.0630  BEST VAL Loss: 0.0630  Val_Acc: 98.431

Epoch 17: Validation loss decreased (0.063026 --> 0.062130).  Saving model ...
	 Train_Loss: 0.1098 Train_Acc: 97.337 Val_Loss: 0.0621  BEST VAL Loss: 0.0621  Val_Acc: 98.193

Epoch 18: Validation loss decreased (0.062130 --> 0.061692).  Saving model ...
	 Train_Loss: 0.1076 Train_Acc: 97.456 Val_Loss: 0.0617  BEST VAL Loss: 0.0617  Val_Acc: 98.050

Epoch 19: Validation loss decreased (0.061692 --> 0.061411).  Saving model ...
	 Train_Loss: 0.1056 Train_Acc: 97.563 Val_Loss: 0.0614  BEST VAL Loss: 0.0614  Val_Acc: 97.908

Epoch 20: Validation loss decreased (0.061411 --> 0.060826).  Saving model ...
	 Train_Loss: 0.1037 Train_Acc: 97.599 Val_Loss: 0.0608  BEST VAL Loss: 0.0608  Val_Acc: 98.193

Epoch 21: Validation loss decreased (0.060826 --> 0.060781).  Saving model ...
	 Train_Loss: 0.1020 Train_Acc: 97.545 Val_Loss: 0.0608  BEST VAL Loss: 0.0608  Val_Acc: 98.003

Epoch 22: Validation loss decreased (0.060781 --> 0.060404).  Saving model ...
	 Train_Loss: 0.1004 Train_Acc: 97.789 Val_Loss: 0.0604  BEST VAL Loss: 0.0604  Val_Acc: 98.193

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0988 Train_Acc: 97.932 Val_Loss: 0.0606  BEST VAL Loss: 0.0604  Val_Acc: 98.431

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.0972 Train_Acc: 98.223 Val_Loss: 0.0605  BEST VAL Loss: 0.0604  Val_Acc: 98.098

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.0958 Train_Acc: 97.973 Val_Loss: 0.0606  BEST VAL Loss: 0.0604  Val_Acc: 98.098

Epoch 26: Validation loss decreased (0.060404 --> 0.060176).  Saving model ...
	 Train_Loss: 0.0945 Train_Acc: 98.033 Val_Loss: 0.0602  BEST VAL Loss: 0.0602  Val_Acc: 98.193

Epoch 27: Validation loss decreased (0.060176 --> 0.060158).  Saving model ...
	 Train_Loss: 0.0933 Train_Acc: 97.961 Val_Loss: 0.0602  BEST VAL Loss: 0.0602  Val_Acc: 98.431

Epoch 28: Validation loss decreased (0.060158 --> 0.059962).  Saving model ...
	 Train_Loss: 0.0922 Train_Acc: 97.890 Val_Loss: 0.0600  BEST VAL Loss: 0.0600  Val_Acc: 98.288

Epoch 29: Validation loss decreased (0.059962 --> 0.059560).  Saving model ...
	 Train_Loss: 0.0914 Train_Acc: 97.587 Val_Loss: 0.0596  BEST VAL Loss: 0.0596  Val_Acc: 98.241

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0905 Train_Acc: 97.932 Val_Loss: 0.0597  BEST VAL Loss: 0.0596  Val_Acc: 98.193

Epoch 31: Validation loss decreased (0.059560 --> 0.059456).  Saving model ...
	 Train_Loss: 0.0896 Train_Acc: 98.074 Val_Loss: 0.0595  BEST VAL Loss: 0.0595  Val_Acc: 97.908

Epoch 32: Validation loss decreased (0.059456 --> 0.059360).  Saving model ...
	 Train_Loss: 0.0889 Train_Acc: 97.860 Val_Loss: 0.0594  BEST VAL Loss: 0.0594  Val_Acc: 98.098

Epoch 33: Validation loss decreased (0.059360 --> 0.059088).  Saving model ...
	 Train_Loss: 0.0881 Train_Acc: 97.920 Val_Loss: 0.0591  BEST VAL Loss: 0.0591  Val_Acc: 98.003

Epoch 34: Validation loss decreased (0.059088 --> 0.058964).  Saving model ...
	 Train_Loss: 0.0873 Train_Acc: 98.015 Val_Loss: 0.0590  BEST VAL Loss: 0.0590  Val_Acc: 98.241

Epoch 35: Validation loss decreased (0.058964 --> 0.058521).  Saving model ...
	 Train_Loss: 0.0865 Train_Acc: 98.068 Val_Loss: 0.0585  BEST VAL Loss: 0.0585  Val_Acc: 98.241

Epoch 36: Validation loss decreased (0.058521 --> 0.058440).  Saving model ...
	 Train_Loss: 0.0857 Train_Acc: 98.122 Val_Loss: 0.0584  BEST VAL Loss: 0.0584  Val_Acc: 98.573

Epoch 37: Validation loss decreased (0.058440 --> 0.058305).  Saving model ...
	 Train_Loss: 0.0848 Train_Acc: 98.258 Val_Loss: 0.0583  BEST VAL Loss: 0.0583  Val_Acc: 98.288

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0842 Train_Acc: 97.967 Val_Loss: 0.0583  BEST VAL Loss: 0.0583  Val_Acc: 98.336

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0835 Train_Acc: 98.110 Val_Loss: 0.0583  BEST VAL Loss: 0.0583  Val_Acc: 97.955

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0829 Train_Acc: 98.140 Val_Loss: 0.0586  BEST VAL Loss: 0.0583  Val_Acc: 98.050

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0823 Train_Acc: 98.092 Val_Loss: 0.0583  BEST VAL Loss: 0.0583  Val_Acc: 98.336

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0818 Train_Acc: 98.092 Val_Loss: 0.0584  BEST VAL Loss: 0.0583  Val_Acc: 98.431

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0811 Train_Acc: 98.383 Val_Loss: 0.0586  BEST VAL Loss: 0.0583  Val_Acc: 98.431

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.0806 Train_Acc: 98.122 Val_Loss: 0.0587  BEST VAL Loss: 0.0583  Val_Acc: 97.860

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.0800 Train_Acc: 98.294 Val_Loss: 0.0588  BEST VAL Loss: 0.0583  Val_Acc: 98.383

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.0795 Train_Acc: 98.181 Val_Loss: 0.0587  BEST VAL Loss: 0.0583  Val_Acc: 98.241

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.0791 Train_Acc: 98.027 Val_Loss: 0.0587  BEST VAL Loss: 0.0583  Val_Acc: 98.193

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.0787 Train_Acc: 98.122 Val_Loss: 0.0586  BEST VAL Loss: 0.0583  Val_Acc: 98.288

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.0783 Train_Acc: 98.205 Val_Loss: 0.0587  BEST VAL Loss: 0.0583  Val_Acc: 98.146

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.0779 Train_Acc: 97.997 Val_Loss: 0.0588  BEST VAL Loss: 0.0583  Val_Acc: 97.955

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.0774 Train_Acc: 98.258 Val_Loss: 0.0589  BEST VAL Loss: 0.0583  Val_Acc: 98.383

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.0771 Train_Acc: 97.991 Val_Loss: 0.0587  BEST VAL Loss: 0.0583  Val_Acc: 98.383

Epoch 53: Validation loss did not decrease
Early stopped at epoch : 53
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      8453
           1       1.00      1.00      1.00      8371

    accuracy                           1.00     16824
   macro avg       1.00      1.00      1.00     16824
weighted avg       1.00      1.00      1.00     16824

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1057
           1       0.98      0.98      0.98      1046

    accuracy                           0.98      2103
   macro avg       0.98      0.98      0.98      2103
weighted avg       0.98      0.98      0.98      2103

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1057
           1       0.98      0.98      0.98      1046

    accuracy                           0.98      2103
   macro avg       0.98      0.98      0.98      2103
weighted avg       0.98      0.98      0.98      2103

              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1057
           1       0.98      0.98      0.98      1046

    accuracy                           0.98      2103
   macro avg       0.98      0.98      0.98      2103
weighted avg       0.98      0.98      0.98      2103

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      3835
           1       0.98      0.99      0.98      3303

    accuracy                           0.99      7138
   macro avg       0.99      0.99      0.99      7138
weighted avg       0.99      0.99      0.99      7138

              precision    recall  f1-score   support

           0       0.99      0.98      0.99      3835
           1       0.98      0.99      0.98      3303

    accuracy                           0.99      7138
   macro avg       0.99      0.99      0.99      7138
weighted avg       0.99      0.99      0.99      7138

completed
