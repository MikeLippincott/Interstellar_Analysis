[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18688 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Writing 40901 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:254: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:281: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1015: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1016: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:571: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:585: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:645: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:854: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:856: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:859: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:890: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:932: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1124: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
  warnings.warn(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1131: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1133: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1136: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1213: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1400: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1402: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1405: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1482: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
PBMC MultiClass_MLP True
[0.9436581681188537, 0.5245113046249099, 0.5318305272562364]
Data Subset Is Off
(1483474,) (370869,) (1966775,) 3821118     858323
3821119     858324
3821120     858325
3821121     858326
3821122     858327
            ...   
4061834    4538951
4061835    4538952
4061836    4538953
4061837    4538954
4061838    4538955
Name: labeled_data_index, Length: 240721, dtype: int64 (1536843,)
(1483474,) (370869,) (1966775,) 3821118     858323
3821119     858324
3821120     858325
3821121     858326
3821122     858327
            ...   
4061834    4538951
4061835    4538952
4061836    4538953
4061837    4538954
4061838    4538955
Name: labeled_data_index, Length: 240721, dtype: int64 (1536843,)
5598682
(95928,) (749319,) (638227,)
(23982,) (187329,) (159558,)
(119911,) (936644,) (910220,)
(0,) (0,) (240721,)
(75619,) (788818,) (672406,)
(1483474, 1245) (370869, 1245) (1966775, 1245) (240721, 1245) (1536843, 1245)
(1483474,) (370869,) (1966775,) (240721,) (1536843,)
Number of in features:  1245
Number of out features:  3
Multi_Class
Adam
Epoch 0: Validation loss decreased (inf --> 0.586423).  Saving model ...
	 Train_Loss: 0.6688 Train_Acc: 72.724 Val_Loss: 0.5864  BEST VAL Loss: 0.5864  Val_Acc: 76.615

Epoch 1: Validation loss decreased (0.586423 --> 0.569003).  Saving model ...
	 Train_Loss: 0.6346 Train_Acc: 75.593 Val_Loss: 0.5690  BEST VAL Loss: 0.5690  Val_Acc: 77.926

Epoch 2: Validation loss decreased (0.569003 --> 0.557149).  Saving model ...
	 Train_Loss: 0.6156 Train_Acc: 76.577 Val_Loss: 0.5571  BEST VAL Loss: 0.5571  Val_Acc: 78.657

Epoch 3: Validation loss decreased (0.557149 --> 0.549175).  Saving model ...
	 Train_Loss: 0.6029 Train_Acc: 77.215 Val_Loss: 0.5492  BEST VAL Loss: 0.5492  Val_Acc: 79.309

Epoch 4: Validation loss decreased (0.549175 --> 0.544445).  Saving model ...
	 Train_Loss: 0.5933 Train_Acc: 77.580 Val_Loss: 0.5444  BEST VAL Loss: 0.5444  Val_Acc: 79.676

Epoch 5: Validation loss decreased (0.544445 --> 0.539287).  Saving model ...
	 Train_Loss: 0.5858 Train_Acc: 77.933 Val_Loss: 0.5393  BEST VAL Loss: 0.5393  Val_Acc: 79.724

Epoch 6: Validation loss decreased (0.539287 --> 0.534625).  Saving model ...
	 Train_Loss: 0.5795 Train_Acc: 78.170 Val_Loss: 0.5346  BEST VAL Loss: 0.5346  Val_Acc: 79.938

Epoch 7: Validation loss decreased (0.534625 --> 0.530923).  Saving model ...
	 Train_Loss: 0.5741 Train_Acc: 78.345 Val_Loss: 0.5309  BEST VAL Loss: 0.5309  Val_Acc: 80.181

Epoch 8: Validation loss decreased (0.530923 --> 0.527450).  Saving model ...
	 Train_Loss: 0.5695 Train_Acc: 78.589 Val_Loss: 0.5274  BEST VAL Loss: 0.5274  Val_Acc: 80.389

Epoch 9: Validation loss decreased (0.527450 --> 0.524497).  Saving model ...
	 Train_Loss: 0.5654 Train_Acc: 78.720 Val_Loss: 0.5245  BEST VAL Loss: 0.5245  Val_Acc: 80.552

Epoch 10: Validation loss decreased (0.524497 --> 0.521698).  Saving model ...
	 Train_Loss: 0.5618 Train_Acc: 78.824 Val_Loss: 0.5217  BEST VAL Loss: 0.5217  Val_Acc: 80.524

Epoch 11: Validation loss decreased (0.521698 --> 0.518967).  Saving model ...
	 Train_Loss: 0.5586 Train_Acc: 78.948 Val_Loss: 0.5190  BEST VAL Loss: 0.5190  Val_Acc: 80.848

Epoch 12: Validation loss decreased (0.518967 --> 0.516755).  Saving model ...
	 Train_Loss: 0.5557 Train_Acc: 79.086 Val_Loss: 0.5168  BEST VAL Loss: 0.5168  Val_Acc: 80.813

Epoch 13: Validation loss decreased (0.516755 --> 0.514696).  Saving model ...
	 Train_Loss: 0.5530 Train_Acc: 79.169 Val_Loss: 0.5147  BEST VAL Loss: 0.5147  Val_Acc: 80.944

Epoch 14: Validation loss decreased (0.514696 --> 0.512276).  Saving model ...
	 Train_Loss: 0.5506 Train_Acc: 79.265 Val_Loss: 0.5123  BEST VAL Loss: 0.5123  Val_Acc: 81.172

Epoch 15: Validation loss decreased (0.512276 --> 0.510492).  Saving model ...
	 Train_Loss: 0.5483 Train_Acc: 79.333 Val_Loss: 0.5105  BEST VAL Loss: 0.5105  Val_Acc: 80.916

Epoch 16: Validation loss decreased (0.510492 --> 0.509047).  Saving model ...
	 Train_Loss: 0.5462 Train_Acc: 79.393 Val_Loss: 0.5090  BEST VAL Loss: 0.5090  Val_Acc: 81.237

Epoch 17: Validation loss decreased (0.509047 --> 0.507277).  Saving model ...
	 Train_Loss: 0.5443 Train_Acc: 79.475 Val_Loss: 0.5073  BEST VAL Loss: 0.5073  Val_Acc: 81.402

Epoch 18: Validation loss decreased (0.507277 --> 0.505749).  Saving model ...
	 Train_Loss: 0.5425 Train_Acc: 79.521 Val_Loss: 0.5057  BEST VAL Loss: 0.5057  Val_Acc: 81.413

Epoch 19: Validation loss decreased (0.505749 --> 0.504773).  Saving model ...
	 Train_Loss: 0.5408 Train_Acc: 79.571 Val_Loss: 0.5048  BEST VAL Loss: 0.5048  Val_Acc: 81.440

Epoch 20: Validation loss decreased (0.504773 --> 0.503830).  Saving model ...
	 Train_Loss: 0.5391 Train_Acc: 79.667 Val_Loss: 0.5038  BEST VAL Loss: 0.5038  Val_Acc: 81.206

Epoch 21: Validation loss decreased (0.503830 --> 0.503025).  Saving model ...
	 Train_Loss: 0.5376 Train_Acc: 79.706 Val_Loss: 0.5030  BEST VAL Loss: 0.5030  Val_Acc: 81.144

Epoch 22: Validation loss decreased (0.503025 --> 0.502149).  Saving model ...
	 Train_Loss: 0.5362 Train_Acc: 79.754 Val_Loss: 0.5021  BEST VAL Loss: 0.5021  Val_Acc: 81.580

Epoch 23: Validation loss decreased (0.502149 --> 0.501293).  Saving model ...
	 Train_Loss: 0.5349 Train_Acc: 79.752 Val_Loss: 0.5013  BEST VAL Loss: 0.5013  Val_Acc: 81.562

Epoch 24: Validation loss decreased (0.501293 --> 0.500485).  Saving model ...
	 Train_Loss: 0.5337 Train_Acc: 79.837 Val_Loss: 0.5005  BEST VAL Loss: 0.5005  Val_Acc: 81.301

Epoch 25: Validation loss decreased (0.500485 --> 0.499290).  Saving model ...
	 Train_Loss: 0.5324 Train_Acc: 79.899 Val_Loss: 0.4993  BEST VAL Loss: 0.4993  Val_Acc: 81.634

Epoch 26: Validation loss decreased (0.499290 --> 0.498249).  Saving model ...
	 Train_Loss: 0.5313 Train_Acc: 79.879 Val_Loss: 0.4982  BEST VAL Loss: 0.4982  Val_Acc: 81.609

Epoch 27: Validation loss decreased (0.498249 --> 0.497442).  Saving model ...
	 Train_Loss: 0.5302 Train_Acc: 79.915 Val_Loss: 0.4974  BEST VAL Loss: 0.4974  Val_Acc: 81.639

Epoch 28: Validation loss decreased (0.497442 --> 0.496625).  Saving model ...
	 Train_Loss: 0.5292 Train_Acc: 79.969 Val_Loss: 0.4966  BEST VAL Loss: 0.4966  Val_Acc: 81.601

Epoch 29: Validation loss decreased (0.496625 --> 0.496084).  Saving model ...
	 Train_Loss: 0.5282 Train_Acc: 79.969 Val_Loss: 0.4961  BEST VAL Loss: 0.4961  Val_Acc: 81.810

Epoch 30: Validation loss decreased (0.496084 --> 0.495376).  Saving model ...
	 Train_Loss: 0.5273 Train_Acc: 80.036 Val_Loss: 0.4954  BEST VAL Loss: 0.4954  Val_Acc: 81.366

Epoch 31: Validation loss decreased (0.495376 --> 0.494602).  Saving model ...
	 Train_Loss: 0.5264 Train_Acc: 80.009 Val_Loss: 0.4946  BEST VAL Loss: 0.4946  Val_Acc: 81.803

Epoch 32: Validation loss decreased (0.494602 --> 0.493950).  Saving model ...
	 Train_Loss: 0.5256 Train_Acc: 80.004 Val_Loss: 0.4939  BEST VAL Loss: 0.4939  Val_Acc: 81.752

Epoch 33: Validation loss decreased (0.493950 --> 0.493405).  Saving model ...
	 Train_Loss: 0.5247 Train_Acc: 80.079 Val_Loss: 0.4934  BEST VAL Loss: 0.4934  Val_Acc: 81.815

Epoch 34: Validation loss decreased (0.493405 --> 0.492716).  Saving model ...
	 Train_Loss: 0.5239 Train_Acc: 80.082 Val_Loss: 0.4927  BEST VAL Loss: 0.4927  Val_Acc: 81.929

Epoch 35: Validation loss decreased (0.492716 --> 0.492185).  Saving model ...
	 Train_Loss: 0.5232 Train_Acc: 80.125 Val_Loss: 0.4922  BEST VAL Loss: 0.4922  Val_Acc: 81.904

Epoch 36: Validation loss decreased (0.492185 --> 0.491561).  Saving model ...
	 Train_Loss: 0.5224 Train_Acc: 80.173 Val_Loss: 0.4916  BEST VAL Loss: 0.4916  Val_Acc: 81.751

Epoch 37: Validation loss decreased (0.491561 --> 0.491227).  Saving model ...
	 Train_Loss: 0.5217 Train_Acc: 80.197 Val_Loss: 0.4912  BEST VAL Loss: 0.4912  Val_Acc: 81.906

Epoch 38: Validation loss decreased (0.491227 --> 0.490758).  Saving model ...
	 Train_Loss: 0.5211 Train_Acc: 80.162 Val_Loss: 0.4908  BEST VAL Loss: 0.4908  Val_Acc: 82.040

Epoch 39: Validation loss decreased (0.490758 --> 0.490187).  Saving model ...
	 Train_Loss: 0.5204 Train_Acc: 80.156 Val_Loss: 0.4902  BEST VAL Loss: 0.4902  Val_Acc: 82.114

Epoch 40: Validation loss decreased (0.490187 --> 0.489746).  Saving model ...
	 Train_Loss: 0.5198 Train_Acc: 80.131 Val_Loss: 0.4897  BEST VAL Loss: 0.4897  Val_Acc: 81.713

Epoch 41: Validation loss decreased (0.489746 --> 0.489098).  Saving model ...
	 Train_Loss: 0.5192 Train_Acc: 80.150 Val_Loss: 0.4891  BEST VAL Loss: 0.4891  Val_Acc: 81.928

Epoch 42: Validation loss decreased (0.489098 --> 0.488612).  Saving model ...
	 Train_Loss: 0.5187 Train_Acc: 80.178 Val_Loss: 0.4886  BEST VAL Loss: 0.4886  Val_Acc: 81.755

Epoch 43: Validation loss decreased (0.488612 --> 0.488459).  Saving model ...
	 Train_Loss: 0.5182 Train_Acc: 80.115 Val_Loss: 0.4885  BEST VAL Loss: 0.4885  Val_Acc: 81.754

Epoch 44: Validation loss decreased (0.488459 --> 0.488141).  Saving model ...
	 Train_Loss: 0.5177 Train_Acc: 80.149 Val_Loss: 0.4881  BEST VAL Loss: 0.4881  Val_Acc: 81.977

Epoch 45: Validation loss decreased (0.488141 --> 0.487800).  Saving model ...
	 Train_Loss: 0.5172 Train_Acc: 80.177 Val_Loss: 0.4878  BEST VAL Loss: 0.4878  Val_Acc: 81.980

Epoch 46: Validation loss decreased (0.487800 --> 0.487657).  Saving model ...
	 Train_Loss: 0.5167 Train_Acc: 80.185 Val_Loss: 0.4877  BEST VAL Loss: 0.4877  Val_Acc: 81.899

Epoch 47: Validation loss decreased (0.487657 --> 0.487180).  Saving model ...
	 Train_Loss: 0.5163 Train_Acc: 80.174 Val_Loss: 0.4872  BEST VAL Loss: 0.4872  Val_Acc: 81.943

Epoch 48: Validation loss decreased (0.487180 --> 0.486942).  Saving model ...
	 Train_Loss: 0.5158 Train_Acc: 80.271 Val_Loss: 0.4869  BEST VAL Loss: 0.4869  Val_Acc: 82.128

Epoch 49: Validation loss decreased (0.486942 --> 0.486734).  Saving model ...
	 Train_Loss: 0.5154 Train_Acc: 80.292 Val_Loss: 0.4867  BEST VAL Loss: 0.4867  Val_Acc: 81.852

Epoch 50: Validation loss decreased (0.486734 --> 0.486387).  Saving model ...
	 Train_Loss: 0.5149 Train_Acc: 80.299 Val_Loss: 0.4864  BEST VAL Loss: 0.4864  Val_Acc: 81.923

Epoch 51: Validation loss decreased (0.486387 --> 0.486090).  Saving model ...
	 Train_Loss: 0.5145 Train_Acc: 80.282 Val_Loss: 0.4861  BEST VAL Loss: 0.4861  Val_Acc: 81.934

Epoch 52: Validation loss decreased (0.486090 --> 0.485991).  Saving model ...
	 Train_Loss: 0.5141 Train_Acc: 80.280 Val_Loss: 0.4860  BEST VAL Loss: 0.4860  Val_Acc: 81.997

Epoch 53: Validation loss decreased (0.485991 --> 0.485617).  Saving model ...
	 Train_Loss: 0.5137 Train_Acc: 80.260 Val_Loss: 0.4856  BEST VAL Loss: 0.4856  Val_Acc: 81.770

Epoch 54: Validation loss decreased (0.485617 --> 0.485233).  Saving model ...
	 Train_Loss: 0.5134 Train_Acc: 80.281 Val_Loss: 0.4852  BEST VAL Loss: 0.4852  Val_Acc: 81.911

Epoch 55: Validation loss decreased (0.485233 --> 0.485052).  Saving model ...
	 Train_Loss: 0.5130 Train_Acc: 80.305 Val_Loss: 0.4851  BEST VAL Loss: 0.4851  Val_Acc: 82.017

Epoch 56: Validation loss decreased (0.485052 --> 0.484751).  Saving model ...
	 Train_Loss: 0.5126 Train_Acc: 80.307 Val_Loss: 0.4848  BEST VAL Loss: 0.4848  Val_Acc: 81.959

Epoch 57: Validation loss decreased (0.484751 --> 0.484515).  Saving model ...
	 Train_Loss: 0.5123 Train_Acc: 80.318 Val_Loss: 0.4845  BEST VAL Loss: 0.4845  Val_Acc: 82.142

Epoch 58: Validation loss decreased (0.484515 --> 0.484148).  Saving model ...
	 Train_Loss: 0.5119 Train_Acc: 80.366 Val_Loss: 0.4841  BEST VAL Loss: 0.4841  Val_Acc: 82.142

Epoch 59: Validation loss decreased (0.484148 --> 0.483799).  Saving model ...
	 Train_Loss: 0.5116 Train_Acc: 80.341 Val_Loss: 0.4838  BEST VAL Loss: 0.4838  Val_Acc: 82.194

Epoch 60: Validation loss decreased (0.483799 --> 0.483370).  Saving model ...
	 Train_Loss: 0.5112 Train_Acc: 80.427 Val_Loss: 0.4834  BEST VAL Loss: 0.4834  Val_Acc: 82.249

Epoch 61: Validation loss decreased (0.483370 --> 0.483173).  Saving model ...
	 Train_Loss: 0.5109 Train_Acc: 80.358 Val_Loss: 0.4832  BEST VAL Loss: 0.4832  Val_Acc: 82.073

Epoch 62: Validation loss decreased (0.483173 --> 0.482857).  Saving model ...
	 Train_Loss: 0.5106 Train_Acc: 80.311 Val_Loss: 0.4829  BEST VAL Loss: 0.4829  Val_Acc: 82.084

Epoch 63: Validation loss decreased (0.482857 --> 0.482708).  Saving model ...
	 Train_Loss: 0.5103 Train_Acc: 80.420 Val_Loss: 0.4827  BEST VAL Loss: 0.4827  Val_Acc: 82.261

Epoch 64: Validation loss decreased (0.482708 --> 0.482412).  Saving model ...
	 Train_Loss: 0.5100 Train_Acc: 80.438 Val_Loss: 0.4824  BEST VAL Loss: 0.4824  Val_Acc: 82.246

Epoch 65: Validation loss decreased (0.482412 --> 0.482378).  Saving model ...
	 Train_Loss: 0.5097 Train_Acc: 80.391 Val_Loss: 0.4824  BEST VAL Loss: 0.4824  Val_Acc: 81.896

Epoch 66: Validation loss decreased (0.482378 --> 0.482335).  Saving model ...
	 Train_Loss: 0.5094 Train_Acc: 80.439 Val_Loss: 0.4823  BEST VAL Loss: 0.4823  Val_Acc: 82.083

Epoch 67: Validation loss decreased (0.482335 --> 0.482060).  Saving model ...
	 Train_Loss: 0.5092 Train_Acc: 80.324 Val_Loss: 0.4821  BEST VAL Loss: 0.4821  Val_Acc: 82.124

Epoch 68: Validation loss decreased (0.482060 --> 0.481741).  Saving model ...
	 Train_Loss: 0.5089 Train_Acc: 80.363 Val_Loss: 0.4817  BEST VAL Loss: 0.4817  Val_Acc: 82.247

Epoch 69: Validation loss decreased (0.481741 --> 0.481386).  Saving model ...
	 Train_Loss: 0.5087 Train_Acc: 80.344 Val_Loss: 0.4814  BEST VAL Loss: 0.4814  Val_Acc: 82.198

Epoch 70: Validation loss decreased (0.481386 --> 0.481268).  Saving model ...
	 Train_Loss: 0.5084 Train_Acc: 80.395 Val_Loss: 0.4813  BEST VAL Loss: 0.4813  Val_Acc: 82.279

Epoch 71: Validation loss decreased (0.481268 --> 0.480955).  Saving model ...
	 Train_Loss: 0.5081 Train_Acc: 80.417 Val_Loss: 0.4810  BEST VAL Loss: 0.4810  Val_Acc: 82.227

Epoch 72: Validation loss decreased (0.480955 --> 0.480824).  Saving model ...
	 Train_Loss: 0.5079 Train_Acc: 80.363 Val_Loss: 0.4808  BEST VAL Loss: 0.4808  Val_Acc: 82.172

Epoch 73: Validation loss decreased (0.480824 --> 0.480661).  Saving model ...
	 Train_Loss: 0.5077 Train_Acc: 80.425 Val_Loss: 0.4807  BEST VAL Loss: 0.4807  Val_Acc: 81.872

Epoch 74: Validation loss decreased (0.480661 --> 0.480438).  Saving model ...
	 Train_Loss: 0.5075 Train_Acc: 80.500 Val_Loss: 0.4804  BEST VAL Loss: 0.4804  Val_Acc: 82.330

Epoch 75: Validation loss decreased (0.480438 --> 0.480266).  Saving model ...
	 Train_Loss: 0.5072 Train_Acc: 80.418 Val_Loss: 0.4803  BEST VAL Loss: 0.4803  Val_Acc: 82.233

Epoch 76: Validation loss decreased (0.480266 --> 0.480139).  Saving model ...
	 Train_Loss: 0.5070 Train_Acc: 80.398 Val_Loss: 0.4801  BEST VAL Loss: 0.4801  Val_Acc: 82.049

Epoch 77: Validation loss decreased (0.480139 --> 0.480036).  Saving model ...
	 Train_Loss: 0.5068 Train_Acc: 80.377 Val_Loss: 0.4800  BEST VAL Loss: 0.4800  Val_Acc: 82.044

Epoch 78: Validation loss decreased (0.480036 --> 0.480022).  Saving model ...
	 Train_Loss: 0.5066 Train_Acc: 80.418 Val_Loss: 0.4800  BEST VAL Loss: 0.4800  Val_Acc: 82.179

Epoch 79: Validation loss decreased (0.480022 --> 0.479800).  Saving model ...
	 Train_Loss: 0.5064 Train_Acc: 80.436 Val_Loss: 0.4798  BEST VAL Loss: 0.4798  Val_Acc: 82.054

Epoch 80: Validation loss decreased (0.479800 --> 0.479692).  Saving model ...
	 Train_Loss: 0.5062 Train_Acc: 80.347 Val_Loss: 0.4797  BEST VAL Loss: 0.4797  Val_Acc: 81.888

Epoch 81: Validation loss decreased (0.479692 --> 0.479501).  Saving model ...
	 Train_Loss: 0.5060 Train_Acc: 80.387 Val_Loss: 0.4795  BEST VAL Loss: 0.4795  Val_Acc: 81.932

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.5059 Train_Acc: 80.359 Val_Loss: 0.4796  BEST VAL Loss: 0.4795  Val_Acc: 81.933

Epoch 83: Validation loss decreased (0.479501 --> 0.479447).  Saving model ...
	 Train_Loss: 0.5057 Train_Acc: 80.367 Val_Loss: 0.4794  BEST VAL Loss: 0.4794  Val_Acc: 82.111

Epoch 84: Validation loss decreased (0.479447 --> 0.479347).  Saving model ...
	 Train_Loss: 0.5055 Train_Acc: 80.422 Val_Loss: 0.4793  BEST VAL Loss: 0.4793  Val_Acc: 82.260

Epoch 85: Validation loss decreased (0.479347 --> 0.479219).  Saving model ...
	 Train_Loss: 0.5053 Train_Acc: 80.450 Val_Loss: 0.4792  BEST VAL Loss: 0.4792  Val_Acc: 82.278

Epoch 86: Validation loss decreased (0.479219 --> 0.479187).  Saving model ...
	 Train_Loss: 0.5052 Train_Acc: 80.354 Val_Loss: 0.4792  BEST VAL Loss: 0.4792  Val_Acc: 81.557

Epoch 87: Validation loss decreased (0.479187 --> 0.478948).  Saving model ...
	 Train_Loss: 0.5050 Train_Acc: 80.341 Val_Loss: 0.4789  BEST VAL Loss: 0.4789  Val_Acc: 82.162

Epoch 88: Validation loss decreased (0.478948 --> 0.478903).  Saving model ...
	 Train_Loss: 0.5048 Train_Acc: 80.446 Val_Loss: 0.4789  BEST VAL Loss: 0.4789  Val_Acc: 81.918

Epoch 89: Validation loss decreased (0.478903 --> 0.478799).  Saving model ...
	 Train_Loss: 0.5047 Train_Acc: 80.481 Val_Loss: 0.4788  BEST VAL Loss: 0.4788  Val_Acc: 81.947

Epoch 90: Validation loss decreased (0.478799 --> 0.478755).  Saving model ...
	 Train_Loss: 0.5045 Train_Acc: 80.430 Val_Loss: 0.4788  BEST VAL Loss: 0.4788  Val_Acc: 82.213

Epoch 91: Validation loss decreased (0.478755 --> 0.478624).  Saving model ...
	 Train_Loss: 0.5044 Train_Acc: 80.540 Val_Loss: 0.4786  BEST VAL Loss: 0.4786  Val_Acc: 82.135

Epoch 92: Validation loss decreased (0.478624 --> 0.478508).  Saving model ...
	 Train_Loss: 0.5042 Train_Acc: 80.519 Val_Loss: 0.4785  BEST VAL Loss: 0.4785  Val_Acc: 81.688

Epoch 93: Validation loss decreased (0.478508 --> 0.478369).  Saving model ...
	 Train_Loss: 0.5040 Train_Acc: 80.476 Val_Loss: 0.4784  BEST VAL Loss: 0.4784  Val_Acc: 82.088

Epoch 94: Validation loss decreased (0.478369 --> 0.478262).  Saving model ...
	 Train_Loss: 0.5039 Train_Acc: 80.504 Val_Loss: 0.4783  BEST VAL Loss: 0.4783  Val_Acc: 82.085

Epoch 95: Validation loss decreased (0.478262 --> 0.478191).  Saving model ...
	 Train_Loss: 0.5037 Train_Acc: 80.506 Val_Loss: 0.4782  BEST VAL Loss: 0.4782  Val_Acc: 82.121

Epoch 96: Validation loss decreased (0.478191 --> 0.478160).  Saving model ...
	 Train_Loss: 0.5036 Train_Acc: 80.426 Val_Loss: 0.4782  BEST VAL Loss: 0.4782  Val_Acc: 82.182

Epoch 97: Validation loss decreased (0.478160 --> 0.478005).  Saving model ...
	 Train_Loss: 0.5034 Train_Acc: 80.507 Val_Loss: 0.4780  BEST VAL Loss: 0.4780  Val_Acc: 81.870

Epoch 98: Validation loss decreased (0.478005 --> 0.477874).  Saving model ...
	 Train_Loss: 0.5033 Train_Acc: 80.490 Val_Loss: 0.4779  BEST VAL Loss: 0.4779  Val_Acc: 82.381

Epoch 99: Validation loss decreased (0.477874 --> 0.477766).  Saving model ...
	 Train_Loss: 0.5032 Train_Acc: 80.518 Val_Loss: 0.4778  BEST VAL Loss: 0.4778  Val_Acc: 82.176

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.06      0.06      0.06     95928
           1       0.51      0.53      0.52    749319
           2       0.43      0.41      0.42    638227

    accuracy                           0.45   1483474
   macro avg       0.33      0.33      0.33   1483474
weighted avg       0.44      0.45      0.45   1483474

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.07      0.06      0.06     23982
           1       0.51      0.54      0.52    187329
           2       0.43      0.41      0.42    159558

    accuracy                           0.45    370869
   macro avg       0.33      0.33      0.33    370869
weighted avg       0.45      0.45      0.45    370869

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.06      0.06      0.06    119911
           1       0.48      0.52      0.50    936644
           2       0.46      0.42      0.44    910220

    accuracy                           0.45   1966775
   macro avg       0.33      0.33      0.33   1966775
weighted avg       0.44      0.45      0.44   1966775

Precision for class 0: 0.061243494456145235
Recall for class 0: 0.0586851915170418
Precision for class 1: 0.475982400302134
Recall for class 1: 0.5193905048236043
Precision for class 2: 0.46270833082271845
Recall for class 2: 0.4218320845509877
3
              precision    recall  f1-score   support

           0       0.06      0.06      0.06    119911
           1       0.48      0.52      0.50    936644
           2       0.46      0.42      0.44    910220

    accuracy                           0.45   1966775
   macro avg       0.33      0.33      0.33   1966775
weighted avg       0.44      0.45      0.44   1966775

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.83      0.91    240721

    accuracy                           0.83    240721
   macro avg       0.33      0.28      0.30    240721
weighted avg       1.00      0.83      0.91    240721

Precision for class 0: 0.0
Recall for class 0: 0.0
Precision for class 1: 0.0
Recall for class 1: 0.0
Precision for class 2: 1.0
Recall for class 2: 0.8338159113662705
3
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.83      0.91    240721

    accuracy                           0.83    240721
   macro avg       0.33      0.28      0.30    240721
weighted avg       1.00      0.83      0.91    240721

MultiClass_MLP_shuffle
              precision    recall  f1-score   support

           0       0.05      0.05      0.05     75619
           1       0.51      0.48      0.50    788818
           2       0.44      0.47      0.45    672406

    accuracy                           0.45   1536843
   macro avg       0.33      0.33      0.33   1536843
weighted avg       0.46      0.45      0.46   1536843

Precision for class 0: 0.04937531755233914
Recall for class 0: 0.05011967891667438
Precision for class 1: 0.5135909402698013
Recall for class 1: 0.48409138736692114
Precision for class 2: 0.4376505427213379
Recall for class 2: 0.4663982772313156
3
              precision    recall  f1-score   support

           0       0.05      0.05      0.05     75619
           1       0.51      0.48      0.50    788818
           2       0.44      0.47      0.45    672406

    accuracy                           0.45   1536843
   macro avg       0.33      0.33      0.33   1536843
weighted avg       0.46      0.45      0.46   1536843

Done
