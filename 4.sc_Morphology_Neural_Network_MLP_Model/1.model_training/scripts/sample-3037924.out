[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'bfaf6f4c'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '89b2c962'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a2607afe'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '1a334dcd'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: Flagellin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: Flagellin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (29575, 1276)
Number of total missing values across all columns: 59150
Data Subset Is Off
Wells held out for testing: ['D14' 'M22']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'M18' 'M19' 'M23']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.403691).  Saving model ...
	 Train_Loss: 0.5706 Train_Acc: 68.275 Val_Loss: 0.4037  BEST VAL Loss: 0.4037  Val_Acc: 82.075

Epoch 1: Validation loss decreased (0.403691 --> 0.375390).  Saving model ...
	 Train_Loss: 0.5186 Train_Acc: 78.391 Val_Loss: 0.3754  BEST VAL Loss: 0.3754  Val_Acc: 83.872

Epoch 2: Validation loss decreased (0.375390 --> 0.351540).  Saving model ...
	 Train_Loss: 0.4890 Train_Acc: 80.284 Val_Loss: 0.3515  BEST VAL Loss: 0.3515  Val_Acc: 86.208

Epoch 3: Validation loss decreased (0.351540 --> 0.340784).  Saving model ...
	 Train_Loss: 0.4672 Train_Acc: 80.621 Val_Loss: 0.3408  BEST VAL Loss: 0.3408  Val_Acc: 86.433

Epoch 4: Validation loss decreased (0.340784 --> 0.332208).  Saving model ...
	 Train_Loss: 0.4517 Train_Acc: 82.896 Val_Loss: 0.3322  BEST VAL Loss: 0.3322  Val_Acc: 87.017

Epoch 5: Validation loss decreased (0.332208 --> 0.323904).  Saving model ...
	 Train_Loss: 0.4416 Train_Acc: 82.655 Val_Loss: 0.3239  BEST VAL Loss: 0.3239  Val_Acc: 87.242

Epoch 6: Validation loss decreased (0.323904 --> 0.316423).  Saving model ...
	 Train_Loss: 0.4318 Train_Acc: 83.621 Val_Loss: 0.3164  BEST VAL Loss: 0.3164  Val_Acc: 87.691

Epoch 7: Validation loss decreased (0.316423 --> 0.311253).  Saving model ...
	 Train_Loss: 0.4247 Train_Acc: 83.536 Val_Loss: 0.3113  BEST VAL Loss: 0.3113  Val_Acc: 87.197

Epoch 8: Validation loss decreased (0.311253 --> 0.309568).  Saving model ...
	 Train_Loss: 0.4184 Train_Acc: 83.761 Val_Loss: 0.3096  BEST VAL Loss: 0.3096  Val_Acc: 87.781

Epoch 9: Validation loss decreased (0.309568 --> 0.306111).  Saving model ...
	 Train_Loss: 0.4125 Train_Acc: 84.042 Val_Loss: 0.3061  BEST VAL Loss: 0.3061  Val_Acc: 87.062

Epoch 10: Validation loss decreased (0.306111 --> 0.302354).  Saving model ...
	 Train_Loss: 0.4082 Train_Acc: 83.750 Val_Loss: 0.3024  BEST VAL Loss: 0.3024  Val_Acc: 88.724

Epoch 11: Validation loss decreased (0.302354 --> 0.298550).  Saving model ...
	 Train_Loss: 0.4035 Train_Acc: 84.778 Val_Loss: 0.2985  BEST VAL Loss: 0.2985  Val_Acc: 88.949

Epoch 12: Validation loss decreased (0.298550 --> 0.294566).  Saving model ...
	 Train_Loss: 0.3987 Train_Acc: 84.986 Val_Loss: 0.2946  BEST VAL Loss: 0.2946  Val_Acc: 88.455

Epoch 13: Validation loss decreased (0.294566 --> 0.291262).  Saving model ...
	 Train_Loss: 0.3947 Train_Acc: 84.952 Val_Loss: 0.2913  BEST VAL Loss: 0.2913  Val_Acc: 88.500

Epoch 14: Validation loss decreased (0.291262 --> 0.289712).  Saving model ...
	 Train_Loss: 0.3909 Train_Acc: 85.255 Val_Loss: 0.2897  BEST VAL Loss: 0.2897  Val_Acc: 89.039

Epoch 15: Validation loss decreased (0.289712 --> 0.287720).  Saving model ...
	 Train_Loss: 0.3870 Train_Acc: 85.553 Val_Loss: 0.2877  BEST VAL Loss: 0.2877  Val_Acc: 87.601

Epoch 16: Validation loss decreased (0.287720 --> 0.285047).  Saving model ...
	 Train_Loss: 0.3837 Train_Acc: 85.486 Val_Loss: 0.2850  BEST VAL Loss: 0.2850  Val_Acc: 88.589

Epoch 17: Validation loss decreased (0.285047 --> 0.283274).  Saving model ...
	 Train_Loss: 0.3804 Train_Acc: 85.766 Val_Loss: 0.2833  BEST VAL Loss: 0.2833  Val_Acc: 88.410

Epoch 18: Validation loss decreased (0.283274 --> 0.281365).  Saving model ...
	 Train_Loss: 0.3777 Train_Acc: 85.469 Val_Loss: 0.2814  BEST VAL Loss: 0.2814  Val_Acc: 89.937

Epoch 19: Validation loss decreased (0.281365 --> 0.279164).  Saving model ...
	 Train_Loss: 0.3752 Train_Acc: 85.823 Val_Loss: 0.2792  BEST VAL Loss: 0.2792  Val_Acc: 89.443

Epoch 20: Validation loss decreased (0.279164 --> 0.277952).  Saving model ...
	 Train_Loss: 0.3735 Train_Acc: 85.250 Val_Loss: 0.2780  BEST VAL Loss: 0.2780  Val_Acc: 88.230

Epoch 21: Validation loss decreased (0.277952 --> 0.277262).  Saving model ...
	 Train_Loss: 0.3720 Train_Acc: 85.323 Val_Loss: 0.2773  BEST VAL Loss: 0.2773  Val_Acc: 88.050

Epoch 22: Validation loss decreased (0.277262 --> 0.275855).  Saving model ...
	 Train_Loss: 0.3701 Train_Acc: 85.244 Val_Loss: 0.2759  BEST VAL Loss: 0.2759  Val_Acc: 88.769

Epoch 23: Validation loss decreased (0.275855 --> 0.274880).  Saving model ...
	 Train_Loss: 0.3686 Train_Acc: 85.379 Val_Loss: 0.2749  BEST VAL Loss: 0.2749  Val_Acc: 89.488

Epoch 24: Validation loss decreased (0.274880 --> 0.273216).  Saving model ...
	 Train_Loss: 0.3668 Train_Acc: 85.969 Val_Loss: 0.2732  BEST VAL Loss: 0.2732  Val_Acc: 89.802

Epoch 25: Validation loss decreased (0.273216 --> 0.272072).  Saving model ...
	 Train_Loss: 0.3651 Train_Acc: 85.693 Val_Loss: 0.2721  BEST VAL Loss: 0.2721  Val_Acc: 89.263

Epoch 26: Validation loss decreased (0.272072 --> 0.270914).  Saving model ...
	 Train_Loss: 0.3632 Train_Acc: 86.087 Val_Loss: 0.2709  BEST VAL Loss: 0.2709  Val_Acc: 88.769

Epoch 27: Validation loss decreased (0.270914 --> 0.269823).  Saving model ...
	 Train_Loss: 0.3615 Train_Acc: 86.497 Val_Loss: 0.2698  BEST VAL Loss: 0.2698  Val_Acc: 88.365

Epoch 28: Validation loss decreased (0.269823 --> 0.269046).  Saving model ...
	 Train_Loss: 0.3599 Train_Acc: 86.025 Val_Loss: 0.2690  BEST VAL Loss: 0.2690  Val_Acc: 87.826

Epoch 29: Validation loss decreased (0.269046 --> 0.268421).  Saving model ...
	 Train_Loss: 0.3584 Train_Acc: 86.176 Val_Loss: 0.2684  BEST VAL Loss: 0.2684  Val_Acc: 89.353

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.3572 Train_Acc: 86.497 Val_Loss: 0.2686  BEST VAL Loss: 0.2684  Val_Acc: 87.826

Epoch 31: Validation loss decreased (0.268421 --> 0.268157).  Saving model ...
	 Train_Loss: 0.3560 Train_Acc: 85.963 Val_Loss: 0.2682  BEST VAL Loss: 0.2682  Val_Acc: 88.410

Epoch 32: Validation loss decreased (0.268157 --> 0.267393).  Saving model ...
	 Train_Loss: 0.3548 Train_Acc: 86.328 Val_Loss: 0.2674  BEST VAL Loss: 0.2674  Val_Acc: 89.263

Epoch 33: Validation loss decreased (0.267393 --> 0.267388).  Saving model ...
	 Train_Loss: 0.3536 Train_Acc: 86.143 Val_Loss: 0.2674  BEST VAL Loss: 0.2674  Val_Acc: 88.230

Epoch 34: Validation loss decreased (0.267388 --> 0.267327).  Saving model ...
	 Train_Loss: 0.3529 Train_Acc: 85.980 Val_Loss: 0.2673  BEST VAL Loss: 0.2673  Val_Acc: 88.455

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.3524 Train_Acc: 85.536 Val_Loss: 0.2673  BEST VAL Loss: 0.2673  Val_Acc: 88.230

Epoch 36: Validation loss decreased (0.267327 --> 0.266825).  Saving model ...
	 Train_Loss: 0.3516 Train_Acc: 85.789 Val_Loss: 0.2668  BEST VAL Loss: 0.2668  Val_Acc: 89.353

Epoch 37: Validation loss decreased (0.266825 --> 0.266716).  Saving model ...
	 Train_Loss: 0.3507 Train_Acc: 86.339 Val_Loss: 0.2667  BEST VAL Loss: 0.2667  Val_Acc: 89.488

Epoch 38: Validation loss decreased (0.266716 --> 0.266311).  Saving model ...
	 Train_Loss: 0.3499 Train_Acc: 86.508 Val_Loss: 0.2663  BEST VAL Loss: 0.2663  Val_Acc: 88.544

Epoch 39: Validation loss decreased (0.266311 --> 0.265937).  Saving model ...
	 Train_Loss: 0.3493 Train_Acc: 85.486 Val_Loss: 0.2659  BEST VAL Loss: 0.2659  Val_Acc: 88.859

Epoch 40: Validation loss decreased (0.265937 --> 0.265701).  Saving model ...
	 Train_Loss: 0.3487 Train_Acc: 85.424 Val_Loss: 0.2657  BEST VAL Loss: 0.2657  Val_Acc: 89.263

Epoch 41: Validation loss decreased (0.265701 --> 0.265403).  Saving model ...
	 Train_Loss: 0.3484 Train_Acc: 85.519 Val_Loss: 0.2654  BEST VAL Loss: 0.2654  Val_Acc: 88.455

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.3488 Train_Acc: 85.924 Val_Loss: 0.2693  BEST VAL Loss: 0.2654  Val_Acc: 88.994

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.3503 Train_Acc: 83.025 Val_Loss: 0.2695  BEST VAL Loss: 0.2654  Val_Acc: 88.230

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.3510 Train_Acc: 84.418 Val_Loss: 0.2698  BEST VAL Loss: 0.2654  Val_Acc: 88.320

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.3512 Train_Acc: 85.693 Val_Loss: 0.2695  BEST VAL Loss: 0.2654  Val_Acc: 88.904

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.3516 Train_Acc: 85.334 Val_Loss: 0.2700  BEST VAL Loss: 0.2654  Val_Acc: 87.736

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.3522 Train_Acc: 84.935 Val_Loss: 0.2704  BEST VAL Loss: 0.2654  Val_Acc: 87.556

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.3533 Train_Acc: 85.250 Val_Loss: 0.2718  BEST VAL Loss: 0.2654  Val_Acc: 88.544

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.3543 Train_Acc: 83.082 Val_Loss: 0.2727  BEST VAL Loss: 0.2654  Val_Acc: 88.185

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.3562 Train_Acc: 82.537 Val_Loss: 0.2751  BEST VAL Loss: 0.2654  Val_Acc: 84.996

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.3584 Train_Acc: 81.952 Val_Loss: 0.2777  BEST VAL Loss: 0.2654  Val_Acc: 82.570

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.3616 Train_Acc: 78.992 Val_Loss: 0.2807  BEST VAL Loss: 0.2654  Val_Acc: 82.839

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.3645 Train_Acc: 77.380 Val_Loss: 0.2835  BEST VAL Loss: 0.2654  Val_Acc: 83.603

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.3667 Train_Acc: 79.296 Val_Loss: 0.2864  BEST VAL Loss: 0.2654  Val_Acc: 85.490

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.3687 Train_Acc: 79.318 Val_Loss: 0.2881  BEST VAL Loss: 0.2654  Val_Acc: 85.490

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.3707 Train_Acc: 78.768 Val_Loss: 0.2899  BEST VAL Loss: 0.2654  Val_Acc: 85.265

Epoch 57: Validation loss did not decrease
Early stopped at epoch : 57
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.90      0.94      0.92      9433
           1       0.93      0.88      0.90      8370

    accuracy                           0.91     17803
   macro avg       0.91      0.91      0.91     17803
weighted avg       0.91      0.91      0.91     17803

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.87      0.92      0.89      1179
           1       0.90      0.85      0.87      1047

    accuracy                           0.88      2226
   macro avg       0.89      0.88      0.88      2226
weighted avg       0.89      0.88      0.88      2226

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.88      0.92      0.90      1180
           1       0.91      0.86      0.88      1046

    accuracy                           0.89      2226
   macro avg       0.89      0.89      0.89      2226
weighted avg       0.89      0.89      0.89      2226

              precision    recall  f1-score   support

           0       0.88      0.92      0.90      1180
           1       0.91      0.86      0.88      1046

    accuracy                           0.89      2226
   macro avg       0.89      0.89      0.89      2226
weighted avg       0.89      0.89      0.89      2226

Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Flagellin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.89      0.88      0.88      4017
           1       0.86      0.86      0.86      3303

    accuracy                           0.87      7320
   macro avg       0.87      0.87      0.87      7320
weighted avg       0.87      0.87      0.87      7320

              precision    recall  f1-score   support

           0       0.89      0.88      0.88      4017
           1       0.86      0.86      0.86      3303

    accuracy                           0.87      7320
   macro avg       0.87      0.87      0.87      7320
weighted avg       0.87      0.87      0.87      7320

completed
