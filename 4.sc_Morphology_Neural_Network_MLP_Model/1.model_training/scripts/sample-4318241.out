[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18694 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
[NbConvertApp] Writing 61992 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:276: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:303: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1017: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1017: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:693: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:698: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:750: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  main_prob_df = pd.concat([main_prob_df, prob_df])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:761: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:826: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:840: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:999: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1005: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1083: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1249: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1255: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1445: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1447: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1450: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1475: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1511: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1631: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1637: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1124: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1844: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1961: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1967: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2153: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2155: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2158: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2229: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
PBMC MultiClass_MLP False
[0.9436581681188537, 0.5416430152668075, 0.5146988166143389]
Data Subset Is Off
(1411998,) (353000,) (1877429,) 3642427     858323
3642428     858324
3642429     858325
3642430     858326
3642431     858327
            ...
4061834    5498161
4061835    5498162
4061836    5498163
4061837    5498164
4061838    5498165
Name: labeled_data_index, Length: 419412, dtype: int64 (1536843,)
(1411998,) (353000,) (1877429,) 3642427     858323
3642428     858324
3642429     858325
3642430     858326
3642431     858327
            ...
4061834    5498161
4061835    5498162
4061836    5498163
4061837    5498164
4061838    5498165
Name: labeled_data_index, Length: 419412, dtype: int64 (1536843,)
5598682
(95928,) (722887,) (593183,)
(23982,) (180722,) (148296,)
(119911,) (903609,) (853909,)
(0,) (0,) (419412,)
(75619,) (758977,) (702247,)
(1411998, 1245) (353000, 1245) (1877429, 1245) (419412, 1245) (1536843, 1245)
(1411998,) (353000,) (1877429,) (419412,) (1536843,)
3
Number of in features:  1245
Number of out features:  3
Multi_Class
Adam
Epoch 0: Validation loss decreased (inf --> 0.313681).  Saving model ...
	 Train_Loss: 0.3610 Train_Acc: 0.002 Val_Loss: 0.3137  BEST VAL Loss: 0.3137  Val_Acc: 0.000

Epoch 1: Validation loss decreased (0.313681 --> 0.303537).  Saving model ...
	 Train_Loss: 0.3425 Train_Acc: 0.000 Val_Loss: 0.3035  BEST VAL Loss: 0.3035  Val_Acc: 0.000

Epoch 2: Validation loss decreased (0.303537 --> 0.296914).  Saving model ...
	 Train_Loss: 0.3320 Train_Acc: 0.000 Val_Loss: 0.2969  BEST VAL Loss: 0.2969  Val_Acc: 0.000

Epoch 3: Validation loss decreased (0.296914 --> 0.291693).  Saving model ...
	 Train_Loss: 0.3248 Train_Acc: 0.000 Val_Loss: 0.2917  BEST VAL Loss: 0.2917  Val_Acc: 0.000

Epoch 4: Validation loss decreased (0.291693 --> 0.287563).  Saving model ...
	 Train_Loss: 0.3193 Train_Acc: 0.000 Val_Loss: 0.2876  BEST VAL Loss: 0.2876  Val_Acc: 0.000

Epoch 5: Validation loss decreased (0.287563 --> 0.284247).  Saving model ...
	 Train_Loss: 0.3150 Train_Acc: 0.000 Val_Loss: 0.2842  BEST VAL Loss: 0.2842  Val_Acc: 0.001

Epoch 6: Validation loss decreased (0.284247 --> 0.281469).  Saving model ...
	 Train_Loss: 0.3115 Train_Acc: 0.000 Val_Loss: 0.2815  BEST VAL Loss: 0.2815  Val_Acc: 0.000

Epoch 7: Validation loss decreased (0.281469 --> 0.279269).  Saving model ...
	 Train_Loss: 0.3085 Train_Acc: 0.000 Val_Loss: 0.2793  BEST VAL Loss: 0.2793  Val_Acc: 0.000

Epoch 8: Validation loss decreased (0.279269 --> 0.277243).  Saving model ...
	 Train_Loss: 0.3059 Train_Acc: 0.000 Val_Loss: 0.2772  BEST VAL Loss: 0.2772  Val_Acc: 0.001

Epoch 9: Validation loss decreased (0.277243 --> 0.275377).  Saving model ...
	 Train_Loss: 0.3036 Train_Acc: 0.000 Val_Loss: 0.2754  BEST VAL Loss: 0.2754  Val_Acc: 0.000

Epoch 10: Validation loss decreased (0.275377 --> 0.273743).  Saving model ...
	 Train_Loss: 0.3016 Train_Acc: 0.000 Val_Loss: 0.2737  BEST VAL Loss: 0.2737  Val_Acc: 0.000

Epoch 11: Validation loss decreased (0.273743 --> 0.272368).  Saving model ...
	 Train_Loss: 0.2998 Train_Acc: 0.000 Val_Loss: 0.2724  BEST VAL Loss: 0.2724  Val_Acc: 0.001

Epoch 12: Validation loss decreased (0.272368 --> 0.271102).  Saving model ...
	 Train_Loss: 0.2981 Train_Acc: 0.000 Val_Loss: 0.2711  BEST VAL Loss: 0.2711  Val_Acc: 0.001

Epoch 13: Validation loss decreased (0.271102 --> 0.269904).  Saving model ...
	 Train_Loss: 0.2967 Train_Acc: 0.001 Val_Loss: 0.2699  BEST VAL Loss: 0.2699  Val_Acc: 0.001

Epoch 14: Validation loss decreased (0.269904 --> 0.268811).  Saving model ...
	 Train_Loss: 0.2953 Train_Acc: 0.000 Val_Loss: 0.2688  BEST VAL Loss: 0.2688  Val_Acc: 0.001

Epoch 15: Validation loss decreased (0.268811 --> 0.267674).  Saving model ...
	 Train_Loss: 0.2940 Train_Acc: 0.001 Val_Loss: 0.2677  BEST VAL Loss: 0.2677  Val_Acc: 0.000

Epoch 16: Validation loss decreased (0.267674 --> 0.266666).  Saving model ...
	 Train_Loss: 0.2928 Train_Acc: 0.000 Val_Loss: 0.2667  BEST VAL Loss: 0.2667  Val_Acc: 0.000

Epoch 17: Validation loss decreased (0.266666 --> 0.265756).  Saving model ...
	 Train_Loss: 0.2917 Train_Acc: 0.001 Val_Loss: 0.2658  BEST VAL Loss: 0.2658  Val_Acc: 0.000

Epoch 18: Validation loss decreased (0.265756 --> 0.264903).  Saving model ...
	 Train_Loss: 0.2907 Train_Acc: 0.000 Val_Loss: 0.2649  BEST VAL Loss: 0.2649  Val_Acc: 0.000

Epoch 19: Validation loss decreased (0.264903 --> 0.264127).  Saving model ...
	 Train_Loss: 0.2897 Train_Acc: 0.000 Val_Loss: 0.2641  BEST VAL Loss: 0.2641  Val_Acc: 0.000

Epoch 20: Validation loss decreased (0.264127 --> 0.263350).  Saving model ...
	 Train_Loss: 0.2888 Train_Acc: 0.001 Val_Loss: 0.2634  BEST VAL Loss: 0.2634  Val_Acc: 0.000

Epoch 21: Validation loss decreased (0.263350 --> 0.262590).  Saving model ...
	 Train_Loss: 0.2880 Train_Acc: 0.001 Val_Loss: 0.2626  BEST VAL Loss: 0.2626  Val_Acc: 0.001

Epoch 22: Validation loss decreased (0.262590 --> 0.261875).  Saving model ...
	 Train_Loss: 0.2872 Train_Acc: 0.001 Val_Loss: 0.2619  BEST VAL Loss: 0.2619  Val_Acc: 0.001

Epoch 23: Validation loss decreased (0.261875 --> 0.261332).  Saving model ...
	 Train_Loss: 0.2864 Train_Acc: 0.000 Val_Loss: 0.2613  BEST VAL Loss: 0.2613  Val_Acc: 0.000

Epoch 24: Validation loss decreased (0.261332 --> 0.260686).  Saving model ...
	 Train_Loss: 0.2857 Train_Acc: 0.001 Val_Loss: 0.2607  BEST VAL Loss: 0.2607  Val_Acc: 0.000

Epoch 25: Validation loss decreased (0.260686 --> 0.260050).  Saving model ...
	 Train_Loss: 0.2850 Train_Acc: 0.001 Val_Loss: 0.2600  BEST VAL Loss: 0.2600  Val_Acc: 0.000

Epoch 26: Validation loss decreased (0.260050 --> 0.259442).  Saving model ...
	 Train_Loss: 0.2843 Train_Acc: 0.001 Val_Loss: 0.2594  BEST VAL Loss: 0.2594  Val_Acc: 0.000

Epoch 27: Validation loss decreased (0.259442 --> 0.258862).  Saving model ...
	 Train_Loss: 0.2836 Train_Acc: 0.000 Val_Loss: 0.2589  BEST VAL Loss: 0.2589  Val_Acc: 0.002

Epoch 28: Validation loss decreased (0.258862 --> 0.258401).  Saving model ...
	 Train_Loss: 0.2830 Train_Acc: 0.001 Val_Loss: 0.2584  BEST VAL Loss: 0.2584  Val_Acc: 0.001

Epoch 29: Validation loss decreased (0.258401 --> 0.257916).  Saving model ...
	 Train_Loss: 0.2824 Train_Acc: 0.001 Val_Loss: 0.2579  BEST VAL Loss: 0.2579  Val_Acc: 0.001

Epoch 30: Validation loss decreased (0.257916 --> 0.257452).  Saving model ...
	 Train_Loss: 0.2819 Train_Acc: 0.001 Val_Loss: 0.2575  BEST VAL Loss: 0.2575  Val_Acc: 0.001

Epoch 31: Validation loss decreased (0.257452 --> 0.256985).  Saving model ...
	 Train_Loss: 0.2813 Train_Acc: 0.001 Val_Loss: 0.2570  BEST VAL Loss: 0.2570  Val_Acc: 0.001

Epoch 32: Validation loss decreased (0.256985 --> 0.256498).  Saving model ...
	 Train_Loss: 0.2808 Train_Acc: 0.000 Val_Loss: 0.2565  BEST VAL Loss: 0.2565  Val_Acc: 0.001

Epoch 33: Validation loss decreased (0.256498 --> 0.256027).  Saving model ...
	 Train_Loss: 0.2803 Train_Acc: 0.001 Val_Loss: 0.2560  BEST VAL Loss: 0.2560  Val_Acc: 0.001

Epoch 34: Validation loss decreased (0.256027 --> 0.255553).  Saving model ...
	 Train_Loss: 0.2798 Train_Acc: 0.001 Val_Loss: 0.2556  BEST VAL Loss: 0.2556  Val_Acc: 0.001

Epoch 35: Validation loss decreased (0.255553 --> 0.255175).  Saving model ...
	 Train_Loss: 0.2793 Train_Acc: 0.001 Val_Loss: 0.2552  BEST VAL Loss: 0.2552  Val_Acc: 0.001

Epoch 36: Validation loss decreased (0.255175 --> 0.254752).  Saving model ...
	 Train_Loss: 0.2789 Train_Acc: 0.001 Val_Loss: 0.2548  BEST VAL Loss: 0.2548  Val_Acc: 0.000

Epoch 37: Validation loss decreased (0.254752 --> 0.254435).  Saving model ...
	 Train_Loss: 0.2784 Train_Acc: 0.001 Val_Loss: 0.2544  BEST VAL Loss: 0.2544  Val_Acc: 0.000

Epoch 38: Validation loss decreased (0.254435 --> 0.254027).  Saving model ...
	 Train_Loss: 0.2780 Train_Acc: 0.001 Val_Loss: 0.2540  BEST VAL Loss: 0.2540  Val_Acc: 0.001

Epoch 39: Validation loss decreased (0.254027 --> 0.253660).  Saving model ...
	 Train_Loss: 0.2776 Train_Acc: 0.001 Val_Loss: 0.2537  BEST VAL Loss: 0.2537  Val_Acc: 0.001

Epoch 40: Validation loss decreased (0.253660 --> 0.253310).  Saving model ...
	 Train_Loss: 0.2772 Train_Acc: 0.001 Val_Loss: 0.2533  BEST VAL Loss: 0.2533  Val_Acc: 0.001

Epoch 41: Validation loss decreased (0.253310 --> 0.252935).  Saving model ...
	 Train_Loss: 0.2768 Train_Acc: 0.001 Val_Loss: 0.2529  BEST VAL Loss: 0.2529  Val_Acc: 0.001

Epoch 42: Validation loss decreased (0.252935 --> 0.252571).  Saving model ...
	 Train_Loss: 0.2764 Train_Acc: 0.000 Val_Loss: 0.2526  BEST VAL Loss: 0.2526  Val_Acc: 0.000

Epoch 43: Validation loss decreased (0.252571 --> 0.252291).  Saving model ...
	 Train_Loss: 0.2761 Train_Acc: 0.001 Val_Loss: 0.2523  BEST VAL Loss: 0.2523  Val_Acc: 0.001

Epoch 44: Validation loss decreased (0.252291 --> 0.251976).  Saving model ...
	 Train_Loss: 0.2757 Train_Acc: 0.001 Val_Loss: 0.2520  BEST VAL Loss: 0.2520  Val_Acc: 0.000

Epoch 45: Validation loss decreased (0.251976 --> 0.251670).  Saving model ...
	 Train_Loss: 0.2753 Train_Acc: 0.001 Val_Loss: 0.2517  BEST VAL Loss: 0.2517  Val_Acc: 0.001

Epoch 46: Validation loss decreased (0.251670 --> 0.251382).  Saving model ...
	 Train_Loss: 0.2750 Train_Acc: 0.001 Val_Loss: 0.2514  BEST VAL Loss: 0.2514  Val_Acc: 0.000

Epoch 47: Validation loss decreased (0.251382 --> 0.251088).  Saving model ...
	 Train_Loss: 0.2747 Train_Acc: 0.001 Val_Loss: 0.2511  BEST VAL Loss: 0.2511  Val_Acc: 0.001

Epoch 48: Validation loss decreased (0.251088 --> 0.250789).  Saving model ...
	 Train_Loss: 0.2743 Train_Acc: 0.001 Val_Loss: 0.2508  BEST VAL Loss: 0.2508  Val_Acc: 0.001

Epoch 49: Validation loss decreased (0.250789 --> 0.250513).  Saving model ...
	 Train_Loss: 0.2740 Train_Acc: 0.000 Val_Loss: 0.2505  BEST VAL Loss: 0.2505  Val_Acc: 0.000

Epoch 50: Validation loss decreased (0.250513 --> 0.250221).  Saving model ...
	 Train_Loss: 0.2737 Train_Acc: 0.001 Val_Loss: 0.2502  BEST VAL Loss: 0.2502  Val_Acc: 0.001

Epoch 51: Validation loss decreased (0.250221 --> 0.249932).  Saving model ...
	 Train_Loss: 0.2734 Train_Acc: 0.001 Val_Loss: 0.2499  BEST VAL Loss: 0.2499  Val_Acc: 0.001

Epoch 52: Validation loss decreased (0.249932 --> 0.249658).  Saving model ...
	 Train_Loss: 0.2731 Train_Acc: 0.001 Val_Loss: 0.2497  BEST VAL Loss: 0.2497  Val_Acc: 0.000

Epoch 53: Validation loss decreased (0.249658 --> 0.249400).  Saving model ...
	 Train_Loss: 0.2728 Train_Acc: 0.001 Val_Loss: 0.2494  BEST VAL Loss: 0.2494  Val_Acc: 0.001

Epoch 54: Validation loss decreased (0.249400 --> 0.249193).  Saving model ...
	 Train_Loss: 0.2726 Train_Acc: 0.001 Val_Loss: 0.2492  BEST VAL Loss: 0.2492  Val_Acc: 0.001

Epoch 55: Validation loss decreased (0.249193 --> 0.248919).  Saving model ...
	 Train_Loss: 0.2723 Train_Acc: 0.001 Val_Loss: 0.2489  BEST VAL Loss: 0.2489  Val_Acc: 0.000

Epoch 56: Validation loss decreased (0.248919 --> 0.248683).  Saving model ...
	 Train_Loss: 0.2720 Train_Acc: 0.001 Val_Loss: 0.2487  BEST VAL Loss: 0.2487  Val_Acc: 0.001

Epoch 57: Validation loss decreased (0.248683 --> 0.248447).  Saving model ...
	 Train_Loss: 0.2717 Train_Acc: 0.001 Val_Loss: 0.2484  BEST VAL Loss: 0.2484  Val_Acc: 0.001

Epoch 58: Validation loss decreased (0.248447 --> 0.248210).  Saving model ...
	 Train_Loss: 0.2715 Train_Acc: 0.001 Val_Loss: 0.2482  BEST VAL Loss: 0.2482  Val_Acc: 0.001

Epoch 59: Validation loss decreased (0.248210 --> 0.247984).  Saving model ...
	 Train_Loss: 0.2712 Train_Acc: 0.001 Val_Loss: 0.2480  BEST VAL Loss: 0.2480  Val_Acc: 0.001

Epoch 60: Validation loss decreased (0.247984 --> 0.247780).  Saving model ...
	 Train_Loss: 0.2710 Train_Acc: 0.001 Val_Loss: 0.2478  BEST VAL Loss: 0.2478  Val_Acc: 0.001

Epoch 61: Validation loss decreased (0.247780 --> 0.247547).  Saving model ...
	 Train_Loss: 0.2707 Train_Acc: 0.001 Val_Loss: 0.2475  BEST VAL Loss: 0.2475  Val_Acc: 0.001

Epoch 62: Validation loss decreased (0.247547 --> 0.247338).  Saving model ...
	 Train_Loss: 0.2705 Train_Acc: 0.001 Val_Loss: 0.2473  BEST VAL Loss: 0.2473  Val_Acc: 0.001

Epoch 63: Validation loss decreased (0.247338 --> 0.247136).  Saving model ...
	 Train_Loss: 0.2703 Train_Acc: 0.001 Val_Loss: 0.2471  BEST VAL Loss: 0.2471  Val_Acc: 0.001

Epoch 64: Validation loss decreased (0.247136 --> 0.246948).  Saving model ...
	 Train_Loss: 0.2700 Train_Acc: 0.001 Val_Loss: 0.2469  BEST VAL Loss: 0.2469  Val_Acc: 0.001

Epoch 65: Validation loss decreased (0.246948 --> 0.246726).  Saving model ...
	 Train_Loss: 0.2698 Train_Acc: 0.001 Val_Loss: 0.2467  BEST VAL Loss: 0.2467  Val_Acc: 0.001

Epoch 66: Validation loss decreased (0.246726 --> 0.246520).  Saving model ...
	 Train_Loss: 0.2696 Train_Acc: 0.001 Val_Loss: 0.2465  BEST VAL Loss: 0.2465  Val_Acc: 0.001

Epoch 67: Validation loss decreased (0.246520 --> 0.246366).  Saving model ...
	 Train_Loss: 0.2694 Train_Acc: 0.001 Val_Loss: 0.2464  BEST VAL Loss: 0.2464  Val_Acc: 0.001

Epoch 68: Validation loss decreased (0.246366 --> 0.246192).  Saving model ...
	 Train_Loss: 0.2691 Train_Acc: 0.001 Val_Loss: 0.2462  BEST VAL Loss: 0.2462  Val_Acc: 0.001

Epoch 69: Validation loss decreased (0.246192 --> 0.246006).  Saving model ...
	 Train_Loss: 0.2689 Train_Acc: 0.001 Val_Loss: 0.2460  BEST VAL Loss: 0.2460  Val_Acc: 0.001

Epoch 70: Validation loss decreased (0.246006 --> 0.245836).  Saving model ...
	 Train_Loss: 0.2687 Train_Acc: 0.001 Val_Loss: 0.2458  BEST VAL Loss: 0.2458  Val_Acc: 0.001

Epoch 71: Validation loss decreased (0.245836 --> 0.245683).  Saving model ...
	 Train_Loss: 0.2685 Train_Acc: 0.001 Val_Loss: 0.2457  BEST VAL Loss: 0.2457  Val_Acc: 0.001

Epoch 72: Validation loss decreased (0.245683 --> 0.245493).  Saving model ...
	 Train_Loss: 0.2683 Train_Acc: 0.001 Val_Loss: 0.2455  BEST VAL Loss: 0.2455  Val_Acc: 0.001

Epoch 73: Validation loss decreased (0.245493 --> 0.245304).  Saving model ...
	 Train_Loss: 0.2681 Train_Acc: 0.000 Val_Loss: 0.2453  BEST VAL Loss: 0.2453  Val_Acc: 0.001

Epoch 74: Validation loss decreased (0.245304 --> 0.245112).  Saving model ...
	 Train_Loss: 0.2679 Train_Acc: 0.001 Val_Loss: 0.2451  BEST VAL Loss: 0.2451  Val_Acc: 0.001

Epoch 75: Validation loss decreased (0.245112 --> 0.244935).  Saving model ...
	 Train_Loss: 0.2677 Train_Acc: 0.001 Val_Loss: 0.2449  BEST VAL Loss: 0.2449  Val_Acc: 0.001

Epoch 76: Validation loss decreased (0.244935 --> 0.244773).  Saving model ...
	 Train_Loss: 0.2675 Train_Acc: 0.001 Val_Loss: 0.2448  BEST VAL Loss: 0.2448  Val_Acc: 0.001

Epoch 77: Validation loss decreased (0.244773 --> 0.244629).  Saving model ...
	 Train_Loss: 0.2673 Train_Acc: 0.001 Val_Loss: 0.2446  BEST VAL Loss: 0.2446  Val_Acc: 0.001

Epoch 78: Validation loss decreased (0.244629 --> 0.244461).  Saving model ...
	 Train_Loss: 0.2671 Train_Acc: 0.001 Val_Loss: 0.2445  BEST VAL Loss: 0.2445  Val_Acc: 0.001

Epoch 79: Validation loss decreased (0.244461 --> 0.244289).  Saving model ...
	 Train_Loss: 0.2669 Train_Acc: 0.001 Val_Loss: 0.2443  BEST VAL Loss: 0.2443  Val_Acc: 0.001

Epoch 80: Validation loss decreased (0.244289 --> 0.244142).  Saving model ...
	 Train_Loss: 0.2668 Train_Acc: 0.001 Val_Loss: 0.2441  BEST VAL Loss: 0.2441  Val_Acc: 0.001

Epoch 81: Validation loss decreased (0.244142 --> 0.243966).  Saving model ...
	 Train_Loss: 0.2666 Train_Acc: 0.001 Val_Loss: 0.2440  BEST VAL Loss: 0.2440  Val_Acc: 0.001

Epoch 82: Validation loss decreased (0.243966 --> 0.243795).  Saving model ...
	 Train_Loss: 0.2664 Train_Acc: 0.001 Val_Loss: 0.2438  BEST VAL Loss: 0.2438  Val_Acc: 0.001

Epoch 83: Validation loss decreased (0.243795 --> 0.243648).  Saving model ...
	 Train_Loss: 0.2662 Train_Acc: 0.001 Val_Loss: 0.2436  BEST VAL Loss: 0.2436  Val_Acc: 0.001

Epoch 84: Validation loss decreased (0.243648 --> 0.243488).  Saving model ...
	 Train_Loss: 0.2661 Train_Acc: 0.001 Val_Loss: 0.2435  BEST VAL Loss: 0.2435  Val_Acc: 0.001

Epoch 85: Validation loss decreased (0.243488 --> 0.243333).  Saving model ...
	 Train_Loss: 0.2659 Train_Acc: 0.001 Val_Loss: 0.2433  BEST VAL Loss: 0.2433  Val_Acc: 0.001

Epoch 86: Validation loss decreased (0.243333 --> 0.243168).  Saving model ...
	 Train_Loss: 0.2657 Train_Acc: 0.001 Val_Loss: 0.2432  BEST VAL Loss: 0.2432  Val_Acc: 0.000

Epoch 87: Validation loss decreased (0.243168 --> 0.243004).  Saving model ...
	 Train_Loss: 0.2656 Train_Acc: 0.001 Val_Loss: 0.2430  BEST VAL Loss: 0.2430  Val_Acc: 0.000

Epoch 88: Validation loss decreased (0.243004 --> 0.242872).  Saving model ...
	 Train_Loss: 0.2654 Train_Acc: 0.001 Val_Loss: 0.2429  BEST VAL Loss: 0.2429  Val_Acc: 0.001

Epoch 89: Validation loss decreased (0.242872 --> 0.242731).  Saving model ...
	 Train_Loss: 0.2652 Train_Acc: 0.001 Val_Loss: 0.2427  BEST VAL Loss: 0.2427  Val_Acc: 0.001

Epoch 90: Validation loss decreased (0.242731 --> 0.242579).  Saving model ...
	 Train_Loss: 0.2651 Train_Acc: 0.001 Val_Loss: 0.2426  BEST VAL Loss: 0.2426  Val_Acc: 0.001

Epoch 91: Validation loss decreased (0.242579 --> 0.242434).  Saving model ...
	 Train_Loss: 0.2649 Train_Acc: 0.001 Val_Loss: 0.2424  BEST VAL Loss: 0.2424  Val_Acc: 0.001

Epoch 92: Validation loss decreased (0.242434 --> 0.242280).  Saving model ...
	 Train_Loss: 0.2648 Train_Acc: 0.001 Val_Loss: 0.2423  BEST VAL Loss: 0.2423  Val_Acc: 0.001

Epoch 93: Validation loss decreased (0.242280 --> 0.242174).  Saving model ...
	 Train_Loss: 0.2646 Train_Acc: 0.001 Val_Loss: 0.2422  BEST VAL Loss: 0.2422  Val_Acc: 0.000

Epoch 94: Validation loss decreased (0.242174 --> 0.242033).  Saving model ...
	 Train_Loss: 0.2645 Train_Acc: 0.001 Val_Loss: 0.2420  BEST VAL Loss: 0.2420  Val_Acc: 0.001

Epoch 95: Validation loss decreased (0.242033 --> 0.241894).  Saving model ...
	 Train_Loss: 0.2643 Train_Acc: 0.001 Val_Loss: 0.2419  BEST VAL Loss: 0.2419  Val_Acc: 0.001

Epoch 96: Validation loss decreased (0.241894 --> 0.241743).  Saving model ...
	 Train_Loss: 0.2642 Train_Acc: 0.001 Val_Loss: 0.2417  BEST VAL Loss: 0.2417  Val_Acc: 0.001

Epoch 97: Validation loss decreased (0.241743 --> 0.241623).  Saving model ...
	 Train_Loss: 0.2640 Train_Acc: 0.001 Val_Loss: 0.2416  BEST VAL Loss: 0.2416  Val_Acc: 0.001

Epoch 98: Validation loss decreased (0.241623 --> 0.241488).  Saving model ...
	 Train_Loss: 0.2639 Train_Acc: 0.001 Val_Loss: 0.2415  BEST VAL Loss: 0.2415  Val_Acc: 0.001

Epoch 99: Validation loss decreased (0.241488 --> 0.241356).  Saving model ...
	 Train_Loss: 0.2637 Train_Acc: 0.001 Val_Loss: 0.2414  BEST VAL Loss: 0.2414  Val_Acc: 0.001

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.71      0.67      0.69     95928
           1       0.86      0.89      0.88    722887
           2       0.87      0.84      0.85    593183

    accuracy                           0.85   1411998
   macro avg       0.81      0.80      0.81   1411998
weighted avg       0.85      0.85      0.85   1411998

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.67      0.63      0.65     23982
           1       0.85      0.88      0.87    180722
           2       0.86      0.83      0.84    148296

    accuracy                           0.84    353000
   macro avg       0.79      0.78      0.79    353000
weighted avg       0.84      0.84      0.84    353000

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.63      0.63      0.63    118868
           1       0.88      0.83      0.85    962975
           2       0.81      0.87      0.84    795586

    accuracy                           0.83   1877429
   macro avg       0.77      0.78      0.78   1877429
weighted avg       0.84      0.83      0.83   1877429

Precision for class 0: 0.6345021368240401
Recall for class 0: 0.6289831625121965
Precision for class 1: 0.8276351930216256
Recall for class 1: 0.8820098073392363
Precision for class 2: 0.8709366429273516
Recall for class 2: 0.8114506346695023
3
              precision    recall  f1-score   support

           0       0.63      0.63      0.63    119911
           1       0.83      0.88      0.85    903609
           2       0.87      0.81      0.84    853909

    accuracy                           0.83   1877429
   macro avg       0.78      0.77      0.78   1877429
weighted avg       0.83      0.83      0.83   1877429

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.00      0.00      0.00     31589
           1       0.00      0.00      0.00    114686
           2       0.65      1.00      0.79    273137

    accuracy                           0.65    419412
   macro avg       0.22      0.33      0.26    419412
weighted avg       0.42      0.65      0.51    419412

Precision for class 0: 0.0
Recall for class 0: 0.0
Precision for class 1: 0.0
Recall for class 1: 0.0
Precision for class 2: 1.0
Recall for class 2: 0.6512379235691873
3
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.65      0.79    419412

    accuracy                           0.65    419412
   macro avg       0.33      0.22      0.26    419412
weighted avg       1.00      0.65      0.79    419412

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.36      0.28      0.31     97005
           1       0.76      0.78      0.77    740008
           2       0.75      0.76      0.75    699830

    accuracy                           0.74   1536843
   macro avg       0.62      0.61      0.61   1536843
weighted avg       0.73      0.74      0.74   1536843

Precision for class 0: 0.27958352662233904
Recall for class 0: 0.3586532485221968
Precision for class 1: 0.7833604501572956
Recall for class 1: 0.7637820381908806
Precision for class 2: 0.7550790906362973
Recall for class 2: 0.7524802526746287
3
              precision    recall  f1-score   support

           0       0.28      0.36      0.31     75619
           1       0.78      0.76      0.77    758977
           2       0.76      0.75      0.75    702247

    accuracy                           0.74   1536843
   macro avg       0.61      0.62      0.61   1536843
weighted avg       0.75      0.74      0.74   1536843

Done
