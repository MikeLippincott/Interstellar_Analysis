[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '99e61c0f'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '9624f552'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '391ebbc1'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c9fdb532'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Thapsigargin_10.000_DMSO_0.025']
The dimensions of the data are: (26978, 1276)
Number of total missing values across all columns: 53956
Data Subset Is Off
Wells held out for testing: ['E14' 'K14']
Wells to use for training, validation, and testing ['D14' 'D15' 'E15' 'L14' 'K15' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.626026).  Saving model ...
	 Train_Loss: 0.6754 Train_Acc: 55.808 Val_Loss: 0.6260  BEST VAL Loss: 0.6260  Val_Acc: 64.335

Epoch 1: Validation loss decreased (0.626026 --> 0.617908).  Saving model ...
	 Train_Loss: 0.6528 Train_Acc: 61.236 Val_Loss: 0.6179  BEST VAL Loss: 0.6179  Val_Acc: 68.601

Epoch 2: Validation loss decreased (0.617908 --> 0.608549).  Saving model ...
	 Train_Loss: 0.6362 Train_Acc: 63.983 Val_Loss: 0.6085  BEST VAL Loss: 0.6085  Val_Acc: 70.585

Epoch 3: Validation loss decreased (0.608549 --> 0.595800).  Saving model ...
	 Train_Loss: 0.6229 Train_Acc: 65.856 Val_Loss: 0.5958  BEST VAL Loss: 0.5958  Val_Acc: 71.875

Epoch 4: Validation loss decreased (0.595800 --> 0.589469).  Saving model ...
	 Train_Loss: 0.6116 Train_Acc: 67.208 Val_Loss: 0.5895  BEST VAL Loss: 0.5895  Val_Acc: 72.321

Epoch 5: Validation loss decreased (0.589469 --> 0.583092).  Saving model ...
	 Train_Loss: 0.6031 Train_Acc: 67.140 Val_Loss: 0.5831  BEST VAL Loss: 0.5831  Val_Acc: 73.661

Epoch 6: Validation loss decreased (0.583092 --> 0.580217).  Saving model ...
	 Train_Loss: 0.5945 Train_Acc: 67.506 Val_Loss: 0.5802  BEST VAL Loss: 0.5802  Val_Acc: 73.909

Epoch 7: Validation loss decreased (0.580217 --> 0.576374).  Saving model ...
	 Train_Loss: 0.5880 Train_Acc: 67.599 Val_Loss: 0.5764  BEST VAL Loss: 0.5764  Val_Acc: 72.768

Epoch 8: Validation loss decreased (0.576374 --> 0.572775).  Saving model ...
	 Train_Loss: 0.5822 Train_Acc: 68.430 Val_Loss: 0.5728  BEST VAL Loss: 0.5728  Val_Acc: 74.851

Epoch 9: Validation loss decreased (0.572775 --> 0.567855).  Saving model ...
	 Train_Loss: 0.5764 Train_Acc: 68.436 Val_Loss: 0.5679  BEST VAL Loss: 0.5679  Val_Acc: 72.520

Epoch 10: Validation loss decreased (0.567855 --> 0.564600).  Saving model ...
	 Train_Loss: 0.5715 Train_Acc: 69.367 Val_Loss: 0.5646  BEST VAL Loss: 0.5646  Val_Acc: 73.562

Epoch 11: Validation loss decreased (0.564600 --> 0.561301).  Saving model ...
	 Train_Loss: 0.5672 Train_Acc: 69.013 Val_Loss: 0.5613  BEST VAL Loss: 0.5613  Val_Acc: 73.462

Epoch 12: Validation loss decreased (0.561301 --> 0.559091).  Saving model ...
	 Train_Loss: 0.5628 Train_Acc: 69.559 Val_Loss: 0.5591  BEST VAL Loss: 0.5591  Val_Acc: 73.909

Epoch 13: Validation loss decreased (0.559091 --> 0.554881).  Saving model ...
	 Train_Loss: 0.5587 Train_Acc: 69.913 Val_Loss: 0.5549  BEST VAL Loss: 0.5549  Val_Acc: 74.107

Epoch 14: Validation loss decreased (0.554881 --> 0.553380).  Saving model ...
	 Train_Loss: 0.5550 Train_Acc: 70.489 Val_Loss: 0.5534  BEST VAL Loss: 0.5534  Val_Acc: 73.909

Epoch 15: Validation loss decreased (0.553380 --> 0.550789).  Saving model ...
	 Train_Loss: 0.5515 Train_Acc: 70.099 Val_Loss: 0.5508  BEST VAL Loss: 0.5508  Val_Acc: 73.958

Epoch 16: Validation loss decreased (0.550789 --> 0.547300).  Saving model ...
	 Train_Loss: 0.5486 Train_Acc: 70.372 Val_Loss: 0.5473  BEST VAL Loss: 0.5473  Val_Acc: 73.363

Epoch 17: Validation loss decreased (0.547300 --> 0.545408).  Saving model ...
	 Train_Loss: 0.5458 Train_Acc: 70.489 Val_Loss: 0.5454  BEST VAL Loss: 0.5454  Val_Acc: 74.950

Epoch 18: Validation loss decreased (0.545408 --> 0.542834).  Saving model ...
	 Train_Loss: 0.5429 Train_Acc: 70.638 Val_Loss: 0.5428  BEST VAL Loss: 0.5428  Val_Acc: 74.653

Epoch 19: Validation loss decreased (0.542834 --> 0.540415).  Saving model ...
	 Train_Loss: 0.5403 Train_Acc: 70.427 Val_Loss: 0.5404  BEST VAL Loss: 0.5404  Val_Acc: 74.702

Epoch 20: Validation loss decreased (0.540415 --> 0.538836).  Saving model ...
	 Train_Loss: 0.5375 Train_Acc: 71.345 Val_Loss: 0.5388  BEST VAL Loss: 0.5388  Val_Acc: 74.504

Epoch 21: Validation loss decreased (0.538836 --> 0.537550).  Saving model ...
	 Train_Loss: 0.5350 Train_Acc: 71.190 Val_Loss: 0.5375  BEST VAL Loss: 0.5375  Val_Acc: 74.851

Epoch 22: Validation loss decreased (0.537550 --> 0.535659).  Saving model ...
	 Train_Loss: 0.5327 Train_Acc: 71.159 Val_Loss: 0.5357  BEST VAL Loss: 0.5357  Val_Acc: 75.050

Epoch 23: Validation loss decreased (0.535659 --> 0.533678).  Saving model ...
	 Train_Loss: 0.5304 Train_Acc: 71.296 Val_Loss: 0.5337  BEST VAL Loss: 0.5337  Val_Acc: 74.653

Epoch 24: Validation loss decreased (0.533678 --> 0.532473).  Saving model ...
	 Train_Loss: 0.5284 Train_Acc: 71.010 Val_Loss: 0.5325  BEST VAL Loss: 0.5325  Val_Acc: 74.901

Epoch 25: Validation loss decreased (0.532473 --> 0.530788).  Saving model ...
	 Train_Loss: 0.5266 Train_Acc: 71.370 Val_Loss: 0.5308  BEST VAL Loss: 0.5308  Val_Acc: 75.595

Epoch 26: Validation loss decreased (0.530788 --> 0.528455).  Saving model ...
	 Train_Loss: 0.5248 Train_Acc: 71.141 Val_Loss: 0.5285  BEST VAL Loss: 0.5285  Val_Acc: 75.000

Epoch 27: Validation loss decreased (0.528455 --> 0.526819).  Saving model ...
	 Train_Loss: 0.5232 Train_Acc: 71.277 Val_Loss: 0.5268  BEST VAL Loss: 0.5268  Val_Acc: 75.496

Epoch 28: Validation loss decreased (0.526819 --> 0.525974).  Saving model ...
	 Train_Loss: 0.5215 Train_Acc: 71.550 Val_Loss: 0.5260  BEST VAL Loss: 0.5260  Val_Acc: 74.157

Epoch 29: Validation loss decreased (0.525974 --> 0.524843).  Saving model ...
	 Train_Loss: 0.5198 Train_Acc: 71.997 Val_Loss: 0.5248  BEST VAL Loss: 0.5248  Val_Acc: 74.157

Epoch 30: Validation loss decreased (0.524843 --> 0.524367).  Saving model ...
	 Train_Loss: 0.5180 Train_Acc: 72.276 Val_Loss: 0.5244  BEST VAL Loss: 0.5244  Val_Acc: 75.149

Epoch 31: Validation loss decreased (0.524367 --> 0.523687).  Saving model ...
	 Train_Loss: 0.5163 Train_Acc: 71.730 Val_Loss: 0.5237  BEST VAL Loss: 0.5237  Val_Acc: 74.454

Epoch 32: Validation loss decreased (0.523687 --> 0.523499).  Saving model ...
	 Train_Loss: 0.5149 Train_Acc: 71.972 Val_Loss: 0.5235  BEST VAL Loss: 0.5235  Val_Acc: 74.157

Epoch 33: Validation loss decreased (0.523499 --> 0.522648).  Saving model ...
	 Train_Loss: 0.5135 Train_Acc: 72.840 Val_Loss: 0.5226  BEST VAL Loss: 0.5226  Val_Acc: 73.810

Epoch 34: Validation loss decreased (0.522648 --> 0.522245).  Saving model ...
	 Train_Loss: 0.5120 Train_Acc: 72.319 Val_Loss: 0.5222  BEST VAL Loss: 0.5222  Val_Acc: 74.355

Epoch 35: Validation loss decreased (0.522245 --> 0.521679).  Saving model ...
	 Train_Loss: 0.5108 Train_Acc: 71.395 Val_Loss: 0.5217  BEST VAL Loss: 0.5217  Val_Acc: 73.710

Epoch 36: Validation loss decreased (0.521679 --> 0.521279).  Saving model ...
	 Train_Loss: 0.5093 Train_Acc: 72.598 Val_Loss: 0.5213  BEST VAL Loss: 0.5213  Val_Acc: 73.909

Epoch 37: Validation loss decreased (0.521279 --> 0.520997).  Saving model ...
	 Train_Loss: 0.5082 Train_Acc: 71.432 Val_Loss: 0.5210  BEST VAL Loss: 0.5210  Val_Acc: 74.355

Epoch 38: Validation loss decreased (0.520997 --> 0.520565).  Saving model ...
	 Train_Loss: 0.5069 Train_Acc: 72.660 Val_Loss: 0.5206  BEST VAL Loss: 0.5206  Val_Acc: 73.958

Epoch 39: Validation loss decreased (0.520565 --> 0.520336).  Saving model ...
	 Train_Loss: 0.5056 Train_Acc: 72.276 Val_Loss: 0.5203  BEST VAL Loss: 0.5203  Val_Acc: 73.611

Epoch 40: Validation loss decreased (0.520336 --> 0.519639).  Saving model ...
	 Train_Loss: 0.5043 Train_Acc: 72.735 Val_Loss: 0.5196  BEST VAL Loss: 0.5196  Val_Acc: 73.413

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.5031 Train_Acc: 72.958 Val_Loss: 0.5198  BEST VAL Loss: 0.5196  Val_Acc: 74.008

Epoch 42: Validation loss decreased (0.519639 --> 0.519567).  Saving model ...
	 Train_Loss: 0.5018 Train_Acc: 72.790 Val_Loss: 0.5196  BEST VAL Loss: 0.5196  Val_Acc: 73.958

Epoch 43: Validation loss decreased (0.519567 --> 0.519358).  Saving model ...
	 Train_Loss: 0.5007 Train_Acc: 72.877 Val_Loss: 0.5194  BEST VAL Loss: 0.5194  Val_Acc: 73.810

Epoch 44: Validation loss decreased (0.519358 --> 0.518641).  Saving model ...
	 Train_Loss: 0.4996 Train_Acc: 72.728 Val_Loss: 0.5186  BEST VAL Loss: 0.5186  Val_Acc: 74.454

Epoch 45: Validation loss decreased (0.518641 --> 0.517695).  Saving model ...
	 Train_Loss: 0.4984 Train_Acc: 73.076 Val_Loss: 0.5177  BEST VAL Loss: 0.5177  Val_Acc: 74.107

Epoch 46: Validation loss decreased (0.517695 --> 0.516729).  Saving model ...
	 Train_Loss: 0.4972 Train_Acc: 73.107 Val_Loss: 0.5167  BEST VAL Loss: 0.5167  Val_Acc: 74.206

Epoch 47: Validation loss decreased (0.516729 --> 0.516326).  Saving model ...
	 Train_Loss: 0.4962 Train_Acc: 73.119 Val_Loss: 0.5163  BEST VAL Loss: 0.5163  Val_Acc: 74.355

Epoch 48: Validation loss decreased (0.516326 --> 0.516203).  Saving model ...
	 Train_Loss: 0.4952 Train_Acc: 72.375 Val_Loss: 0.5162  BEST VAL Loss: 0.5162  Val_Acc: 74.206

Epoch 49: Validation loss decreased (0.516203 --> 0.515625).  Saving model ...
	 Train_Loss: 0.4942 Train_Acc: 72.127 Val_Loss: 0.5156  BEST VAL Loss: 0.5156  Val_Acc: 75.893

Epoch 50: Validation loss decreased (0.515625 --> 0.515116).  Saving model ...
	 Train_Loss: 0.4931 Train_Acc: 72.586 Val_Loss: 0.5151  BEST VAL Loss: 0.5151  Val_Acc: 74.504

Epoch 51: Validation loss decreased (0.515116 --> 0.514707).  Saving model ...
	 Train_Loss: 0.4922 Train_Acc: 72.859 Val_Loss: 0.5147  BEST VAL Loss: 0.5147  Val_Acc: 75.099

Epoch 52: Validation loss decreased (0.514707 --> 0.514399).  Saving model ...
	 Train_Loss: 0.4912 Train_Acc: 73.045 Val_Loss: 0.5144  BEST VAL Loss: 0.5144  Val_Acc: 75.149

Epoch 53: Validation loss decreased (0.514399 --> 0.513645).  Saving model ...
	 Train_Loss: 0.4903 Train_Acc: 73.063 Val_Loss: 0.5136  BEST VAL Loss: 0.5136  Val_Acc: 74.405

Epoch 54: Validation loss decreased (0.513645 --> 0.513069).  Saving model ...
	 Train_Loss: 0.4894 Train_Acc: 73.305 Val_Loss: 0.5131  BEST VAL Loss: 0.5131  Val_Acc: 74.306

Epoch 55: Validation loss decreased (0.513069 --> 0.512711).  Saving model ...
	 Train_Loss: 0.4885 Train_Acc: 73.008 Val_Loss: 0.5127  BEST VAL Loss: 0.5127  Val_Acc: 74.603

Epoch 56: Validation loss decreased (0.512711 --> 0.512465).  Saving model ...
	 Train_Loss: 0.4878 Train_Acc: 72.412 Val_Loss: 0.5125  BEST VAL Loss: 0.5125  Val_Acc: 73.958

Epoch 57: Validation loss decreased (0.512465 --> 0.512263).  Saving model ...
	 Train_Loss: 0.4869 Train_Acc: 73.001 Val_Loss: 0.5123  BEST VAL Loss: 0.5123  Val_Acc: 74.306

Epoch 58: Validation loss decreased (0.512263 --> 0.512085).  Saving model ...
	 Train_Loss: 0.4860 Train_Acc: 73.466 Val_Loss: 0.5121  BEST VAL Loss: 0.5121  Val_Acc: 74.603

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.4851 Train_Acc: 73.063 Val_Loss: 0.5122  BEST VAL Loss: 0.5121  Val_Acc: 75.298

Epoch 60: Validation loss decreased (0.512085 --> 0.512036).  Saving model ...
	 Train_Loss: 0.4844 Train_Acc: 73.212 Val_Loss: 0.5120  BEST VAL Loss: 0.5120  Val_Acc: 74.752

Epoch 61: Validation loss decreased (0.512036 --> 0.511732).  Saving model ...
	 Train_Loss: 0.4836 Train_Acc: 73.733 Val_Loss: 0.5117  BEST VAL Loss: 0.5117  Val_Acc: 74.554

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.4829 Train_Acc: 73.008 Val_Loss: 0.5119  BEST VAL Loss: 0.5117  Val_Acc: 74.157

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.4822 Train_Acc: 73.504 Val_Loss: 0.5119  BEST VAL Loss: 0.5117  Val_Acc: 74.008

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.4815 Train_Acc: 73.566 Val_Loss: 0.5118  BEST VAL Loss: 0.5117  Val_Acc: 74.554

Epoch 65: Validation loss decreased (0.511732 --> 0.511579).  Saving model ...
	 Train_Loss: 0.4807 Train_Acc: 73.274 Val_Loss: 0.5116  BEST VAL Loss: 0.5116  Val_Acc: 74.901

Epoch 66: Validation loss decreased (0.511579 --> 0.511427).  Saving model ...
	 Train_Loss: 0.4800 Train_Acc: 73.708 Val_Loss: 0.5114  BEST VAL Loss: 0.5114  Val_Acc: 74.256

Epoch 67: Validation loss decreased (0.511427 --> 0.511238).  Saving model ...
	 Train_Loss: 0.4792 Train_Acc: 73.770 Val_Loss: 0.5112  BEST VAL Loss: 0.5112  Val_Acc: 74.752

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.4784 Train_Acc: 74.112 Val_Loss: 0.5115  BEST VAL Loss: 0.5112  Val_Acc: 73.562

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.4778 Train_Acc: 73.113 Val_Loss: 0.5119  BEST VAL Loss: 0.5112  Val_Acc: 73.363

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.4771 Train_Acc: 73.665 Val_Loss: 0.5120  BEST VAL Loss: 0.5112  Val_Acc: 74.157

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.4765 Train_Acc: 73.280 Val_Loss: 0.5118  BEST VAL Loss: 0.5112  Val_Acc: 74.504

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.4757 Train_Acc: 73.888 Val_Loss: 0.5120  BEST VAL Loss: 0.5112  Val_Acc: 73.958

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.4750 Train_Acc: 74.453 Val_Loss: 0.5120  BEST VAL Loss: 0.5112  Val_Acc: 73.909

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.4743 Train_Acc: 73.857 Val_Loss: 0.5123  BEST VAL Loss: 0.5112  Val_Acc: 74.206

Epoch 75: Validation loss did not decrease
	 Train_Loss: 0.4737 Train_Acc: 73.752 Val_Loss: 0.5123  BEST VAL Loss: 0.5112  Val_Acc: 74.653

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.4730 Train_Acc: 74.006 Val_Loss: 0.5126  BEST VAL Loss: 0.5112  Val_Acc: 73.859

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.4725 Train_Acc: 73.820 Val_Loss: 0.5126  BEST VAL Loss: 0.5112  Val_Acc: 73.810

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.4719 Train_Acc: 73.398 Val_Loss: 0.5128  BEST VAL Loss: 0.5112  Val_Acc: 74.405

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.4713 Train_Acc: 73.777 Val_Loss: 0.5131  BEST VAL Loss: 0.5112  Val_Acc: 74.504

Epoch 80: Validation loss did not decrease
	 Train_Loss: 0.4708 Train_Acc: 73.758 Val_Loss: 0.5134  BEST VAL Loss: 0.5112  Val_Acc: 73.909

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.4702 Train_Acc: 73.820 Val_Loss: 0.5137  BEST VAL Loss: 0.5112  Val_Acc: 74.008

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.4696 Train_Acc: 73.832 Val_Loss: 0.5138  BEST VAL Loss: 0.5112  Val_Acc: 74.554

Epoch 83: Validation loss did not decrease
Early stopped at epoch : 83
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.83      0.93      0.88      8273
           1       0.92      0.80      0.86      7850

    accuracy                           0.87     16123
   macro avg       0.88      0.87      0.87     16123
weighted avg       0.87      0.87      0.87     16123

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.72      0.84      0.77      1034
           1       0.79      0.65      0.71       982

    accuracy                           0.75      2016
   macro avg       0.76      0.75      0.74      2016
weighted avg       0.75      0.75      0.74      2016

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.74      0.84      0.79      1034
           1       0.80      0.69      0.74       982

    accuracy                           0.77      2016
   macro avg       0.77      0.76      0.76      2016
weighted avg       0.77      0.77      0.76      2016

              precision    recall  f1-score   support

           0       0.74      0.84      0.79      1034
           1       0.80      0.69      0.74       982

    accuracy                           0.77      2016
   macro avg       0.77      0.76      0.76      2016
weighted avg       0.77      0.77      0.76      2016

Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.72      0.80      0.76      3425
           1       0.77      0.69      0.73      3398

    accuracy                           0.75      6823
   macro avg       0.75      0.75      0.75      6823
weighted avg       0.75      0.75      0.75      6823

              precision    recall  f1-score   support

           0       0.72      0.80      0.76      3425
           1       0.77      0.69      0.73      3398

    accuracy                           0.75      6823
   macro avg       0.75      0.75      0.75      6823
weighted avg       0.75      0.75      0.75      6823

completed
