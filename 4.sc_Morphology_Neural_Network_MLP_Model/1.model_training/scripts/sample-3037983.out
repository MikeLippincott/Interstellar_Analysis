[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'e598a171'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '93439d5a'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '89529b01'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e785b92d'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (332997, 1270)
Number of total missing values across all columns: 333968
Data Subset Is Off
Wells held out for testing: ['K09' 'L06']
Wells to use for training, validation, and testing ['E06' 'E07' 'K02' 'K03' 'K08' 'L07']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.167242).  Saving model ...
	 Train_Loss: 0.3472 Train_Acc: 85.627 Val_Loss: 0.1672  BEST VAL Loss: 0.1672  Val_Acc: 93.618

Epoch 1: Validation loss decreased (0.167242 --> 0.150923).  Saving model ...
	 Train_Loss: 0.2747 Train_Acc: 92.087 Val_Loss: 0.1509  BEST VAL Loss: 0.1509  Val_Acc: 95.016

Epoch 2: Validation loss decreased (0.150923 --> 0.142027).  Saving model ...
	 Train_Loss: 0.2442 Train_Acc: 92.987 Val_Loss: 0.1420  BEST VAL Loss: 0.1420  Val_Acc: 95.459

Epoch 3: Validation loss decreased (0.142027 --> 0.136741).  Saving model ...
	 Train_Loss: 0.2253 Train_Acc: 93.587 Val_Loss: 0.1367  BEST VAL Loss: 0.1367  Val_Acc: 95.552

Epoch 4: Validation loss decreased (0.136741 --> 0.131892).  Saving model ...
	 Train_Loss: 0.2125 Train_Acc: 93.920 Val_Loss: 0.1319  BEST VAL Loss: 0.1319  Val_Acc: 95.834

Epoch 5: Validation loss decreased (0.131892 --> 0.127808).  Saving model ...
	 Train_Loss: 0.2030 Train_Acc: 94.193 Val_Loss: 0.1278  BEST VAL Loss: 0.1278  Val_Acc: 96.056

Epoch 6: Validation loss decreased (0.127808 --> 0.125161).  Saving model ...
	 Train_Loss: 0.1959 Train_Acc: 94.266 Val_Loss: 0.1252  BEST VAL Loss: 0.1252  Val_Acc: 96.007

Epoch 7: Validation loss decreased (0.125161 --> 0.123087).  Saving model ...
	 Train_Loss: 0.1900 Train_Acc: 94.419 Val_Loss: 0.1231  BEST VAL Loss: 0.1231  Val_Acc: 96.173

Epoch 8: Validation loss decreased (0.123087 --> 0.121300).  Saving model ...
	 Train_Loss: 0.1851 Train_Acc: 94.573 Val_Loss: 0.1213  BEST VAL Loss: 0.1213  Val_Acc: 96.152

Epoch 9: Validation loss decreased (0.121300 --> 0.120131).  Saving model ...
	 Train_Loss: 0.1811 Train_Acc: 94.643 Val_Loss: 0.1201  BEST VAL Loss: 0.1201  Val_Acc: 96.277

Epoch 10: Validation loss decreased (0.120131 --> 0.118030).  Saving model ...
	 Train_Loss: 0.1774 Train_Acc: 94.695 Val_Loss: 0.1180  BEST VAL Loss: 0.1180  Val_Acc: 96.644

Epoch 11: Validation loss decreased (0.118030 --> 0.117119).  Saving model ...
	 Train_Loss: 0.1746 Train_Acc: 94.698 Val_Loss: 0.1171  BEST VAL Loss: 0.1171  Val_Acc: 96.169

Epoch 12: Validation loss did not decrease
	 Train_Loss: 0.1718 Train_Acc: 94.877 Val_Loss: 0.1174  BEST VAL Loss: 0.1171  Val_Acc: 96.221

Epoch 13: Validation loss decreased (0.117119 --> 0.115930).  Saving model ...
	 Train_Loss: 0.1694 Train_Acc: 94.949 Val_Loss: 0.1159  BEST VAL Loss: 0.1159  Val_Acc: 96.632

Epoch 14: Validation loss decreased (0.115930 --> 0.114978).  Saving model ...
	 Train_Loss: 0.1674 Train_Acc: 94.873 Val_Loss: 0.1150  BEST VAL Loss: 0.1150  Val_Acc: 96.249

Epoch 15: Validation loss decreased (0.114978 --> 0.113761).  Saving model ...
	 Train_Loss: 0.1653 Train_Acc: 95.066 Val_Loss: 0.1138  BEST VAL Loss: 0.1138  Val_Acc: 96.535

Epoch 16: Validation loss decreased (0.113761 --> 0.112654).  Saving model ...
	 Train_Loss: 0.1634 Train_Acc: 95.132 Val_Loss: 0.1127  BEST VAL Loss: 0.1127  Val_Acc: 96.640

Epoch 17: Validation loss decreased (0.112654 --> 0.112233).  Saving model ...
	 Train_Loss: 0.1618 Train_Acc: 95.008 Val_Loss: 0.1122  BEST VAL Loss: 0.1122  Val_Acc: 96.438

Epoch 18: Validation loss decreased (0.112233 --> 0.111912).  Saving model ...
	 Train_Loss: 0.1604 Train_Acc: 95.084 Val_Loss: 0.1119  BEST VAL Loss: 0.1119  Val_Acc: 96.354

Epoch 19: Validation loss decreased (0.111912 --> 0.111207).  Saving model ...
	 Train_Loss: 0.1590 Train_Acc: 95.198 Val_Loss: 0.1112  BEST VAL Loss: 0.1112  Val_Acc: 96.608

Epoch 20: Validation loss decreased (0.111207 --> 0.110596).  Saving model ...
	 Train_Loss: 0.1577 Train_Acc: 95.118 Val_Loss: 0.1106  BEST VAL Loss: 0.1106  Val_Acc: 96.624

Epoch 21: Validation loss decreased (0.110596 --> 0.109838).  Saving model ...
	 Train_Loss: 0.1567 Train_Acc: 95.098 Val_Loss: 0.1098  BEST VAL Loss: 0.1098  Val_Acc: 96.592

Epoch 22: Validation loss decreased (0.109838 --> 0.109460).  Saving model ...
	 Train_Loss: 0.1555 Train_Acc: 95.225 Val_Loss: 0.1095  BEST VAL Loss: 0.1095  Val_Acc: 96.563

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1545 Train_Acc: 95.263 Val_Loss: 0.1096  BEST VAL Loss: 0.1095  Val_Acc: 96.467

Epoch 24: Validation loss decreased (0.109460 --> 0.109247).  Saving model ...
	 Train_Loss: 0.1536 Train_Acc: 95.172 Val_Loss: 0.1092  BEST VAL Loss: 0.1092  Val_Acc: 96.418

Epoch 25: Validation loss decreased (0.109247 --> 0.108822).  Saving model ...
	 Train_Loss: 0.1526 Train_Acc: 95.337 Val_Loss: 0.1088  BEST VAL Loss: 0.1088  Val_Acc: 96.676

Epoch 26: Validation loss decreased (0.108822 --> 0.108175).  Saving model ...
	 Train_Loss: 0.1517 Train_Acc: 95.369 Val_Loss: 0.1082  BEST VAL Loss: 0.1082  Val_Acc: 96.906

Epoch 27: Validation loss decreased (0.108175 --> 0.107481).  Saving model ...
	 Train_Loss: 0.1509 Train_Acc: 95.250 Val_Loss: 0.1075  BEST VAL Loss: 0.1075  Val_Acc: 96.958

Epoch 28: Validation loss decreased (0.107481 --> 0.107460).  Saving model ...
	 Train_Loss: 0.1500 Train_Acc: 95.362 Val_Loss: 0.1075  BEST VAL Loss: 0.1075  Val_Acc: 96.765

Epoch 29: Validation loss decreased (0.107460 --> 0.106878).  Saving model ...
	 Train_Loss: 0.1493 Train_Acc: 95.294 Val_Loss: 0.1069  BEST VAL Loss: 0.1069  Val_Acc: 96.866

Epoch 30: Validation loss decreased (0.106878 --> 0.106366).  Saving model ...
	 Train_Loss: 0.1486 Train_Acc: 95.349 Val_Loss: 0.1064  BEST VAL Loss: 0.1064  Val_Acc: 96.753

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1481 Train_Acc: 95.234 Val_Loss: 0.1065  BEST VAL Loss: 0.1064  Val_Acc: 96.688

Epoch 32: Validation loss decreased (0.106366 --> 0.106318).  Saving model ...
	 Train_Loss: 0.1474 Train_Acc: 95.540 Val_Loss: 0.1063  BEST VAL Loss: 0.1063  Val_Acc: 96.817

Epoch 33: Validation loss decreased (0.106318 --> 0.106146).  Saving model ...
	 Train_Loss: 0.1468 Train_Acc: 95.378 Val_Loss: 0.1061  BEST VAL Loss: 0.1061  Val_Acc: 96.906

Epoch 34: Validation loss decreased (0.106146 --> 0.105835).  Saving model ...
	 Train_Loss: 0.1461 Train_Acc: 95.557 Val_Loss: 0.1058  BEST VAL Loss: 0.1058  Val_Acc: 96.990

Epoch 35: Validation loss decreased (0.105835 --> 0.105710).  Saving model ...
	 Train_Loss: 0.1455 Train_Acc: 95.516 Val_Loss: 0.1057  BEST VAL Loss: 0.1057  Val_Acc: 96.769

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1449 Train_Acc: 95.492 Val_Loss: 0.1058  BEST VAL Loss: 0.1057  Val_Acc: 96.934

Epoch 37: Validation loss decreased (0.105710 --> 0.105621).  Saving model ...
	 Train_Loss: 0.1444 Train_Acc: 95.422 Val_Loss: 0.1056  BEST VAL Loss: 0.1056  Val_Acc: 96.866

Epoch 38: Validation loss decreased (0.105621 --> 0.105278).  Saving model ...
	 Train_Loss: 0.1437 Train_Acc: 95.707 Val_Loss: 0.1053  BEST VAL Loss: 0.1053  Val_Acc: 96.958

Epoch 39: Validation loss decreased (0.105278 --> 0.104947).  Saving model ...
	 Train_Loss: 0.1431 Train_Acc: 95.703 Val_Loss: 0.1049  BEST VAL Loss: 0.1049  Val_Acc: 97.111

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1426 Train_Acc: 95.560 Val_Loss: 0.1051  BEST VAL Loss: 0.1049  Val_Acc: 96.922

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1422 Train_Acc: 95.493 Val_Loss: 0.1050  BEST VAL Loss: 0.1049  Val_Acc: 96.994

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1417 Train_Acc: 95.608 Val_Loss: 0.1050  BEST VAL Loss: 0.1049  Val_Acc: 96.886

Epoch 43: Validation loss decreased (0.104947 --> 0.104915).  Saving model ...
	 Train_Loss: 0.1412 Train_Acc: 95.664 Val_Loss: 0.1049  BEST VAL Loss: 0.1049  Val_Acc: 96.322

Epoch 44: Validation loss decreased (0.104915 --> 0.104827).  Saving model ...
	 Train_Loss: 0.1408 Train_Acc: 95.515 Val_Loss: 0.1048  BEST VAL Loss: 0.1048  Val_Acc: 96.870

Epoch 45: Validation loss decreased (0.104827 --> 0.104747).  Saving model ...
	 Train_Loss: 0.1403 Train_Acc: 95.665 Val_Loss: 0.1047  BEST VAL Loss: 0.1047  Val_Acc: 96.672

Epoch 46: Validation loss decreased (0.104747 --> 0.104657).  Saving model ...
	 Train_Loss: 0.1398 Train_Acc: 95.723 Val_Loss: 0.1047  BEST VAL Loss: 0.1047  Val_Acc: 97.216

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1394 Train_Acc: 95.613 Val_Loss: 0.1049  BEST VAL Loss: 0.1047  Val_Acc: 96.958

Epoch 48: Validation loss decreased (0.104657 --> 0.104647).  Saving model ...
	 Train_Loss: 0.1391 Train_Acc: 95.613 Val_Loss: 0.1046  BEST VAL Loss: 0.1046  Val_Acc: 96.910

Epoch 49: Validation loss decreased (0.104647 --> 0.104497).  Saving model ...
	 Train_Loss: 0.1388 Train_Acc: 95.613 Val_Loss: 0.1045  BEST VAL Loss: 0.1045  Val_Acc: 96.729

Epoch 50: Validation loss decreased (0.104497 --> 0.104294).  Saving model ...
	 Train_Loss: 0.1384 Train_Acc: 95.668 Val_Loss: 0.1043  BEST VAL Loss: 0.1043  Val_Acc: 96.882

Epoch 51: Validation loss decreased (0.104294 --> 0.103975).  Saving model ...
	 Train_Loss: 0.1380 Train_Acc: 95.803 Val_Loss: 0.1040  BEST VAL Loss: 0.1040  Val_Acc: 97.043

Epoch 52: Validation loss decreased (0.103975 --> 0.103828).  Saving model ...
	 Train_Loss: 0.1376 Train_Acc: 95.615 Val_Loss: 0.1038  BEST VAL Loss: 0.1038  Val_Acc: 97.095

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1373 Train_Acc: 95.679 Val_Loss: 0.1039  BEST VAL Loss: 0.1038  Val_Acc: 96.692

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1374 Train_Acc: 95.547 Val_Loss: 0.1039  BEST VAL Loss: 0.1038  Val_Acc: 96.302

Epoch 55: Validation loss decreased (0.103828 --> 0.103768).  Saving model ...
	 Train_Loss: 0.1371 Train_Acc: 95.557 Val_Loss: 0.1038  BEST VAL Loss: 0.1038  Val_Acc: 96.994

Epoch 56: Validation loss decreased (0.103768 --> 0.103661).  Saving model ...
	 Train_Loss: 0.1368 Train_Acc: 95.788 Val_Loss: 0.1037  BEST VAL Loss: 0.1037  Val_Acc: 96.970

Epoch 57: Validation loss decreased (0.103661 --> 0.103562).  Saving model ...
	 Train_Loss: 0.1365 Train_Acc: 95.674 Val_Loss: 0.1036  BEST VAL Loss: 0.1036  Val_Acc: 96.914

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.1361 Train_Acc: 95.689 Val_Loss: 0.1036  BEST VAL Loss: 0.1036  Val_Acc: 97.055

Epoch 59: Validation loss decreased (0.103562 --> 0.103520).  Saving model ...
	 Train_Loss: 0.1358 Train_Acc: 95.867 Val_Loss: 0.1035  BEST VAL Loss: 0.1035  Val_Acc: 96.624

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.1355 Train_Acc: 95.624 Val_Loss: 0.1037  BEST VAL Loss: 0.1035  Val_Acc: 96.459

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.1353 Train_Acc: 95.650 Val_Loss: 0.1038  BEST VAL Loss: 0.1035  Val_Acc: 96.878

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.1351 Train_Acc: 95.597 Val_Loss: 0.1037  BEST VAL Loss: 0.1035  Val_Acc: 97.027

Epoch 63: Validation loss did not decrease
	 Train_Loss: 0.1348 Train_Acc: 95.822 Val_Loss: 0.1036  BEST VAL Loss: 0.1035  Val_Acc: 97.043

Epoch 64: Validation loss decreased (0.103520 --> 0.103481).  Saving model ...
	 Train_Loss: 0.1345 Train_Acc: 95.891 Val_Loss: 0.1035  BEST VAL Loss: 0.1035  Val_Acc: 97.043

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.1342 Train_Acc: 95.831 Val_Loss: 0.1037  BEST VAL Loss: 0.1035  Val_Acc: 97.043

Epoch 66: Validation loss did not decrease
	 Train_Loss: 0.1339 Train_Acc: 95.770 Val_Loss: 0.1037  BEST VAL Loss: 0.1035  Val_Acc: 96.809

Epoch 67: Validation loss decreased (0.103481 --> 0.103471).  Saving model ...
	 Train_Loss: 0.1337 Train_Acc: 95.708 Val_Loss: 0.1035  BEST VAL Loss: 0.1035  Val_Acc: 97.007

Epoch 68: Validation loss decreased (0.103471 --> 0.103295).  Saving model ...
	 Train_Loss: 0.1334 Train_Acc: 95.830 Val_Loss: 0.1033  BEST VAL Loss: 0.1033  Val_Acc: 97.140

Epoch 69: Validation loss decreased (0.103295 --> 0.103245).  Saving model ...
	 Train_Loss: 0.1331 Train_Acc: 95.877 Val_Loss: 0.1032  BEST VAL Loss: 0.1032  Val_Acc: 96.902

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.1329 Train_Acc: 95.846 Val_Loss: 0.1032  BEST VAL Loss: 0.1032  Val_Acc: 97.144

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.1326 Train_Acc: 95.872 Val_Loss: 0.1033  BEST VAL Loss: 0.1032  Val_Acc: 96.841

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.1325 Train_Acc: 95.656 Val_Loss: 0.1036  BEST VAL Loss: 0.1032  Val_Acc: 96.970

Epoch 73: Validation loss did not decrease
	 Train_Loss: 0.1325 Train_Acc: 95.430 Val_Loss: 0.1040  BEST VAL Loss: 0.1032  Val_Acc: 96.700

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.1323 Train_Acc: 95.773 Val_Loss: 0.1039  BEST VAL Loss: 0.1032  Val_Acc: 97.091

Epoch 75: Validation loss did not decrease
	 Train_Loss: 0.1320 Train_Acc: 95.890 Val_Loss: 0.1039  BEST VAL Loss: 0.1032  Val_Acc: 97.015

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.1318 Train_Acc: 95.948 Val_Loss: 0.1040  BEST VAL Loss: 0.1032  Val_Acc: 97.051

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.1317 Train_Acc: 95.715 Val_Loss: 0.1045  BEST VAL Loss: 0.1032  Val_Acc: 97.007

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.1318 Train_Acc: 95.495 Val_Loss: 0.1046  BEST VAL Loss: 0.1032  Val_Acc: 96.966

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.1316 Train_Acc: 95.708 Val_Loss: 0.1046  BEST VAL Loss: 0.1032  Val_Acc: 97.055

Epoch 80: Validation loss did not decrease
	 Train_Loss: 0.1314 Train_Acc: 95.966 Val_Loss: 0.1046  BEST VAL Loss: 0.1032  Val_Acc: 97.111

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.1312 Train_Acc: 95.984 Val_Loss: 0.1046  BEST VAL Loss: 0.1032  Val_Acc: 97.188

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.1310 Train_Acc: 95.955 Val_Loss: 0.1047  BEST VAL Loss: 0.1032  Val_Acc: 96.938

Epoch 83: Validation loss did not decrease
	 Train_Loss: 0.1307 Train_Acc: 95.916 Val_Loss: 0.1047  BEST VAL Loss: 0.1032  Val_Acc: 97.107

Epoch 84: Validation loss did not decrease
	 Train_Loss: 0.1305 Train_Acc: 96.006 Val_Loss: 0.1047  BEST VAL Loss: 0.1032  Val_Acc: 96.930

Epoch 85: Validation loss did not decrease
Early stopped at epoch : 85
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.50      0.50    100908
           1       0.49      0.50      0.50     97655

    accuracy                           0.50    198563
   macro avg       0.50      0.50      0.50    198563
weighted avg       0.50      0.50      0.50    198563

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.50      0.51     12614
           1       0.49      0.50      0.50     12207

    accuracy                           0.50     24821
   macro avg       0.50      0.50      0.50     24821
weighted avg       0.50      0.50      0.50     24821

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.50      0.50     12614
           1       0.49      0.49      0.49     12207

    accuracy                           0.50     24821
   macro avg       0.50      0.50      0.50     24821
weighted avg       0.50      0.50      0.50     24821

              precision    recall  f1-score   support

           0       0.51      0.50      0.50     12614
           1       0.49      0.49      0.49     12207

    accuracy                           0.50     24821
   macro avg       0.50      0.50      0.50     24821
weighted avg       0.50      0.50      0.50     24821

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.47      0.46      0.46     39877
           1       0.53      0.54      0.54     44915

    accuracy                           0.50     84792
   macro avg       0.50      0.50      0.50     84792
weighted avg       0.50      0.50      0.50     84792

              precision    recall  f1-score   support

           0       0.47      0.46      0.46     39877
           1       0.53      0.54      0.54     44915

    accuracy                           0.50     84792
   macro avg       0.50      0.50      0.50     84792
weighted avg       0.50      0.50      0.50     84792

completed
