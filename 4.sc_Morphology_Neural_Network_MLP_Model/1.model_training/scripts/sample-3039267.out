[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '4a5d1e48'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f740d2ce'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'ee4b70c2'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '02db3299'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Thapsigargin_10.000_DMSO_0.025']
The dimensions of the data are: (26978, 1276)
Number of total missing values across all columns: 53956
Data Subset Is Off
Wells held out for testing: ['E14' 'K14']
Wells to use for training, validation, and testing ['D14' 'D15' 'E15' 'L14' 'K15' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.615598).  Saving model ...
	 Train_Loss: 0.6683 Train_Acc: 51.312 Val_Loss: 0.6156  BEST VAL Loss: 0.6156  Val_Acc: 51.290

Epoch 1: Validation loss decreased (0.615598 --> 0.610199).  Saving model ...
	 Train_Loss: 0.6461 Train_Acc: 65.707 Val_Loss: 0.6102  BEST VAL Loss: 0.6102  Val_Acc: 69.891

Epoch 2: Validation loss decreased (0.610199 --> 0.584527).  Saving model ...
	 Train_Loss: 0.6309 Train_Acc: 68.740 Val_Loss: 0.5845  BEST VAL Loss: 0.5845  Val_Acc: 71.627

Epoch 3: Validation loss did not decrease
	 Train_Loss: 0.6182 Train_Acc: 70.043 Val_Loss: 0.5849  BEST VAL Loss: 0.5845  Val_Acc: 70.933

Epoch 4: Validation loss decreased (0.584527 --> 0.570520).  Saving model ...
	 Train_Loss: 0.6076 Train_Acc: 71.755 Val_Loss: 0.5705  BEST VAL Loss: 0.5705  Val_Acc: 72.470

Epoch 5: Validation loss decreased (0.570520 --> 0.568005).  Saving model ...
	 Train_Loss: 0.5988 Train_Acc: 72.090 Val_Loss: 0.5680  BEST VAL Loss: 0.5680  Val_Acc: 72.718

Epoch 6: Validation loss did not decrease
	 Train_Loss: 0.5908 Train_Acc: 72.995 Val_Loss: 0.5694  BEST VAL Loss: 0.5680  Val_Acc: 72.718

Epoch 7: Validation loss decreased (0.568005 --> 0.561358).  Saving model ...
	 Train_Loss: 0.5845 Train_Acc: 73.094 Val_Loss: 0.5614  BEST VAL Loss: 0.5614  Val_Acc: 73.958

Epoch 8: Validation loss decreased (0.561358 --> 0.555886).  Saving model ...
	 Train_Loss: 0.5779 Train_Acc: 74.136 Val_Loss: 0.5559  BEST VAL Loss: 0.5559  Val_Acc: 72.669

Epoch 9: Validation loss decreased (0.555886 --> 0.551155).  Saving model ...
	 Train_Loss: 0.5722 Train_Acc: 73.913 Val_Loss: 0.5512  BEST VAL Loss: 0.5512  Val_Acc: 74.157

Epoch 10: Validation loss decreased (0.551155 --> 0.549788).  Saving model ...
	 Train_Loss: 0.5675 Train_Acc: 74.521 Val_Loss: 0.5498  BEST VAL Loss: 0.5498  Val_Acc: 73.909

Epoch 11: Validation loss decreased (0.549788 --> 0.542680).  Saving model ...
	 Train_Loss: 0.5629 Train_Acc: 75.154 Val_Loss: 0.5427  BEST VAL Loss: 0.5427  Val_Acc: 74.901

Epoch 12: Validation loss decreased (0.542680 --> 0.542527).  Saving model ...
	 Train_Loss: 0.5581 Train_Acc: 75.656 Val_Loss: 0.5425  BEST VAL Loss: 0.5425  Val_Acc: 75.843

Epoch 13: Validation loss did not decrease
	 Train_Loss: 0.5538 Train_Acc: 75.972 Val_Loss: 0.5434  BEST VAL Loss: 0.5425  Val_Acc: 74.702

Epoch 14: Validation loss decreased (0.542527 --> 0.539747).  Saving model ...
	 Train_Loss: 0.5501 Train_Acc: 76.003 Val_Loss: 0.5397  BEST VAL Loss: 0.5397  Val_Acc: 75.446

Epoch 15: Validation loss decreased (0.539747 --> 0.533724).  Saving model ...
	 Train_Loss: 0.5469 Train_Acc: 76.003 Val_Loss: 0.5337  BEST VAL Loss: 0.5337  Val_Acc: 75.645

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.5441 Train_Acc: 76.109 Val_Loss: 0.5383  BEST VAL Loss: 0.5337  Val_Acc: 75.099

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.5415 Train_Acc: 76.090 Val_Loss: 0.5375  BEST VAL Loss: 0.5337  Val_Acc: 74.851

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.5388 Train_Acc: 76.028 Val_Loss: 0.5364  BEST VAL Loss: 0.5337  Val_Acc: 75.794

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.5365 Train_Acc: 76.630 Val_Loss: 0.5397  BEST VAL Loss: 0.5337  Val_Acc: 75.248

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.5341 Train_Acc: 76.878 Val_Loss: 0.5404  BEST VAL Loss: 0.5337  Val_Acc: 75.595

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.5317 Train_Acc: 77.095 Val_Loss: 0.5408  BEST VAL Loss: 0.5337  Val_Acc: 75.694

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.5294 Train_Acc: 77.213 Val_Loss: 0.5424  BEST VAL Loss: 0.5337  Val_Acc: 75.992

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.5276 Train_Acc: 77.417 Val_Loss: 0.5409  BEST VAL Loss: 0.5337  Val_Acc: 75.198

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.5254 Train_Acc: 77.765 Val_Loss: 0.5408  BEST VAL Loss: 0.5337  Val_Acc: 76.141

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.5235 Train_Acc: 77.523 Val_Loss: 0.5380  BEST VAL Loss: 0.5337  Val_Acc: 76.042

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.5219 Train_Acc: 77.659 Val_Loss: 0.5367  BEST VAL Loss: 0.5337  Val_Acc: 75.546

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.5201 Train_Acc: 77.864 Val_Loss: 0.5386  BEST VAL Loss: 0.5337  Val_Acc: 75.694

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.5187 Train_Acc: 77.591 Val_Loss: 0.5362  BEST VAL Loss: 0.5337  Val_Acc: 75.050

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.5173 Train_Acc: 77.665 Val_Loss: 0.5377  BEST VAL Loss: 0.5337  Val_Acc: 75.347

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.5161 Train_Acc: 77.380 Val_Loss: 0.5374  BEST VAL Loss: 0.5337  Val_Acc: 76.042

Epoch 31: Validation loss did not decrease
Early stopped at epoch : 31
Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.41      0.46      8273
           1       0.49      0.59      0.53      7850

    accuracy                           0.50     16123
   macro avg       0.50      0.50      0.50     16123
weighted avg       0.50      0.50      0.49     16123

Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.44      0.47      1034
           1       0.48      0.56      0.52       982

    accuracy                           0.50      2016
   macro avg       0.50      0.50      0.49      2016
weighted avg       0.50      0.50      0.49      2016

Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.50      0.40      0.45      1034
           1       0.48      0.58      0.53       982

    accuracy                           0.49      2016
   macro avg       0.49      0.49      0.49      2016
weighted avg       0.49      0.49      0.49      2016

              precision    recall  f1-score   support

           0       0.50      0.40      0.45      1034
           1       0.48      0.58      0.53       982

    accuracy                           0.49      2016
   macro avg       0.49      0.49      0.49      2016
weighted avg       0.49      0.49      0.49      2016

Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.41      0.45      3425
           1       0.50      0.61      0.55      3398

    accuracy                           0.51      6823
   macro avg       0.51      0.51      0.50      6823
weighted avg       0.51      0.51      0.50      6823

              precision    recall  f1-score   support

           0       0.51      0.41      0.45      3425
           1       0.50      0.61      0.55      3398

    accuracy                           0.51      6823
   macro avg       0.51      0.51      0.50      6823
weighted avg       0.51      0.51      0.50      6823

completed
