[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '89035038'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'b0d79269'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '494a2017'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c2d69dd1'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Flagellin_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_3.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_Nigericin_1.000_3.0_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (278808, 1270)
Number of total missing values across all columns: 245710
Data Subset Is Off
Wells held out for testing: ['L08' 'M10']
Wells to use for training, validation, and testing ['L02' 'L03' 'M05' 'L09' 'M11']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.074410).  Saving model ...
	 Train_Loss: 0.1566 Train_Acc: 94.680 Val_Loss: 0.0744  BEST VAL Loss: 0.0744  Val_Acc: 97.641

Epoch 1: Validation loss decreased (0.074410 --> 0.068416).  Saving model ...
	 Train_Loss: 0.1150 Train_Acc: 97.597 Val_Loss: 0.0684  BEST VAL Loss: 0.0684  Val_Acc: 98.193

Epoch 2: Validation loss decreased (0.068416 --> 0.062408).  Saving model ...
	 Train_Loss: 0.0964 Train_Acc: 98.037 Val_Loss: 0.0624  BEST VAL Loss: 0.0624  Val_Acc: 98.466

Epoch 3: Validation loss decreased (0.062408 --> 0.058549).  Saving model ...
	 Train_Loss: 0.0850 Train_Acc: 98.271 Val_Loss: 0.0585  BEST VAL Loss: 0.0585  Val_Acc: 98.517

Epoch 4: Validation loss decreased (0.058549 --> 0.055762).  Saving model ...
	 Train_Loss: 0.0772 Train_Acc: 98.420 Val_Loss: 0.0558  BEST VAL Loss: 0.0558  Val_Acc: 98.633

Epoch 5: Validation loss decreased (0.055762 --> 0.053409).  Saving model ...
	 Train_Loss: 0.0713 Train_Acc: 98.562 Val_Loss: 0.0534  BEST VAL Loss: 0.0534  Val_Acc: 98.729

Epoch 6: Validation loss decreased (0.053409 --> 0.051302).  Saving model ...
	 Train_Loss: 0.0665 Train_Acc: 98.686 Val_Loss: 0.0513  BEST VAL Loss: 0.0513  Val_Acc: 98.775

Epoch 7: Validation loss decreased (0.051302 --> 0.049520).  Saving model ...
	 Train_Loss: 0.0627 Train_Acc: 98.743 Val_Loss: 0.0495  BEST VAL Loss: 0.0495  Val_Acc: 98.897

Epoch 8: Validation loss decreased (0.049520 --> 0.048817).  Saving model ...
	 Train_Loss: 0.0595 Train_Acc: 98.803 Val_Loss: 0.0488  BEST VAL Loss: 0.0488  Val_Acc: 98.836

Epoch 9: Validation loss decreased (0.048817 --> 0.048776).  Saving model ...
	 Train_Loss: 0.0571 Train_Acc: 98.798 Val_Loss: 0.0488  BEST VAL Loss: 0.0488  Val_Acc: 98.350

Epoch 10: Validation loss decreased (0.048776 --> 0.048581).  Saving model ...
	 Train_Loss: 0.0555 Train_Acc: 98.636 Val_Loss: 0.0486  BEST VAL Loss: 0.0486  Val_Acc: 98.805

Epoch 11: Validation loss decreased (0.048581 --> 0.047944).  Saving model ...
	 Train_Loss: 0.0538 Train_Acc: 98.831 Val_Loss: 0.0479  BEST VAL Loss: 0.0479  Val_Acc: 98.942

Epoch 12: Validation loss decreased (0.047944 --> 0.047424).  Saving model ...
	 Train_Loss: 0.0522 Train_Acc: 98.863 Val_Loss: 0.0474  BEST VAL Loss: 0.0474  Val_Acc: 98.826

Epoch 13: Validation loss decreased (0.047424 --> 0.047083).  Saving model ...
	 Train_Loss: 0.0507 Train_Acc: 98.867 Val_Loss: 0.0471  BEST VAL Loss: 0.0471  Val_Acc: 98.912

Epoch 14: Validation loss decreased (0.047083 --> 0.046824).  Saving model ...
	 Train_Loss: 0.0491 Train_Acc: 99.046 Val_Loss: 0.0468  BEST VAL Loss: 0.0468  Val_Acc: 98.871

Epoch 15: Validation loss decreased (0.046824 --> 0.046454).  Saving model ...
	 Train_Loss: 0.0478 Train_Acc: 99.037 Val_Loss: 0.0465  BEST VAL Loss: 0.0465  Val_Acc: 98.967

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.0466 Train_Acc: 99.022 Val_Loss: 0.0466  BEST VAL Loss: 0.0465  Val_Acc: 98.861

Epoch 17: Validation loss decreased (0.046454 --> 0.046179).  Saving model ...
	 Train_Loss: 0.0455 Train_Acc: 99.038 Val_Loss: 0.0462  BEST VAL Loss: 0.0462  Val_Acc: 98.952

Epoch 18: Validation loss decreased (0.046179 --> 0.046151).  Saving model ...
	 Train_Loss: 0.0445 Train_Acc: 99.040 Val_Loss: 0.0462  BEST VAL Loss: 0.0462  Val_Acc: 98.805

Epoch 19: Validation loss decreased (0.046151 --> 0.045925).  Saving model ...
	 Train_Loss: 0.0436 Train_Acc: 99.077 Val_Loss: 0.0459  BEST VAL Loss: 0.0459  Val_Acc: 98.922

Epoch 20: Validation loss decreased (0.045925 --> 0.045656).  Saving model ...
	 Train_Loss: 0.0428 Train_Acc: 99.038 Val_Loss: 0.0457  BEST VAL Loss: 0.0457  Val_Acc: 98.907

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.0421 Train_Acc: 99.067 Val_Loss: 0.0457  BEST VAL Loss: 0.0457  Val_Acc: 98.988

Epoch 22: Validation loss decreased (0.045656 --> 0.045555).  Saving model ...
	 Train_Loss: 0.0413 Train_Acc: 99.153 Val_Loss: 0.0456  BEST VAL Loss: 0.0456  Val_Acc: 98.962

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0407 Train_Acc: 99.041 Val_Loss: 0.0457  BEST VAL Loss: 0.0456  Val_Acc: 98.952

Epoch 24: Validation loss decreased (0.045555 --> 0.045505).  Saving model ...
	 Train_Loss: 0.0402 Train_Acc: 98.986 Val_Loss: 0.0455  BEST VAL Loss: 0.0455  Val_Acc: 98.957

Epoch 25: Validation loss decreased (0.045505 --> 0.045212).  Saving model ...
	 Train_Loss: 0.0396 Train_Acc: 99.142 Val_Loss: 0.0452  BEST VAL Loss: 0.0452  Val_Acc: 99.048

Epoch 26: Validation loss decreased (0.045212 --> 0.045152).  Saving model ...
	 Train_Loss: 0.0390 Train_Acc: 99.151 Val_Loss: 0.0452  BEST VAL Loss: 0.0452  Val_Acc: 98.912

Epoch 27: Validation loss decreased (0.045152 --> 0.044999).  Saving model ...
	 Train_Loss: 0.0384 Train_Acc: 99.148 Val_Loss: 0.0450  BEST VAL Loss: 0.0450  Val_Acc: 98.866

Epoch 28: Validation loss decreased (0.044999 --> 0.044916).  Saving model ...
	 Train_Loss: 0.0379 Train_Acc: 99.249 Val_Loss: 0.0449  BEST VAL Loss: 0.0449  Val_Acc: 98.897

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.0373 Train_Acc: 99.203 Val_Loss: 0.0450  BEST VAL Loss: 0.0449  Val_Acc: 98.917

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0368 Train_Acc: 99.222 Val_Loss: 0.0451  BEST VAL Loss: 0.0449  Val_Acc: 99.023

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.0365 Train_Acc: 99.131 Val_Loss: 0.0450  BEST VAL Loss: 0.0449  Val_Acc: 98.947

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0361 Train_Acc: 99.169 Val_Loss: 0.0450  BEST VAL Loss: 0.0449  Val_Acc: 98.755

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0360 Train_Acc: 98.919 Val_Loss: 0.0451  BEST VAL Loss: 0.0449  Val_Acc: 98.810

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0358 Train_Acc: 98.934 Val_Loss: 0.0451  BEST VAL Loss: 0.0449  Val_Acc: 99.008

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.0354 Train_Acc: 99.241 Val_Loss: 0.0453  BEST VAL Loss: 0.0449  Val_Acc: 98.917

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.0351 Train_Acc: 99.197 Val_Loss: 0.0453  BEST VAL Loss: 0.0449  Val_Acc: 98.978

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.0346 Train_Acc: 99.305 Val_Loss: 0.0452  BEST VAL Loss: 0.0449  Val_Acc: 98.962

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0343 Train_Acc: 99.268 Val_Loss: 0.0452  BEST VAL Loss: 0.0449  Val_Acc: 98.856

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0339 Train_Acc: 99.310 Val_Loss: 0.0453  BEST VAL Loss: 0.0449  Val_Acc: 98.993

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0336 Train_Acc: 99.302 Val_Loss: 0.0454  BEST VAL Loss: 0.0449  Val_Acc: 98.876

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0335 Train_Acc: 98.966 Val_Loss: 0.0455  BEST VAL Loss: 0.0449  Val_Acc: 98.886

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0332 Train_Acc: 99.337 Val_Loss: 0.0457  BEST VAL Loss: 0.0449  Val_Acc: 98.841

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0329 Train_Acc: 99.330 Val_Loss: 0.0457  BEST VAL Loss: 0.0449  Val_Acc: 98.902

Epoch 44: Validation loss did not decrease
Early stopped at epoch : 44
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.35      0.35      0.35     56122
           1       0.64      0.64      0.64    101921

    accuracy                           0.54    158043
   macro avg       0.50      0.50      0.50    158043
weighted avg       0.54      0.54      0.54    158043

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.35      0.35      0.35      7015
           1       0.64      0.65      0.65     12741

    accuracy                           0.54     19756
   macro avg       0.50      0.50      0.50     19756
weighted avg       0.54      0.54      0.54     19756

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.35      0.35      0.35      7016
           1       0.64      0.65      0.64     12740

    accuracy                           0.54     19756
   macro avg       0.50      0.50      0.50     19756
weighted avg       0.54      0.54      0.54     19756

              precision    recall  f1-score   support

           0       0.35      0.35      0.35      7016
           1       0.64      0.65      0.64     12740

    accuracy                           0.54     19756
   macro avg       0.50      0.50      0.50     19756
weighted avg       0.54      0.54      0.54     19756

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_3.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.42      0.43      0.43     34394
           1       0.57      0.56      0.57     46859

    accuracy                           0.51     81253
   macro avg       0.50      0.50      0.50     81253
weighted avg       0.51      0.51      0.51     81253

              precision    recall  f1-score   support

           0       0.42      0.43      0.43     34394
           1       0.57      0.56      0.57     46859

    accuracy                           0.51     81253
   macro avg       0.50      0.50      0.50     81253
weighted avg       0.51      0.51      0.51     81253

completed
