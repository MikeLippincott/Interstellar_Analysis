[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '770e8446'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '79b5fc76'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '7417eb96'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '1894471e'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: DMSO_0.100_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: DMSO_0.100_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['DMSO_0.100_DMSO_0.025' 'Thapsigargin_10.000_DMSO_0.025']
The dimensions of the data are: (48855, 1276)
Number of total missing values across all columns: 97710
Data Subset Is Off
Wells held out for testing: ['I14' 'L14']
Wells to use for training, validation, and testing ['B14' 'C14' 'E14' 'B15' 'C15' 'E15' 'J14' 'I15' 'J15' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.314610).  Saving model ...
	 Train_Loss: 0.5345 Train_Acc: 64.470 Val_Loss: 0.3146  BEST VAL Loss: 0.3146  Val_Acc: 75.737

Epoch 1: Validation loss decreased (0.314610 --> 0.271466).  Saving model ...
	 Train_Loss: 0.4585 Train_Acc: 81.322 Val_Loss: 0.2715  BEST VAL Loss: 0.2715  Val_Acc: 92.502

Epoch 2: Validation loss decreased (0.271466 --> 0.244827).  Saving model ...
	 Train_Loss: 0.4161 Train_Acc: 83.511 Val_Loss: 0.2448  BEST VAL Loss: 0.2448  Val_Acc: 93.117

Epoch 3: Validation loss decreased (0.244827 --> 0.226813).  Saving model ...
	 Train_Loss: 0.3889 Train_Acc: 84.482 Val_Loss: 0.2268  BEST VAL Loss: 0.2268  Val_Acc: 93.363

Epoch 4: Validation loss decreased (0.226813 --> 0.211953).  Saving model ...
	 Train_Loss: 0.3660 Train_Acc: 88.382 Val_Loss: 0.2120  BEST VAL Loss: 0.2120  Val_Acc: 94.174

Epoch 5: Validation loss decreased (0.211953 --> 0.201215).  Saving model ...
	 Train_Loss: 0.3477 Train_Acc: 89.811 Val_Loss: 0.2012  BEST VAL Loss: 0.2012  Val_Acc: 94.027

Epoch 6: Validation loss decreased (0.201215 --> 0.191746).  Saving model ...
	 Train_Loss: 0.3337 Train_Acc: 89.980 Val_Loss: 0.1917  BEST VAL Loss: 0.1917  Val_Acc: 94.444

Epoch 7: Validation loss decreased (0.191746 --> 0.184938).  Saving model ...
	 Train_Loss: 0.3216 Train_Acc: 90.266 Val_Loss: 0.1849  BEST VAL Loss: 0.1849  Val_Acc: 94.518

Epoch 8: Validation loss decreased (0.184938 --> 0.178798).  Saving model ...
	 Train_Loss: 0.3089 Train_Acc: 90.524 Val_Loss: 0.1788  BEST VAL Loss: 0.1788  Val_Acc: 94.862

Epoch 9: Validation loss decreased (0.178798 --> 0.174302).  Saving model ...
	 Train_Loss: 0.2966 Train_Acc: 90.816 Val_Loss: 0.1743  BEST VAL Loss: 0.1743  Val_Acc: 94.862

Epoch 10: Validation loss decreased (0.174302 --> 0.171112).  Saving model ...
	 Train_Loss: 0.2865 Train_Acc: 90.924 Val_Loss: 0.1711  BEST VAL Loss: 0.1711  Val_Acc: 94.789

Epoch 11: Validation loss decreased (0.171112 --> 0.168536).  Saving model ...
	 Train_Loss: 0.2773 Train_Acc: 90.945 Val_Loss: 0.1685  BEST VAL Loss: 0.1685  Val_Acc: 94.395

Epoch 12: Validation loss decreased (0.168536 --> 0.166180).  Saving model ...
	 Train_Loss: 0.2691 Train_Acc: 91.250 Val_Loss: 0.1662  BEST VAL Loss: 0.1662  Val_Acc: 94.690

Epoch 13: Validation loss decreased (0.166180 --> 0.163414).  Saving model ...
	 Train_Loss: 0.2621 Train_Acc: 91.093 Val_Loss: 0.1634  BEST VAL Loss: 0.1634  Val_Acc: 94.715

Epoch 14: Validation loss decreased (0.163414 --> 0.161901).  Saving model ...
	 Train_Loss: 0.2555 Train_Acc: 91.336 Val_Loss: 0.1619  BEST VAL Loss: 0.1619  Val_Acc: 94.567

Epoch 15: Validation loss decreased (0.161901 --> 0.160234).  Saving model ...
	 Train_Loss: 0.2498 Train_Acc: 91.539 Val_Loss: 0.1602  BEST VAL Loss: 0.1602  Val_Acc: 94.936

Epoch 16: Validation loss decreased (0.160234 --> 0.158257).  Saving model ...
	 Train_Loss: 0.2442 Train_Acc: 91.858 Val_Loss: 0.1583  BEST VAL Loss: 0.1583  Val_Acc: 95.305

Epoch 17: Validation loss decreased (0.158257 --> 0.157084).  Saving model ...
	 Train_Loss: 0.2392 Train_Acc: 92.289 Val_Loss: 0.1571  BEST VAL Loss: 0.1571  Val_Acc: 94.420

Epoch 18: Validation loss decreased (0.157084 --> 0.155922).  Saving model ...
	 Train_Loss: 0.2349 Train_Acc: 92.697 Val_Loss: 0.1559  BEST VAL Loss: 0.1559  Val_Acc: 94.985

Epoch 19: Validation loss decreased (0.155922 --> 0.154536).  Saving model ...
	 Train_Loss: 0.2308 Train_Acc: 92.989 Val_Loss: 0.1545  BEST VAL Loss: 0.1545  Val_Acc: 94.912

Epoch 20: Validation loss decreased (0.154536 --> 0.153992).  Saving model ...
	 Train_Loss: 0.2271 Train_Acc: 92.780 Val_Loss: 0.1540  BEST VAL Loss: 0.1540  Val_Acc: 94.936

Epoch 21: Validation loss decreased (0.153992 --> 0.153157).  Saving model ...
	 Train_Loss: 0.2235 Train_Acc: 93.020 Val_Loss: 0.1532  BEST VAL Loss: 0.1532  Val_Acc: 94.936

Epoch 22: Validation loss decreased (0.153157 --> 0.152227).  Saving model ...
	 Train_Loss: 0.2202 Train_Acc: 92.992 Val_Loss: 0.1522  BEST VAL Loss: 0.1522  Val_Acc: 95.305

Epoch 23: Validation loss decreased (0.152227 --> 0.151596).  Saving model ...
	 Train_Loss: 0.2173 Train_Acc: 92.989 Val_Loss: 0.1516  BEST VAL Loss: 0.1516  Val_Acc: 95.059

Epoch 24: Validation loss decreased (0.151596 --> 0.151220).  Saving model ...
	 Train_Loss: 0.2143 Train_Acc: 93.309 Val_Loss: 0.1512  BEST VAL Loss: 0.1512  Val_Acc: 95.084

Epoch 25: Validation loss decreased (0.151220 --> 0.150829).  Saving model ...
	 Train_Loss: 0.2117 Train_Acc: 93.343 Val_Loss: 0.1508  BEST VAL Loss: 0.1508  Val_Acc: 95.157

Epoch 26: Validation loss decreased (0.150829 --> 0.150138).  Saving model ...
	 Train_Loss: 0.2092 Train_Acc: 93.180 Val_Loss: 0.1501  BEST VAL Loss: 0.1501  Val_Acc: 94.813

Epoch 27: Validation loss decreased (0.150138 --> 0.149663).  Saving model ...
	 Train_Loss: 0.2069 Train_Acc: 93.171 Val_Loss: 0.1497  BEST VAL Loss: 0.1497  Val_Acc: 94.789

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.2047 Train_Acc: 93.241 Val_Loss: 0.1497  BEST VAL Loss: 0.1497  Val_Acc: 95.010

Epoch 29: Validation loss decreased (0.149663 --> 0.149099).  Saving model ...
	 Train_Loss: 0.2027 Train_Acc: 93.291 Val_Loss: 0.1491  BEST VAL Loss: 0.1491  Val_Acc: 94.961

Epoch 30: Validation loss decreased (0.149099 --> 0.148657).  Saving model ...
	 Train_Loss: 0.2005 Train_Acc: 93.509 Val_Loss: 0.1487  BEST VAL Loss: 0.1487  Val_Acc: 95.108

Epoch 31: Validation loss decreased (0.148657 --> 0.148173).  Saving model ...
	 Train_Loss: 0.1985 Train_Acc: 93.582 Val_Loss: 0.1482  BEST VAL Loss: 0.1482  Val_Acc: 95.256

Epoch 32: Validation loss decreased (0.148173 --> 0.147615).  Saving model ...
	 Train_Loss: 0.1966 Train_Acc: 93.782 Val_Loss: 0.1476  BEST VAL Loss: 0.1476  Val_Acc: 95.256

Epoch 33: Validation loss decreased (0.147615 --> 0.147254).  Saving model ...
	 Train_Loss: 0.1948 Train_Acc: 93.626 Val_Loss: 0.1473  BEST VAL Loss: 0.1473  Val_Acc: 95.084

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1932 Train_Acc: 93.521 Val_Loss: 0.1473  BEST VAL Loss: 0.1473  Val_Acc: 95.280

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1916 Train_Acc: 93.570 Val_Loss: 0.1474  BEST VAL Loss: 0.1473  Val_Acc: 94.985

Epoch 36: Validation loss decreased (0.147254 --> 0.147039).  Saving model ...
	 Train_Loss: 0.1900 Train_Acc: 93.613 Val_Loss: 0.1470  BEST VAL Loss: 0.1470  Val_Acc: 95.354

Epoch 37: Validation loss decreased (0.147039 --> 0.146955).  Saving model ...
	 Train_Loss: 0.1884 Train_Acc: 93.718 Val_Loss: 0.1470  BEST VAL Loss: 0.1470  Val_Acc: 95.305

Epoch 38: Validation loss decreased (0.146955 --> 0.146947).  Saving model ...
	 Train_Loss: 0.1870 Train_Acc: 93.687 Val_Loss: 0.1469  BEST VAL Loss: 0.1469  Val_Acc: 95.133

Epoch 39: Validation loss decreased (0.146947 --> 0.146846).  Saving model ...
	 Train_Loss: 0.1857 Train_Acc: 93.669 Val_Loss: 0.1468  BEST VAL Loss: 0.1468  Val_Acc: 95.182

Epoch 40: Validation loss decreased (0.146846 --> 0.146833).  Saving model ...
	 Train_Loss: 0.1846 Train_Acc: 93.324 Val_Loss: 0.1468  BEST VAL Loss: 0.1468  Val_Acc: 95.256

Epoch 41: Validation loss decreased (0.146833 --> 0.146724).  Saving model ...
	 Train_Loss: 0.1834 Train_Acc: 93.782 Val_Loss: 0.1467  BEST VAL Loss: 0.1467  Val_Acc: 95.206

Epoch 42: Validation loss decreased (0.146724 --> 0.146553).  Saving model ...
	 Train_Loss: 0.1822 Train_Acc: 93.859 Val_Loss: 0.1466  BEST VAL Loss: 0.1466  Val_Acc: 95.182

Epoch 43: Validation loss decreased (0.146553 --> 0.146478).  Saving model ...
	 Train_Loss: 0.1810 Train_Acc: 93.816 Val_Loss: 0.1465  BEST VAL Loss: 0.1465  Val_Acc: 95.329

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1799 Train_Acc: 93.825 Val_Loss: 0.1465  BEST VAL Loss: 0.1465  Val_Acc: 95.256

Epoch 45: Validation loss decreased (0.146478 --> 0.146435).  Saving model ...
	 Train_Loss: 0.1788 Train_Acc: 93.927 Val_Loss: 0.1464  BEST VAL Loss: 0.1464  Val_Acc: 95.157

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1778 Train_Acc: 93.712 Val_Loss: 0.1466  BEST VAL Loss: 0.1464  Val_Acc: 95.305

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1767 Train_Acc: 94.077 Val_Loss: 0.1466  BEST VAL Loss: 0.1464  Val_Acc: 95.256

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1757 Train_Acc: 93.924 Val_Loss: 0.1466  BEST VAL Loss: 0.1464  Val_Acc: 95.354

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1749 Train_Acc: 93.893 Val_Loss: 0.1468  BEST VAL Loss: 0.1464  Val_Acc: 94.764

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1739 Train_Acc: 93.994 Val_Loss: 0.1469  BEST VAL Loss: 0.1464  Val_Acc: 95.256

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1730 Train_Acc: 94.074 Val_Loss: 0.1470  BEST VAL Loss: 0.1464  Val_Acc: 95.379

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1722 Train_Acc: 93.924 Val_Loss: 0.1471  BEST VAL Loss: 0.1464  Val_Acc: 95.206

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1715 Train_Acc: 93.490 Val_Loss: 0.1475  BEST VAL Loss: 0.1464  Val_Acc: 94.912

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1707 Train_Acc: 93.838 Val_Loss: 0.1477  BEST VAL Loss: 0.1464  Val_Acc: 95.280

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.1699 Train_Acc: 94.056 Val_Loss: 0.1479  BEST VAL Loss: 0.1464  Val_Acc: 95.084

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.1692 Train_Acc: 93.699 Val_Loss: 0.1481  BEST VAL Loss: 0.1464  Val_Acc: 94.936

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.1685 Train_Acc: 94.099 Val_Loss: 0.1481  BEST VAL Loss: 0.1464  Val_Acc: 95.501

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.1678 Train_Acc: 93.964 Val_Loss: 0.1481  BEST VAL Loss: 0.1464  Val_Acc: 95.256

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.1670 Train_Acc: 94.320 Val_Loss: 0.1481  BEST VAL Loss: 0.1464  Val_Acc: 95.010

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.1664 Train_Acc: 93.924 Val_Loss: 0.1482  BEST VAL Loss: 0.1464  Val_Acc: 95.010

Epoch 61: Validation loss did not decrease
Early stopped at epoch : 61
DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.99      0.99     24644
           1       0.95      0.97      0.96      7892

    accuracy                           0.98     32536
   macro avg       0.97      0.98      0.98     32536
weighted avg       0.98      0.98      0.98     32536

DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      3081
           1       0.90      0.90      0.90       987

    accuracy                           0.95      4068
   macro avg       0.93      0.94      0.93      4068
weighted avg       0.95      0.95      0.95      4068

DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      3081
           1       0.90      0.92      0.91       987

    accuracy                           0.96      4068
   macro avg       0.94      0.94      0.94      4068
weighted avg       0.96      0.96      0.96      4068

              precision    recall  f1-score   support

           0       0.97      0.97      0.97      3081
           1       0.90      0.92      0.91       987

    accuracy                           0.96      4068
   macro avg       0.94      0.94      0.94      4068
weighted avg       0.96      0.96      0.96      4068

DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
DMSO_0.100_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.98      0.96      4837
           1       0.97      0.92      0.94      3346

    accuracy                           0.95      8183
   macro avg       0.96      0.95      0.95      8183
weighted avg       0.96      0.95      0.95      8183

              precision    recall  f1-score   support

           0       0.94      0.98      0.96      4837
           1       0.97      0.92      0.94      3346

    accuracy                           0.95      8183
   macro avg       0.96      0.95      0.95      8183
weighted avg       0.96      0.95      0.95      8183

completed
