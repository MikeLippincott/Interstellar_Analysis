[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '99a50942'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '266fe029'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '0413108d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'a10de25d'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_0.100_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_0.100_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_0.100_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (31912, 1276)
Number of total missing values across all columns: 35020
Data Subset Is Off
Wells held out for testing: ['C20' 'M16']
Wells to use for training, validation, and testing ['C16' 'C17' 'C21' 'M17' 'M20' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.298002).  Saving model ...
	 Train_Loss: 0.5592 Train_Acc: 66.296 Val_Loss: 0.2980  BEST VAL Loss: 0.2980  Val_Acc: 89.331

Epoch 1: Validation loss decreased (0.298002 --> 0.285551).  Saving model ...
	 Train_Loss: 0.5044 Train_Acc: 72.681 Val_Loss: 0.2856  BEST VAL Loss: 0.2856  Val_Acc: 90.855

Epoch 2: Validation loss decreased (0.285551 --> 0.263415).  Saving model ...
	 Train_Loss: 0.4726 Train_Acc: 74.036 Val_Loss: 0.2634  BEST VAL Loss: 0.2634  Val_Acc: 92.506

Epoch 3: Validation loss decreased (0.263415 --> 0.242081).  Saving model ...
	 Train_Loss: 0.4514 Train_Acc: 74.846 Val_Loss: 0.2421  BEST VAL Loss: 0.2421  Val_Acc: 93.861

Epoch 4: Validation loss decreased (0.242081 --> 0.226181).  Saving model ...
	 Train_Loss: 0.4351 Train_Acc: 75.085 Val_Loss: 0.2262  BEST VAL Loss: 0.2262  Val_Acc: 92.549

Epoch 5: Validation loss decreased (0.226181 --> 0.212557).  Saving model ...
	 Train_Loss: 0.4238 Train_Acc: 75.566 Val_Loss: 0.2126  BEST VAL Loss: 0.2126  Val_Acc: 95.004

Epoch 6: Validation loss decreased (0.212557 --> 0.201088).  Saving model ...
	 Train_Loss: 0.4135 Train_Acc: 76.239 Val_Loss: 0.2011  BEST VAL Loss: 0.2011  Val_Acc: 94.877

Epoch 7: Validation loss decreased (0.201088 --> 0.193270).  Saving model ...
	 Train_Loss: 0.4051 Train_Acc: 76.938 Val_Loss: 0.1933  BEST VAL Loss: 0.1933  Val_Acc: 95.258

Epoch 8: Validation loss decreased (0.193270 --> 0.185049).  Saving model ...
	 Train_Loss: 0.3983 Train_Acc: 76.514 Val_Loss: 0.1850  BEST VAL Loss: 0.1850  Val_Acc: 95.470

Epoch 9: Validation loss decreased (0.185049 --> 0.177280).  Saving model ...
	 Train_Loss: 0.3926 Train_Acc: 76.657 Val_Loss: 0.1773  BEST VAL Loss: 0.1773  Val_Acc: 96.105

Epoch 10: Validation loss decreased (0.177280 --> 0.170461).  Saving model ...
	 Train_Loss: 0.3879 Train_Acc: 76.763 Val_Loss: 0.1705  BEST VAL Loss: 0.1705  Val_Acc: 96.020

Epoch 11: Validation loss decreased (0.170461 --> 0.164659).  Saving model ...
	 Train_Loss: 0.3836 Train_Acc: 77.086 Val_Loss: 0.1647  BEST VAL Loss: 0.1647  Val_Acc: 96.063

Epoch 12: Validation loss decreased (0.164659 --> 0.160046).  Saving model ...
	 Train_Loss: 0.3802 Train_Acc: 76.594 Val_Loss: 0.1600  BEST VAL Loss: 0.1600  Val_Acc: 95.978

Epoch 13: Validation loss decreased (0.160046 --> 0.156410).  Saving model ...
	 Train_Loss: 0.3767 Train_Acc: 77.531 Val_Loss: 0.1564  BEST VAL Loss: 0.1564  Val_Acc: 96.232

Epoch 14: Validation loss decreased (0.156410 --> 0.153058).  Saving model ...
	 Train_Loss: 0.3735 Train_Acc: 77.271 Val_Loss: 0.1531  BEST VAL Loss: 0.1531  Val_Acc: 95.978

Epoch 15: Validation loss decreased (0.153058 --> 0.149771).  Saving model ...
	 Train_Loss: 0.3709 Train_Acc: 77.128 Val_Loss: 0.1498  BEST VAL Loss: 0.1498  Val_Acc: 96.063

Epoch 16: Validation loss decreased (0.149771 --> 0.146479).  Saving model ...
	 Train_Loss: 0.3686 Train_Acc: 77.176 Val_Loss: 0.1465  BEST VAL Loss: 0.1465  Val_Acc: 96.655

Epoch 17: Validation loss decreased (0.146479 --> 0.144104).  Saving model ...
	 Train_Loss: 0.3662 Train_Acc: 77.695 Val_Loss: 0.1441  BEST VAL Loss: 0.1441  Val_Acc: 96.190

Epoch 18: Validation loss decreased (0.144104 --> 0.141410).  Saving model ...
	 Train_Loss: 0.3643 Train_Acc: 76.943 Val_Loss: 0.1414  BEST VAL Loss: 0.1414  Val_Acc: 96.359

Epoch 19: Validation loss decreased (0.141410 --> 0.139146).  Saving model ...
	 Train_Loss: 0.3624 Train_Acc: 77.234 Val_Loss: 0.1391  BEST VAL Loss: 0.1391  Val_Acc: 96.486

Epoch 20: Validation loss decreased (0.139146 --> 0.137378).  Saving model ...
	 Train_Loss: 0.3608 Train_Acc: 76.726 Val_Loss: 0.1374  BEST VAL Loss: 0.1374  Val_Acc: 95.682

Epoch 21: Validation loss decreased (0.137378 --> 0.135180).  Saving model ...
	 Train_Loss: 0.3593 Train_Acc: 77.287 Val_Loss: 0.1352  BEST VAL Loss: 0.1352  Val_Acc: 96.232

Epoch 22: Validation loss decreased (0.135180 --> 0.133319).  Saving model ...
	 Train_Loss: 0.3578 Train_Acc: 77.674 Val_Loss: 0.1333  BEST VAL Loss: 0.1333  Val_Acc: 96.190

Epoch 23: Validation loss decreased (0.133319 --> 0.132655).  Saving model ...
	 Train_Loss: 0.3563 Train_Acc: 77.785 Val_Loss: 0.1327  BEST VAL Loss: 0.1327  Val_Acc: 96.147

Epoch 24: Validation loss decreased (0.132655 --> 0.130913).  Saving model ...
	 Train_Loss: 0.3551 Train_Acc: 77.499 Val_Loss: 0.1309  BEST VAL Loss: 0.1309  Val_Acc: 96.613

Epoch 25: Validation loss decreased (0.130913 --> 0.129942).  Saving model ...
	 Train_Loss: 0.3537 Train_Acc: 77.827 Val_Loss: 0.1299  BEST VAL Loss: 0.1299  Val_Acc: 95.978

Epoch 26: Validation loss decreased (0.129942 --> 0.129022).  Saving model ...
	 Train_Loss: 0.3522 Train_Acc: 79.659 Val_Loss: 0.1290  BEST VAL Loss: 0.1290  Val_Acc: 95.682

Epoch 27: Validation loss decreased (0.129022 --> 0.127831).  Saving model ...
	 Train_Loss: 0.3509 Train_Acc: 79.490 Val_Loss: 0.1278  BEST VAL Loss: 0.1278  Val_Acc: 96.401

Epoch 28: Validation loss decreased (0.127831 --> 0.127038).  Saving model ...
	 Train_Loss: 0.3492 Train_Acc: 80.792 Val_Loss: 0.1270  BEST VAL Loss: 0.1270  Val_Acc: 96.274

Epoch 29: Validation loss decreased (0.127038 --> 0.126113).  Saving model ...
	 Train_Loss: 0.3475 Train_Acc: 81.094 Val_Loss: 0.1261  BEST VAL Loss: 0.1261  Val_Acc: 95.893

Epoch 30: Validation loss decreased (0.126113 --> 0.125095).  Saving model ...
	 Train_Loss: 0.3460 Train_Acc: 80.406 Val_Loss: 0.1251  BEST VAL Loss: 0.1251  Val_Acc: 96.571

Epoch 31: Validation loss decreased (0.125095 --> 0.124026).  Saving model ...
	 Train_Loss: 0.3447 Train_Acc: 80.575 Val_Loss: 0.1240  BEST VAL Loss: 0.1240  Val_Acc: 96.317

Epoch 32: Validation loss decreased (0.124026 --> 0.123015).  Saving model ...
	 Train_Loss: 0.3435 Train_Acc: 80.686 Val_Loss: 0.1230  BEST VAL Loss: 0.1230  Val_Acc: 96.147

Epoch 33: Validation loss decreased (0.123015 --> 0.122633).  Saving model ...
	 Train_Loss: 0.3422 Train_Acc: 80.713 Val_Loss: 0.1226  BEST VAL Loss: 0.1226  Val_Acc: 96.190

Epoch 34: Validation loss decreased (0.122633 --> 0.121705).  Saving model ...
	 Train_Loss: 0.3409 Train_Acc: 81.491 Val_Loss: 0.1217  BEST VAL Loss: 0.1217  Val_Acc: 96.782

Epoch 35: Validation loss decreased (0.121705 --> 0.120703).  Saving model ...
	 Train_Loss: 0.3397 Train_Acc: 80.977 Val_Loss: 0.1207  BEST VAL Loss: 0.1207  Val_Acc: 96.274

Epoch 36: Validation loss decreased (0.120703 --> 0.119938).  Saving model ...
	 Train_Loss: 0.3386 Train_Acc: 81.157 Val_Loss: 0.1199  BEST VAL Loss: 0.1199  Val_Acc: 96.444

Epoch 37: Validation loss decreased (0.119938 --> 0.119032).  Saving model ...
	 Train_Loss: 0.3376 Train_Acc: 81.009 Val_Loss: 0.1190  BEST VAL Loss: 0.1190  Val_Acc: 96.486

Epoch 38: Validation loss decreased (0.119032 --> 0.118412).  Saving model ...
	 Train_Loss: 0.3366 Train_Acc: 80.877 Val_Loss: 0.1184  BEST VAL Loss: 0.1184  Val_Acc: 96.274

Epoch 39: Validation loss decreased (0.118412 --> 0.117599).  Saving model ...
	 Train_Loss: 0.3355 Train_Acc: 81.094 Val_Loss: 0.1176  BEST VAL Loss: 0.1176  Val_Acc: 96.232

Epoch 40: Validation loss decreased (0.117599 --> 0.117425).  Saving model ...
	 Train_Loss: 0.3345 Train_Acc: 81.337 Val_Loss: 0.1174  BEST VAL Loss: 0.1174  Val_Acc: 96.740

Epoch 41: Validation loss decreased (0.117425 --> 0.117347).  Saving model ...
	 Train_Loss: 0.3335 Train_Acc: 81.406 Val_Loss: 0.1173  BEST VAL Loss: 0.1173  Val_Acc: 96.274

Epoch 42: Validation loss decreased (0.117347 --> 0.116763).  Saving model ...
	 Train_Loss: 0.3327 Train_Acc: 80.691 Val_Loss: 0.1168  BEST VAL Loss: 0.1168  Val_Acc: 96.867

Epoch 43: Validation loss decreased (0.116763 --> 0.115979).  Saving model ...
	 Train_Loss: 0.3317 Train_Acc: 81.300 Val_Loss: 0.1160  BEST VAL Loss: 0.1160  Val_Acc: 96.994

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.3310 Train_Acc: 81.253 Val_Loss: 0.1173  BEST VAL Loss: 0.1160  Val_Acc: 96.825

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.3306 Train_Acc: 80.734 Val_Loss: 0.1193  BEST VAL Loss: 0.1160  Val_Acc: 95.724

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.3314 Train_Acc: 80.008 Val_Loss: 0.1231  BEST VAL Loss: 0.1160  Val_Acc: 94.835

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.3336 Train_Acc: 78.791 Val_Loss: 0.1255  BEST VAL Loss: 0.1160  Val_Acc: 94.115

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.3357 Train_Acc: 77.764 Val_Loss: 0.1286  BEST VAL Loss: 0.1160  Val_Acc: 94.454

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.3377 Train_Acc: 77.769 Val_Loss: 0.1302  BEST VAL Loss: 0.1160  Val_Acc: 93.565

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.3386 Train_Acc: 78.357 Val_Loss: 0.1308  BEST VAL Loss: 0.1160  Val_Acc: 93.819

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.3389 Train_Acc: 79.040 Val_Loss: 0.1308  BEST VAL Loss: 0.1160  Val_Acc: 95.131

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.3388 Train_Acc: 79.574 Val_Loss: 0.1307  BEST VAL Loss: 0.1160  Val_Acc: 95.301

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.3385 Train_Acc: 79.987 Val_Loss: 0.1302  BEST VAL Loss: 0.1160  Val_Acc: 95.639

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.3383 Train_Acc: 80.067 Val_Loss: 0.1297  BEST VAL Loss: 0.1160  Val_Acc: 95.893

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.3378 Train_Acc: 80.607 Val_Loss: 0.1293  BEST VAL Loss: 0.1160  Val_Acc: 95.512

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.3373 Train_Acc: 80.649 Val_Loss: 0.1289  BEST VAL Loss: 0.1160  Val_Acc: 96.020

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.3367 Train_Acc: 80.856 Val_Loss: 0.1286  BEST VAL Loss: 0.1160  Val_Acc: 95.978

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.3362 Train_Acc: 80.771 Val_Loss: 0.1280  BEST VAL Loss: 0.1160  Val_Acc: 96.359

Epoch 59: Validation loss did not decrease
Early stopped at epoch : 59
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55     10452
           1       0.45      0.45      0.45      8436

    accuracy                           0.51     18888
   macro avg       0.50      0.50      0.50     18888
weighted avg       0.51      0.51      0.51     18888

LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.54      0.54      1307
           1       0.43      0.42      0.42      1055

    accuracy                           0.49      2362
   macro avg       0.48      0.48      0.48      2362
weighted avg       0.49      0.49      0.49      2362

LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.56      0.56      1306
           1       0.46      0.46      0.46      1055

    accuracy                           0.52      2361
   macro avg       0.51      0.51      0.51      2361
weighted avg       0.52      0.52      0.52      2361

              precision    recall  f1-score   support

           0       0.56      0.56      0.56      1306
           1       0.46      0.46      0.46      1055

    accuracy                           0.52      2361
   macro avg       0.51      0.51      0.51      2361
weighted avg       0.52      0.52      0.52      2361

LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_0.100_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.56      0.55      4445
           1       0.47      0.45      0.46      3856

    accuracy                           0.51      8301
   macro avg       0.51      0.51      0.51      8301
weighted avg       0.51      0.51      0.51      8301

              precision    recall  f1-score   support

           0       0.54      0.56      0.55      4445
           1       0.47      0.45      0.46      3856

    accuracy                           0.51      8301
   macro avg       0.51      0.51      0.51      8301
weighted avg       0.51      0.51      0.51      8301

completed
