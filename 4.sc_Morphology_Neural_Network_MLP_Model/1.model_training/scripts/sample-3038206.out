[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '66a3a1ec'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '0b06a53b'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'bd082fb2'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'a50f6c58'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: LPS_100.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_100.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_100.000_DMSO_0.025']
The dimensions of the data are: (29753, 1276)
Number of total missing values across all columns: 59506
Data Subset Is Off
Wells held out for testing: ['D14' 'J20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'J16' 'J17' 'J21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.213724).  Saving model ...
	 Train_Loss: 0.4357 Train_Acc: 70.469 Val_Loss: 0.2137  BEST VAL Loss: 0.2137  Val_Acc: 92.641

Epoch 1: Validation loss decreased (0.213724 --> 0.184455).  Saving model ...
	 Train_Loss: 0.3666 Train_Acc: 85.692 Val_Loss: 0.1845  BEST VAL Loss: 0.1845  Val_Acc: 94.569

Epoch 2: Validation loss decreased (0.184455 --> 0.166447).  Saving model ...
	 Train_Loss: 0.3278 Train_Acc: 90.226 Val_Loss: 0.1664  BEST VAL Loss: 0.1664  Val_Acc: 95.488

Epoch 3: Validation loss decreased (0.166447 --> 0.163093).  Saving model ...
	 Train_Loss: 0.3043 Train_Acc: 91.140 Val_Loss: 0.1631  BEST VAL Loss: 0.1631  Val_Acc: 95.007

Epoch 4: Validation loss decreased (0.163093 --> 0.152681).  Saving model ...
	 Train_Loss: 0.2873 Train_Acc: 91.485 Val_Loss: 0.1527  BEST VAL Loss: 0.1527  Val_Acc: 95.883

Epoch 5: Validation loss decreased (0.152681 --> 0.145207).  Saving model ...
	 Train_Loss: 0.2734 Train_Acc: 92.230 Val_Loss: 0.1452  BEST VAL Loss: 0.1452  Val_Acc: 96.102

Epoch 6: Validation loss decreased (0.145207 --> 0.137859).  Saving model ...
	 Train_Loss: 0.2616 Train_Acc: 92.082 Val_Loss: 0.1379  BEST VAL Loss: 0.1379  Val_Acc: 96.627

Epoch 7: Validation loss decreased (0.137859 --> 0.132462).  Saving model ...
	 Train_Loss: 0.2517 Train_Acc: 92.306 Val_Loss: 0.1325  BEST VAL Loss: 0.1325  Val_Acc: 96.671

Epoch 8: Validation loss decreased (0.132462 --> 0.129193).  Saving model ...
	 Train_Loss: 0.2434 Train_Acc: 92.575 Val_Loss: 0.1292  BEST VAL Loss: 0.1292  Val_Acc: 96.934

Epoch 9: Validation loss decreased (0.129193 --> 0.127070).  Saving model ...
	 Train_Loss: 0.2360 Train_Acc: 93.051 Val_Loss: 0.1271  BEST VAL Loss: 0.1271  Val_Acc: 96.934

Epoch 10: Validation loss decreased (0.127070 --> 0.124295).  Saving model ...
	 Train_Loss: 0.2301 Train_Acc: 92.788 Val_Loss: 0.1243  BEST VAL Loss: 0.1243  Val_Acc: 96.846

Epoch 11: Validation loss decreased (0.124295 --> 0.122008).  Saving model ...
	 Train_Loss: 0.2253 Train_Acc: 93.144 Val_Loss: 0.1220  BEST VAL Loss: 0.1220  Val_Acc: 96.846

Epoch 12: Validation loss decreased (0.122008 --> 0.120481).  Saving model ...
	 Train_Loss: 0.2208 Train_Acc: 93.265 Val_Loss: 0.1205  BEST VAL Loss: 0.1205  Val_Acc: 97.197

Epoch 13: Validation loss decreased (0.120481 --> 0.118772).  Saving model ...
	 Train_Loss: 0.2168 Train_Acc: 93.062 Val_Loss: 0.1188  BEST VAL Loss: 0.1188  Val_Acc: 97.065

Epoch 14: Validation loss decreased (0.118772 --> 0.117269).  Saving model ...
	 Train_Loss: 0.2128 Train_Acc: 93.484 Val_Loss: 0.1173  BEST VAL Loss: 0.1173  Val_Acc: 96.978

Epoch 15: Validation loss decreased (0.117269 --> 0.115731).  Saving model ...
	 Train_Loss: 0.2095 Train_Acc: 93.292 Val_Loss: 0.1157  BEST VAL Loss: 0.1157  Val_Acc: 97.197

Epoch 16: Validation loss decreased (0.115731 --> 0.113945).  Saving model ...
	 Train_Loss: 0.2063 Train_Acc: 93.599 Val_Loss: 0.1139  BEST VAL Loss: 0.1139  Val_Acc: 97.459

Epoch 17: Validation loss decreased (0.113945 --> 0.112872).  Saving model ...
	 Train_Loss: 0.2037 Train_Acc: 93.434 Val_Loss: 0.1129  BEST VAL Loss: 0.1129  Val_Acc: 97.416

Epoch 18: Validation loss decreased (0.112872 --> 0.112685).  Saving model ...
	 Train_Loss: 0.2011 Train_Acc: 93.664 Val_Loss: 0.1127  BEST VAL Loss: 0.1127  Val_Acc: 97.503

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1990 Train_Acc: 93.243 Val_Loss: 0.1138  BEST VAL Loss: 0.1127  Val_Acc: 96.496

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1970 Train_Acc: 93.413 Val_Loss: 0.1138  BEST VAL Loss: 0.1127  Val_Acc: 97.328

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1950 Train_Acc: 93.577 Val_Loss: 0.1151  BEST VAL Loss: 0.1127  Val_Acc: 97.240

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1931 Train_Acc: 93.434 Val_Loss: 0.1139  BEST VAL Loss: 0.1127  Val_Acc: 97.459

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1912 Train_Acc: 93.774 Val_Loss: 0.1131  BEST VAL Loss: 0.1127  Val_Acc: 97.372

Epoch 24: Validation loss decreased (0.112685 --> 0.112587).  Saving model ...
	 Train_Loss: 0.1894 Train_Acc: 93.911 Val_Loss: 0.1126  BEST VAL Loss: 0.1126  Val_Acc: 97.722

Epoch 25: Validation loss decreased (0.112587 --> 0.112105).  Saving model ...
	 Train_Loss: 0.1880 Train_Acc: 93.632 Val_Loss: 0.1121  BEST VAL Loss: 0.1121  Val_Acc: 97.197

Epoch 26: Validation loss decreased (0.112105 --> 0.111895).  Saving model ...
	 Train_Loss: 0.1867 Train_Acc: 93.664 Val_Loss: 0.1119  BEST VAL Loss: 0.1119  Val_Acc: 97.240

Epoch 27: Validation loss decreased (0.111895 --> 0.111748).  Saving model ...
	 Train_Loss: 0.1854 Train_Acc: 93.533 Val_Loss: 0.1117  BEST VAL Loss: 0.1117  Val_Acc: 97.416

Epoch 28: Validation loss decreased (0.111748 --> 0.111603).  Saving model ...
	 Train_Loss: 0.1841 Train_Acc: 93.774 Val_Loss: 0.1116  BEST VAL Loss: 0.1116  Val_Acc: 97.591

Epoch 29: Validation loss decreased (0.111603 --> 0.111549).  Saving model ...
	 Train_Loss: 0.1829 Train_Acc: 93.692 Val_Loss: 0.1115  BEST VAL Loss: 0.1115  Val_Acc: 97.284

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1819 Train_Acc: 93.829 Val_Loss: 0.1119  BEST VAL Loss: 0.1115  Val_Acc: 97.372

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1808 Train_Acc: 93.752 Val_Loss: 0.1120  BEST VAL Loss: 0.1115  Val_Acc: 97.065

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1798 Train_Acc: 93.801 Val_Loss: 0.1116  BEST VAL Loss: 0.1115  Val_Acc: 97.197

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1788 Train_Acc: 93.949 Val_Loss: 0.1116  BEST VAL Loss: 0.1115  Val_Acc: 97.372

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1777 Train_Acc: 94.152 Val_Loss: 0.1117  BEST VAL Loss: 0.1115  Val_Acc: 97.240

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1768 Train_Acc: 93.664 Val_Loss: 0.1123  BEST VAL Loss: 0.1115  Val_Acc: 97.197

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1761 Train_Acc: 93.533 Val_Loss: 0.1128  BEST VAL Loss: 0.1115  Val_Acc: 97.547

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1753 Train_Acc: 93.823 Val_Loss: 0.1134  BEST VAL Loss: 0.1115  Val_Acc: 97.328

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1747 Train_Acc: 93.807 Val_Loss: 0.1135  BEST VAL Loss: 0.1115  Val_Acc: 97.459

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1738 Train_Acc: 94.075 Val_Loss: 0.1134  BEST VAL Loss: 0.1115  Val_Acc: 97.766

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1732 Train_Acc: 93.719 Val_Loss: 0.1139  BEST VAL Loss: 0.1115  Val_Acc: 97.416

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1726 Train_Acc: 93.752 Val_Loss: 0.1147  BEST VAL Loss: 0.1115  Val_Acc: 97.547

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1720 Train_Acc: 93.845 Val_Loss: 0.1149  BEST VAL Loss: 0.1115  Val_Acc: 97.547

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1712 Train_Acc: 94.026 Val_Loss: 0.1159  BEST VAL Loss: 0.1115  Val_Acc: 97.284

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1707 Train_Acc: 93.719 Val_Loss: 0.1158  BEST VAL Loss: 0.1115  Val_Acc: 97.503

Epoch 45: Validation loss did not decrease
Early stopped at epoch : 45
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      9891
           1       1.00      0.99      0.99      8371

    accuracy                           0.99     18262
   macro avg       0.99      0.99      0.99     18262
weighted avg       0.99      0.99      0.99     18262

Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.98      1237
           1       0.98      0.96      0.97      1046

    accuracy                           0.97      2283
   macro avg       0.97      0.97      0.97      2283
weighted avg       0.97      0.97      0.97      2283

Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.98      1237
           1       0.98      0.97      0.97      1046

    accuracy                           0.97      2283
   macro avg       0.97      0.97      0.97      2283
weighted avg       0.97      0.97      0.97      2283

              precision    recall  f1-score   support

           0       0.97      0.98      0.98      1237
           1       0.98      0.97      0.97      1046

    accuracy                           0.97      2283
   macro avg       0.97      0.97      0.97      2283
weighted avg       0.97      0.97      0.97      2283

Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_LPS_100.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.94      0.96      3622
           1       0.94      0.97      0.95      3303

    accuracy                           0.96      6925
   macro avg       0.96      0.96      0.96      6925
weighted avg       0.96      0.96      0.96      6925

              precision    recall  f1-score   support

           0       0.97      0.94      0.96      3622
           1       0.94      0.97      0.95      3303

    accuracy                           0.96      6925
   macro avg       0.96      0.96      0.96      6925
weighted avg       0.96      0.96      0.96      6925

completed
