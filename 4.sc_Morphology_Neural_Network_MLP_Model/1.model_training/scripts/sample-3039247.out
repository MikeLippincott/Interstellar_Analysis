[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '2126de69'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '9e0e1d64'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '54189203'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '497c929a'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_100.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_100.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_100.000_DMSO_0.025']
The dimensions of the data are: (29753, 1276)
Number of total missing values across all columns: 59506
Data Subset Is Off
Wells held out for testing: ['D14' 'J20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'J16' 'J17' 'J21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.329618).  Saving model ...
	 Train_Loss: 0.5185 Train_Acc: 79.904 Val_Loss: 0.3296  BEST VAL Loss: 0.3296  Val_Acc: 91.283

Epoch 1: Validation loss decreased (0.329618 --> 0.270287).  Saving model ...
	 Train_Loss: 0.4192 Train_Acc: 89.152 Val_Loss: 0.2703  BEST VAL Loss: 0.2703  Val_Acc: 92.816

Epoch 2: Validation loss decreased (0.270287 --> 0.236175).  Saving model ...
	 Train_Loss: 0.3624 Train_Acc: 91.518 Val_Loss: 0.2362  BEST VAL Loss: 0.2362  Val_Acc: 94.087

Epoch 3: Validation loss decreased (0.236175 --> 0.210212).  Saving model ...
	 Train_Loss: 0.3221 Train_Acc: 92.991 Val_Loss: 0.2102  BEST VAL Loss: 0.2102  Val_Acc: 95.007

Epoch 4: Validation loss decreased (0.210212 --> 0.191586).  Saving model ...
	 Train_Loss: 0.2935 Train_Acc: 93.681 Val_Loss: 0.1916  BEST VAL Loss: 0.1916  Val_Acc: 95.357

Epoch 5: Validation loss decreased (0.191586 --> 0.179851).  Saving model ...
	 Train_Loss: 0.2734 Train_Acc: 94.168 Val_Loss: 0.1799  BEST VAL Loss: 0.1799  Val_Acc: 95.839

Epoch 6: Validation loss decreased (0.179851 --> 0.169850).  Saving model ...
	 Train_Loss: 0.2559 Train_Acc: 94.650 Val_Loss: 0.1699  BEST VAL Loss: 0.1699  Val_Acc: 96.364

Epoch 7: Validation loss decreased (0.169850 --> 0.162533).  Saving model ...
	 Train_Loss: 0.2421 Train_Acc: 94.716 Val_Loss: 0.1625  BEST VAL Loss: 0.1625  Val_Acc: 96.145

Epoch 8: Validation loss decreased (0.162533 --> 0.156390).  Saving model ...
	 Train_Loss: 0.2311 Train_Acc: 94.897 Val_Loss: 0.1564  BEST VAL Loss: 0.1564  Val_Acc: 96.233

Epoch 9: Validation loss decreased (0.156390 --> 0.153150).  Saving model ...
	 Train_Loss: 0.2227 Train_Acc: 95.335 Val_Loss: 0.1531  BEST VAL Loss: 0.1531  Val_Acc: 96.014

Epoch 10: Validation loss decreased (0.153150 --> 0.149980).  Saving model ...
	 Train_Loss: 0.2150 Train_Acc: 95.362 Val_Loss: 0.1500  BEST VAL Loss: 0.1500  Val_Acc: 96.233

Epoch 11: Validation loss decreased (0.149980 --> 0.148462).  Saving model ...
	 Train_Loss: 0.2080 Train_Acc: 95.296 Val_Loss: 0.1485  BEST VAL Loss: 0.1485  Val_Acc: 96.540

Epoch 12: Validation loss decreased (0.148462 --> 0.145802).  Saving model ...
	 Train_Loss: 0.2013 Train_Acc: 95.641 Val_Loss: 0.1458  BEST VAL Loss: 0.1458  Val_Acc: 97.021

Epoch 13: Validation loss decreased (0.145802 --> 0.142976).  Saving model ...
	 Train_Loss: 0.1954 Train_Acc: 95.948 Val_Loss: 0.1430  BEST VAL Loss: 0.1430  Val_Acc: 96.452

Epoch 14: Validation loss decreased (0.142976 --> 0.140761).  Saving model ...
	 Train_Loss: 0.1907 Train_Acc: 95.636 Val_Loss: 0.1408  BEST VAL Loss: 0.1408  Val_Acc: 96.671

Epoch 15: Validation loss decreased (0.140761 --> 0.139978).  Saving model ...
	 Train_Loss: 0.1861 Train_Acc: 95.931 Val_Loss: 0.1400  BEST VAL Loss: 0.1400  Val_Acc: 96.364

Epoch 16: Validation loss decreased (0.139978 --> 0.137674).  Saving model ...
	 Train_Loss: 0.1821 Train_Acc: 95.811 Val_Loss: 0.1377  BEST VAL Loss: 0.1377  Val_Acc: 96.890

Epoch 17: Validation loss decreased (0.137674 --> 0.135798).  Saving model ...
	 Train_Loss: 0.1782 Train_Acc: 95.953 Val_Loss: 0.1358  BEST VAL Loss: 0.1358  Val_Acc: 96.934

Epoch 18: Validation loss decreased (0.135798 --> 0.133531).  Saving model ...
	 Train_Loss: 0.1748 Train_Acc: 96.030 Val_Loss: 0.1335  BEST VAL Loss: 0.1335  Val_Acc: 96.671

Epoch 19: Validation loss decreased (0.133531 --> 0.132512).  Saving model ...
	 Train_Loss: 0.1714 Train_Acc: 96.337 Val_Loss: 0.1325  BEST VAL Loss: 0.1325  Val_Acc: 96.934

Epoch 20: Validation loss decreased (0.132512 --> 0.131909).  Saving model ...
	 Train_Loss: 0.1685 Train_Acc: 96.003 Val_Loss: 0.1319  BEST VAL Loss: 0.1319  Val_Acc: 96.934

Epoch 21: Validation loss decreased (0.131909 --> 0.130434).  Saving model ...
	 Train_Loss: 0.1658 Train_Acc: 95.904 Val_Loss: 0.1304  BEST VAL Loss: 0.1304  Val_Acc: 96.890

Epoch 22: Validation loss decreased (0.130434 --> 0.129753).  Saving model ...
	 Train_Loss: 0.1633 Train_Acc: 96.118 Val_Loss: 0.1298  BEST VAL Loss: 0.1298  Val_Acc: 97.065

Epoch 23: Validation loss decreased (0.129753 --> 0.129189).  Saving model ...
	 Train_Loss: 0.1609 Train_Acc: 96.298 Val_Loss: 0.1292  BEST VAL Loss: 0.1292  Val_Acc: 96.978

Epoch 24: Validation loss decreased (0.129189 --> 0.128901).  Saving model ...
	 Train_Loss: 0.1585 Train_Acc: 96.342 Val_Loss: 0.1289  BEST VAL Loss: 0.1289  Val_Acc: 96.802

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1565 Train_Acc: 96.025 Val_Loss: 0.1293  BEST VAL Loss: 0.1289  Val_Acc: 96.583

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1544 Train_Acc: 96.233 Val_Loss: 0.1295  BEST VAL Loss: 0.1289  Val_Acc: 96.715

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1523 Train_Acc: 96.583 Val_Loss: 0.1296  BEST VAL Loss: 0.1289  Val_Acc: 96.890

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1503 Train_Acc: 96.753 Val_Loss: 0.1298  BEST VAL Loss: 0.1289  Val_Acc: 96.846

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1485 Train_Acc: 96.506 Val_Loss: 0.1298  BEST VAL Loss: 0.1289  Val_Acc: 97.284

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1466 Train_Acc: 96.720 Val_Loss: 0.1299  BEST VAL Loss: 0.1289  Val_Acc: 97.284

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1452 Train_Acc: 96.211 Val_Loss: 0.1305  BEST VAL Loss: 0.1289  Val_Acc: 96.627

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1440 Train_Acc: 96.216 Val_Loss: 0.1305  BEST VAL Loss: 0.1289  Val_Acc: 96.978

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1426 Train_Acc: 96.441 Val_Loss: 0.1303  BEST VAL Loss: 0.1289  Val_Acc: 96.802

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1411 Train_Acc: 96.649 Val_Loss: 0.1305  BEST VAL Loss: 0.1289  Val_Acc: 96.583

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1399 Train_Acc: 96.660 Val_Loss: 0.1299  BEST VAL Loss: 0.1289  Val_Acc: 97.328

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1386 Train_Acc: 96.731 Val_Loss: 0.1295  BEST VAL Loss: 0.1289  Val_Acc: 97.722

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.1373 Train_Acc: 96.742 Val_Loss: 0.1297  BEST VAL Loss: 0.1289  Val_Acc: 97.416

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1360 Train_Acc: 96.961 Val_Loss: 0.1296  BEST VAL Loss: 0.1289  Val_Acc: 97.284

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1347 Train_Acc: 96.994 Val_Loss: 0.1296  BEST VAL Loss: 0.1289  Val_Acc: 97.153

Epoch 40: Validation loss did not decrease
Early stopped at epoch : 40
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.99      0.99      9891
           1       0.99      0.98      0.98      8371

    accuracy                           0.98     18262
   macro avg       0.98      0.98      0.98     18262
weighted avg       0.98      0.98      0.98     18262

LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1237
           1       0.97      0.96      0.96      1046

    accuracy                           0.97      2283
   macro avg       0.97      0.97      0.97      2283
weighted avg       0.97      0.97      0.97      2283

LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1237
           1       0.97      0.96      0.97      1046

    accuracy                           0.97      2283
   macro avg       0.97      0.97      0.97      2283
weighted avg       0.97      0.97      0.97      2283

              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1237
           1       0.97      0.96      0.97      1046

    accuracy                           0.97      2283
   macro avg       0.97      0.97      0.97      2283
weighted avg       0.97      0.97      0.97      2283

LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.93      0.95      3622
           1       0.93      0.97      0.95      3303

    accuracy                           0.95      6925
   macro avg       0.95      0.95      0.95      6925
weighted avg       0.95      0.95      0.95      6925

              precision    recall  f1-score   support

           0       0.97      0.93      0.95      3622
           1       0.93      0.97      0.95      3303

    accuracy                           0.95      6925
   macro avg       0.95      0.95      0.95      6925
weighted avg       0.95      0.95      0.95      6925

completed
