[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '742a4766'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f1ff2621'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a2a5d1f2'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '5bffed63'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: LPS_0.100_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_0.100_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_0.100_DMSO_0.025' 'Thapsigargin_1.000_DMSO_0.025']
The dimensions of the data are: (287053, 1270)
Number of total missing values across all columns: 574106
Data Subset Is Off
Wells held out for testing: ['C08' 'K06']
Wells to use for training, validation, and testing ['C02' 'C03' 'C09' 'D06' 'D07' 'K07']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.660173).  Saving model ...
	 Train_Loss: 0.6869 Train_Acc: 51.489 Val_Loss: 0.6602  BEST VAL Loss: 0.6602  Val_Acc: 51.501

Epoch 1: Validation loss decreased (0.660173 --> 0.640853).  Saving model ...
	 Train_Loss: 0.6693 Train_Acc: 52.068 Val_Loss: 0.6409  BEST VAL Loss: 0.6409  Val_Acc: 52.263

Epoch 2: Validation loss decreased (0.640853 --> 0.612297).  Saving model ...
	 Train_Loss: 0.6484 Train_Acc: 63.981 Val_Loss: 0.6123  BEST VAL Loss: 0.6123  Val_Acc: 73.302

Epoch 3: Validation loss decreased (0.612297 --> 0.590417).  Saving model ...
	 Train_Loss: 0.6292 Train_Acc: 70.233 Val_Loss: 0.5904  BEST VAL Loss: 0.5904  Val_Acc: 74.804

Epoch 4: Validation loss decreased (0.590417 --> 0.573762).  Saving model ...
	 Train_Loss: 0.6147 Train_Acc: 71.865 Val_Loss: 0.5738  BEST VAL Loss: 0.5738  Val_Acc: 76.328

Epoch 5: Validation loss decreased (0.573762 --> 0.560376).  Saving model ...
	 Train_Loss: 0.6025 Train_Acc: 73.213 Val_Loss: 0.5604  BEST VAL Loss: 0.5604  Val_Acc: 77.282

Epoch 6: Validation loss decreased (0.560376 --> 0.548995).  Saving model ...
	 Train_Loss: 0.5921 Train_Acc: 74.605 Val_Loss: 0.5490  BEST VAL Loss: 0.5490  Val_Acc: 77.918

Epoch 7: Validation loss decreased (0.548995 --> 0.539454).  Saving model ...
	 Train_Loss: 0.5830 Train_Acc: 76.079 Val_Loss: 0.5395  BEST VAL Loss: 0.5395  Val_Acc: 78.306

Epoch 8: Validation loss decreased (0.539454 --> 0.530955).  Saving model ...
	 Train_Loss: 0.5750 Train_Acc: 76.596 Val_Loss: 0.5310  BEST VAL Loss: 0.5310  Val_Acc: 79.227

Epoch 9: Validation loss decreased (0.530955 --> 0.523419).  Saving model ...
	 Train_Loss: 0.5677 Train_Acc: 77.275 Val_Loss: 0.5234  BEST VAL Loss: 0.5234  Val_Acc: 79.536

Epoch 10: Validation loss decreased (0.523419 --> 0.516522).  Saving model ...
	 Train_Loss: 0.5613 Train_Acc: 77.678 Val_Loss: 0.5165  BEST VAL Loss: 0.5165  Val_Acc: 79.882

Epoch 11: Validation loss decreased (0.516522 --> 0.510308).  Saving model ...
	 Train_Loss: 0.5555 Train_Acc: 77.985 Val_Loss: 0.5103  BEST VAL Loss: 0.5103  Val_Acc: 80.181

Epoch 12: Validation loss decreased (0.510308 --> 0.504634).  Saving model ...
	 Train_Loss: 0.5503 Train_Acc: 78.170 Val_Loss: 0.5046  BEST VAL Loss: 0.5046  Val_Acc: 80.570

Epoch 13: Validation loss decreased (0.504634 --> 0.499297).  Saving model ...
	 Train_Loss: 0.5454 Train_Acc: 78.560 Val_Loss: 0.4993  BEST VAL Loss: 0.4993  Val_Acc: 81.033

Epoch 14: Validation loss decreased (0.499297 --> 0.494611).  Saving model ...
	 Train_Loss: 0.5408 Train_Acc: 78.820 Val_Loss: 0.4946  BEST VAL Loss: 0.4946  Val_Acc: 81.140

Epoch 15: Validation loss decreased (0.494611 --> 0.490225).  Saving model ...
	 Train_Loss: 0.5367 Train_Acc: 78.994 Val_Loss: 0.4902  BEST VAL Loss: 0.4902  Val_Acc: 81.135

Epoch 16: Validation loss decreased (0.490225 --> 0.486067).  Saving model ...
	 Train_Loss: 0.5327 Train_Acc: 79.226 Val_Loss: 0.4861  BEST VAL Loss: 0.4861  Val_Acc: 81.510

Epoch 17: Validation loss decreased (0.486067 --> 0.482046).  Saving model ...
	 Train_Loss: 0.5290 Train_Acc: 79.417 Val_Loss: 0.4820  BEST VAL Loss: 0.4820  Val_Acc: 81.566

Epoch 18: Validation loss decreased (0.482046 --> 0.478329).  Saving model ...
	 Train_Loss: 0.5255 Train_Acc: 79.609 Val_Loss: 0.4783  BEST VAL Loss: 0.4783  Val_Acc: 81.926

Epoch 19: Validation loss decreased (0.478329 --> 0.475069).  Saving model ...
	 Train_Loss: 0.5222 Train_Acc: 79.818 Val_Loss: 0.4751  BEST VAL Loss: 0.4751  Val_Acc: 81.973

Epoch 20: Validation loss decreased (0.475069 --> 0.471897).  Saving model ...
	 Train_Loss: 0.5192 Train_Acc: 79.711 Val_Loss: 0.4719  BEST VAL Loss: 0.4719  Val_Acc: 81.940

Epoch 21: Validation loss decreased (0.471897 --> 0.468945).  Saving model ...
	 Train_Loss: 0.5163 Train_Acc: 79.847 Val_Loss: 0.4689  BEST VAL Loss: 0.4689  Val_Acc: 82.141

Epoch 22: Validation loss decreased (0.468945 --> 0.466053).  Saving model ...
	 Train_Loss: 0.5136 Train_Acc: 79.839 Val_Loss: 0.4661  BEST VAL Loss: 0.4661  Val_Acc: 81.996

Epoch 23: Validation loss decreased (0.466053 --> 0.463472).  Saving model ...
	 Train_Loss: 0.5109 Train_Acc: 80.109 Val_Loss: 0.4635  BEST VAL Loss: 0.4635  Val_Acc: 82.258

Epoch 24: Validation loss decreased (0.463472 --> 0.460902).  Saving model ...
	 Train_Loss: 0.5084 Train_Acc: 80.106 Val_Loss: 0.4609  BEST VAL Loss: 0.4609  Val_Acc: 82.473

Epoch 25: Validation loss decreased (0.460902 --> 0.458723).  Saving model ...
	 Train_Loss: 0.5061 Train_Acc: 80.309 Val_Loss: 0.4587  BEST VAL Loss: 0.4587  Val_Acc: 82.393

Epoch 26: Validation loss decreased (0.458723 --> 0.456419).  Saving model ...
	 Train_Loss: 0.5039 Train_Acc: 80.238 Val_Loss: 0.4564  BEST VAL Loss: 0.4564  Val_Acc: 82.473

Epoch 27: Validation loss decreased (0.456419 --> 0.454268).  Saving model ...
	 Train_Loss: 0.5018 Train_Acc: 80.544 Val_Loss: 0.4543  BEST VAL Loss: 0.4543  Val_Acc: 82.440

Epoch 28: Validation loss decreased (0.454268 --> 0.452141).  Saving model ...
	 Train_Loss: 0.4997 Train_Acc: 80.574 Val_Loss: 0.4521  BEST VAL Loss: 0.4521  Val_Acc: 82.585

Epoch 29: Validation loss decreased (0.452141 --> 0.450105).  Saving model ...
	 Train_Loss: 0.4978 Train_Acc: 80.414 Val_Loss: 0.4501  BEST VAL Loss: 0.4501  Val_Acc: 82.459

Epoch 30: Validation loss decreased (0.450105 --> 0.448138).  Saving model ...
	 Train_Loss: 0.4959 Train_Acc: 80.696 Val_Loss: 0.4481  BEST VAL Loss: 0.4481  Val_Acc: 82.618

Epoch 31: Validation loss decreased (0.448138 --> 0.446401).  Saving model ...
	 Train_Loss: 0.4942 Train_Acc: 80.624 Val_Loss: 0.4464  BEST VAL Loss: 0.4464  Val_Acc: 82.454

Epoch 32: Validation loss decreased (0.446401 --> 0.444753).  Saving model ...
	 Train_Loss: 0.4925 Train_Acc: 80.751 Val_Loss: 0.4448  BEST VAL Loss: 0.4448  Val_Acc: 82.510

Epoch 33: Validation loss decreased (0.444753 --> 0.443198).  Saving model ...
	 Train_Loss: 0.4909 Train_Acc: 80.731 Val_Loss: 0.4432  BEST VAL Loss: 0.4432  Val_Acc: 82.449

Epoch 34: Validation loss decreased (0.443198 --> 0.441617).  Saving model ...
	 Train_Loss: 0.4893 Train_Acc: 80.729 Val_Loss: 0.4416  BEST VAL Loss: 0.4416  Val_Acc: 83.062

Epoch 35: Validation loss decreased (0.441617 --> 0.440143).  Saving model ...
	 Train_Loss: 0.4878 Train_Acc: 80.847 Val_Loss: 0.4401  BEST VAL Loss: 0.4401  Val_Acc: 82.688

Epoch 36: Validation loss decreased (0.440143 --> 0.438814).  Saving model ...
	 Train_Loss: 0.4864 Train_Acc: 80.745 Val_Loss: 0.4388  BEST VAL Loss: 0.4388  Val_Acc: 82.520

Epoch 37: Validation loss decreased (0.438814 --> 0.437410).  Saving model ...
	 Train_Loss: 0.4851 Train_Acc: 80.748 Val_Loss: 0.4374  BEST VAL Loss: 0.4374  Val_Acc: 82.796

Epoch 38: Validation loss decreased (0.437410 --> 0.436093).  Saving model ...
	 Train_Loss: 0.4837 Train_Acc: 80.904 Val_Loss: 0.4361  BEST VAL Loss: 0.4361  Val_Acc: 82.997

Epoch 39: Validation loss decreased (0.436093 --> 0.434885).  Saving model ...
	 Train_Loss: 0.4824 Train_Acc: 80.991 Val_Loss: 0.4349  BEST VAL Loss: 0.4349  Val_Acc: 82.824

Epoch 40: Validation loss decreased (0.434885 --> 0.433661).  Saving model ...
	 Train_Loss: 0.4812 Train_Acc: 80.965 Val_Loss: 0.4337  BEST VAL Loss: 0.4337  Val_Acc: 82.931

Epoch 41: Validation loss decreased (0.433661 --> 0.432556).  Saving model ...
	 Train_Loss: 0.4800 Train_Acc: 81.023 Val_Loss: 0.4326  BEST VAL Loss: 0.4326  Val_Acc: 82.992

Epoch 42: Validation loss decreased (0.432556 --> 0.431386).  Saving model ...
	 Train_Loss: 0.4788 Train_Acc: 81.148 Val_Loss: 0.4314  BEST VAL Loss: 0.4314  Val_Acc: 83.067

Epoch 43: Validation loss decreased (0.431386 --> 0.430281).  Saving model ...
	 Train_Loss: 0.4777 Train_Acc: 81.221 Val_Loss: 0.4303  BEST VAL Loss: 0.4303  Val_Acc: 82.997

Epoch 44: Validation loss decreased (0.430281 --> 0.429309).  Saving model ...
	 Train_Loss: 0.4767 Train_Acc: 81.020 Val_Loss: 0.4293  BEST VAL Loss: 0.4293  Val_Acc: 82.997

Epoch 45: Validation loss decreased (0.429309 --> 0.428332).  Saving model ...
	 Train_Loss: 0.4756 Train_Acc: 81.297 Val_Loss: 0.4283  BEST VAL Loss: 0.4283  Val_Acc: 83.100

Epoch 46: Validation loss decreased (0.428332 --> 0.427315).  Saving model ...
	 Train_Loss: 0.4746 Train_Acc: 81.331 Val_Loss: 0.4273  BEST VAL Loss: 0.4273  Val_Acc: 83.254

Epoch 47: Validation loss decreased (0.427315 --> 0.426327).  Saving model ...
	 Train_Loss: 0.4736 Train_Acc: 81.349 Val_Loss: 0.4263  BEST VAL Loss: 0.4263  Val_Acc: 83.357

Epoch 48: Validation loss decreased (0.426327 --> 0.425346).  Saving model ...
	 Train_Loss: 0.4726 Train_Acc: 81.247 Val_Loss: 0.4253  BEST VAL Loss: 0.4253  Val_Acc: 83.464

Epoch 49: Validation loss decreased (0.425346 --> 0.424462).  Saving model ...
	 Train_Loss: 0.4717 Train_Acc: 81.279 Val_Loss: 0.4245  BEST VAL Loss: 0.4245  Val_Acc: 83.366

Epoch 50: Validation loss decreased (0.424462 --> 0.423610).  Saving model ...
	 Train_Loss: 0.4708 Train_Acc: 81.135 Val_Loss: 0.4236  BEST VAL Loss: 0.4236  Val_Acc: 83.114

Epoch 51: Validation loss decreased (0.423610 --> 0.422788).  Saving model ...
	 Train_Loss: 0.4700 Train_Acc: 81.190 Val_Loss: 0.4228  BEST VAL Loss: 0.4228  Val_Acc: 83.221

Epoch 52: Validation loss decreased (0.422788 --> 0.422007).  Saving model ...
	 Train_Loss: 0.4692 Train_Acc: 81.305 Val_Loss: 0.4220  BEST VAL Loss: 0.4220  Val_Acc: 83.296

Epoch 53: Validation loss decreased (0.422007 --> 0.421233).  Saving model ...
	 Train_Loss: 0.4684 Train_Acc: 81.404 Val_Loss: 0.4212  BEST VAL Loss: 0.4212  Val_Acc: 83.329

Epoch 54: Validation loss decreased (0.421233 --> 0.420467).  Saving model ...
	 Train_Loss: 0.4675 Train_Acc: 81.450 Val_Loss: 0.4205  BEST VAL Loss: 0.4205  Val_Acc: 83.469

Epoch 55: Validation loss decreased (0.420467 --> 0.419760).  Saving model ...
	 Train_Loss: 0.4668 Train_Acc: 81.357 Val_Loss: 0.4198  BEST VAL Loss: 0.4198  Val_Acc: 83.104

Epoch 56: Validation loss decreased (0.419760 --> 0.419032).  Saving model ...
	 Train_Loss: 0.4660 Train_Acc: 81.468 Val_Loss: 0.4190  BEST VAL Loss: 0.4190  Val_Acc: 83.385

Epoch 57: Validation loss decreased (0.419032 --> 0.418390).  Saving model ...
	 Train_Loss: 0.4653 Train_Acc: 81.411 Val_Loss: 0.4184  BEST VAL Loss: 0.4184  Val_Acc: 83.357

Epoch 58: Validation loss decreased (0.418390 --> 0.417674).  Saving model ...
	 Train_Loss: 0.4646 Train_Acc: 81.394 Val_Loss: 0.4177  BEST VAL Loss: 0.4177  Val_Acc: 83.492

Epoch 59: Validation loss decreased (0.417674 --> 0.416969).  Saving model ...
	 Train_Loss: 0.4639 Train_Acc: 81.448 Val_Loss: 0.4170  BEST VAL Loss: 0.4170  Val_Acc: 83.661

Epoch 60: Validation loss decreased (0.416969 --> 0.416295).  Saving model ...
	 Train_Loss: 0.4632 Train_Acc: 81.573 Val_Loss: 0.4163  BEST VAL Loss: 0.4163  Val_Acc: 83.511

Epoch 61: Validation loss decreased (0.416295 --> 0.415685).  Saving model ...
	 Train_Loss: 0.4625 Train_Acc: 81.533 Val_Loss: 0.4157  BEST VAL Loss: 0.4157  Val_Acc: 83.380

Epoch 62: Validation loss decreased (0.415685 --> 0.415066).  Saving model ...
	 Train_Loss: 0.4619 Train_Acc: 81.435 Val_Loss: 0.4151  BEST VAL Loss: 0.4151  Val_Acc: 83.628

Epoch 63: Validation loss decreased (0.415066 --> 0.414450).  Saving model ...
	 Train_Loss: 0.4612 Train_Acc: 81.577 Val_Loss: 0.4145  BEST VAL Loss: 0.4145  Val_Acc: 83.675

Epoch 64: Validation loss decreased (0.414450 --> 0.413906).  Saving model ...
	 Train_Loss: 0.4606 Train_Acc: 81.447 Val_Loss: 0.4139  BEST VAL Loss: 0.4139  Val_Acc: 83.357

Epoch 65: Validation loss decreased (0.413906 --> 0.413344).  Saving model ...
	 Train_Loss: 0.4600 Train_Acc: 81.460 Val_Loss: 0.4133  BEST VAL Loss: 0.4133  Val_Acc: 83.553

Epoch 66: Validation loss decreased (0.413344 --> 0.412787).  Saving model ...
	 Train_Loss: 0.4594 Train_Acc: 81.503 Val_Loss: 0.4128  BEST VAL Loss: 0.4128  Val_Acc: 83.577

Epoch 67: Validation loss decreased (0.412787 --> 0.412234).  Saving model ...
	 Train_Loss: 0.4588 Train_Acc: 81.686 Val_Loss: 0.4122  BEST VAL Loss: 0.4122  Val_Acc: 83.520

Epoch 68: Validation loss decreased (0.412234 --> 0.411749).  Saving model ...
	 Train_Loss: 0.4583 Train_Acc: 81.537 Val_Loss: 0.4117  BEST VAL Loss: 0.4117  Val_Acc: 83.586

Epoch 69: Validation loss decreased (0.411749 --> 0.411249).  Saving model ...
	 Train_Loss: 0.4577 Train_Acc: 81.691 Val_Loss: 0.4112  BEST VAL Loss: 0.4112  Val_Acc: 83.703

Epoch 70: Validation loss decreased (0.411249 --> 0.410702).  Saving model ...
	 Train_Loss: 0.4571 Train_Acc: 81.698 Val_Loss: 0.4107  BEST VAL Loss: 0.4107  Val_Acc: 83.782

Epoch 71: Validation loss decreased (0.410702 --> 0.410233).  Saving model ...
	 Train_Loss: 0.4566 Train_Acc: 81.628 Val_Loss: 0.4102  BEST VAL Loss: 0.4102  Val_Acc: 83.642

Epoch 72: Validation loss decreased (0.410233 --> 0.409748).  Saving model ...
	 Train_Loss: 0.4561 Train_Acc: 81.776 Val_Loss: 0.4097  BEST VAL Loss: 0.4097  Val_Acc: 83.913

Epoch 73: Validation loss decreased (0.409748 --> 0.409289).  Saving model ...
	 Train_Loss: 0.4555 Train_Acc: 81.712 Val_Loss: 0.4093  BEST VAL Loss: 0.4093  Val_Acc: 83.614

Epoch 74: Validation loss decreased (0.409289 --> 0.408804).  Saving model ...
	 Train_Loss: 0.4550 Train_Acc: 81.755 Val_Loss: 0.4088  BEST VAL Loss: 0.4088  Val_Acc: 83.857

Epoch 75: Validation loss decreased (0.408804 --> 0.408330).  Saving model ...
	 Train_Loss: 0.4545 Train_Acc: 81.774 Val_Loss: 0.4083  BEST VAL Loss: 0.4083  Val_Acc: 83.871

Epoch 76: Validation loss decreased (0.408330 --> 0.407886).  Saving model ...
	 Train_Loss: 0.4540 Train_Acc: 81.937 Val_Loss: 0.4079  BEST VAL Loss: 0.4079  Val_Acc: 83.866

Epoch 77: Validation loss decreased (0.407886 --> 0.407444).  Saving model ...
	 Train_Loss: 0.4535 Train_Acc: 81.872 Val_Loss: 0.4074  BEST VAL Loss: 0.4074  Val_Acc: 83.717

Epoch 78: Validation loss decreased (0.407444 --> 0.406992).  Saving model ...
	 Train_Loss: 0.4531 Train_Acc: 81.749 Val_Loss: 0.4070  BEST VAL Loss: 0.4070  Val_Acc: 83.852

Epoch 79: Validation loss decreased (0.406992 --> 0.406586).  Saving model ...
	 Train_Loss: 0.4526 Train_Acc: 81.878 Val_Loss: 0.4066  BEST VAL Loss: 0.4066  Val_Acc: 83.801

Epoch 80: Validation loss decreased (0.406586 --> 0.406189).  Saving model ...
	 Train_Loss: 0.4521 Train_Acc: 81.909 Val_Loss: 0.4062  BEST VAL Loss: 0.4062  Val_Acc: 83.764

Epoch 81: Validation loss decreased (0.406189 --> 0.405786).  Saving model ...
	 Train_Loss: 0.4517 Train_Acc: 81.747 Val_Loss: 0.4058  BEST VAL Loss: 0.4058  Val_Acc: 83.885

Epoch 82: Validation loss decreased (0.405786 --> 0.405418).  Saving model ...
	 Train_Loss: 0.4512 Train_Acc: 81.858 Val_Loss: 0.4054  BEST VAL Loss: 0.4054  Val_Acc: 83.838

Epoch 83: Validation loss decreased (0.405418 --> 0.405020).  Saving model ...
	 Train_Loss: 0.4508 Train_Acc: 81.781 Val_Loss: 0.4050  BEST VAL Loss: 0.4050  Val_Acc: 83.955

Epoch 84: Validation loss decreased (0.405020 --> 0.404604).  Saving model ...
	 Train_Loss: 0.4504 Train_Acc: 81.867 Val_Loss: 0.4046  BEST VAL Loss: 0.4046  Val_Acc: 84.212

Epoch 85: Validation loss decreased (0.404604 --> 0.404248).  Saving model ...
	 Train_Loss: 0.4500 Train_Acc: 81.775 Val_Loss: 0.4042  BEST VAL Loss: 0.4042  Val_Acc: 83.862

Epoch 86: Validation loss decreased (0.404248 --> 0.403848).  Saving model ...
	 Train_Loss: 0.4496 Train_Acc: 81.898 Val_Loss: 0.4038  BEST VAL Loss: 0.4038  Val_Acc: 83.885

Epoch 87: Validation loss decreased (0.403848 --> 0.403515).  Saving model ...
	 Train_Loss: 0.4492 Train_Acc: 81.891 Val_Loss: 0.4035  BEST VAL Loss: 0.4035  Val_Acc: 83.993

Epoch 88: Validation loss decreased (0.403515 --> 0.403182).  Saving model ...
	 Train_Loss: 0.4488 Train_Acc: 81.861 Val_Loss: 0.4032  BEST VAL Loss: 0.4032  Val_Acc: 84.128

Epoch 89: Validation loss decreased (0.403182 --> 0.402790).  Saving model ...
	 Train_Loss: 0.4484 Train_Acc: 81.986 Val_Loss: 0.4028  BEST VAL Loss: 0.4028  Val_Acc: 84.152

Epoch 90: Validation loss decreased (0.402790 --> 0.402404).  Saving model ...
	 Train_Loss: 0.4480 Train_Acc: 81.856 Val_Loss: 0.4024  BEST VAL Loss: 0.4024  Val_Acc: 84.306

Epoch 91: Validation loss decreased (0.402404 --> 0.402090).  Saving model ...
	 Train_Loss: 0.4476 Train_Acc: 81.996 Val_Loss: 0.4021  BEST VAL Loss: 0.4021  Val_Acc: 83.890

Epoch 92: Validation loss decreased (0.402090 --> 0.401729).  Saving model ...
	 Train_Loss: 0.4472 Train_Acc: 82.043 Val_Loss: 0.4017  BEST VAL Loss: 0.4017  Val_Acc: 84.096

Epoch 93: Validation loss decreased (0.401729 --> 0.401418).  Saving model ...
	 Train_Loss: 0.4469 Train_Acc: 82.019 Val_Loss: 0.4014  BEST VAL Loss: 0.4014  Val_Acc: 83.918

Epoch 94: Validation loss decreased (0.401418 --> 0.401090).  Saving model ...
	 Train_Loss: 0.4465 Train_Acc: 82.057 Val_Loss: 0.4011  BEST VAL Loss: 0.4011  Val_Acc: 84.068

Epoch 95: Validation loss decreased (0.401090 --> 0.400786).  Saving model ...
	 Train_Loss: 0.4462 Train_Acc: 81.958 Val_Loss: 0.4008  BEST VAL Loss: 0.4008  Val_Acc: 84.039

Epoch 96: Validation loss decreased (0.400786 --> 0.400528).  Saving model ...
	 Train_Loss: 0.4458 Train_Acc: 82.046 Val_Loss: 0.4005  BEST VAL Loss: 0.4005  Val_Acc: 83.815

Epoch 97: Validation loss decreased (0.400528 --> 0.400232).  Saving model ...
	 Train_Loss: 0.4455 Train_Acc: 81.937 Val_Loss: 0.4002  BEST VAL Loss: 0.4002  Val_Acc: 83.895

Epoch 98: Validation loss decreased (0.400232 --> 0.399922).  Saving model ...
	 Train_Loss: 0.4452 Train_Acc: 81.923 Val_Loss: 0.3999  BEST VAL Loss: 0.3999  Val_Acc: 84.147

Epoch 99: Validation loss decreased (0.399922 --> 0.399637).  Saving model ...
	 Train_Loss: 0.4448 Train_Acc: 82.043 Val_Loss: 0.3996  BEST VAL Loss: 0.3996  Val_Acc: 83.988

Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.51      0.50     82968
           1       0.51      0.49      0.50     88098

    accuracy                           0.50    171066
   macro avg       0.50      0.50      0.50    171066
weighted avg       0.50      0.50      0.50    171066

Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.51      0.50     10371
           1       0.51      0.49      0.50     11013

    accuracy                           0.50     21384
   macro avg       0.50      0.50      0.50     21384
weighted avg       0.50      0.50      0.50     21384

Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.51      0.49     10371
           1       0.51      0.49      0.50     11013

    accuracy                           0.50     21384
   macro avg       0.50      0.50      0.50     21384
weighted avg       0.50      0.50      0.50     21384

              precision    recall  f1-score   support

           0       0.48      0.51      0.49     10371
           1       0.51      0.49      0.50     11013

    accuracy                           0.50     21384
   macro avg       0.50      0.50      0.50     21384
weighted avg       0.50      0.50      0.50     21384

Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_1.000_DMSO_0.025_vs_LPS_0.100_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.53      0.50     34887
           1       0.53      0.48      0.50     38332

    accuracy                           0.50     73219
   macro avg       0.50      0.50      0.50     73219
weighted avg       0.50      0.50      0.50     73219

              precision    recall  f1-score   support

           0       0.48      0.53      0.50     34887
           1       0.53      0.48      0.50     38332

    accuracy                           0.50     73219
   macro avg       0.50      0.50      0.50     73219
weighted avg       0.50      0.50      0.50     73219

completed
