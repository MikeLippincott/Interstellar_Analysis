[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '95d89a6a'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '8c510032'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '5dcc3388'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '86768d4f'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (28168, 1276)
Number of total missing values across all columns: 27532
Data Subset Is Off
Wells held out for testing: ['D14' 'M20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'M16' 'M17' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.213611).  Saving model ...
	 Train_Loss: 0.4211 Train_Acc: 84.694 Val_Loss: 0.2136  BEST VAL Loss: 0.2136  Val_Acc: 93.153

Epoch 1: Validation loss decreased (0.213611 --> 0.160275).  Saving model ...
	 Train_Loss: 0.3056 Train_Acc: 93.319 Val_Loss: 0.1603  BEST VAL Loss: 0.1603  Val_Acc: 95.435

Epoch 2: Validation loss decreased (0.160275 --> 0.146552).  Saving model ...
	 Train_Loss: 0.2518 Train_Acc: 95.096 Val_Loss: 0.1466  BEST VAL Loss: 0.1466  Val_Acc: 96.481

Epoch 3: Validation loss decreased (0.146552 --> 0.132859).  Saving model ...
	 Train_Loss: 0.2224 Train_Acc: 95.542 Val_Loss: 0.1329  BEST VAL Loss: 0.1329  Val_Acc: 96.671

Epoch 4: Validation loss decreased (0.132859 --> 0.129892).  Saving model ...
	 Train_Loss: 0.2019 Train_Acc: 96.267 Val_Loss: 0.1299  BEST VAL Loss: 0.1299  Val_Acc: 96.862

Epoch 5: Validation loss decreased (0.129892 --> 0.122632).  Saving model ...
	 Train_Loss: 0.1876 Train_Acc: 96.463 Val_Loss: 0.1226  BEST VAL Loss: 0.1226  Val_Acc: 97.194

Epoch 6: Validation loss decreased (0.122632 --> 0.118944).  Saving model ...
	 Train_Loss: 0.1756 Train_Acc: 96.582 Val_Loss: 0.1189  BEST VAL Loss: 0.1189  Val_Acc: 96.767

Epoch 7: Validation loss decreased (0.118944 --> 0.114902).  Saving model ...
	 Train_Loss: 0.1657 Train_Acc: 96.868 Val_Loss: 0.1149  BEST VAL Loss: 0.1149  Val_Acc: 97.052

Epoch 8: Validation loss decreased (0.114902 --> 0.112708).  Saving model ...
	 Train_Loss: 0.1577 Train_Acc: 97.171 Val_Loss: 0.1127  BEST VAL Loss: 0.1127  Val_Acc: 96.671

Epoch 9: Validation loss decreased (0.112708 --> 0.110445).  Saving model ...
	 Train_Loss: 0.1517 Train_Acc: 97.064 Val_Loss: 0.1104  BEST VAL Loss: 0.1104  Val_Acc: 96.814

Epoch 10: Validation loss decreased (0.110445 --> 0.106942).  Saving model ...
	 Train_Loss: 0.1466 Train_Acc: 96.945 Val_Loss: 0.1069  BEST VAL Loss: 0.1069  Val_Acc: 97.052

Epoch 11: Validation loss decreased (0.106942 --> 0.104194).  Saving model ...
	 Train_Loss: 0.1429 Train_Acc: 96.689 Val_Loss: 0.1042  BEST VAL Loss: 0.1042  Val_Acc: 96.862

Epoch 12: Validation loss decreased (0.104194 --> 0.101421).  Saving model ...
	 Train_Loss: 0.1391 Train_Acc: 97.034 Val_Loss: 0.1014  BEST VAL Loss: 0.1014  Val_Acc: 97.194

Epoch 13: Validation loss decreased (0.101421 --> 0.098810).  Saving model ...
	 Train_Loss: 0.1349 Train_Acc: 97.563 Val_Loss: 0.0988  BEST VAL Loss: 0.0988  Val_Acc: 97.242

Epoch 14: Validation loss decreased (0.098810 --> 0.097490).  Saving model ...
	 Train_Loss: 0.1309 Train_Acc: 97.747 Val_Loss: 0.0975  BEST VAL Loss: 0.0975  Val_Acc: 97.527

Epoch 15: Validation loss decreased (0.097490 --> 0.097315).  Saving model ...
	 Train_Loss: 0.1275 Train_Acc: 97.961 Val_Loss: 0.0973  BEST VAL Loss: 0.0973  Val_Acc: 97.622

Epoch 16: Validation loss decreased (0.097315 --> 0.095992).  Saving model ...
	 Train_Loss: 0.1242 Train_Acc: 97.830 Val_Loss: 0.0960  BEST VAL Loss: 0.0960  Val_Acc: 97.385

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.1212 Train_Acc: 97.943 Val_Loss: 0.0967  BEST VAL Loss: 0.0960  Val_Acc: 97.099

Epoch 18: Validation loss decreased (0.095992 --> 0.095801).  Saving model ...
	 Train_Loss: 0.1190 Train_Acc: 97.640 Val_Loss: 0.0958  BEST VAL Loss: 0.0958  Val_Acc: 97.527

Epoch 19: Validation loss decreased (0.095801 --> 0.095703).  Saving model ...
	 Train_Loss: 0.1164 Train_Acc: 98.015 Val_Loss: 0.0957  BEST VAL Loss: 0.0957  Val_Acc: 97.527

Epoch 20: Validation loss decreased (0.095703 --> 0.095534).  Saving model ...
	 Train_Loss: 0.1141 Train_Acc: 97.932 Val_Loss: 0.0955  BEST VAL Loss: 0.0955  Val_Acc: 97.337

Epoch 21: Validation loss decreased (0.095534 --> 0.094625).  Saving model ...
	 Train_Loss: 0.1122 Train_Acc: 97.878 Val_Loss: 0.0946  BEST VAL Loss: 0.0946  Val_Acc: 97.432

Epoch 22: Validation loss decreased (0.094625 --> 0.094470).  Saving model ...
	 Train_Loss: 0.1105 Train_Acc: 97.813 Val_Loss: 0.0945  BEST VAL Loss: 0.0945  Val_Acc: 97.242

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1085 Train_Acc: 98.163 Val_Loss: 0.0946  BEST VAL Loss: 0.0945  Val_Acc: 97.099

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1072 Train_Acc: 97.670 Val_Loss: 0.0958  BEST VAL Loss: 0.0945  Val_Acc: 97.670

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1058 Train_Acc: 97.914 Val_Loss: 0.0950  BEST VAL Loss: 0.0945  Val_Acc: 97.337

Epoch 26: Validation loss decreased (0.094470 --> 0.094451).  Saving model ...
	 Train_Loss: 0.1046 Train_Acc: 97.706 Val_Loss: 0.0945  BEST VAL Loss: 0.0945  Val_Acc: 97.718

Epoch 27: Validation loss decreased (0.094451 --> 0.093527).  Saving model ...
	 Train_Loss: 0.1034 Train_Acc: 98.044 Val_Loss: 0.0935  BEST VAL Loss: 0.0935  Val_Acc: 97.337

Epoch 28: Validation loss decreased (0.093527 --> 0.093527).  Saving model ...
	 Train_Loss: 0.1020 Train_Acc: 98.146 Val_Loss: 0.0935  BEST VAL Loss: 0.0935  Val_Acc: 97.575

Epoch 29: Validation loss decreased (0.093527 --> 0.092492).  Saving model ...
	 Train_Loss: 0.1006 Train_Acc: 98.354 Val_Loss: 0.0925  BEST VAL Loss: 0.0925  Val_Acc: 97.385

Epoch 30: Validation loss decreased (0.092492 --> 0.091978).  Saving model ...
	 Train_Loss: 0.0991 Train_Acc: 98.514 Val_Loss: 0.0920  BEST VAL Loss: 0.0920  Val_Acc: 97.860

Epoch 31: Validation loss decreased (0.091978 --> 0.091454).  Saving model ...
	 Train_Loss: 0.0981 Train_Acc: 98.134 Val_Loss: 0.0915  BEST VAL Loss: 0.0915  Val_Acc: 97.718

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0971 Train_Acc: 98.199 Val_Loss: 0.0932  BEST VAL Loss: 0.0915  Val_Acc: 97.860

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0960 Train_Acc: 98.217 Val_Loss: 0.0929  BEST VAL Loss: 0.0915  Val_Acc: 97.718

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0948 Train_Acc: 98.556 Val_Loss: 0.0927  BEST VAL Loss: 0.0915  Val_Acc: 97.718

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.0935 Train_Acc: 98.591 Val_Loss: 0.0927  BEST VAL Loss: 0.0915  Val_Acc: 97.908

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.0924 Train_Acc: 98.431 Val_Loss: 0.0948  BEST VAL Loss: 0.0915  Val_Acc: 97.813

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.0915 Train_Acc: 98.395 Val_Loss: 0.0964  BEST VAL Loss: 0.0915  Val_Acc: 97.860

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0907 Train_Acc: 98.187 Val_Loss: 0.0971  BEST VAL Loss: 0.0915  Val_Acc: 97.575

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0899 Train_Acc: 98.437 Val_Loss: 0.0978  BEST VAL Loss: 0.0915  Val_Acc: 97.860

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0891 Train_Acc: 98.431 Val_Loss: 0.0975  BEST VAL Loss: 0.0915  Val_Acc: 97.622

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0882 Train_Acc: 98.556 Val_Loss: 0.0967  BEST VAL Loss: 0.0915  Val_Acc: 97.955

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0873 Train_Acc: 98.728 Val_Loss: 0.0967  BEST VAL Loss: 0.0915  Val_Acc: 97.908

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0864 Train_Acc: 98.793 Val_Loss: 0.0972  BEST VAL Loss: 0.0915  Val_Acc: 98.098

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.0854 Train_Acc: 98.995 Val_Loss: 0.0968  BEST VAL Loss: 0.0915  Val_Acc: 97.955

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.0846 Train_Acc: 98.532 Val_Loss: 0.0961  BEST VAL Loss: 0.0915  Val_Acc: 97.622

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.0841 Train_Acc: 98.395 Val_Loss: 0.0980  BEST VAL Loss: 0.0915  Val_Acc: 97.575

Epoch 47: Validation loss did not decrease
Early stopped at epoch : 47
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.52      0.51      8453
           1       0.50      0.49      0.50      8371

    accuracy                           0.50     16824
   macro avg       0.50      0.50      0.50     16824
weighted avg       0.50      0.50      0.50     16824

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.51      0.52      0.51      1057
           1       0.50      0.48      0.49      1046

    accuracy                           0.50      2103
   macro avg       0.50      0.50      0.50      2103
weighted avg       0.50      0.50      0.50      2103

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.50      0.51      0.51      1057
           1       0.50      0.49      0.50      1046

    accuracy                           0.50      2103
   macro avg       0.50      0.50      0.50      2103
weighted avg       0.50      0.50      0.50      2103

              precision    recall  f1-score   support

           0       0.50      0.51      0.51      1057
           1       0.50      0.49      0.50      1046

    accuracy                           0.50      2103
   macro avg       0.50      0.50      0.50      2103
weighted avg       0.50      0.50      0.50      2103

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.54      0.54      3835
           1       0.46      0.45      0.45      3303

    accuracy                           0.50      7138
   macro avg       0.50      0.50      0.50      7138
weighted avg       0.50      0.50      0.50      7138

              precision    recall  f1-score   support

           0       0.53      0.54      0.54      3835
           1       0.46      0.45      0.45      3303

    accuracy                           0.50      7138
   macro avg       0.50      0.50      0.50      7138
weighted avg       0.50      0.50      0.50      7138

completed
