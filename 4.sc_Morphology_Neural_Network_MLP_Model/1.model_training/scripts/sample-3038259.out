[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd6ea6fcb'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '30a433db'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '5b74299a'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'a7318c5c'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_10.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_10.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_10.000_DMSO_0.025']
The dimensions of the data are: (30022, 1276)
Number of total missing values across all columns: 60044
Data Subset Is Off
Wells held out for testing: ['E14' 'E20']
Wells to use for training, validation, and testing ['E15' 'E16' 'E17' 'E21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.354297).  Saving model ...
	 Train_Loss: 0.5002 Train_Acc: 73.091 Val_Loss: 0.3543  BEST VAL Loss: 0.3543  Val_Acc: 86.153

Epoch 1: Validation loss decreased (0.354297 --> 0.334377).  Saving model ...
	 Train_Loss: 0.4460 Train_Acc: 78.334 Val_Loss: 0.3344  BEST VAL Loss: 0.3344  Val_Acc: 88.736

Epoch 2: Validation loss decreased (0.334377 --> 0.307862).  Saving model ...
	 Train_Loss: 0.4098 Train_Acc: 85.627 Val_Loss: 0.3079  BEST VAL Loss: 0.3079  Val_Acc: 90.383

Epoch 3: Validation loss decreased (0.307862 --> 0.291376).  Saving model ...
	 Train_Loss: 0.3833 Train_Acc: 87.458 Val_Loss: 0.2914  BEST VAL Loss: 0.2914  Val_Acc: 90.962

Epoch 4: Validation loss decreased (0.291376 --> 0.277991).  Saving model ...
	 Train_Loss: 0.3640 Train_Acc: 88.065 Val_Loss: 0.2780  BEST VAL Loss: 0.2780  Val_Acc: 91.407

Epoch 5: Validation loss decreased (0.277991 --> 0.270433).  Saving model ...
	 Train_Loss: 0.3486 Train_Acc: 88.510 Val_Loss: 0.2704  BEST VAL Loss: 0.2704  Val_Acc: 91.496

Epoch 6: Validation loss decreased (0.270433 --> 0.265426).  Saving model ...
	 Train_Loss: 0.3361 Train_Acc: 88.933 Val_Loss: 0.2654  BEST VAL Loss: 0.2654  Val_Acc: 92.342

Epoch 7: Validation loss decreased (0.265426 --> 0.258338).  Saving model ...
	 Train_Loss: 0.3256 Train_Acc: 89.340 Val_Loss: 0.2583  BEST VAL Loss: 0.2583  Val_Acc: 92.164

Epoch 8: Validation loss decreased (0.258338 --> 0.253957).  Saving model ...
	 Train_Loss: 0.3167 Train_Acc: 89.668 Val_Loss: 0.2540  BEST VAL Loss: 0.2540  Val_Acc: 91.941

Epoch 9: Validation loss decreased (0.253957 --> 0.247607).  Saving model ...
	 Train_Loss: 0.3088 Train_Acc: 89.774 Val_Loss: 0.2476  BEST VAL Loss: 0.2476  Val_Acc: 92.431

Epoch 10: Validation loss decreased (0.247607 --> 0.244262).  Saving model ...
	 Train_Loss: 0.3023 Train_Acc: 89.780 Val_Loss: 0.2443  BEST VAL Loss: 0.2443  Val_Acc: 92.342

Epoch 11: Validation loss decreased (0.244262 --> 0.240410).  Saving model ...
	 Train_Loss: 0.2964 Train_Acc: 90.069 Val_Loss: 0.2404  BEST VAL Loss: 0.2404  Val_Acc: 93.232

Epoch 12: Validation loss decreased (0.240410 --> 0.238193).  Saving model ...
	 Train_Loss: 0.2905 Train_Acc: 90.898 Val_Loss: 0.2382  BEST VAL Loss: 0.2382  Val_Acc: 92.743

Epoch 13: Validation loss decreased (0.238193 --> 0.235611).  Saving model ...
	 Train_Loss: 0.2850 Train_Acc: 90.943 Val_Loss: 0.2356  BEST VAL Loss: 0.2356  Val_Acc: 92.431

Epoch 14: Validation loss decreased (0.235611 --> 0.233461).  Saving model ...
	 Train_Loss: 0.2804 Train_Acc: 90.726 Val_Loss: 0.2335  BEST VAL Loss: 0.2335  Val_Acc: 93.455

Epoch 15: Validation loss decreased (0.233461 --> 0.231338).  Saving model ...
	 Train_Loss: 0.2763 Train_Acc: 90.999 Val_Loss: 0.2313  BEST VAL Loss: 0.2313  Val_Acc: 92.609

Epoch 16: Validation loss decreased (0.231338 --> 0.229849).  Saving model ...
	 Train_Loss: 0.2728 Train_Acc: 90.676 Val_Loss: 0.2298  BEST VAL Loss: 0.2298  Val_Acc: 93.232

Epoch 17: Validation loss decreased (0.229849 --> 0.227842).  Saving model ...
	 Train_Loss: 0.2690 Train_Acc: 91.088 Val_Loss: 0.2278  BEST VAL Loss: 0.2278  Val_Acc: 93.678

Epoch 18: Validation loss decreased (0.227842 --> 0.226129).  Saving model ...
	 Train_Loss: 0.2655 Train_Acc: 91.077 Val_Loss: 0.2261  BEST VAL Loss: 0.2261  Val_Acc: 93.099

Epoch 19: Validation loss decreased (0.226129 --> 0.225381).  Saving model ...
	 Train_Loss: 0.2622 Train_Acc: 91.600 Val_Loss: 0.2254  BEST VAL Loss: 0.2254  Val_Acc: 93.099

Epoch 20: Validation loss decreased (0.225381 --> 0.224719).  Saving model ...
	 Train_Loss: 0.2595 Train_Acc: 91.238 Val_Loss: 0.2247  BEST VAL Loss: 0.2247  Val_Acc: 93.277

Epoch 21: Validation loss decreased (0.224719 --> 0.223072).  Saving model ...
	 Train_Loss: 0.2571 Train_Acc: 90.798 Val_Loss: 0.2231  BEST VAL Loss: 0.2231  Val_Acc: 93.232

Epoch 22: Validation loss decreased (0.223072 --> 0.222206).  Saving model ...
	 Train_Loss: 0.2544 Train_Acc: 91.533 Val_Loss: 0.2222  BEST VAL Loss: 0.2222  Val_Acc: 93.544

Epoch 23: Validation loss decreased (0.222206 --> 0.221350).  Saving model ...
	 Train_Loss: 0.2518 Train_Acc: 91.733 Val_Loss: 0.2214  BEST VAL Loss: 0.2214  Val_Acc: 93.678

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.2493 Train_Acc: 91.873 Val_Loss: 0.2214  BEST VAL Loss: 0.2214  Val_Acc: 93.099

Epoch 25: Validation loss decreased (0.221350 --> 0.220834).  Saving model ...
	 Train_Loss: 0.2470 Train_Acc: 91.928 Val_Loss: 0.2208  BEST VAL Loss: 0.2208  Val_Acc: 93.277

Epoch 26: Validation loss decreased (0.220834 --> 0.220214).  Saving model ...
	 Train_Loss: 0.2450 Train_Acc: 91.522 Val_Loss: 0.2202  BEST VAL Loss: 0.2202  Val_Acc: 93.054

Epoch 27: Validation loss decreased (0.220214 --> 0.219145).  Saving model ...
	 Train_Loss: 0.2433 Train_Acc: 91.633 Val_Loss: 0.2191  BEST VAL Loss: 0.2191  Val_Acc: 93.767

Epoch 28: Validation loss decreased (0.219145 --> 0.217993).  Saving model ...
	 Train_Loss: 0.2415 Train_Acc: 91.494 Val_Loss: 0.2180  BEST VAL Loss: 0.2180  Val_Acc: 93.054

Epoch 29: Validation loss decreased (0.217993 --> 0.217634).  Saving model ...
	 Train_Loss: 0.2396 Train_Acc: 91.912 Val_Loss: 0.2176  BEST VAL Loss: 0.2176  Val_Acc: 93.366

Epoch 30: Validation loss decreased (0.217634 --> 0.217495).  Saving model ...
	 Train_Loss: 0.2377 Train_Acc: 92.207 Val_Loss: 0.2175  BEST VAL Loss: 0.2175  Val_Acc: 92.965

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.2359 Train_Acc: 92.151 Val_Loss: 0.2176  BEST VAL Loss: 0.2175  Val_Acc: 92.609

Epoch 32: Validation loss decreased (0.217495 --> 0.216709).  Saving model ...
	 Train_Loss: 0.2344 Train_Acc: 91.583 Val_Loss: 0.2167  BEST VAL Loss: 0.2167  Val_Acc: 92.921

Epoch 33: Validation loss decreased (0.216709 --> 0.216642).  Saving model ...
	 Train_Loss: 0.2329 Train_Acc: 91.795 Val_Loss: 0.2166  BEST VAL Loss: 0.2166  Val_Acc: 93.277

Epoch 34: Validation loss decreased (0.216642 --> 0.216226).  Saving model ...
	 Train_Loss: 0.2314 Train_Acc: 91.978 Val_Loss: 0.2162  BEST VAL Loss: 0.2162  Val_Acc: 93.589

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.2300 Train_Acc: 92.001 Val_Loss: 0.2163  BEST VAL Loss: 0.2162  Val_Acc: 93.277

Epoch 36: Validation loss decreased (0.216226 --> 0.215760).  Saving model ...
	 Train_Loss: 0.2287 Train_Acc: 91.967 Val_Loss: 0.2158  BEST VAL Loss: 0.2158  Val_Acc: 94.078

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.2273 Train_Acc: 92.034 Val_Loss: 0.2162  BEST VAL Loss: 0.2158  Val_Acc: 93.500

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2260 Train_Acc: 92.424 Val_Loss: 0.2166  BEST VAL Loss: 0.2158  Val_Acc: 93.366

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.2247 Train_Acc: 92.468 Val_Loss: 0.2175  BEST VAL Loss: 0.2158  Val_Acc: 93.722

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2234 Train_Acc: 92.452 Val_Loss: 0.2174  BEST VAL Loss: 0.2158  Val_Acc: 93.366

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2220 Train_Acc: 92.802 Val_Loss: 0.2186  BEST VAL Loss: 0.2158  Val_Acc: 93.767

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2208 Train_Acc: 92.602 Val_Loss: 0.2196  BEST VAL Loss: 0.2158  Val_Acc: 93.989

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.2195 Train_Acc: 92.758 Val_Loss: 0.2199  BEST VAL Loss: 0.2158  Val_Acc: 93.633

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.2185 Train_Acc: 92.318 Val_Loss: 0.2200  BEST VAL Loss: 0.2158  Val_Acc: 93.633

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2175 Train_Acc: 92.390 Val_Loss: 0.2206  BEST VAL Loss: 0.2158  Val_Acc: 93.633

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.2165 Train_Acc: 92.635 Val_Loss: 0.2210  BEST VAL Loss: 0.2158  Val_Acc: 93.678

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.2155 Train_Acc: 92.118 Val_Loss: 0.2208  BEST VAL Loss: 0.2158  Val_Acc: 93.945

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.2146 Train_Acc: 92.541 Val_Loss: 0.2210  BEST VAL Loss: 0.2158  Val_Acc: 93.678

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2136 Train_Acc: 92.641 Val_Loss: 0.2214  BEST VAL Loss: 0.2158  Val_Acc: 93.856

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.2126 Train_Acc: 92.825 Val_Loss: 0.2217  BEST VAL Loss: 0.2158  Val_Acc: 93.811

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.2118 Train_Acc: 92.418 Val_Loss: 0.2214  BEST VAL Loss: 0.2158  Val_Acc: 93.544

Epoch 52: Validation loss did not decrease
Early stopped at epoch : 52
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.57      0.57      0.57     10114
           1       0.44      0.44      0.44      7850

    accuracy                           0.51     17964
   macro avg       0.50      0.50      0.50     17964
weighted avg       0.51      0.51      0.51     17964

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1264
           1       0.43      0.43      0.43       982

    accuracy                           0.50      2246
   macro avg       0.49      0.49      0.49      2246
weighted avg       0.50      0.50      0.50      2246

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.58      0.58      0.58      1264
           1       0.46      0.46      0.46       982

    accuracy                           0.53      2246
   macro avg       0.52      0.52      0.52      2246
weighted avg       0.53      0.53      0.53      2246

              precision    recall  f1-score   support

           0       0.58      0.58      0.58      1264
           1       0.46      0.46      0.46       982

    accuracy                           0.53      2246
   macro avg       0.52      0.52      0.52      2246
weighted avg       0.53      0.53      0.53      2246

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.57      0.56      4168
           1       0.45      0.44      0.44      3398

    accuracy                           0.51      7566
   macro avg       0.50      0.50      0.50      7566
weighted avg       0.51      0.51      0.51      7566

              precision    recall  f1-score   support

           0       0.55      0.57      0.56      4168
           1       0.45      0.44      0.44      3398

    accuracy                           0.51      7566
   macro avg       0.50      0.50      0.50      7566
weighted avg       0.51      0.51      0.51      7566

completed
