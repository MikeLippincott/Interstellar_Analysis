[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'fa8a89e2'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '0279780f'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd156f038'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c5e88746'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_Nigericin_1.000_1.0_DMSO_0.025'
 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (347179, 1270)
Number of total missing values across all columns: 0
Data Subset Is Off
Wells held out for testing: ['K08' 'M09']
Wells to use for training, validation, and testing ['K02' 'K03' 'K09' 'M02' 'M03' 'M08']
Number of in features:  1245
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.572667).  Saving model ...
	 Train_Loss: 0.6403 Train_Acc: 62.267 Val_Loss: 0.5727  BEST VAL Loss: 0.5727  Val_Acc: 69.079

Epoch 1: Validation loss decreased (0.572667 --> 0.565493).  Saving model ...
	 Train_Loss: 0.6117 Train_Acc: 67.823 Val_Loss: 0.5655  BEST VAL Loss: 0.5655  Val_Acc: 71.410

Epoch 2: Validation loss decreased (0.565493 --> 0.553108).  Saving model ...
	 Train_Loss: 0.5943 Train_Acc: 70.730 Val_Loss: 0.5531  BEST VAL Loss: 0.5531  Val_Acc: 73.605

Epoch 3: Validation loss decreased (0.553108 --> 0.543484).  Saving model ...
	 Train_Loss: 0.5823 Train_Acc: 72.179 Val_Loss: 0.5435  BEST VAL Loss: 0.5435  Val_Acc: 74.920

Epoch 4: Validation loss decreased (0.543484 --> 0.536331).  Saving model ...
	 Train_Loss: 0.5730 Train_Acc: 73.186 Val_Loss: 0.5363  BEST VAL Loss: 0.5363  Val_Acc: 74.722

Epoch 5: Validation loss decreased (0.536331 --> 0.531950).  Saving model ...
	 Train_Loss: 0.5654 Train_Acc: 73.820 Val_Loss: 0.5319  BEST VAL Loss: 0.5319  Val_Acc: 74.648

Epoch 6: Validation loss decreased (0.531950 --> 0.526249).  Saving model ...
	 Train_Loss: 0.5591 Train_Acc: 74.348 Val_Loss: 0.5262  BEST VAL Loss: 0.5262  Val_Acc: 76.228

Epoch 7: Validation loss decreased (0.526249 --> 0.520818).  Saving model ...
	 Train_Loss: 0.5532 Train_Acc: 74.775 Val_Loss: 0.5208  BEST VAL Loss: 0.5208  Val_Acc: 76.535

Epoch 8: Validation loss decreased (0.520818 --> 0.515136).  Saving model ...
	 Train_Loss: 0.5478 Train_Acc: 75.297 Val_Loss: 0.5151  BEST VAL Loss: 0.5151  Val_Acc: 77.263

Epoch 9: Validation loss decreased (0.515136 --> 0.511802).  Saving model ...
	 Train_Loss: 0.5429 Train_Acc: 75.662 Val_Loss: 0.5118  BEST VAL Loss: 0.5118  Val_Acc: 77.150

Epoch 10: Validation loss decreased (0.511802 --> 0.508341).  Saving model ...
	 Train_Loss: 0.5387 Train_Acc: 75.742 Val_Loss: 0.5083  BEST VAL Loss: 0.5083  Val_Acc: 76.967

Epoch 11: Validation loss decreased (0.508341 --> 0.504338).  Saving model ...
	 Train_Loss: 0.5347 Train_Acc: 76.064 Val_Loss: 0.5043  BEST VAL Loss: 0.5043  Val_Acc: 77.590

Epoch 12: Validation loss decreased (0.504338 --> 0.501075).  Saving model ...
	 Train_Loss: 0.5311 Train_Acc: 76.288 Val_Loss: 0.5011  BEST VAL Loss: 0.5011  Val_Acc: 77.920

Epoch 13: Validation loss decreased (0.501075 --> 0.499133).  Saving model ...
	 Train_Loss: 0.5277 Train_Acc: 76.525 Val_Loss: 0.4991  BEST VAL Loss: 0.4991  Val_Acc: 76.146

Epoch 14: Validation loss decreased (0.499133 --> 0.496125).  Saving model ...
	 Train_Loss: 0.5246 Train_Acc: 76.645 Val_Loss: 0.4961  BEST VAL Loss: 0.4961  Val_Acc: 77.991

Epoch 15: Validation loss decreased (0.496125 --> 0.493320).  Saving model ...
	 Train_Loss: 0.5216 Train_Acc: 76.978 Val_Loss: 0.4933  BEST VAL Loss: 0.4933  Val_Acc: 78.115

Epoch 16: Validation loss decreased (0.493320 --> 0.492370).  Saving model ...
	 Train_Loss: 0.5189 Train_Acc: 77.061 Val_Loss: 0.4924  BEST VAL Loss: 0.4924  Val_Acc: 76.566

Epoch 17: Validation loss decreased (0.492370 --> 0.489982).  Saving model ...
	 Train_Loss: 0.5163 Train_Acc: 77.147 Val_Loss: 0.4900  BEST VAL Loss: 0.4900  Val_Acc: 78.582

Epoch 18: Validation loss decreased (0.489982 --> 0.487668).  Saving model ...
	 Train_Loss: 0.5138 Train_Acc: 77.363 Val_Loss: 0.4877  BEST VAL Loss: 0.4877  Val_Acc: 78.080

Epoch 19: Validation loss decreased (0.487668 --> 0.485782).  Saving model ...
	 Train_Loss: 0.5116 Train_Acc: 77.530 Val_Loss: 0.4858  BEST VAL Loss: 0.4858  Val_Acc: 78.037

Epoch 20: Validation loss decreased (0.485782 --> 0.483768).  Saving model ...
	 Train_Loss: 0.5095 Train_Acc: 77.639 Val_Loss: 0.4838  BEST VAL Loss: 0.4838  Val_Acc: 78.360

Epoch 21: Validation loss decreased (0.483768 --> 0.481562).  Saving model ...
	 Train_Loss: 0.5075 Train_Acc: 77.698 Val_Loss: 0.4816  BEST VAL Loss: 0.4816  Val_Acc: 78.625

Epoch 22: Validation loss decreased (0.481562 --> 0.479646).  Saving model ...
	 Train_Loss: 0.5056 Train_Acc: 77.957 Val_Loss: 0.4796  BEST VAL Loss: 0.4796  Val_Acc: 78.683

Epoch 23: Validation loss decreased (0.479646 --> 0.478007).  Saving model ...
	 Train_Loss: 0.5037 Train_Acc: 78.103 Val_Loss: 0.4780  BEST VAL Loss: 0.4780  Val_Acc: 78.563

Epoch 24: Validation loss decreased (0.478007 --> 0.476468).  Saving model ...
	 Train_Loss: 0.5019 Train_Acc: 78.066 Val_Loss: 0.4765  BEST VAL Loss: 0.4765  Val_Acc: 78.944

Epoch 25: Validation loss decreased (0.476468 --> 0.474859).  Saving model ...
	 Train_Loss: 0.5003 Train_Acc: 78.175 Val_Loss: 0.4749  BEST VAL Loss: 0.4749  Val_Acc: 79.263

Epoch 26: Validation loss decreased (0.474859 --> 0.473729).  Saving model ...
	 Train_Loss: 0.4987 Train_Acc: 78.322 Val_Loss: 0.4737  BEST VAL Loss: 0.4737  Val_Acc: 78.430

Epoch 27: Validation loss decreased (0.473729 --> 0.472418).  Saving model ...
	 Train_Loss: 0.4971 Train_Acc: 78.353 Val_Loss: 0.4724  BEST VAL Loss: 0.4724  Val_Acc: 79.205

Epoch 28: Validation loss decreased (0.472418 --> 0.471240).  Saving model ...
	 Train_Loss: 0.4956 Train_Acc: 78.403 Val_Loss: 0.4712  BEST VAL Loss: 0.4712  Val_Acc: 78.862

Epoch 29: Validation loss decreased (0.471240 --> 0.470048).  Saving model ...
	 Train_Loss: 0.4943 Train_Acc: 78.511 Val_Loss: 0.4700  BEST VAL Loss: 0.4700  Val_Acc: 78.535

Epoch 30: Validation loss decreased (0.470048 --> 0.469714).  Saving model ...
	 Train_Loss: 0.4929 Train_Acc: 78.568 Val_Loss: 0.4697  BEST VAL Loss: 0.4697  Val_Acc: 78.777

Epoch 31: Validation loss decreased (0.469714 --> 0.468727).  Saving model ...
	 Train_Loss: 0.4916 Train_Acc: 78.599 Val_Loss: 0.4687  BEST VAL Loss: 0.4687  Val_Acc: 78.699

Epoch 32: Validation loss decreased (0.468727 --> 0.467539).  Saving model ...
	 Train_Loss: 0.4904 Train_Acc: 78.712 Val_Loss: 0.4675  BEST VAL Loss: 0.4675  Val_Acc: 79.430

Epoch 33: Validation loss decreased (0.467539 --> 0.466504).  Saving model ...
	 Train_Loss: 0.4892 Train_Acc: 78.643 Val_Loss: 0.4665  BEST VAL Loss: 0.4665  Val_Acc: 79.065

Epoch 34: Validation loss decreased (0.466504 --> 0.465420).  Saving model ...
	 Train_Loss: 0.4881 Train_Acc: 78.782 Val_Loss: 0.4654  BEST VAL Loss: 0.4654  Val_Acc: 78.780

Epoch 35: Validation loss decreased (0.465420 --> 0.464342).  Saving model ...
	 Train_Loss: 0.4869 Train_Acc: 78.833 Val_Loss: 0.4643  BEST VAL Loss: 0.4643  Val_Acc: 79.590

Epoch 36: Validation loss decreased (0.464342 --> 0.463897).  Saving model ...
	 Train_Loss: 0.4859 Train_Acc: 78.920 Val_Loss: 0.4639  BEST VAL Loss: 0.4639  Val_Acc: 78.477

Epoch 37: Validation loss decreased (0.463897 --> 0.463023).  Saving model ...
	 Train_Loss: 0.4848 Train_Acc: 78.952 Val_Loss: 0.4630  BEST VAL Loss: 0.4630  Val_Acc: 79.290

Epoch 38: Validation loss decreased (0.463023 --> 0.462285).  Saving model ...
	 Train_Loss: 0.4838 Train_Acc: 78.959 Val_Loss: 0.4623  BEST VAL Loss: 0.4623  Val_Acc: 78.913

Epoch 39: Validation loss decreased (0.462285 --> 0.461389).  Saving model ...
	 Train_Loss: 0.4828 Train_Acc: 79.032 Val_Loss: 0.4614  BEST VAL Loss: 0.4614  Val_Acc: 79.388

Epoch 40: Validation loss decreased (0.461389 --> 0.460885).  Saving model ...
	 Train_Loss: 0.4818 Train_Acc: 79.064 Val_Loss: 0.4609  BEST VAL Loss: 0.4609  Val_Acc: 78.924

Epoch 41: Validation loss decreased (0.460885 --> 0.460109).  Saving model ...
	 Train_Loss: 0.4809 Train_Acc: 79.150 Val_Loss: 0.4601  BEST VAL Loss: 0.4601  Val_Acc: 79.485

Epoch 42: Validation loss decreased (0.460109 --> 0.459191).  Saving model ...
	 Train_Loss: 0.4800 Train_Acc: 79.173 Val_Loss: 0.4592  BEST VAL Loss: 0.4592  Val_Acc: 79.426

Epoch 43: Validation loss decreased (0.459191 --> 0.458413).  Saving model ...
	 Train_Loss: 0.4791 Train_Acc: 79.201 Val_Loss: 0.4584  BEST VAL Loss: 0.4584  Val_Acc: 79.637

Epoch 44: Validation loss decreased (0.458413 --> 0.457614).  Saving model ...
	 Train_Loss: 0.4782 Train_Acc: 79.377 Val_Loss: 0.4576  BEST VAL Loss: 0.4576  Val_Acc: 79.512

Epoch 45: Validation loss decreased (0.457614 --> 0.457005).  Saving model ...
	 Train_Loss: 0.4774 Train_Acc: 79.356 Val_Loss: 0.4570  BEST VAL Loss: 0.4570  Val_Acc: 79.006

Epoch 46: Validation loss decreased (0.457005 --> 0.456215).  Saving model ...
	 Train_Loss: 0.4766 Train_Acc: 79.426 Val_Loss: 0.4562  BEST VAL Loss: 0.4562  Val_Acc: 79.808

Epoch 47: Validation loss decreased (0.456215 --> 0.455383).  Saving model ...
	 Train_Loss: 0.4758 Train_Acc: 79.496 Val_Loss: 0.4554  BEST VAL Loss: 0.4554  Val_Acc: 80.310

Epoch 48: Validation loss decreased (0.455383 --> 0.454667).  Saving model ...
	 Train_Loss: 0.4750 Train_Acc: 79.573 Val_Loss: 0.4547  BEST VAL Loss: 0.4547  Val_Acc: 79.652

Epoch 49: Validation loss decreased (0.454667 --> 0.453962).  Saving model ...
	 Train_Loss: 0.4742 Train_Acc: 79.581 Val_Loss: 0.4540  BEST VAL Loss: 0.4540  Val_Acc: 79.777

Epoch 50: Validation loss decreased (0.453962 --> 0.453319).  Saving model ...
	 Train_Loss: 0.4735 Train_Acc: 79.685 Val_Loss: 0.4533  BEST VAL Loss: 0.4533  Val_Acc: 79.644

Epoch 51: Validation loss decreased (0.453319 --> 0.452878).  Saving model ...
	 Train_Loss: 0.4728 Train_Acc: 79.760 Val_Loss: 0.4529  BEST VAL Loss: 0.4529  Val_Acc: 79.738

Epoch 52: Validation loss decreased (0.452878 --> 0.452252).  Saving model ...
	 Train_Loss: 0.4721 Train_Acc: 79.719 Val_Loss: 0.4523  BEST VAL Loss: 0.4523  Val_Acc: 79.940

Epoch 53: Validation loss decreased (0.452252 --> 0.451615).  Saving model ...
	 Train_Loss: 0.4714 Train_Acc: 79.731 Val_Loss: 0.4516  BEST VAL Loss: 0.4516  Val_Acc: 80.037

Epoch 54: Validation loss decreased (0.451615 --> 0.451098).  Saving model ...
	 Train_Loss: 0.4707 Train_Acc: 79.819 Val_Loss: 0.4511  BEST VAL Loss: 0.4511  Val_Acc: 79.368

Epoch 55: Validation loss decreased (0.451098 --> 0.450412).  Saving model ...
	 Train_Loss: 0.4701 Train_Acc: 79.704 Val_Loss: 0.4504  BEST VAL Loss: 0.4504  Val_Acc: 80.088

Epoch 56: Validation loss decreased (0.450412 --> 0.449899).  Saving model ...
	 Train_Loss: 0.4694 Train_Acc: 79.718 Val_Loss: 0.4499  BEST VAL Loss: 0.4499  Val_Acc: 79.948

Epoch 57: Validation loss decreased (0.449899 --> 0.449428).  Saving model ...
	 Train_Loss: 0.4688 Train_Acc: 79.841 Val_Loss: 0.4494  BEST VAL Loss: 0.4494  Val_Acc: 80.391

Epoch 58: Validation loss decreased (0.449428 --> 0.448864).  Saving model ...
	 Train_Loss: 0.4682 Train_Acc: 79.968 Val_Loss: 0.4489  BEST VAL Loss: 0.4489  Val_Acc: 79.960

Epoch 59: Validation loss decreased (0.448864 --> 0.448331).  Saving model ...
	 Train_Loss: 0.4676 Train_Acc: 79.794 Val_Loss: 0.4483  BEST VAL Loss: 0.4483  Val_Acc: 80.302

Epoch 60: Validation loss decreased (0.448331 --> 0.447766).  Saving model ...
	 Train_Loss: 0.4670 Train_Acc: 79.918 Val_Loss: 0.4478  BEST VAL Loss: 0.4478  Val_Acc: 80.286

Epoch 61: Validation loss decreased (0.447766 --> 0.447399).  Saving model ...
	 Train_Loss: 0.4664 Train_Acc: 79.906 Val_Loss: 0.4474  BEST VAL Loss: 0.4474  Val_Acc: 80.107

Epoch 62: Validation loss decreased (0.447399 --> 0.446859).  Saving model ...
	 Train_Loss: 0.4658 Train_Acc: 80.061 Val_Loss: 0.4469  BEST VAL Loss: 0.4469  Val_Acc: 80.193

Epoch 63: Validation loss decreased (0.446859 --> 0.446336).  Saving model ...
	 Train_Loss: 0.4653 Train_Acc: 79.963 Val_Loss: 0.4463  BEST VAL Loss: 0.4463  Val_Acc: 80.613

Epoch 64: Validation loss decreased (0.446336 --> 0.445813).  Saving model ...
	 Train_Loss: 0.4648 Train_Acc: 80.062 Val_Loss: 0.4458  BEST VAL Loss: 0.4458  Val_Acc: 80.216

Epoch 65: Validation loss decreased (0.445813 --> 0.445353).  Saving model ...
	 Train_Loss: 0.4642 Train_Acc: 80.226 Val_Loss: 0.4454  BEST VAL Loss: 0.4454  Val_Acc: 80.150

Epoch 66: Validation loss decreased (0.445353 --> 0.444862).  Saving model ...
	 Train_Loss: 0.4637 Train_Acc: 80.090 Val_Loss: 0.4449  BEST VAL Loss: 0.4449  Val_Acc: 80.376

Epoch 67: Validation loss decreased (0.444862 --> 0.444412).  Saving model ...
	 Train_Loss: 0.4632 Train_Acc: 80.133 Val_Loss: 0.4444  BEST VAL Loss: 0.4444  Val_Acc: 80.142

Epoch 68: Validation loss decreased (0.444412 --> 0.443956).  Saving model ...
	 Train_Loss: 0.4627 Train_Acc: 80.159 Val_Loss: 0.4440  BEST VAL Loss: 0.4440  Val_Acc: 80.244

Epoch 69: Validation loss decreased (0.443956 --> 0.443542).  Saving model ...
	 Train_Loss: 0.4622 Train_Acc: 80.131 Val_Loss: 0.4435  BEST VAL Loss: 0.4435  Val_Acc: 80.294

Epoch 70: Validation loss decreased (0.443542 --> 0.443202).  Saving model ...
	 Train_Loss: 0.4617 Train_Acc: 80.234 Val_Loss: 0.4432  BEST VAL Loss: 0.4432  Val_Acc: 80.033

Epoch 71: Validation loss decreased (0.443202 --> 0.442723).  Saving model ...
	 Train_Loss: 0.4612 Train_Acc: 80.159 Val_Loss: 0.4427  BEST VAL Loss: 0.4427  Val_Acc: 80.399

Epoch 72: Validation loss decreased (0.442723 --> 0.442407).  Saving model ...
	 Train_Loss: 0.4608 Train_Acc: 80.187 Val_Loss: 0.4424  BEST VAL Loss: 0.4424  Val_Acc: 80.061

Epoch 73: Validation loss decreased (0.442407 --> 0.441958).  Saving model ...
	 Train_Loss: 0.4603 Train_Acc: 80.183 Val_Loss: 0.4420  BEST VAL Loss: 0.4420  Val_Acc: 80.388

Epoch 74: Validation loss decreased (0.441958 --> 0.441635).  Saving model ...
	 Train_Loss: 0.4599 Train_Acc: 80.462 Val_Loss: 0.4416  BEST VAL Loss: 0.4416  Val_Acc: 80.142

Epoch 75: Validation loss decreased (0.441635 --> 0.441176).  Saving model ...
	 Train_Loss: 0.4594 Train_Acc: 80.344 Val_Loss: 0.4412  BEST VAL Loss: 0.4412  Val_Acc: 80.524

Epoch 76: Validation loss decreased (0.441176 --> 0.440808).  Saving model ...
	 Train_Loss: 0.4590 Train_Acc: 80.389 Val_Loss: 0.4408  BEST VAL Loss: 0.4408  Val_Acc: 80.111

Epoch 77: Validation loss decreased (0.440808 --> 0.440368).  Saving model ...
	 Train_Loss: 0.4585 Train_Acc: 80.373 Val_Loss: 0.4404  BEST VAL Loss: 0.4404  Val_Acc: 80.574

Epoch 78: Validation loss decreased (0.440368 --> 0.440002).  Saving model ...
	 Train_Loss: 0.4581 Train_Acc: 80.416 Val_Loss: 0.4400  BEST VAL Loss: 0.4400  Val_Acc: 80.247

Epoch 79: Validation loss decreased (0.440002 --> 0.439611).  Saving model ...
	 Train_Loss: 0.4577 Train_Acc: 80.356 Val_Loss: 0.4396  BEST VAL Loss: 0.4396  Val_Acc: 80.582

Epoch 80: Validation loss decreased (0.439611 --> 0.439381).  Saving model ...
	 Train_Loss: 0.4573 Train_Acc: 80.524 Val_Loss: 0.4394  BEST VAL Loss: 0.4394  Val_Acc: 79.625

Epoch 81: Validation loss decreased (0.439381 --> 0.438980).  Saving model ...
	 Train_Loss: 0.4569 Train_Acc: 80.382 Val_Loss: 0.4390  BEST VAL Loss: 0.4390  Val_Acc: 80.683

Epoch 82: Validation loss decreased (0.438980 --> 0.438640).  Saving model ...
	 Train_Loss: 0.4565 Train_Acc: 80.343 Val_Loss: 0.4386  BEST VAL Loss: 0.4386  Val_Acc: 80.784

Epoch 83: Validation loss decreased (0.438640 --> 0.438388).  Saving model ...
	 Train_Loss: 0.4561 Train_Acc: 80.417 Val_Loss: 0.4384  BEST VAL Loss: 0.4384  Val_Acc: 80.072

Epoch 84: Validation loss decreased (0.438388 --> 0.438007).  Saving model ...
	 Train_Loss: 0.4557 Train_Acc: 80.495 Val_Loss: 0.4380  BEST VAL Loss: 0.4380  Val_Acc: 80.773

Epoch 85: Validation loss decreased (0.438007 --> 0.437700).  Saving model ...
	 Train_Loss: 0.4554 Train_Acc: 80.430 Val_Loss: 0.4377  BEST VAL Loss: 0.4377  Val_Acc: 80.415

Epoch 86: Validation loss decreased (0.437700 --> 0.437600).  Saving model ...
	 Train_Loss: 0.4550 Train_Acc: 80.564 Val_Loss: 0.4376  BEST VAL Loss: 0.4376  Val_Acc: 80.508

Epoch 87: Validation loss decreased (0.437600 --> 0.437318).  Saving model ...
	 Train_Loss: 0.4546 Train_Acc: 80.443 Val_Loss: 0.4373  BEST VAL Loss: 0.4373  Val_Acc: 80.310

Epoch 88: Validation loss decreased (0.437318 --> 0.437058).  Saving model ...
	 Train_Loss: 0.4543 Train_Acc: 80.446 Val_Loss: 0.4371  BEST VAL Loss: 0.4371  Val_Acc: 80.757

Epoch 89: Validation loss decreased (0.437058 --> 0.436907).  Saving model ...
	 Train_Loss: 0.4539 Train_Acc: 80.516 Val_Loss: 0.4369  BEST VAL Loss: 0.4369  Val_Acc: 80.049

Epoch 90: Validation loss decreased (0.436907 --> 0.436616).  Saving model ...
	 Train_Loss: 0.4536 Train_Acc: 80.434 Val_Loss: 0.4366  BEST VAL Loss: 0.4366  Val_Acc: 80.465

Epoch 91: Validation loss decreased (0.436616 --> 0.436411).  Saving model ...
	 Train_Loss: 0.4532 Train_Acc: 80.560 Val_Loss: 0.4364  BEST VAL Loss: 0.4364  Val_Acc: 80.862

Epoch 92: Validation loss decreased (0.436411 --> 0.436365).  Saving model ...
	 Train_Loss: 0.4529 Train_Acc: 80.538 Val_Loss: 0.4364  BEST VAL Loss: 0.4364  Val_Acc: 79.372

Epoch 93: Validation loss decreased (0.436365 --> 0.436101).  Saving model ...
	 Train_Loss: 0.4526 Train_Acc: 80.565 Val_Loss: 0.4361  BEST VAL Loss: 0.4361  Val_Acc: 80.384

Epoch 94: Validation loss decreased (0.436101 --> 0.435806).  Saving model ...
	 Train_Loss: 0.4522 Train_Acc: 80.779 Val_Loss: 0.4358  BEST VAL Loss: 0.4358  Val_Acc: 80.862

Epoch 95: Validation loss decreased (0.435806 --> 0.435524).  Saving model ...
	 Train_Loss: 0.4519 Train_Acc: 80.525 Val_Loss: 0.4355  BEST VAL Loss: 0.4355  Val_Acc: 80.668

Epoch 96: Validation loss decreased (0.435524 --> 0.435403).  Saving model ...
	 Train_Loss: 0.4516 Train_Acc: 80.773 Val_Loss: 0.4354  BEST VAL Loss: 0.4354  Val_Acc: 80.084

Epoch 97: Validation loss decreased (0.435403 --> 0.435175).  Saving model ...
	 Train_Loss: 0.4513 Train_Acc: 80.726 Val_Loss: 0.4352  BEST VAL Loss: 0.4352  Val_Acc: 80.656

Epoch 98: Validation loss decreased (0.435175 --> 0.434895).  Saving model ...
	 Train_Loss: 0.4510 Train_Acc: 80.714 Val_Loss: 0.4349  BEST VAL Loss: 0.4349  Val_Acc: 80.613

Epoch 99: Validation loss decreased (0.434895 --> 0.434672).  Saving model ...
	 Train_Loss: 0.4507 Train_Acc: 80.776 Val_Loss: 0.4347  BEST VAL Loss: 0.4347  Val_Acc: 80.520

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.49      0.47      0.48    100339
           1       0.51      0.53      0.52    105242

    accuracy                           0.50    205581
   macro avg       0.50      0.50      0.50    205581
weighted avg       0.50      0.50      0.50    205581

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.49      0.47      0.48     12543
           1       0.51      0.53      0.52     13155

    accuracy                           0.50     25698
   macro avg       0.50      0.50      0.50     25698
weighted avg       0.50      0.50      0.50     25698

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.47      0.48     12543
           1       0.51      0.52      0.52     13155

    accuracy                           0.50     25698
   macro avg       0.50      0.50      0.50     25698
weighted avg       0.50      0.50      0.50     25698

              precision    recall  f1-score   support

           0       0.48      0.47      0.48     12543
           1       0.51      0.52      0.52     13155

    accuracy                           0.50     25698
   macro avg       0.50      0.50      0.50     25698
weighted avg       0.50      0.50      0.50     25698

LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
LPS_Nigericin_1.000_10.0_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.45      0.32      0.37     40588
           1       0.55      0.68      0.61     49614

    accuracy                           0.52     90202
   macro avg       0.50      0.50      0.49     90202
weighted avg       0.50      0.52      0.50     90202

              precision    recall  f1-score   support

           0       0.45      0.32      0.37     40588
           1       0.55      0.68      0.61     49614

    accuracy                           0.52     90202
   macro avg       0.50      0.50      0.49     90202
weighted avg       0.50      0.52      0.50     90202

completed
