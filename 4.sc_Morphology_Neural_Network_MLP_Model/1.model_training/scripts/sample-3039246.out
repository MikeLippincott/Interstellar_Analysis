[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '56306e0e'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'b676e698'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'e1947acc'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '30ddd543'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_100.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_100.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_100.000_DMSO_0.025']
The dimensions of the data are: (29753, 1276)
Number of total missing values across all columns: 59506
Data Subset Is Off
Wells held out for testing: ['D14' 'J20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'J16' 'J17' 'J21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.277133).  Saving model ...
	 Train_Loss: 0.5564 Train_Acc: 68.821 Val_Loss: 0.2771  BEST VAL Loss: 0.2771  Val_Acc: 90.802

Epoch 1: Validation loss decreased (0.277133 --> 0.230050).  Saving model ...
	 Train_Loss: 0.4587 Train_Acc: 83.989 Val_Loss: 0.2301  BEST VAL Loss: 0.2301  Val_Acc: 92.335

Epoch 2: Validation loss decreased (0.230050 --> 0.197128).  Saving model ...
	 Train_Loss: 0.3939 Train_Acc: 88.813 Val_Loss: 0.1971  BEST VAL Loss: 0.1971  Val_Acc: 94.831

Epoch 3: Validation loss decreased (0.197128 --> 0.176273).  Saving model ...
	 Train_Loss: 0.3502 Train_Acc: 90.603 Val_Loss: 0.1763  BEST VAL Loss: 0.1763  Val_Acc: 95.313

Epoch 4: Validation loss decreased (0.176273 --> 0.162392).  Saving model ...
	 Train_Loss: 0.3236 Train_Acc: 91.293 Val_Loss: 0.1624  BEST VAL Loss: 0.1624  Val_Acc: 95.926

Epoch 5: Validation loss decreased (0.162392 --> 0.152936).  Saving model ...
	 Train_Loss: 0.2984 Train_Acc: 93.237 Val_Loss: 0.1529  BEST VAL Loss: 0.1529  Val_Acc: 96.102

Epoch 6: Validation loss decreased (0.152936 --> 0.145084).  Saving model ...
	 Train_Loss: 0.2811 Train_Acc: 93.511 Val_Loss: 0.1451  BEST VAL Loss: 0.1451  Val_Acc: 96.452

Epoch 7: Validation loss decreased (0.145084 --> 0.139622).  Saving model ...
	 Train_Loss: 0.2660 Train_Acc: 93.988 Val_Loss: 0.1396  BEST VAL Loss: 0.1396  Val_Acc: 96.627

Epoch 8: Validation loss decreased (0.139622 --> 0.135107).  Saving model ...
	 Train_Loss: 0.2532 Train_Acc: 94.447 Val_Loss: 0.1351  BEST VAL Loss: 0.1351  Val_Acc: 96.671

Epoch 9: Validation loss decreased (0.135107 --> 0.131582).  Saving model ...
	 Train_Loss: 0.2432 Train_Acc: 94.415 Val_Loss: 0.1316  BEST VAL Loss: 0.1316  Val_Acc: 96.759

Epoch 10: Validation loss decreased (0.131582 --> 0.127043).  Saving model ...
	 Train_Loss: 0.2344 Train_Acc: 94.480 Val_Loss: 0.1270  BEST VAL Loss: 0.1270  Val_Acc: 97.021

Epoch 11: Validation loss decreased (0.127043 --> 0.124568).  Saving model ...
	 Train_Loss: 0.2263 Train_Acc: 94.864 Val_Loss: 0.1246  BEST VAL Loss: 0.1246  Val_Acc: 96.890

Epoch 12: Validation loss decreased (0.124568 --> 0.122201).  Saving model ...
	 Train_Loss: 0.2191 Train_Acc: 94.940 Val_Loss: 0.1222  BEST VAL Loss: 0.1222  Val_Acc: 97.021

Epoch 13: Validation loss decreased (0.122201 --> 0.121580).  Saving model ...
	 Train_Loss: 0.2132 Train_Acc: 94.940 Val_Loss: 0.1216  BEST VAL Loss: 0.1216  Val_Acc: 96.321

Epoch 14: Validation loss decreased (0.121580 --> 0.120018).  Saving model ...
	 Train_Loss: 0.2078 Train_Acc: 94.957 Val_Loss: 0.1200  BEST VAL Loss: 0.1200  Val_Acc: 97.197

Epoch 15: Validation loss decreased (0.120018 --> 0.119365).  Saving model ...
	 Train_Loss: 0.2032 Train_Acc: 95.203 Val_Loss: 0.1194  BEST VAL Loss: 0.1194  Val_Acc: 96.890

Epoch 16: Validation loss decreased (0.119365 --> 0.117766).  Saving model ...
	 Train_Loss: 0.1991 Train_Acc: 94.803 Val_Loss: 0.1178  BEST VAL Loss: 0.1178  Val_Acc: 97.021

Epoch 17: Validation loss decreased (0.117766 --> 0.116635).  Saving model ...
	 Train_Loss: 0.1948 Train_Acc: 95.346 Val_Loss: 0.1166  BEST VAL Loss: 0.1166  Val_Acc: 97.197

Epoch 18: Validation loss decreased (0.116635 --> 0.116139).  Saving model ...
	 Train_Loss: 0.1909 Train_Acc: 95.400 Val_Loss: 0.1161  BEST VAL Loss: 0.1161  Val_Acc: 96.802

Epoch 19: Validation loss decreased (0.116139 --> 0.115576).  Saving model ...
	 Train_Loss: 0.1873 Train_Acc: 95.450 Val_Loss: 0.1156  BEST VAL Loss: 0.1156  Val_Acc: 97.065

Epoch 20: Validation loss decreased (0.115576 --> 0.115568).  Saving model ...
	 Train_Loss: 0.1843 Train_Acc: 95.137 Val_Loss: 0.1156  BEST VAL Loss: 0.1156  Val_Acc: 96.759

Epoch 21: Validation loss decreased (0.115568 --> 0.115265).  Saving model ...
	 Train_Loss: 0.1814 Train_Acc: 95.466 Val_Loss: 0.1153  BEST VAL Loss: 0.1153  Val_Acc: 97.284

Epoch 22: Validation loss decreased (0.115265 --> 0.114061).  Saving model ...
	 Train_Loss: 0.1790 Train_Acc: 95.132 Val_Loss: 0.1141  BEST VAL Loss: 0.1141  Val_Acc: 97.109

Epoch 23: Validation loss decreased (0.114061 --> 0.113761).  Saving model ...
	 Train_Loss: 0.1768 Train_Acc: 95.061 Val_Loss: 0.1138  BEST VAL Loss: 0.1138  Val_Acc: 96.890

Epoch 24: Validation loss decreased (0.113761 --> 0.112908).  Saving model ...
	 Train_Loss: 0.1741 Train_Acc: 95.740 Val_Loss: 0.1129  BEST VAL Loss: 0.1129  Val_Acc: 97.197

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1719 Train_Acc: 95.576 Val_Loss: 0.1129  BEST VAL Loss: 0.1129  Val_Acc: 97.109

Epoch 26: Validation loss decreased (0.112908 --> 0.112237).  Saving model ...
	 Train_Loss: 0.1697 Train_Acc: 95.784 Val_Loss: 0.1122  BEST VAL Loss: 0.1122  Val_Acc: 97.240

Epoch 27: Validation loss decreased (0.112237 --> 0.111896).  Saving model ...
	 Train_Loss: 0.1678 Train_Acc: 95.565 Val_Loss: 0.1119  BEST VAL Loss: 0.1119  Val_Acc: 97.021

Epoch 28: Validation loss decreased (0.111896 --> 0.111061).  Saving model ...
	 Train_Loss: 0.1660 Train_Acc: 95.488 Val_Loss: 0.1111  BEST VAL Loss: 0.1111  Val_Acc: 97.328

Epoch 29: Validation loss decreased (0.111061 --> 0.110199).  Saving model ...
	 Train_Loss: 0.1642 Train_Acc: 95.663 Val_Loss: 0.1102  BEST VAL Loss: 0.1102  Val_Acc: 97.284

Epoch 30: Validation loss decreased (0.110199 --> 0.109427).  Saving model ...
	 Train_Loss: 0.1624 Train_Acc: 95.570 Val_Loss: 0.1094  BEST VAL Loss: 0.1094  Val_Acc: 97.197

Epoch 31: Validation loss decreased (0.109427 --> 0.109208).  Saving model ...
	 Train_Loss: 0.1609 Train_Acc: 95.674 Val_Loss: 0.1092  BEST VAL Loss: 0.1092  Val_Acc: 97.416

Epoch 32: Validation loss decreased (0.109208 --> 0.108610).  Saving model ...
	 Train_Loss: 0.1595 Train_Acc: 95.356 Val_Loss: 0.1086  BEST VAL Loss: 0.1086  Val_Acc: 97.284

Epoch 33: Validation loss decreased (0.108610 --> 0.108350).  Saving model ...
	 Train_Loss: 0.1582 Train_Acc: 95.241 Val_Loss: 0.1083  BEST VAL Loss: 0.1083  Val_Acc: 97.021

Epoch 34: Validation loss decreased (0.108350 --> 0.107850).  Saving model ...
	 Train_Loss: 0.1569 Train_Acc: 95.751 Val_Loss: 0.1079  BEST VAL Loss: 0.1079  Val_Acc: 97.153

Epoch 35: Validation loss decreased (0.107850 --> 0.107277).  Saving model ...
	 Train_Loss: 0.1555 Train_Acc: 95.816 Val_Loss: 0.1073  BEST VAL Loss: 0.1073  Val_Acc: 97.240

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.1542 Train_Acc: 95.680 Val_Loss: 0.1074  BEST VAL Loss: 0.1073  Val_Acc: 96.890

Epoch 37: Validation loss decreased (0.107277 --> 0.107264).  Saving model ...
	 Train_Loss: 0.1531 Train_Acc: 95.773 Val_Loss: 0.1073  BEST VAL Loss: 0.1073  Val_Acc: 97.328

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.1519 Train_Acc: 95.707 Val_Loss: 0.1077  BEST VAL Loss: 0.1073  Val_Acc: 96.978

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1508 Train_Acc: 95.543 Val_Loss: 0.1076  BEST VAL Loss: 0.1073  Val_Acc: 96.934

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1497 Train_Acc: 95.822 Val_Loss: 0.1081  BEST VAL Loss: 0.1073  Val_Acc: 97.372

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.1485 Train_Acc: 96.123 Val_Loss: 0.1081  BEST VAL Loss: 0.1073  Val_Acc: 96.978

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.1475 Train_Acc: 96.025 Val_Loss: 0.1085  BEST VAL Loss: 0.1073  Val_Acc: 97.065

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.1467 Train_Acc: 95.439 Val_Loss: 0.1085  BEST VAL Loss: 0.1073  Val_Acc: 97.328

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1458 Train_Acc: 95.685 Val_Loss: 0.1087  BEST VAL Loss: 0.1073  Val_Acc: 97.065

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.1450 Train_Acc: 95.576 Val_Loss: 0.1086  BEST VAL Loss: 0.1073  Val_Acc: 97.109

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1441 Train_Acc: 95.811 Val_Loss: 0.1091  BEST VAL Loss: 0.1073  Val_Acc: 97.153

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.1431 Train_Acc: 96.145 Val_Loss: 0.1090  BEST VAL Loss: 0.1073  Val_Acc: 97.328

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1423 Train_Acc: 95.893 Val_Loss: 0.1092  BEST VAL Loss: 0.1073  Val_Acc: 97.284

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1414 Train_Acc: 96.172 Val_Loss: 0.1091  BEST VAL Loss: 0.1073  Val_Acc: 97.416

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1407 Train_Acc: 95.877 Val_Loss: 0.1092  BEST VAL Loss: 0.1073  Val_Acc: 97.284

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1400 Train_Acc: 95.816 Val_Loss: 0.1093  BEST VAL Loss: 0.1073  Val_Acc: 97.372

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1392 Train_Acc: 96.233 Val_Loss: 0.1091  BEST VAL Loss: 0.1073  Val_Acc: 96.934

Epoch 53: Validation loss did not decrease
Early stopped at epoch : 53
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.54      0.54      9891
           1       0.46      0.46      0.46      8371

    accuracy                           0.50     18262
   macro avg       0.50      0.50      0.50     18262
weighted avg       0.51      0.50      0.51     18262

LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.53      0.54      1237
           1       0.46      0.46      0.46      1046

    accuracy                           0.50      2283
   macro avg       0.50      0.50      0.50      2283
weighted avg       0.50      0.50      0.50      2283

LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.53      0.52      0.53      1237
           1       0.44      0.44      0.44      1046

    accuracy                           0.49      2283
   macro avg       0.48      0.48      0.48      2283
weighted avg       0.49      0.49      0.49      2283

              precision    recall  f1-score   support

           0       0.53      0.52      0.53      1237
           1       0.44      0.44      0.44      1046

    accuracy                           0.49      2283
   macro avg       0.48      0.48      0.48      2283
weighted avg       0.49      0.49      0.49      2283

LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_100.000_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.52      0.49      0.50      3622
           1       0.47      0.50      0.48      3303

    accuracy                           0.49      6925
   macro avg       0.49      0.49      0.49      6925
weighted avg       0.49      0.49      0.49      6925

              precision    recall  f1-score   support

           0       0.52      0.49      0.50      3622
           1       0.47      0.50      0.48      3303

    accuracy                           0.49      6925
   macro avg       0.49      0.49      0.49      6925
weighted avg       0.49      0.49      0.49      6925

completed
