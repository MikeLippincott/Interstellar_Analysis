[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 18688 bytes to train_binary_model.py
[NbConvertApp] Converting notebook ../notebooks/train_multiclass_model.ipynb to script
[NbConvertApp] Writing 57614 bytes to train_multiclass_model.py
[NbConvertApp] Converting notebook ../notebooks/train_regression_model.ipynb to script
[NbConvertApp] Writing 25120 bytes to train_regression_model.py
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:260: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_descriptive["labels"] = df1["labels"]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:287: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1019: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1019: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:671: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:676: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:728: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  main_prob_df = pd.concat([main_prob_df, prob_df])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:739: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:804: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:818: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:915: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:921: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:999: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1110: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1116: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1311: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1313: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1316: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1341: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  data_split_conf_mat_df_all = pd.concat(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1377: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1497: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1503: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1133: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1124: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
  warnings.warn(
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1713: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1830: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:1836: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  tmp_df.drop_duplicates(inplace=True)
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2013: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["apoptosis"] = confusion_matrix_df["apoptosis"] / sum_of_columns[0]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2015: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["healthy"] = confusion_matrix_df["healthy"] / sum_of_columns[1]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2018: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  confusion_matrix_df["pyroptosis"] / sum_of_columns[2]
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/train_multiclass_model.py:2089: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_labels["new_labels"] = df_labels["new_labels"].astype(str)
[0.954878893196544, 0.505315252332322, 0.539805854471134]
Data Subset Is Off
(152275,) (38069,) (202050,) 392394     93196
392395     93197
392396     93198
392397     93199
392398     93200
           ...
446996    594342
446997    594343
446998    594344
446999    594345
447000    594346
Name: labeled_data_index, Length: 54607, dtype: int64 (150901,)
(152275,) (38069,) (202050,) 392394     93196
392395     93197
392396     93198
392397     93199
392398     93200
           ...
446996    594342
446997    594343
446998    594344
446999    594345
447000    594346
Name: labeled_data_index, Length: 54607, dtype: int64 (150901,)
597902
(7972,) (89086,) (55217,)
(1993,) (22273,) (13803,)
(9965,) (111360,) (80725,)
(0,) (0,) (54607,)
(7048,) (73054,) (70799,)
(152275, 1251) (38069, 1251) (202050, 1251) (54607, 1251) (150901, 1251)
(152275,) (38069,) (202050,) (54607,) (150901,)
3
Number of in features:  1251
Number of out features:  3
Multi_Class
SGD
Epoch 0: Validation loss decreased (inf --> 0.312703).  Saving model ...
	 Train_Loss: 0.3501 Train_Acc: 0.002 Val_Loss: 0.3127  BEST VAL Loss: 0.3127  Val_Acc: 0.000

Epoch 1: Validation loss decreased (0.312703 --> 0.308718).  Saving model ...
	 Train_Loss: 0.3325 Train_Acc: 0.002 Val_Loss: 0.3087  BEST VAL Loss: 0.3087  Val_Acc: 0.000

Epoch 2: Validation loss decreased (0.308718 --> 0.306700).  Saving model ...
	 Train_Loss: 0.3228 Train_Acc: 0.001 Val_Loss: 0.3067  BEST VAL Loss: 0.3067  Val_Acc: 0.000

Epoch 3: Validation loss decreased (0.306700 --> 0.304733).  Saving model ...
	 Train_Loss: 0.3158 Train_Acc: 0.001 Val_Loss: 0.3047  BEST VAL Loss: 0.3047  Val_Acc: 0.000

Epoch 4: Validation loss decreased (0.304733 --> 0.303297).  Saving model ...
	 Train_Loss: 0.3104 Train_Acc: 0.001 Val_Loss: 0.3033  BEST VAL Loss: 0.3033  Val_Acc: 0.000

Epoch 5: Validation loss decreased (0.303297 --> 0.301355).  Saving model ...
	 Train_Loss: 0.3058 Train_Acc: 0.001 Val_Loss: 0.3014  BEST VAL Loss: 0.3014  Val_Acc: 0.000

Epoch 6: Validation loss decreased (0.301355 --> 0.300233).  Saving model ...
	 Train_Loss: 0.3021 Train_Acc: 0.000 Val_Loss: 0.3002  BEST VAL Loss: 0.3002  Val_Acc: 0.000

Epoch 7: Validation loss decreased (0.300233 --> 0.298864).  Saving model ...
	 Train_Loss: 0.2990 Train_Acc: 0.001 Val_Loss: 0.2989  BEST VAL Loss: 0.2989  Val_Acc: 0.000

Epoch 8: Validation loss decreased (0.298864 --> 0.297971).  Saving model ...
	 Train_Loss: 0.2962 Train_Acc: 0.001 Val_Loss: 0.2980  BEST VAL Loss: 0.2980  Val_Acc: 0.000

Epoch 9: Validation loss decreased (0.297971 --> 0.296896).  Saving model ...
	 Train_Loss: 0.2937 Train_Acc: 0.000 Val_Loss: 0.2969  BEST VAL Loss: 0.2969  Val_Acc: 0.000

Epoch 10: Validation loss decreased (0.296896 --> 0.295659).  Saving model ...
	 Train_Loss: 0.2914 Train_Acc: 0.001 Val_Loss: 0.2957  BEST VAL Loss: 0.2957  Val_Acc: 0.000

Epoch 11: Validation loss decreased (0.295659 --> 0.295408).  Saving model ...
	 Train_Loss: 0.2893 Train_Acc: 0.001 Val_Loss: 0.2954  BEST VAL Loss: 0.2954  Val_Acc: 0.000

Epoch 12: Validation loss decreased (0.295408 --> 0.294517).  Saving model ...
	 Train_Loss: 0.2874 Train_Acc: 0.000 Val_Loss: 0.2945  BEST VAL Loss: 0.2945  Val_Acc: 0.000

Epoch 13: Validation loss decreased (0.294517 --> 0.293606).  Saving model ...
	 Train_Loss: 0.2856 Train_Acc: 0.001 Val_Loss: 0.2936  BEST VAL Loss: 0.2936  Val_Acc: 0.000

Epoch 14: Validation loss decreased (0.293606 --> 0.292809).  Saving model ...
	 Train_Loss: 0.2840 Train_Acc: 0.001 Val_Loss: 0.2928  BEST VAL Loss: 0.2928  Val_Acc: 0.000

Epoch 15: Validation loss decreased (0.292809 --> 0.292086).  Saving model ...
	 Train_Loss: 0.2824 Train_Acc: 0.000 Val_Loss: 0.2921  BEST VAL Loss: 0.2921  Val_Acc: 0.000

Epoch 16: Validation loss decreased (0.292086 --> 0.291804).  Saving model ...
	 Train_Loss: 0.2810 Train_Acc: 0.001 Val_Loss: 0.2918  BEST VAL Loss: 0.2918  Val_Acc: 0.000

Epoch 17: Validation loss decreased (0.291804 --> 0.291184).  Saving model ...
	 Train_Loss: 0.2797 Train_Acc: 0.001 Val_Loss: 0.2912  BEST VAL Loss: 0.2912  Val_Acc: 0.000

Epoch 18: Validation loss decreased (0.291184 --> 0.290549).  Saving model ...
	 Train_Loss: 0.2783 Train_Acc: 0.000 Val_Loss: 0.2905  BEST VAL Loss: 0.2905  Val_Acc: 0.000

Epoch 19: Validation loss decreased (0.290549 --> 0.290079).  Saving model ...
	 Train_Loss: 0.2771 Train_Acc: 0.001 Val_Loss: 0.2901  BEST VAL Loss: 0.2901  Val_Acc: 0.000

Epoch 20: Validation loss decreased (0.290079 --> 0.289455).  Saving model ...
	 Train_Loss: 0.2760 Train_Acc: 0.001 Val_Loss: 0.2895  BEST VAL Loss: 0.2895  Val_Acc: 0.000

Epoch 21: Validation loss decreased (0.289455 --> 0.289149).  Saving model ...
	 Train_Loss: 0.2748 Train_Acc: 0.000 Val_Loss: 0.2891  BEST VAL Loss: 0.2891  Val_Acc: 0.000

Epoch 22: Validation loss decreased (0.289149 --> 0.288642).  Saving model ...
	 Train_Loss: 0.2737 Train_Acc: 0.001 Val_Loss: 0.2886  BEST VAL Loss: 0.2886  Val_Acc: 0.000

Epoch 23: Validation loss decreased (0.288642 --> 0.288145).  Saving model ...
	 Train_Loss: 0.2727 Train_Acc: 0.000 Val_Loss: 0.2881  BEST VAL Loss: 0.2881  Val_Acc: 0.000

Epoch 24: Validation loss decreased (0.288145 --> 0.287782).  Saving model ...
	 Train_Loss: 0.2717 Train_Acc: 0.000 Val_Loss: 0.2878  BEST VAL Loss: 0.2878  Val_Acc: 0.000

Epoch 25: Validation loss decreased (0.287782 --> 0.287532).  Saving model ...
	 Train_Loss: 0.2708 Train_Acc: 0.001 Val_Loss: 0.2875  BEST VAL Loss: 0.2875  Val_Acc: 0.000

Epoch 26: Validation loss decreased (0.287532 --> 0.287120).  Saving model ...
	 Train_Loss: 0.2699 Train_Acc: 0.001 Val_Loss: 0.2871  BEST VAL Loss: 0.2871  Val_Acc: 0.000

Epoch 27: Validation loss decreased (0.287120 --> 0.286804).  Saving model ...
	 Train_Loss: 0.2690 Train_Acc: 0.001 Val_Loss: 0.2868  BEST VAL Loss: 0.2868  Val_Acc: 0.000

Epoch 28: Validation loss decreased (0.286804 --> 0.286747).  Saving model ...
	 Train_Loss: 0.2681 Train_Acc: 0.000 Val_Loss: 0.2867  BEST VAL Loss: 0.2867  Val_Acc: 0.000

Epoch 29: Validation loss decreased (0.286747 --> 0.286649).  Saving model ...
	 Train_Loss: 0.2673 Train_Acc: 0.001 Val_Loss: 0.2866  BEST VAL Loss: 0.2866  Val_Acc: 0.003

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.2665 Train_Acc: 0.001 Val_Loss: 0.2867  BEST VAL Loss: 0.2866  Val_Acc: 0.003

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.2657 Train_Acc: 0.001 Val_Loss: 0.2868  BEST VAL Loss: 0.2866  Val_Acc: 0.003

Epoch 32: Validation loss decreased (0.286649 --> 0.286506).  Saving model ...
	 Train_Loss: 0.2649 Train_Acc: 0.000 Val_Loss: 0.2865  BEST VAL Loss: 0.2865  Val_Acc: 0.003

Epoch 33: Validation loss decreased (0.286506 --> 0.286350).  Saving model ...
	 Train_Loss: 0.2641 Train_Acc: 0.001 Val_Loss: 0.2863  BEST VAL Loss: 0.2863  Val_Acc: 0.003

Epoch 34: Validation loss decreased (0.286350 --> 0.286295).  Saving model ...
	 Train_Loss: 0.2634 Train_Acc: 0.001 Val_Loss: 0.2863  BEST VAL Loss: 0.2863  Val_Acc: 0.003

Epoch 35: Validation loss decreased (0.286295 --> 0.286066).  Saving model ...
	 Train_Loss: 0.2628 Train_Acc: 0.000 Val_Loss: 0.2861  BEST VAL Loss: 0.2861  Val_Acc: 0.003

Epoch 36: Validation loss decreased (0.286066 --> 0.286049).  Saving model ...
	 Train_Loss: 0.2621 Train_Acc: 0.001 Val_Loss: 0.2860  BEST VAL Loss: 0.2860  Val_Acc: 0.003

Epoch 37: Validation loss decreased (0.286049 --> 0.285735).  Saving model ...
	 Train_Loss: 0.2614 Train_Acc: 0.000 Val_Loss: 0.2857  BEST VAL Loss: 0.2857  Val_Acc: 0.003

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2608 Train_Acc: 0.001 Val_Loss: 0.2857  BEST VAL Loss: 0.2857  Val_Acc: 0.003

Epoch 39: Validation loss decreased (0.285735 --> 0.285617).  Saving model ...
	 Train_Loss: 0.2602 Train_Acc: 0.000 Val_Loss: 0.2856  BEST VAL Loss: 0.2856  Val_Acc: 0.003

Epoch 40: Validation loss decreased (0.285617 --> 0.285344).  Saving model ...
	 Train_Loss: 0.2596 Train_Acc: 0.000 Val_Loss: 0.2853  BEST VAL Loss: 0.2853  Val_Acc: 0.003

Epoch 41: Validation loss decreased (0.285344 --> 0.285324).  Saving model ...
	 Train_Loss: 0.2590 Train_Acc: 0.001 Val_Loss: 0.2853  BEST VAL Loss: 0.2853  Val_Acc: 0.003

Epoch 42: Validation loss decreased (0.285324 --> 0.285145).  Saving model ...
	 Train_Loss: 0.2584 Train_Acc: 0.000 Val_Loss: 0.2851  BEST VAL Loss: 0.2851  Val_Acc: 0.003

Epoch 43: Validation loss decreased (0.285145 --> 0.285076).  Saving model ...
	 Train_Loss: 0.2578 Train_Acc: 0.000 Val_Loss: 0.2851  BEST VAL Loss: 0.2851  Val_Acc: 0.003

Epoch 44: Validation loss decreased (0.285076 --> 0.284926).  Saving model ...
	 Train_Loss: 0.2573 Train_Acc: 0.001 Val_Loss: 0.2849  BEST VAL Loss: 0.2849  Val_Acc: 0.003

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2568 Train_Acc: 0.000 Val_Loss: 0.2849  BEST VAL Loss: 0.2849  Val_Acc: 0.003

Epoch 46: Validation loss decreased (0.284926 --> 0.284912).  Saving model ...
	 Train_Loss: 0.2562 Train_Acc: 0.000 Val_Loss: 0.2849  BEST VAL Loss: 0.2849  Val_Acc: 0.003

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.2557 Train_Acc: 0.000 Val_Loss: 0.2849  BEST VAL Loss: 0.2849  Val_Acc: 0.003

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.2552 Train_Acc: 0.001 Val_Loss: 0.2849  BEST VAL Loss: 0.2849  Val_Acc: 0.000

Epoch 49: Validation loss decreased (0.284912 --> 0.284754).  Saving model ...
	 Train_Loss: 0.2547 Train_Acc: 0.001 Val_Loss: 0.2848  BEST VAL Loss: 0.2848  Val_Acc: 0.003

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.2542 Train_Acc: 0.001 Val_Loss: 0.2848  BEST VAL Loss: 0.2848  Val_Acc: 0.003

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.2538 Train_Acc: 0.000 Val_Loss: 0.2848  BEST VAL Loss: 0.2848  Val_Acc: 0.003

Epoch 52: Validation loss decreased (0.284754 --> 0.284676).  Saving model ...
	 Train_Loss: 0.2533 Train_Acc: 0.001 Val_Loss: 0.2847  BEST VAL Loss: 0.2847  Val_Acc: 0.003

Epoch 53: Validation loss decreased (0.284676 --> 0.284650).  Saving model ...
	 Train_Loss: 0.2528 Train_Acc: 0.001 Val_Loss: 0.2847  BEST VAL Loss: 0.2847  Val_Acc: 0.005

Epoch 54: Validation loss decreased (0.284650 --> 0.284596).  Saving model ...
	 Train_Loss: 0.2524 Train_Acc: 0.000 Val_Loss: 0.2846  BEST VAL Loss: 0.2846  Val_Acc: 0.003

Epoch 55: Validation loss decreased (0.284596 --> 0.284585).  Saving model ...
	 Train_Loss: 0.2519 Train_Acc: 0.001 Val_Loss: 0.2846  BEST VAL Loss: 0.2846  Val_Acc: 0.003

Epoch 56: Validation loss decreased (0.284585 --> 0.284463).  Saving model ...
	 Train_Loss: 0.2515 Train_Acc: 0.001 Val_Loss: 0.2845  BEST VAL Loss: 0.2845  Val_Acc: 0.005

Epoch 57: Validation loss decreased (0.284463 --> 0.284354).  Saving model ...
	 Train_Loss: 0.2511 Train_Acc: 0.001 Val_Loss: 0.2844  BEST VAL Loss: 0.2844  Val_Acc: 0.003

Epoch 58: Validation loss decreased (0.284354 --> 0.284302).  Saving model ...
	 Train_Loss: 0.2507 Train_Acc: 0.001 Val_Loss: 0.2843  BEST VAL Loss: 0.2843  Val_Acc: 0.003

Epoch 59: Validation loss decreased (0.284302 --> 0.284247).  Saving model ...
	 Train_Loss: 0.2503 Train_Acc: 0.000 Val_Loss: 0.2842  BEST VAL Loss: 0.2842  Val_Acc: 0.003

Epoch 60: Validation loss decreased (0.284247 --> 0.284184).  Saving model ...
	 Train_Loss: 0.2499 Train_Acc: 0.001 Val_Loss: 0.2842  BEST VAL Loss: 0.2842  Val_Acc: 0.003

Epoch 61: Validation loss decreased (0.284184 --> 0.284168).  Saving model ...
	 Train_Loss: 0.2495 Train_Acc: 0.000 Val_Loss: 0.2842  BEST VAL Loss: 0.2842  Val_Acc: 0.003

Epoch 62: Validation loss decreased (0.284168 --> 0.284081).  Saving model ...
	 Train_Loss: 0.2491 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2841  Val_Acc: 0.003

Epoch 63: Validation loss decreased (0.284081 --> 0.284051).  Saving model ...
	 Train_Loss: 0.2487 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2841  Val_Acc: 0.003

Epoch 64: Validation loss did not decrease
	 Train_Loss: 0.2483 Train_Acc: 0.000 Val_Loss: 0.2841  BEST VAL Loss: 0.2841  Val_Acc: 0.005

Epoch 65: Validation loss did not decrease
	 Train_Loss: 0.2479 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2841  Val_Acc: 0.003

Epoch 66: Validation loss decreased (0.284051 --> 0.284006).  Saving model ...
	 Train_Loss: 0.2476 Train_Acc: 0.001 Val_Loss: 0.2840  BEST VAL Loss: 0.2840  Val_Acc: 0.005

Epoch 67: Validation loss decreased (0.284006 --> 0.283913).  Saving model ...
	 Train_Loss: 0.2472 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 68: Validation loss did not decrease
	 Train_Loss: 0.2469 Train_Acc: 0.001 Val_Loss: 0.2840  BEST VAL Loss: 0.2839  Val_Acc: 0.003

Epoch 69: Validation loss did not decrease
	 Train_Loss: 0.2465 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2839  Val_Acc: 0.003

Epoch 70: Validation loss did not decrease
	 Train_Loss: 0.2462 Train_Acc: 0.000 Val_Loss: 0.2841  BEST VAL Loss: 0.2839  Val_Acc: 0.003

Epoch 71: Validation loss did not decrease
	 Train_Loss: 0.2458 Train_Acc: 0.002 Val_Loss: 0.2840  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 72: Validation loss did not decrease
	 Train_Loss: 0.2455 Train_Acc: 0.001 Val_Loss: 0.2840  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 73: Validation loss decreased (0.283913 --> 0.283907).  Saving model ...
	 Train_Loss: 0.2452 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 74: Validation loss did not decrease
	 Train_Loss: 0.2449 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 75: Validation loss decreased (0.283907 --> 0.283870).  Saving model ...
	 Train_Loss: 0.2445 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.003

Epoch 76: Validation loss did not decrease
	 Train_Loss: 0.2442 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 77: Validation loss did not decrease
	 Train_Loss: 0.2439 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.003

Epoch 78: Validation loss did not decrease
	 Train_Loss: 0.2436 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 79: Validation loss did not decrease
	 Train_Loss: 0.2433 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2839  Val_Acc: 0.005

Epoch 80: Validation loss decreased (0.283870 --> 0.283842).  Saving model ...
	 Train_Loss: 0.2430 Train_Acc: 0.001 Val_Loss: 0.2838  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 81: Validation loss did not decrease
	 Train_Loss: 0.2427 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 82: Validation loss did not decrease
	 Train_Loss: 0.2424 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 83: Validation loss did not decrease
	 Train_Loss: 0.2421 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 84: Validation loss did not decrease
	 Train_Loss: 0.2419 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 85: Validation loss did not decrease
	 Train_Loss: 0.2416 Train_Acc: 0.000 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 86: Validation loss did not decrease
	 Train_Loss: 0.2413 Train_Acc: 0.001 Val_Loss: 0.2838  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 87: Validation loss did not decrease
	 Train_Loss: 0.2410 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.003

Epoch 88: Validation loss did not decrease
	 Train_Loss: 0.2408 Train_Acc: 0.001 Val_Loss: 0.2839  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 89: Validation loss did not decrease
	 Train_Loss: 0.2405 Train_Acc: 0.000 Val_Loss: 0.2840  BEST VAL Loss: 0.2838  Val_Acc: 0.003

Epoch 90: Validation loss did not decrease
	 Train_Loss: 0.2402 Train_Acc: 0.002 Val_Loss: 0.2840  BEST VAL Loss: 0.2838  Val_Acc: 0.003

Epoch 91: Validation loss did not decrease
	 Train_Loss: 0.2400 Train_Acc: 0.000 Val_Loss: 0.2841  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 92: Validation loss did not decrease
	 Train_Loss: 0.2397 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2838  Val_Acc: 0.003

Epoch 93: Validation loss did not decrease
	 Train_Loss: 0.2395 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 94: Validation loss did not decrease
	 Train_Loss: 0.2392 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2838  Val_Acc: 0.003

Epoch 95: Validation loss did not decrease
	 Train_Loss: 0.2390 Train_Acc: 0.001 Val_Loss: 0.2841  BEST VAL Loss: 0.2838  Val_Acc: 0.005

Epoch 96: Validation loss did not decrease
Early stopped at epoch : 96
MultiClass_MLP
              precision    recall  f1-score   support

           0       0.98      0.92      0.95      7972
           1       0.81      0.97      0.88     89086
           2       0.92      0.64      0.75     55217

    accuracy                           0.84    152275
   macro avg       0.90      0.84      0.86    152275
weighted avg       0.86      0.84      0.84    152275

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.87      0.69      0.77      1993
           1       0.76      0.92      0.83     22273
           2       0.82      0.56      0.67     13803

    accuracy                           0.78     38069
   macro avg       0.82      0.72      0.76     38069
weighted avg       0.79      0.78      0.77     38069

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.69      0.88      0.77      7812
           1       0.93      0.72      0.81    142666
           2       0.54      0.84      0.66     51572

    accuracy                           0.76    202050
   macro avg       0.72      0.82      0.75    202050
weighted avg       0.82      0.76      0.77    202050

Precision for class 0: 0.8795442908346134
Recall for class 0: 0.6895132965378826
Precision for class 1: 0.7242790854162869
Recall for class 1: 0.9278915229885057
Precision for class 2: 0.842492049949585
Recall for class 2: 0.5382347475998761
3
              precision    recall  f1-score   support

           0       0.88      0.69      0.77      9965
           1       0.72      0.93      0.81    111360
           2       0.84      0.54      0.66     80725

    accuracy                           0.76    202050
   macro avg       0.82      0.72      0.75    202050
weighted avg       0.78      0.76      0.75    202050

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       384
           1       0.00      0.00      0.00     37978
           2       0.30      1.00      0.46     16245

    accuracy                           0.30     54607
   macro avg       0.10      0.33      0.15     54607
weighted avg       0.09      0.30      0.14     54607

Precision for class 0: 0.0
Recall for class 0: 0.0
Precision for class 1: 0.0
Recall for class 1: 0.0
Precision for class 2: 1.0
Recall for class 2: 0.29748933286941237
3
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         0
           2       1.00      0.30      0.46     54607

    accuracy                           0.30     54607
   macro avg       0.33      0.10      0.15     54607
weighted avg       1.00      0.30      0.46     54607

MultiClass_MLP
              precision    recall  f1-score   support

           0       0.68      0.88      0.77      5392
           1       0.93      0.65      0.76    104617
           2       0.50      0.87      0.64     40892

    accuracy                           0.72    150901
   macro avg       0.70      0.80      0.72    150901
weighted avg       0.80      0.72      0.73    150901

Precision for class 0: 0.8829747774480712
Recall for class 0: 0.6755107832009081
Precision for class 1: 0.6489193916858637
Recall for class 1: 0.9292851863005448
Precision for class 2: 0.8707326616453096
Recall for class 2: 0.5029167078631054
3
              precision    recall  f1-score   support

           0       0.88      0.68      0.77      7048
           1       0.65      0.93      0.76     73054
           2       0.87      0.50      0.64     70799

    accuracy                           0.72    150901
   macro avg       0.80      0.70      0.72    150901
weighted avg       0.76      0.72      0.70    150901

Done
