[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '837aa4a8'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '13042513'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'c119e1c1'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f2749cde'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (29984, 1276)
Number of total missing values across all columns: 27532
Data Subset Is Off
Wells held out for testing: ['D14' 'K20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'K16' 'K17' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.196630).  Saving model ...
	 Train_Loss: 0.3118 Train_Acc: 86.047 Val_Loss: 0.1966  BEST VAL Loss: 0.1966  Val_Acc: 92.023

Epoch 1: Validation loss decreased (0.196630 --> 0.173569).  Saving model ...
	 Train_Loss: 0.2359 Train_Acc: 93.431 Val_Loss: 0.1736  BEST VAL Loss: 0.1736  Val_Acc: 94.094

Epoch 2: Validation loss decreased (0.173569 --> 0.160193).  Saving model ...
	 Train_Loss: 0.1975 Train_Acc: 95.052 Val_Loss: 0.1602  BEST VAL Loss: 0.1602  Val_Acc: 95.064

Epoch 3: Validation loss decreased (0.160193 --> 0.150037).  Saving model ...
	 Train_Loss: 0.1715 Train_Acc: 96.110 Val_Loss: 0.1500  BEST VAL Loss: 0.1500  Val_Acc: 95.593

Epoch 4: Validation loss decreased (0.150037 --> 0.142816).  Saving model ...
	 Train_Loss: 0.1525 Train_Acc: 96.815 Val_Loss: 0.1428  BEST VAL Loss: 0.1428  Val_Acc: 95.813

Epoch 5: Validation loss decreased (0.142816 --> 0.137305).  Saving model ...
	 Train_Loss: 0.1379 Train_Acc: 97.223 Val_Loss: 0.1373  BEST VAL Loss: 0.1373  Val_Acc: 96.386

Epoch 6: Validation loss decreased (0.137305 --> 0.134776).  Saving model ...
	 Train_Loss: 0.1271 Train_Acc: 97.471 Val_Loss: 0.1348  BEST VAL Loss: 0.1348  Val_Acc: 96.695

Epoch 7: Validation loss decreased (0.134776 --> 0.132945).  Saving model ...
	 Train_Loss: 0.1182 Train_Acc: 97.867 Val_Loss: 0.1329  BEST VAL Loss: 0.1329  Val_Acc: 96.695

Epoch 8: Validation loss decreased (0.132945 --> 0.130167).  Saving model ...
	 Train_Loss: 0.1112 Train_Acc: 97.653 Val_Loss: 0.1302  BEST VAL Loss: 0.1302  Val_Acc: 96.695

Epoch 9: Validation loss decreased (0.130167 --> 0.129370).  Saving model ...
	 Train_Loss: 0.1057 Train_Acc: 97.928 Val_Loss: 0.1294  BEST VAL Loss: 0.1294  Val_Acc: 95.813

Epoch 10: Validation loss decreased (0.129370 --> 0.128062).  Saving model ...
	 Train_Loss: 0.1010 Train_Acc: 97.801 Val_Loss: 0.1281  BEST VAL Loss: 0.1281  Val_Acc: 96.695

Epoch 11: Validation loss decreased (0.128062 --> 0.125625).  Saving model ...
	 Train_Loss: 0.0962 Train_Acc: 98.248 Val_Loss: 0.1256  BEST VAL Loss: 0.1256  Val_Acc: 97.268

Epoch 12: Validation loss decreased (0.125625 --> 0.124826).  Saving model ...
	 Train_Loss: 0.0918 Train_Acc: 98.479 Val_Loss: 0.1248  BEST VAL Loss: 0.1248  Val_Acc: 97.047

Epoch 13: Validation loss decreased (0.124826 --> 0.123740).  Saving model ...
	 Train_Loss: 0.0883 Train_Acc: 98.281 Val_Loss: 0.1237  BEST VAL Loss: 0.1237  Val_Acc: 97.400

Epoch 14: Validation loss decreased (0.123740 --> 0.122701).  Saving model ...
	 Train_Loss: 0.0856 Train_Acc: 98.011 Val_Loss: 0.1227  BEST VAL Loss: 0.1227  Val_Acc: 97.003

Epoch 15: Validation loss decreased (0.122701 --> 0.121494).  Saving model ...
	 Train_Loss: 0.0824 Train_Acc: 98.584 Val_Loss: 0.1215  BEST VAL Loss: 0.1215  Val_Acc: 97.091

Epoch 16: Validation loss decreased (0.121494 --> 0.120238).  Saving model ...
	 Train_Loss: 0.0795 Train_Acc: 98.672 Val_Loss: 0.1202  BEST VAL Loss: 0.1202  Val_Acc: 97.003

Epoch 17: Validation loss decreased (0.120238 --> 0.119500).  Saving model ...
	 Train_Loss: 0.0765 Train_Acc: 98.947 Val_Loss: 0.1195  BEST VAL Loss: 0.1195  Val_Acc: 96.827

Epoch 18: Validation loss decreased (0.119500 --> 0.118888).  Saving model ...
	 Train_Loss: 0.0738 Train_Acc: 98.981 Val_Loss: 0.1189  BEST VAL Loss: 0.1189  Val_Acc: 97.268

Epoch 19: Validation loss decreased (0.118888 --> 0.118708).  Saving model ...
	 Train_Loss: 0.0716 Train_Acc: 98.837 Val_Loss: 0.1187  BEST VAL Loss: 0.1187  Val_Acc: 97.356

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.0697 Train_Acc: 98.567 Val_Loss: 0.1195  BEST VAL Loss: 0.1187  Val_Acc: 96.827

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.0681 Train_Acc: 98.518 Val_Loss: 0.1199  BEST VAL Loss: 0.1187  Val_Acc: 96.739

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.0670 Train_Acc: 98.170 Val_Loss: 0.1203  BEST VAL Loss: 0.1187  Val_Acc: 96.606

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0659 Train_Acc: 98.352 Val_Loss: 0.1203  BEST VAL Loss: 0.1187  Val_Acc: 96.651

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.0645 Train_Acc: 98.749 Val_Loss: 0.1196  BEST VAL Loss: 0.1187  Val_Acc: 97.135

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.0632 Train_Acc: 98.909 Val_Loss: 0.1189  BEST VAL Loss: 0.1187  Val_Acc: 97.047

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.0618 Train_Acc: 98.931 Val_Loss: 0.1189  BEST VAL Loss: 0.1187  Val_Acc: 96.959

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.0604 Train_Acc: 99.102 Val_Loss: 0.1189  BEST VAL Loss: 0.1187  Val_Acc: 96.827

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.0591 Train_Acc: 99.096 Val_Loss: 0.1189  BEST VAL Loss: 0.1187  Val_Acc: 97.223

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.0579 Train_Acc: 99.019 Val_Loss: 0.1188  BEST VAL Loss: 0.1187  Val_Acc: 97.135

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0567 Train_Acc: 99.179 Val_Loss: 0.1190  BEST VAL Loss: 0.1187  Val_Acc: 97.223

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.0556 Train_Acc: 99.223 Val_Loss: 0.1197  BEST VAL Loss: 0.1187  Val_Acc: 96.827

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0545 Train_Acc: 99.256 Val_Loss: 0.1203  BEST VAL Loss: 0.1187  Val_Acc: 97.268

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0534 Train_Acc: 99.273 Val_Loss: 0.1210  BEST VAL Loss: 0.1187  Val_Acc: 97.223

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0525 Train_Acc: 99.157 Val_Loss: 0.1209  BEST VAL Loss: 0.1187  Val_Acc: 97.223

Epoch 35: Validation loss did not decrease
Early stopped at epoch : 35
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       1.00      0.99      0.99      9777
           1       0.99      1.00      0.99      8370

    accuracy                           0.99     18147
   macro avg       0.99      0.99      0.99     18147
weighted avg       0.99      0.99      0.99     18147

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1222
           1       0.97      0.98      0.97      1047

    accuracy                           0.97      2269
   macro avg       0.97      0.97      0.97      2269
weighted avg       0.97      0.97      0.97      2269

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1223
           1       0.97      0.97      0.97      1046

    accuracy                           0.97      2269
   macro avg       0.97      0.97      0.97      2269
weighted avg       0.97      0.97      0.97      2269

              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1223
           1       0.97      0.97      0.97      1046

    accuracy                           0.97      2269
   macro avg       0.97      0.97      0.97      2269
weighted avg       0.97      0.97      0.97      2269

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      3996
           1       0.97      0.98      0.97      3303

    accuracy                           0.97      7299
   macro avg       0.97      0.97      0.97      7299
weighted avg       0.97      0.97      0.97      7299

              precision    recall  f1-score   support

           0       0.98      0.97      0.98      3996
           1       0.97      0.98      0.97      3303

    accuracy                           0.97      7299
   macro avg       0.97      0.97      0.97      7299
weighted avg       0.97      0.97      0.97      7299

completed
