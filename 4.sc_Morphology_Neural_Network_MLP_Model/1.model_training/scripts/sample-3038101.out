[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'd7e85a61'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '327a2ec1'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a822282c'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '8d3a472b'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Flagellin_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Flagellin_1.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_Nigericin_1.000_10.0_DMSO_0.025' 'Flagellin_1.000_DMSO_0.025']
The dimensions of the data are: (285713, 1270)
Number of total missing values across all columns: 245710
Data Subset Is Off
Wells held out for testing: ['M08' 'M10']
Wells to use for training, validation, and testing ['M02' 'M03' 'M05' 'M09' 'M11']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.117785).  Saving model ...
	 Train_Loss: 0.1983 Train_Acc: 92.389 Val_Loss: 0.1178  BEST VAL Loss: 0.1178  Val_Acc: 96.075

Epoch 1: Validation loss decreased (0.117785 --> 0.107907).  Saving model ...
	 Train_Loss: 0.1607 Train_Acc: 95.547 Val_Loss: 0.1079  BEST VAL Loss: 0.1079  Val_Acc: 96.751

Epoch 2: Validation loss decreased (0.107907 --> 0.103294).  Saving model ...
	 Train_Loss: 0.1428 Train_Acc: 96.141 Val_Loss: 0.1033  BEST VAL Loss: 0.1033  Val_Acc: 96.742

Epoch 3: Validation loss decreased (0.103294 --> 0.099492).  Saving model ...
	 Train_Loss: 0.1318 Train_Acc: 96.437 Val_Loss: 0.0995  BEST VAL Loss: 0.0995  Val_Acc: 96.988

Epoch 4: Validation loss decreased (0.099492 --> 0.097978).  Saving model ...
	 Train_Loss: 0.1239 Train_Acc: 96.621 Val_Loss: 0.0980  BEST VAL Loss: 0.0980  Val_Acc: 96.877

Epoch 5: Validation loss decreased (0.097978 --> 0.094988).  Saving model ...
	 Train_Loss: 0.1181 Train_Acc: 96.741 Val_Loss: 0.0950  BEST VAL Loss: 0.0950  Val_Acc: 97.205

Epoch 6: Validation loss decreased (0.094988 --> 0.092439).  Saving model ...
	 Train_Loss: 0.1133 Train_Acc: 96.889 Val_Loss: 0.0924  BEST VAL Loss: 0.0924  Val_Acc: 97.258

Epoch 7: Validation loss decreased (0.092439 --> 0.090149).  Saving model ...
	 Train_Loss: 0.1097 Train_Acc: 96.913 Val_Loss: 0.0901  BEST VAL Loss: 0.0901  Val_Acc: 97.417

Epoch 8: Validation loss decreased (0.090149 --> 0.088737).  Saving model ...
	 Train_Loss: 0.1063 Train_Acc: 97.052 Val_Loss: 0.0887  BEST VAL Loss: 0.0887  Val_Acc: 97.437

Epoch 9: Validation loss decreased (0.088737 --> 0.087648).  Saving model ...
	 Train_Loss: 0.1034 Train_Acc: 97.174 Val_Loss: 0.0876  BEST VAL Loss: 0.0876  Val_Acc: 97.388

Epoch 10: Validation loss decreased (0.087648 --> 0.086469).  Saving model ...
	 Train_Loss: 0.1011 Train_Acc: 97.151 Val_Loss: 0.0865  BEST VAL Loss: 0.0865  Val_Acc: 97.442

Epoch 11: Validation loss decreased (0.086469 --> 0.086275).  Saving model ...
	 Train_Loss: 0.0991 Train_Acc: 97.172 Val_Loss: 0.0863  BEST VAL Loss: 0.0863  Val_Acc: 97.345

Epoch 12: Validation loss decreased (0.086275 --> 0.085177).  Saving model ...
	 Train_Loss: 0.0972 Train_Acc: 97.256 Val_Loss: 0.0852  BEST VAL Loss: 0.0852  Val_Acc: 97.601

Epoch 13: Validation loss decreased (0.085177 --> 0.084552).  Saving model ...
	 Train_Loss: 0.0955 Train_Acc: 97.224 Val_Loss: 0.0846  BEST VAL Loss: 0.0846  Val_Acc: 97.311

Epoch 14: Validation loss decreased (0.084552 --> 0.084064).  Saving model ...
	 Train_Loss: 0.0940 Train_Acc: 97.291 Val_Loss: 0.0841  BEST VAL Loss: 0.0841  Val_Acc: 97.335

Epoch 15: Validation loss decreased (0.084064 --> 0.083180).  Saving model ...
	 Train_Loss: 0.0926 Train_Acc: 97.355 Val_Loss: 0.0832  BEST VAL Loss: 0.0832  Val_Acc: 97.596

Epoch 16: Validation loss decreased (0.083180 --> 0.082562).  Saving model ...
	 Train_Loss: 0.0914 Train_Acc: 97.335 Val_Loss: 0.0826  BEST VAL Loss: 0.0826  Val_Acc: 97.417

Epoch 17: Validation loss decreased (0.082562 --> 0.082084).  Saving model ...
	 Train_Loss: 0.0903 Train_Acc: 97.401 Val_Loss: 0.0821  BEST VAL Loss: 0.0821  Val_Acc: 97.485

Epoch 18: Validation loss decreased (0.082084 --> 0.081655).  Saving model ...
	 Train_Loss: 0.0893 Train_Acc: 97.356 Val_Loss: 0.0817  BEST VAL Loss: 0.0817  Val_Acc: 97.538

Epoch 19: Validation loss decreased (0.081655 --> 0.081101).  Saving model ...
	 Train_Loss: 0.0882 Train_Acc: 97.527 Val_Loss: 0.0811  BEST VAL Loss: 0.0811  Val_Acc: 97.567

Epoch 20: Validation loss decreased (0.081101 --> 0.080727).  Saving model ...
	 Train_Loss: 0.0872 Train_Acc: 97.532 Val_Loss: 0.0807  BEST VAL Loss: 0.0807  Val_Acc: 97.538

Epoch 21: Validation loss decreased (0.080727 --> 0.080353).  Saving model ...
	 Train_Loss: 0.0861 Train_Acc: 97.581 Val_Loss: 0.0804  BEST VAL Loss: 0.0804  Val_Acc: 97.606

Epoch 22: Validation loss decreased (0.080353 --> 0.080186).  Saving model ...
	 Train_Loss: 0.0853 Train_Acc: 97.511 Val_Loss: 0.0802  BEST VAL Loss: 0.0802  Val_Acc: 97.446

Epoch 23: Validation loss decreased (0.080186 --> 0.080018).  Saving model ...
	 Train_Loss: 0.0844 Train_Acc: 97.571 Val_Loss: 0.0800  BEST VAL Loss: 0.0800  Val_Acc: 97.586

Epoch 24: Validation loss decreased (0.080018 --> 0.079675).  Saving model ...
	 Train_Loss: 0.0836 Train_Acc: 97.556 Val_Loss: 0.0797  BEST VAL Loss: 0.0797  Val_Acc: 97.582

Epoch 25: Validation loss decreased (0.079675 --> 0.079534).  Saving model ...
	 Train_Loss: 0.0829 Train_Acc: 97.588 Val_Loss: 0.0795  BEST VAL Loss: 0.0795  Val_Acc: 97.413

Epoch 26: Validation loss decreased (0.079534 --> 0.079284).  Saving model ...
	 Train_Loss: 0.0822 Train_Acc: 97.590 Val_Loss: 0.0793  BEST VAL Loss: 0.0793  Val_Acc: 97.586

Epoch 27: Validation loss decreased (0.079284 --> 0.079171).  Saving model ...
	 Train_Loss: 0.0816 Train_Acc: 97.600 Val_Loss: 0.0792  BEST VAL Loss: 0.0792  Val_Acc: 97.528

Epoch 28: Validation loss decreased (0.079171 --> 0.078836).  Saving model ...
	 Train_Loss: 0.0810 Train_Acc: 97.539 Val_Loss: 0.0788  BEST VAL Loss: 0.0788  Val_Acc: 97.659

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.0804 Train_Acc: 97.656 Val_Loss: 0.0789  BEST VAL Loss: 0.0788  Val_Acc: 97.567

Epoch 30: Validation loss decreased (0.078836 --> 0.078807).  Saving model ...
	 Train_Loss: 0.0798 Train_Acc: 97.690 Val_Loss: 0.0788  BEST VAL Loss: 0.0788  Val_Acc: 97.611

Epoch 31: Validation loss decreased (0.078807 --> 0.078777).  Saving model ...
	 Train_Loss: 0.0792 Train_Acc: 97.706 Val_Loss: 0.0788  BEST VAL Loss: 0.0788  Val_Acc: 97.417

Epoch 32: Validation loss decreased (0.078777 --> 0.078521).  Saving model ...
	 Train_Loss: 0.0787 Train_Acc: 97.708 Val_Loss: 0.0785  BEST VAL Loss: 0.0785  Val_Acc: 97.702

Epoch 33: Validation loss decreased (0.078521 --> 0.078361).  Saving model ...
	 Train_Loss: 0.0782 Train_Acc: 97.669 Val_Loss: 0.0784  BEST VAL Loss: 0.0784  Val_Acc: 97.519

Epoch 34: Validation loss decreased (0.078361 --> 0.078284).  Saving model ...
	 Train_Loss: 0.0777 Train_Acc: 97.680 Val_Loss: 0.0783  BEST VAL Loss: 0.0783  Val_Acc: 97.615

Epoch 35: Validation loss decreased (0.078284 --> 0.078090).  Saving model ...
	 Train_Loss: 0.0772 Train_Acc: 97.737 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.664

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.0768 Train_Acc: 97.703 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.533

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.0764 Train_Acc: 97.707 Val_Loss: 0.0782  BEST VAL Loss: 0.0781  Val_Acc: 97.673

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0759 Train_Acc: 97.751 Val_Loss: 0.0783  BEST VAL Loss: 0.0781  Val_Acc: 97.596

Epoch 39: Validation loss decreased (0.078090 --> 0.078062).  Saving model ...
	 Train_Loss: 0.0755 Train_Acc: 97.821 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.866

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0751 Train_Acc: 97.688 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.611

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0748 Train_Acc: 97.752 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.770

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0744 Train_Acc: 97.789 Val_Loss: 0.0782  BEST VAL Loss: 0.0781  Val_Acc: 97.553

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0741 Train_Acc: 97.741 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.722

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.0738 Train_Acc: 97.826 Val_Loss: 0.0781  BEST VAL Loss: 0.0781  Val_Acc: 97.635

Epoch 45: Validation loss decreased (0.078062 --> 0.077993).  Saving model ...
	 Train_Loss: 0.0734 Train_Acc: 97.830 Val_Loss: 0.0780  BEST VAL Loss: 0.0780  Val_Acc: 97.466

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.0731 Train_Acc: 97.813 Val_Loss: 0.0781  BEST VAL Loss: 0.0780  Val_Acc: 97.591

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.0728 Train_Acc: 97.840 Val_Loss: 0.0781  BEST VAL Loss: 0.0780  Val_Acc: 97.770

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.0725 Train_Acc: 97.825 Val_Loss: 0.0781  BEST VAL Loss: 0.0780  Val_Acc: 97.451

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.0722 Train_Acc: 97.750 Val_Loss: 0.0780  BEST VAL Loss: 0.0780  Val_Acc: 97.659

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.0719 Train_Acc: 97.869 Val_Loss: 0.0780  BEST VAL Loss: 0.0780  Val_Acc: 97.678

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.0716 Train_Acc: 97.948 Val_Loss: 0.0782  BEST VAL Loss: 0.0780  Val_Acc: 97.562

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.0713 Train_Acc: 97.888 Val_Loss: 0.0783  BEST VAL Loss: 0.0780  Val_Acc: 97.606

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.0710 Train_Acc: 97.948 Val_Loss: 0.0783  BEST VAL Loss: 0.0780  Val_Acc: 97.528

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.0707 Train_Acc: 97.917 Val_Loss: 0.0783  BEST VAL Loss: 0.0780  Val_Acc: 97.644

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.0705 Train_Acc: 97.909 Val_Loss: 0.0786  BEST VAL Loss: 0.0780  Val_Acc: 97.495

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.0702 Train_Acc: 97.840 Val_Loss: 0.0785  BEST VAL Loss: 0.0780  Val_Acc: 97.857

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.0700 Train_Acc: 97.902 Val_Loss: 0.0785  BEST VAL Loss: 0.0780  Val_Acc: 97.533

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.0697 Train_Acc: 97.889 Val_Loss: 0.0785  BEST VAL Loss: 0.0780  Val_Acc: 97.635

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.0695 Train_Acc: 97.958 Val_Loss: 0.0787  BEST VAL Loss: 0.0780  Val_Acc: 97.625

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.0693 Train_Acc: 97.901 Val_Loss: 0.0788  BEST VAL Loss: 0.0780  Val_Acc: 97.509

Epoch 61: Validation loss did not decrease
Early stopped at epoch : 61
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.34      0.34      0.34     56122
           1       0.66      0.65      0.66    109598

    accuracy                           0.55    165720
   macro avg       0.50      0.50      0.50    165720
weighted avg       0.55      0.55      0.55    165720

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.34      0.35      0.34      7016
           1       0.66      0.66      0.66     13700

    accuracy                           0.55     20716
   macro avg       0.50      0.50      0.50     20716
weighted avg       0.55      0.55      0.55     20716

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.35      0.36      0.36      7015
           1       0.67      0.66      0.66     13700

    accuracy                           0.56     20715
   macro avg       0.51      0.51      0.51     20715
weighted avg       0.56      0.56      0.56     20715

              precision    recall  f1-score   support

           0       0.35      0.36      0.36      7015
           1       0.67      0.66      0.66     13700

    accuracy                           0.56     20715
   macro avg       0.51      0.51      0.51     20715
weighted avg       0.56      0.56      0.56     20715

Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Flagellin_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.43      0.40      0.42     34394
           1       0.56      0.59      0.58     44168

    accuracy                           0.51     78562
   macro avg       0.50      0.50      0.50     78562
weighted avg       0.51      0.51      0.51     78562

              precision    recall  f1-score   support

           0       0.43      0.40      0.42     34394
           1       0.56      0.59      0.58     44168

    accuracy                           0.51     78562
   macro avg       0.50      0.50      0.50     78562
weighted avg       0.51      0.51      0.51     78562

completed
