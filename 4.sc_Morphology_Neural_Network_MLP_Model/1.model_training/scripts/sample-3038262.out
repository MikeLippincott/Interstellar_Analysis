[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'df9f8dfd'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2ef4b35a'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '8a086e90'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c985a01c'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: PBMC control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_10.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: PBMC
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_10.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['LPS_10.000_DMSO_0.025' 'Thapsigargin_10.000_DMSO_0.025']
The dimensions of the data are: (323474, 1270)
Number of total missing values across all columns: 646948
Data Subset Is Off
Wells held out for testing: ['E09' 'L06']
Wells to use for training, validation, and testing ['E02' 'E03' 'E06' 'E07' 'E08' 'L07']
Number of in features:  1245
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.326964).  Saving model ...
	 Train_Loss: 0.4904 Train_Acc: 75.672 Val_Loss: 0.3270  BEST VAL Loss: 0.3270  Val_Acc: 85.865

Epoch 1: Validation loss decreased (0.326964 --> 0.312472).  Saving model ...
	 Train_Loss: 0.4211 Train_Acc: 84.114 Val_Loss: 0.3125  BEST VAL Loss: 0.3125  Val_Acc: 87.269

Epoch 2: Validation loss decreased (0.312472 --> 0.299941).  Saving model ...
	 Train_Loss: 0.3893 Train_Acc: 85.520 Val_Loss: 0.2999  BEST VAL Loss: 0.2999  Val_Acc: 88.268

Epoch 3: Validation loss decreased (0.299941 --> 0.291320).  Saving model ...
	 Train_Loss: 0.3706 Train_Acc: 85.930 Val_Loss: 0.2913  BEST VAL Loss: 0.2913  Val_Acc: 88.858

Epoch 4: Validation loss decreased (0.291320 --> 0.285808).  Saving model ...
	 Train_Loss: 0.3574 Train_Acc: 86.436 Val_Loss: 0.2858  BEST VAL Loss: 0.2858  Val_Acc: 88.714

Epoch 5: Validation loss decreased (0.285808 --> 0.281997).  Saving model ...
	 Train_Loss: 0.3474 Train_Acc: 86.783 Val_Loss: 0.2820  BEST VAL Loss: 0.2820  Val_Acc: 88.853

Epoch 6: Validation loss decreased (0.281997 --> 0.279310).  Saving model ...
	 Train_Loss: 0.3399 Train_Acc: 86.953 Val_Loss: 0.2793  BEST VAL Loss: 0.2793  Val_Acc: 88.605

Epoch 7: Validation loss decreased (0.279310 --> 0.276229).  Saving model ...
	 Train_Loss: 0.3336 Train_Acc: 87.290 Val_Loss: 0.2762  BEST VAL Loss: 0.2762  Val_Acc: 89.233

Epoch 8: Validation loss decreased (0.276229 --> 0.273293).  Saving model ...
	 Train_Loss: 0.3276 Train_Acc: 87.573 Val_Loss: 0.2733  BEST VAL Loss: 0.2733  Val_Acc: 89.393

Epoch 9: Validation loss decreased (0.273293 --> 0.270883).  Saving model ...
	 Train_Loss: 0.3227 Train_Acc: 87.664 Val_Loss: 0.2709  BEST VAL Loss: 0.2709  Val_Acc: 89.228

Epoch 10: Validation loss decreased (0.270883 --> 0.268895).  Saving model ...
	 Train_Loss: 0.3188 Train_Acc: 87.691 Val_Loss: 0.2689  BEST VAL Loss: 0.2689  Val_Acc: 89.549

Epoch 11: Validation loss decreased (0.268895 --> 0.266786).  Saving model ...
	 Train_Loss: 0.3150 Train_Acc: 87.965 Val_Loss: 0.2668  BEST VAL Loss: 0.2668  Val_Acc: 89.835

Epoch 12: Validation loss decreased (0.266786 --> 0.264772).  Saving model ...
	 Train_Loss: 0.3120 Train_Acc: 87.968 Val_Loss: 0.2648  BEST VAL Loss: 0.2648  Val_Acc: 89.911

Epoch 13: Validation loss decreased (0.264772 --> 0.262559).  Saving model ...
	 Train_Loss: 0.3091 Train_Acc: 88.160 Val_Loss: 0.2626  BEST VAL Loss: 0.2626  Val_Acc: 90.080

Epoch 14: Validation loss decreased (0.262559 --> 0.261175).  Saving model ...
	 Train_Loss: 0.3062 Train_Acc: 88.331 Val_Loss: 0.2612  BEST VAL Loss: 0.2612  Val_Acc: 89.806

Epoch 15: Validation loss decreased (0.261175 --> 0.259368).  Saving model ...
	 Train_Loss: 0.3036 Train_Acc: 88.350 Val_Loss: 0.2594  BEST VAL Loss: 0.2594  Val_Acc: 89.915

Epoch 16: Validation loss decreased (0.259368 --> 0.258186).  Saving model ...
	 Train_Loss: 0.3012 Train_Acc: 88.548 Val_Loss: 0.2582  BEST VAL Loss: 0.2582  Val_Acc: 89.970

Epoch 17: Validation loss decreased (0.258186 --> 0.256871).  Saving model ...
	 Train_Loss: 0.2991 Train_Acc: 88.545 Val_Loss: 0.2569  BEST VAL Loss: 0.2569  Val_Acc: 90.139

Epoch 18: Validation loss decreased (0.256871 --> 0.255783).  Saving model ...
	 Train_Loss: 0.2970 Train_Acc: 88.664 Val_Loss: 0.2558  BEST VAL Loss: 0.2558  Val_Acc: 89.903

Epoch 19: Validation loss decreased (0.255783 --> 0.254346).  Saving model ...
	 Train_Loss: 0.2952 Train_Acc: 88.608 Val_Loss: 0.2543  BEST VAL Loss: 0.2543  Val_Acc: 90.101

Epoch 20: Validation loss decreased (0.254346 --> 0.253203).  Saving model ...
	 Train_Loss: 0.2933 Train_Acc: 88.695 Val_Loss: 0.2532  BEST VAL Loss: 0.2532  Val_Acc: 90.429

Epoch 21: Validation loss decreased (0.253203 --> 0.252347).  Saving model ...
	 Train_Loss: 0.2918 Train_Acc: 88.666 Val_Loss: 0.2523  BEST VAL Loss: 0.2523  Val_Acc: 89.970

Epoch 22: Validation loss decreased (0.252347 --> 0.251581).  Saving model ...
	 Train_Loss: 0.2902 Train_Acc: 88.839 Val_Loss: 0.2516  BEST VAL Loss: 0.2516  Val_Acc: 90.134

Epoch 23: Validation loss decreased (0.251581 --> 0.250749).  Saving model ...
	 Train_Loss: 0.2887 Train_Acc: 88.845 Val_Loss: 0.2507  BEST VAL Loss: 0.2507  Val_Acc: 90.122

Epoch 24: Validation loss decreased (0.250749 --> 0.250079).  Saving model ...
	 Train_Loss: 0.2873 Train_Acc: 88.826 Val_Loss: 0.2501  BEST VAL Loss: 0.2501  Val_Acc: 90.071

Epoch 25: Validation loss decreased (0.250079 --> 0.249462).  Saving model ...
	 Train_Loss: 0.2860 Train_Acc: 88.953 Val_Loss: 0.2495  BEST VAL Loss: 0.2495  Val_Acc: 90.118

Epoch 26: Validation loss decreased (0.249462 --> 0.248884).  Saving model ...
	 Train_Loss: 0.2847 Train_Acc: 88.897 Val_Loss: 0.2489  BEST VAL Loss: 0.2489  Val_Acc: 90.193

Epoch 27: Validation loss decreased (0.248884 --> 0.247940).  Saving model ...
	 Train_Loss: 0.2835 Train_Acc: 89.010 Val_Loss: 0.2479  BEST VAL Loss: 0.2479  Val_Acc: 90.493

Epoch 28: Validation loss decreased (0.247940 --> 0.247099).  Saving model ...
	 Train_Loss: 0.2823 Train_Acc: 89.070 Val_Loss: 0.2471  BEST VAL Loss: 0.2471  Val_Acc: 90.526

Epoch 29: Validation loss decreased (0.247099 --> 0.246446).  Saving model ...
	 Train_Loss: 0.2812 Train_Acc: 89.151 Val_Loss: 0.2464  BEST VAL Loss: 0.2464  Val_Acc: 90.703

Epoch 30: Validation loss decreased (0.246446 --> 0.245602).  Saving model ...
	 Train_Loss: 0.2802 Train_Acc: 89.150 Val_Loss: 0.2456  BEST VAL Loss: 0.2456  Val_Acc: 90.518

Epoch 31: Validation loss decreased (0.245602 --> 0.245250).  Saving model ...
	 Train_Loss: 0.2792 Train_Acc: 89.043 Val_Loss: 0.2452  BEST VAL Loss: 0.2452  Val_Acc: 90.333

Epoch 32: Validation loss decreased (0.245250 --> 0.245027).  Saving model ...
	 Train_Loss: 0.2784 Train_Acc: 88.887 Val_Loss: 0.2450  BEST VAL Loss: 0.2450  Val_Acc: 89.920

Epoch 33: Validation loss decreased (0.245027 --> 0.244317).  Saving model ...
	 Train_Loss: 0.2775 Train_Acc: 89.068 Val_Loss: 0.2443  BEST VAL Loss: 0.2443  Val_Acc: 90.615

Epoch 34: Validation loss decreased (0.244317 --> 0.243605).  Saving model ...
	 Train_Loss: 0.2766 Train_Acc: 89.320 Val_Loss: 0.2436  BEST VAL Loss: 0.2436  Val_Acc: 90.590

Epoch 35: Validation loss decreased (0.243605 --> 0.243350).  Saving model ...
	 Train_Loss: 0.2756 Train_Acc: 89.276 Val_Loss: 0.2433  BEST VAL Loss: 0.2433  Val_Acc: 90.349

Epoch 36: Validation loss decreased (0.243350 --> 0.243101).  Saving model ...
	 Train_Loss: 0.2748 Train_Acc: 89.220 Val_Loss: 0.2431  BEST VAL Loss: 0.2431  Val_Acc: 89.886

Epoch 37: Validation loss decreased (0.243101 --> 0.242757).  Saving model ...
	 Train_Loss: 0.2740 Train_Acc: 89.248 Val_Loss: 0.2428  BEST VAL Loss: 0.2428  Val_Acc: 90.379

Epoch 38: Validation loss decreased (0.242757 --> 0.242224).  Saving model ...
	 Train_Loss: 0.2733 Train_Acc: 89.282 Val_Loss: 0.2422  BEST VAL Loss: 0.2422  Val_Acc: 90.518

Epoch 39: Validation loss decreased (0.242224 --> 0.241571).  Saving model ...
	 Train_Loss: 0.2725 Train_Acc: 89.473 Val_Loss: 0.2416  BEST VAL Loss: 0.2416  Val_Acc: 90.817

Epoch 40: Validation loss decreased (0.241571 --> 0.241210).  Saving model ...
	 Train_Loss: 0.2717 Train_Acc: 89.325 Val_Loss: 0.2412  BEST VAL Loss: 0.2412  Val_Acc: 90.510

Epoch 41: Validation loss decreased (0.241210 --> 0.240826).  Saving model ...
	 Train_Loss: 0.2710 Train_Acc: 89.380 Val_Loss: 0.2408  BEST VAL Loss: 0.2408  Val_Acc: 90.771

Epoch 42: Validation loss decreased (0.240826 --> 0.240275).  Saving model ...
	 Train_Loss: 0.2703 Train_Acc: 89.297 Val_Loss: 0.2403  BEST VAL Loss: 0.2403  Val_Acc: 90.577

Epoch 43: Validation loss decreased (0.240275 --> 0.239925).  Saving model ...
	 Train_Loss: 0.2697 Train_Acc: 89.317 Val_Loss: 0.2399  BEST VAL Loss: 0.2399  Val_Acc: 90.501

Epoch 44: Validation loss decreased (0.239925 --> 0.239415).  Saving model ...
	 Train_Loss: 0.2691 Train_Acc: 89.431 Val_Loss: 0.2394  BEST VAL Loss: 0.2394  Val_Acc: 90.556

Epoch 45: Validation loss decreased (0.239415 --> 0.238990).  Saving model ...
	 Train_Loss: 0.2685 Train_Acc: 89.330 Val_Loss: 0.2390  BEST VAL Loss: 0.2390  Val_Acc: 90.493

Epoch 46: Validation loss decreased (0.238990 --> 0.238977).  Saving model ...
	 Train_Loss: 0.2679 Train_Acc: 89.424 Val_Loss: 0.2390  BEST VAL Loss: 0.2390  Val_Acc: 89.557

Epoch 47: Validation loss decreased (0.238977 --> 0.238601).  Saving model ...
	 Train_Loss: 0.2673 Train_Acc: 89.378 Val_Loss: 0.2386  BEST VAL Loss: 0.2386  Val_Acc: 90.379

Epoch 48: Validation loss decreased (0.238601 --> 0.238186).  Saving model ...
	 Train_Loss: 0.2667 Train_Acc: 89.478 Val_Loss: 0.2382  BEST VAL Loss: 0.2382  Val_Acc: 90.383

Epoch 49: Validation loss decreased (0.238186 --> 0.237987).  Saving model ...
	 Train_Loss: 0.2662 Train_Acc: 89.478 Val_Loss: 0.2380  BEST VAL Loss: 0.2380  Val_Acc: 90.248

Epoch 50: Validation loss decreased (0.237987 --> 0.237832).  Saving model ...
	 Train_Loss: 0.2657 Train_Acc: 89.354 Val_Loss: 0.2378  BEST VAL Loss: 0.2378  Val_Acc: 90.198

Epoch 51: Validation loss decreased (0.237832 --> 0.237384).  Saving model ...
	 Train_Loss: 0.2651 Train_Acc: 89.501 Val_Loss: 0.2374  BEST VAL Loss: 0.2374  Val_Acc: 90.842

Epoch 52: Validation loss decreased (0.237384 --> 0.237098).  Saving model ...
	 Train_Loss: 0.2646 Train_Acc: 89.425 Val_Loss: 0.2371  BEST VAL Loss: 0.2371  Val_Acc: 90.387

Epoch 53: Validation loss decreased (0.237098 --> 0.236919).  Saving model ...
	 Train_Loss: 0.2641 Train_Acc: 89.593 Val_Loss: 0.2369  BEST VAL Loss: 0.2369  Val_Acc: 90.729

Epoch 54: Validation loss decreased (0.236919 --> 0.236615).  Saving model ...
	 Train_Loss: 0.2636 Train_Acc: 89.281 Val_Loss: 0.2366  BEST VAL Loss: 0.2366  Val_Acc: 90.653

Epoch 55: Validation loss decreased (0.236615 --> 0.236337).  Saving model ...
	 Train_Loss: 0.2631 Train_Acc: 89.607 Val_Loss: 0.2363  BEST VAL Loss: 0.2363  Val_Acc: 90.429

Epoch 56: Validation loss decreased (0.236337 --> 0.236112).  Saving model ...
	 Train_Loss: 0.2626 Train_Acc: 89.606 Val_Loss: 0.2361  BEST VAL Loss: 0.2361  Val_Acc: 90.809

Epoch 57: Validation loss decreased (0.236112 --> 0.235814).  Saving model ...
	 Train_Loss: 0.2622 Train_Acc: 89.462 Val_Loss: 0.2358  BEST VAL Loss: 0.2358  Val_Acc: 90.476

Epoch 58: Validation loss decreased (0.235814 --> 0.235489).  Saving model ...
	 Train_Loss: 0.2617 Train_Acc: 89.698 Val_Loss: 0.2355  BEST VAL Loss: 0.2355  Val_Acc: 90.581

Epoch 59: Validation loss decreased (0.235489 --> 0.235157).  Saving model ...
	 Train_Loss: 0.2613 Train_Acc: 89.479 Val_Loss: 0.2352  BEST VAL Loss: 0.2352  Val_Acc: 90.834

Epoch 60: Validation loss decreased (0.235157 --> 0.234807).  Saving model ...
	 Train_Loss: 0.2609 Train_Acc: 89.649 Val_Loss: 0.2348  BEST VAL Loss: 0.2348  Val_Acc: 90.830

Epoch 61: Validation loss decreased (0.234807 --> 0.234751).  Saving model ...
	 Train_Loss: 0.2605 Train_Acc: 89.583 Val_Loss: 0.2348  BEST VAL Loss: 0.2348  Val_Acc: 89.928

Epoch 62: Validation loss decreased (0.234751 --> 0.234633).  Saving model ...
	 Train_Loss: 0.2600 Train_Acc: 89.642 Val_Loss: 0.2346  BEST VAL Loss: 0.2346  Val_Acc: 90.421

Epoch 63: Validation loss decreased (0.234633 --> 0.234458).  Saving model ...
	 Train_Loss: 0.2597 Train_Acc: 89.562 Val_Loss: 0.2345  BEST VAL Loss: 0.2345  Val_Acc: 90.387

Epoch 64: Validation loss decreased (0.234458 --> 0.234199).  Saving model ...
	 Train_Loss: 0.2593 Train_Acc: 89.649 Val_Loss: 0.2342  BEST VAL Loss: 0.2342  Val_Acc: 90.632

Epoch 65: Validation loss decreased (0.234199 --> 0.233851).  Saving model ...
	 Train_Loss: 0.2589 Train_Acc: 89.625 Val_Loss: 0.2339  BEST VAL Loss: 0.2339  Val_Acc: 90.880

Epoch 66: Validation loss decreased (0.233851 --> 0.233612).  Saving model ...
	 Train_Loss: 0.2585 Train_Acc: 89.586 Val_Loss: 0.2336  BEST VAL Loss: 0.2336  Val_Acc: 90.737

Epoch 67: Validation loss decreased (0.233612 --> 0.233395).  Saving model ...
	 Train_Loss: 0.2582 Train_Acc: 89.585 Val_Loss: 0.2334  BEST VAL Loss: 0.2334  Val_Acc: 90.623

Epoch 68: Validation loss decreased (0.233395 --> 0.233355).  Saving model ...
	 Train_Loss: 0.2578 Train_Acc: 89.555 Val_Loss: 0.2334  BEST VAL Loss: 0.2334  Val_Acc: 90.252

Epoch 69: Validation loss decreased (0.233355 --> 0.233135).  Saving model ...
	 Train_Loss: 0.2575 Train_Acc: 89.693 Val_Loss: 0.2331  BEST VAL Loss: 0.2331  Val_Acc: 90.927

Epoch 70: Validation loss decreased (0.233135 --> 0.232871).  Saving model ...
	 Train_Loss: 0.2571 Train_Acc: 89.779 Val_Loss: 0.2329  BEST VAL Loss: 0.2329  Val_Acc: 90.922

Epoch 71: Validation loss decreased (0.232871 --> 0.232563).  Saving model ...
	 Train_Loss: 0.2567 Train_Acc: 89.833 Val_Loss: 0.2326  BEST VAL Loss: 0.2326  Val_Acc: 91.070

Epoch 72: Validation loss decreased (0.232563 --> 0.232377).  Saving model ...
	 Train_Loss: 0.2564 Train_Acc: 89.696 Val_Loss: 0.2324  BEST VAL Loss: 0.2324  Val_Acc: 90.847

Epoch 73: Validation loss decreased (0.232377 --> 0.232208).  Saving model ...
	 Train_Loss: 0.2560 Train_Acc: 89.698 Val_Loss: 0.2322  BEST VAL Loss: 0.2322  Val_Acc: 90.602

Epoch 74: Validation loss decreased (0.232208 --> 0.232052).  Saving model ...
	 Train_Loss: 0.2557 Train_Acc: 89.807 Val_Loss: 0.2321  BEST VAL Loss: 0.2321  Val_Acc: 90.476

Epoch 75: Validation loss decreased (0.232052 --> 0.232043).  Saving model ...
	 Train_Loss: 0.2554 Train_Acc: 89.656 Val_Loss: 0.2320  BEST VAL Loss: 0.2320  Val_Acc: 90.977

Epoch 76: Validation loss decreased (0.232043 --> 0.231916).  Saving model ...
	 Train_Loss: 0.2551 Train_Acc: 89.662 Val_Loss: 0.2319  BEST VAL Loss: 0.2319  Val_Acc: 90.598

Epoch 77: Validation loss decreased (0.231916 --> 0.231686).  Saving model ...
	 Train_Loss: 0.2547 Train_Acc: 89.744 Val_Loss: 0.2317  BEST VAL Loss: 0.2317  Val_Acc: 90.960

Epoch 78: Validation loss decreased (0.231686 --> 0.231546).  Saving model ...
	 Train_Loss: 0.2544 Train_Acc: 89.748 Val_Loss: 0.2315  BEST VAL Loss: 0.2315  Val_Acc: 90.783

Epoch 79: Validation loss decreased (0.231546 --> 0.231234).  Saving model ...
	 Train_Loss: 0.2541 Train_Acc: 89.779 Val_Loss: 0.2312  BEST VAL Loss: 0.2312  Val_Acc: 91.091

Epoch 80: Validation loss decreased (0.231234 --> 0.231209).  Saving model ...
	 Train_Loss: 0.2538 Train_Acc: 89.736 Val_Loss: 0.2312  BEST VAL Loss: 0.2312  Val_Acc: 90.564

Epoch 81: Validation loss decreased (0.231209 --> 0.231068).  Saving model ...
	 Train_Loss: 0.2535 Train_Acc: 89.741 Val_Loss: 0.2311  BEST VAL Loss: 0.2311  Val_Acc: 90.501

Epoch 82: Validation loss decreased (0.231068 --> 0.230829).  Saving model ...
	 Train_Loss: 0.2532 Train_Acc: 89.662 Val_Loss: 0.2308  BEST VAL Loss: 0.2308  Val_Acc: 90.750

Epoch 83: Validation loss decreased (0.230829 --> 0.230747).  Saving model ...
	 Train_Loss: 0.2530 Train_Acc: 89.683 Val_Loss: 0.2307  BEST VAL Loss: 0.2307  Val_Acc: 90.594

Epoch 84: Validation loss decreased (0.230747 --> 0.230662).  Saving model ...
	 Train_Loss: 0.2527 Train_Acc: 89.741 Val_Loss: 0.2307  BEST VAL Loss: 0.2307  Val_Acc: 90.783

Epoch 85: Validation loss decreased (0.230662 --> 0.230459).  Saving model ...
	 Train_Loss: 0.2524 Train_Acc: 89.797 Val_Loss: 0.2305  BEST VAL Loss: 0.2305  Val_Acc: 90.826

Epoch 86: Validation loss decreased (0.230459 --> 0.230327).  Saving model ...
	 Train_Loss: 0.2522 Train_Acc: 89.640 Val_Loss: 0.2303  BEST VAL Loss: 0.2303  Val_Acc: 90.737

Epoch 87: Validation loss decreased (0.230327 --> 0.230125).  Saving model ...
	 Train_Loss: 0.2519 Train_Acc: 89.734 Val_Loss: 0.2301  BEST VAL Loss: 0.2301  Val_Acc: 90.792

Epoch 88: Validation loss decreased (0.230125 --> 0.229993).  Saving model ...
	 Train_Loss: 0.2516 Train_Acc: 89.834 Val_Loss: 0.2300  BEST VAL Loss: 0.2300  Val_Acc: 90.994

Epoch 89: Validation loss decreased (0.229993 --> 0.229878).  Saving model ...
	 Train_Loss: 0.2514 Train_Acc: 89.821 Val_Loss: 0.2299  BEST VAL Loss: 0.2299  Val_Acc: 90.922

Epoch 90: Validation loss decreased (0.229878 --> 0.229713).  Saving model ...
	 Train_Loss: 0.2511 Train_Acc: 89.867 Val_Loss: 0.2297  BEST VAL Loss: 0.2297  Val_Acc: 91.091

Epoch 91: Validation loss decreased (0.229713 --> 0.229670).  Saving model ...
	 Train_Loss: 0.2509 Train_Acc: 89.679 Val_Loss: 0.2297  BEST VAL Loss: 0.2297  Val_Acc: 90.842

Epoch 92: Validation loss decreased (0.229670 --> 0.229574).  Saving model ...
	 Train_Loss: 0.2506 Train_Acc: 89.880 Val_Loss: 0.2296  BEST VAL Loss: 0.2296  Val_Acc: 90.918

Epoch 93: Validation loss decreased (0.229574 --> 0.229446).  Saving model ...
	 Train_Loss: 0.2504 Train_Acc: 89.757 Val_Loss: 0.2294  BEST VAL Loss: 0.2294  Val_Acc: 90.750

Epoch 94: Validation loss decreased (0.229446 --> 0.229342).  Saving model ...
	 Train_Loss: 0.2502 Train_Acc: 89.784 Val_Loss: 0.2293  BEST VAL Loss: 0.2293  Val_Acc: 90.821

Epoch 95: Validation loss decreased (0.229342 --> 0.229226).  Saving model ...
	 Train_Loss: 0.2500 Train_Acc: 89.807 Val_Loss: 0.2292  BEST VAL Loss: 0.2292  Val_Acc: 90.977

Epoch 96: Validation loss did not decrease
	 Train_Loss: 0.2497 Train_Acc: 89.933 Val_Loss: 0.2292  BEST VAL Loss: 0.2292  Val_Acc: 90.585

Epoch 97: Validation loss decreased (0.229226 --> 0.229219).  Saving model ...
	 Train_Loss: 0.2495 Train_Acc: 89.816 Val_Loss: 0.2292  BEST VAL Loss: 0.2292  Val_Acc: 90.556

Epoch 98: Validation loss decreased (0.229219 --> 0.229035).  Saving model ...
	 Train_Loss: 0.2493 Train_Acc: 89.826 Val_Loss: 0.2290  BEST VAL Loss: 0.2290  Val_Acc: 91.276

Epoch 99: Validation loss decreased (0.229035 --> 0.228949).  Saving model ...
	 Train_Loss: 0.2490 Train_Acc: 89.914 Val_Loss: 0.2289  BEST VAL Loss: 0.2289  Val_Acc: 90.552

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.49      0.45      0.47     92173
           1       0.52      0.55      0.53     97655

    accuracy                           0.50    189828
   macro avg       0.50      0.50      0.50    189828
weighted avg       0.50      0.50      0.50    189828

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.44      0.46     11522
           1       0.51      0.55      0.53     12207

    accuracy                           0.50     23729
   macro avg       0.50      0.50      0.49     23729
weighted avg       0.50      0.50      0.50     23729

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.49      0.45      0.47     11522
           1       0.52      0.55      0.54     12207

    accuracy                           0.50     23729
   macro avg       0.50      0.50      0.50     23729
weighted avg       0.50      0.50      0.50     23729

              precision    recall  f1-score   support

           0       0.49      0.45      0.47     11522
           1       0.52      0.55      0.54     12207

    accuracy                           0.50     23729
   macro avg       0.50      0.50      0.50     23729
weighted avg       0.50      0.50      0.50     23729

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/PBMC
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.48      0.37      0.42     41273
           1       0.52      0.63      0.57     44915

    accuracy                           0.51     86188
   macro avg       0.50      0.50      0.49     86188
weighted avg       0.50      0.51      0.50     86188

              precision    recall  f1-score   support

           0       0.48      0.37      0.42     41273
           1       0.52      0.63      0.57     44915

    accuracy                           0.51     86188
   macro avg       0.50      0.50      0.49     86188
weighted avg       0.50      0.51      0.50     86188

completed
