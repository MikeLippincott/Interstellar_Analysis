[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'ec1693f5'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '1cd3c9d3'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '9fd18616'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f26106bc'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_1.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (30860, 1276)
Number of total missing values across all columns: 32916
Data Subset Is Off
Wells held out for testing: ['D20' 'M16']
Wells to use for training, validation, and testing ['D16' 'D17' 'D21' 'M17' 'M20' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
RMSprop
Epoch 0: Validation loss decreased (inf --> 0.377641).  Saving model ...
	 Train_Loss: 0.6190 Train_Acc: 68.338 Val_Loss: 0.3776  BEST VAL Loss: 0.3776  Val_Acc: 85.377

Epoch 1: Validation loss decreased (0.377641 --> 0.306427).  Saving model ...
	 Train_Loss: 0.4939 Train_Acc: 81.624 Val_Loss: 0.3064  BEST VAL Loss: 0.3064  Val_Acc: 89.492

Epoch 2: Validation loss decreased (0.306427 --> 0.264881).  Saving model ...
	 Train_Loss: 0.4297 Train_Acc: 84.952 Val_Loss: 0.2649  BEST VAL Loss: 0.2649  Val_Acc: 91.725

Epoch 3: Validation loss decreased (0.264881 --> 0.239982).  Saving model ...
	 Train_Loss: 0.3878 Train_Acc: 86.698 Val_Loss: 0.2400  BEST VAL Loss: 0.2400  Val_Acc: 93.126

Epoch 4: Validation loss decreased (0.239982 --> 0.223098).  Saving model ...
	 Train_Loss: 0.3583 Train_Acc: 88.422 Val_Loss: 0.2231  BEST VAL Loss: 0.2231  Val_Acc: 93.695

Epoch 5: Validation loss decreased (0.223098 --> 0.211641).  Saving model ...
	 Train_Loss: 0.3363 Train_Acc: 88.789 Val_Loss: 0.2116  BEST VAL Loss: 0.2116  Val_Acc: 93.914

Epoch 6: Validation loss decreased (0.211641 --> 0.204975).  Saving model ...
	 Train_Loss: 0.3181 Train_Acc: 89.545 Val_Loss: 0.2050  BEST VAL Loss: 0.2050  Val_Acc: 94.264

Epoch 7: Validation loss decreased (0.204975 --> 0.197785).  Saving model ...
	 Train_Loss: 0.3039 Train_Acc: 89.758 Val_Loss: 0.1978  BEST VAL Loss: 0.1978  Val_Acc: 94.221

Epoch 8: Validation loss decreased (0.197785 --> 0.190299).  Saving model ...
	 Train_Loss: 0.2913 Train_Acc: 90.524 Val_Loss: 0.1903  BEST VAL Loss: 0.1903  Val_Acc: 94.877

Epoch 9: Validation loss decreased (0.190299 --> 0.186199).  Saving model ...
	 Train_Loss: 0.2809 Train_Acc: 90.388 Val_Loss: 0.1862  BEST VAL Loss: 0.1862  Val_Acc: 95.140

Epoch 10: Validation loss decreased (0.186199 --> 0.182764).  Saving model ...
	 Train_Loss: 0.2721 Train_Acc: 90.497 Val_Loss: 0.1828  BEST VAL Loss: 0.1828  Val_Acc: 95.315

Epoch 11: Validation loss decreased (0.182764 --> 0.181039).  Saving model ...
	 Train_Loss: 0.2638 Train_Acc: 91.242 Val_Loss: 0.1810  BEST VAL Loss: 0.1810  Val_Acc: 94.877

Epoch 12: Validation loss decreased (0.181039 --> 0.178269).  Saving model ...
	 Train_Loss: 0.2575 Train_Acc: 90.979 Val_Loss: 0.1783  BEST VAL Loss: 0.1783  Val_Acc: 95.228

Epoch 13: Validation loss decreased (0.178269 --> 0.177348).  Saving model ...
	 Train_Loss: 0.2510 Train_Acc: 91.482 Val_Loss: 0.1773  BEST VAL Loss: 0.1773  Val_Acc: 95.841

Epoch 14: Validation loss decreased (0.177348 --> 0.176485).  Saving model ...
	 Train_Loss: 0.2455 Train_Acc: 91.482 Val_Loss: 0.1765  BEST VAL Loss: 0.1765  Val_Acc: 95.578

Epoch 15: Validation loss decreased (0.176485 --> 0.174904).  Saving model ...
	 Train_Loss: 0.2404 Train_Acc: 91.685 Val_Loss: 0.1749  BEST VAL Loss: 0.1749  Val_Acc: 95.578

Epoch 16: Validation loss decreased (0.174904 --> 0.173903).  Saving model ...
	 Train_Loss: 0.2356 Train_Acc: 91.794 Val_Loss: 0.1739  BEST VAL Loss: 0.1739  Val_Acc: 95.797

Epoch 17: Validation loss decreased (0.173903 --> 0.172924).  Saving model ...
	 Train_Loss: 0.2315 Train_Acc: 91.586 Val_Loss: 0.1729  BEST VAL Loss: 0.1729  Val_Acc: 96.060

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.2275 Train_Acc: 92.112 Val_Loss: 0.1730  BEST VAL Loss: 0.1729  Val_Acc: 95.665

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.2236 Train_Acc: 92.271 Val_Loss: 0.1732  BEST VAL Loss: 0.1729  Val_Acc: 96.454

Epoch 20: Validation loss decreased (0.172924 --> 0.171704).  Saving model ...
	 Train_Loss: 0.2200 Train_Acc: 92.249 Val_Loss: 0.1717  BEST VAL Loss: 0.1717  Val_Acc: 95.928

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.2167 Train_Acc: 92.380 Val_Loss: 0.1724  BEST VAL Loss: 0.1717  Val_Acc: 96.060

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.2139 Train_Acc: 92.369 Val_Loss: 0.1730  BEST VAL Loss: 0.1717  Val_Acc: 96.147

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.2113 Train_Acc: 92.358 Val_Loss: 0.1735  BEST VAL Loss: 0.1717  Val_Acc: 96.278

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.2086 Train_Acc: 92.539 Val_Loss: 0.1767  BEST VAL Loss: 0.1717  Val_Acc: 95.490

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.2065 Train_Acc: 92.249 Val_Loss: 0.1775  BEST VAL Loss: 0.1717  Val_Acc: 96.016

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.2041 Train_Acc: 92.457 Val_Loss: 0.1790  BEST VAL Loss: 0.1717  Val_Acc: 96.016

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.2018 Train_Acc: 92.917 Val_Loss: 0.1795  BEST VAL Loss: 0.1717  Val_Acc: 96.410

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1999 Train_Acc: 92.331 Val_Loss: 0.1791  BEST VAL Loss: 0.1717  Val_Acc: 96.147

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1980 Train_Acc: 92.928 Val_Loss: 0.1807  BEST VAL Loss: 0.1717  Val_Acc: 96.060

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1962 Train_Acc: 92.616 Val_Loss: 0.1821  BEST VAL Loss: 0.1717  Val_Acc: 96.454

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1944 Train_Acc: 92.807 Val_Loss: 0.1833  BEST VAL Loss: 0.1717  Val_Acc: 95.797

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1928 Train_Acc: 92.681 Val_Loss: 0.1838  BEST VAL Loss: 0.1717  Val_Acc: 96.410

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1912 Train_Acc: 92.856 Val_Loss: 0.1849  BEST VAL Loss: 0.1717  Val_Acc: 96.016

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1898 Train_Acc: 92.906 Val_Loss: 0.1847  BEST VAL Loss: 0.1717  Val_Acc: 96.585

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.1885 Train_Acc: 93.037 Val_Loss: 0.1859  BEST VAL Loss: 0.1717  Val_Acc: 96.235

Epoch 36: Validation loss did not decrease
Early stopped at epoch : 36
LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       1.00      0.96      0.98      9832
           1       0.95      1.00      0.97      8436

    accuracy                           0.98     18268
   macro avg       0.97      0.98      0.98     18268
weighted avg       0.98      0.98      0.98     18268

LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.94      0.96      1229
           1       0.93      0.98      0.96      1055

    accuracy                           0.96      2284
   macro avg       0.96      0.96      0.96      2284
weighted avg       0.96      0.96      0.96      2284

LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.93      0.96      1229
           1       0.92      0.99      0.95      1055

    accuracy                           0.95      2284
   macro avg       0.95      0.96      0.95      2284
weighted avg       0.96      0.95      0.95      2284

              precision    recall  f1-score   support

           0       0.99      0.93      0.96      1229
           1       0.92      0.99      0.95      1055

    accuracy                           0.95      2284
   macro avg       0.95      0.96      0.95      2284
weighted avg       0.96      0.95      0.95      2284

LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_1.000_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.94      0.96      4168
           1       0.94      0.98      0.96      3856

    accuracy                           0.96      8024
   macro avg       0.96      0.96      0.96      8024
weighted avg       0.96      0.96      0.96      8024

              precision    recall  f1-score   support

           0       0.98      0.94      0.96      4168
           1       0.94      0.98      0.96      3856

    accuracy                           0.96      8024
   macro avg       0.96      0.96      0.96      8024
weighted avg       0.96      0.96      0.96      8024

completed
