[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'f7c3dc4c'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '8d83bc23'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '45f87ee3'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '60ed8cb9'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_10.000_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_10.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_10.000_DMSO_0.025']
The dimensions of the data are: (30022, 1276)
Number of total missing values across all columns: 60044
Data Subset Is Off
Wells held out for testing: ['E14' 'E20']
Wells to use for training, validation, and testing ['E15' 'E16' 'E17' 'E21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.293021).  Saving model ...
	 Train_Loss: 0.4451 Train_Acc: 78.574 Val_Loss: 0.2930  BEST VAL Loss: 0.2930  Val_Acc: 87.088

Epoch 1: Validation loss decreased (0.293021 --> 0.265589).  Saving model ...
	 Train_Loss: 0.3762 Train_Acc: 85.504 Val_Loss: 0.2656  BEST VAL Loss: 0.2656  Val_Acc: 89.047

Epoch 2: Validation loss decreased (0.265589 --> 0.249583).  Saving model ...
	 Train_Loss: 0.3382 Train_Acc: 87.486 Val_Loss: 0.2496  BEST VAL Loss: 0.2496  Val_Acc: 90.383

Epoch 3: Validation loss decreased (0.249583 --> 0.236998).  Saving model ...
	 Train_Loss: 0.3122 Train_Acc: 88.939 Val_Loss: 0.2370  BEST VAL Loss: 0.2370  Val_Acc: 91.362

Epoch 4: Validation loss decreased (0.236998 --> 0.227509).  Saving model ...
	 Train_Loss: 0.2915 Train_Acc: 90.047 Val_Loss: 0.2275  BEST VAL Loss: 0.2275  Val_Acc: 91.986

Epoch 5: Validation loss decreased (0.227509 --> 0.219574).  Saving model ...
	 Train_Loss: 0.2753 Train_Acc: 90.303 Val_Loss: 0.2196  BEST VAL Loss: 0.2196  Val_Acc: 92.164

Epoch 6: Validation loss decreased (0.219574 --> 0.213905).  Saving model ...
	 Train_Loss: 0.2614 Train_Acc: 91.338 Val_Loss: 0.2139  BEST VAL Loss: 0.2139  Val_Acc: 92.386

Epoch 7: Validation loss decreased (0.213905 --> 0.208998).  Saving model ...
	 Train_Loss: 0.2494 Train_Acc: 91.722 Val_Loss: 0.2090  BEST VAL Loss: 0.2090  Val_Acc: 92.876

Epoch 8: Validation loss decreased (0.208998 --> 0.204931).  Saving model ...
	 Train_Loss: 0.2396 Train_Acc: 91.739 Val_Loss: 0.2049  BEST VAL Loss: 0.2049  Val_Acc: 92.876

Epoch 9: Validation loss decreased (0.204931 --> 0.200903).  Saving model ...
	 Train_Loss: 0.2313 Train_Acc: 91.739 Val_Loss: 0.2009  BEST VAL Loss: 0.2009  Val_Acc: 92.654

Epoch 10: Validation loss decreased (0.200903 --> 0.198869).  Saving model ...
	 Train_Loss: 0.2244 Train_Acc: 93.186 Val_Loss: 0.1989  BEST VAL Loss: 0.1989  Val_Acc: 92.075

Epoch 11: Validation loss decreased (0.198869 --> 0.196834).  Saving model ...
	 Train_Loss: 0.2181 Train_Acc: 93.281 Val_Loss: 0.1968  BEST VAL Loss: 0.1968  Val_Acc: 92.297

Epoch 12: Validation loss decreased (0.196834 --> 0.195075).  Saving model ...
	 Train_Loss: 0.2120 Train_Acc: 93.793 Val_Loss: 0.1951  BEST VAL Loss: 0.1951  Val_Acc: 92.075

Epoch 13: Validation loss decreased (0.195075 --> 0.193926).  Saving model ...
	 Train_Loss: 0.2068 Train_Acc: 93.632 Val_Loss: 0.1939  BEST VAL Loss: 0.1939  Val_Acc: 92.164

Epoch 14: Validation loss decreased (0.193926 --> 0.191857).  Saving model ...
	 Train_Loss: 0.2022 Train_Acc: 93.988 Val_Loss: 0.1919  BEST VAL Loss: 0.1919  Val_Acc: 92.342

Epoch 15: Validation loss decreased (0.191857 --> 0.190545).  Saving model ...
	 Train_Loss: 0.1982 Train_Acc: 93.710 Val_Loss: 0.1905  BEST VAL Loss: 0.1905  Val_Acc: 92.297

Epoch 16: Validation loss decreased (0.190545 --> 0.189027).  Saving model ...
	 Train_Loss: 0.1944 Train_Acc: 94.183 Val_Loss: 0.1890  BEST VAL Loss: 0.1890  Val_Acc: 92.743

Epoch 17: Validation loss decreased (0.189027 --> 0.188635).  Saving model ...
	 Train_Loss: 0.1908 Train_Acc: 94.283 Val_Loss: 0.1886  BEST VAL Loss: 0.1886  Val_Acc: 92.787

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.1872 Train_Acc: 94.534 Val_Loss: 0.1888  BEST VAL Loss: 0.1886  Val_Acc: 92.698

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1840 Train_Acc: 94.595 Val_Loss: 0.1890  BEST VAL Loss: 0.1886  Val_Acc: 92.965

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1812 Train_Acc: 94.511 Val_Loss: 0.1898  BEST VAL Loss: 0.1886  Val_Acc: 92.832

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1786 Train_Acc: 94.367 Val_Loss: 0.1901  BEST VAL Loss: 0.1886  Val_Acc: 92.787

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1760 Train_Acc: 94.734 Val_Loss: 0.1898  BEST VAL Loss: 0.1886  Val_Acc: 92.743

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1736 Train_Acc: 94.762 Val_Loss: 0.1895  BEST VAL Loss: 0.1886  Val_Acc: 93.054

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1712 Train_Acc: 94.706 Val_Loss: 0.1898  BEST VAL Loss: 0.1886  Val_Acc: 92.876

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1689 Train_Acc: 94.662 Val_Loss: 0.1897  BEST VAL Loss: 0.1886  Val_Acc: 92.876

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1666 Train_Acc: 95.062 Val_Loss: 0.1900  BEST VAL Loss: 0.1886  Val_Acc: 93.232

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1644 Train_Acc: 95.374 Val_Loss: 0.1901  BEST VAL Loss: 0.1886  Val_Acc: 92.698

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1622 Train_Acc: 95.441 Val_Loss: 0.1904  BEST VAL Loss: 0.1886  Val_Acc: 92.965

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1600 Train_Acc: 95.485 Val_Loss: 0.1908  BEST VAL Loss: 0.1886  Val_Acc: 92.654

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1580 Train_Acc: 95.435 Val_Loss: 0.1914  BEST VAL Loss: 0.1886  Val_Acc: 93.232

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1561 Train_Acc: 95.708 Val_Loss: 0.1919  BEST VAL Loss: 0.1886  Val_Acc: 92.787

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1545 Train_Acc: 95.385 Val_Loss: 0.1921  BEST VAL Loss: 0.1886  Val_Acc: 92.654

Epoch 33: Validation loss did not decrease
Early stopped at epoch : 33
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.96      0.97     10114
           1       0.95      0.98      0.96      7850

    accuracy                           0.97     17964
   macro avg       0.97      0.97      0.97     17964
weighted avg       0.97      0.97      0.97     17964

LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.91      0.93      1264
           1       0.89      0.95      0.92       982

    accuracy                           0.93      2246
   macro avg       0.93      0.93      0.93      2246
weighted avg       0.93      0.93      0.93      2246

LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.92      0.94      1264
           1       0.91      0.95      0.93       982

    accuracy                           0.93      2246
   macro avg       0.93      0.94      0.93      2246
weighted avg       0.94      0.93      0.93      2246

              precision    recall  f1-score   support

           0       0.96      0.92      0.94      1264
           1       0.91      0.95      0.93       982

    accuracy                           0.93      2246
   macro avg       0.93      0.94      0.93      2246
weighted avg       0.94      0.93      0.93      2246

LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.95      0.95      4168
           1       0.94      0.94      0.94      3398

    accuracy                           0.95      7566
   macro avg       0.95      0.95      0.95      7566
weighted avg       0.95      0.95      0.95      7566

              precision    recall  f1-score   support

           0       0.95      0.95      0.95      4168
           1       0.94      0.94      0.94      3398

    accuracy                           0.95      7566
   macro avg       0.95      0.95      0.95      7566
weighted avg       0.95      0.95      0.95      7566

completed
