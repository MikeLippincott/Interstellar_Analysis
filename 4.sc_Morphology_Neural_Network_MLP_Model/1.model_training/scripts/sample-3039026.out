[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '546bc5fd'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'af424e8b'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '0ef17532'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'fc9c4083'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_1.000_DMSO_0.025']
The dimensions of the data are: (29670, 1276)
Number of total missing values across all columns: 59340
Data Subset Is Off
Wells held out for testing: ['E14' 'D20']
Wells to use for training, validation, and testing ['E15' 'D16' 'D17' 'D21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.322793).  Saving model ...
	 Train_Loss: 0.4550 Train_Acc: 78.249 Val_Loss: 0.3228  BEST VAL Loss: 0.3228  Val_Acc: 85.572

Epoch 1: Validation loss decreased (0.322793 --> 0.290448).  Saving model ...
	 Train_Loss: 0.3792 Train_Acc: 86.687 Val_Loss: 0.2904  BEST VAL Loss: 0.2904  Val_Acc: 89.055

Epoch 2: Validation loss decreased (0.290448 --> 0.272387).  Saving model ...
	 Train_Loss: 0.3360 Train_Acc: 89.368 Val_Loss: 0.2724  BEST VAL Loss: 0.2724  Val_Acc: 90.728

Epoch 3: Validation loss decreased (0.272387 --> 0.256932).  Saving model ...
	 Train_Loss: 0.3070 Train_Acc: 90.578 Val_Loss: 0.2569  BEST VAL Loss: 0.2569  Val_Acc: 90.592

Epoch 4: Validation loss decreased (0.256932 --> 0.249674).  Saving model ...
	 Train_Loss: 0.2862 Train_Acc: 91.522 Val_Loss: 0.2497  BEST VAL Loss: 0.2497  Val_Acc: 90.864

Epoch 5: Validation loss decreased (0.249674 --> 0.242714).  Saving model ...
	 Train_Loss: 0.2695 Train_Acc: 92.077 Val_Loss: 0.2427  BEST VAL Loss: 0.2427  Val_Acc: 91.768

Epoch 6: Validation loss decreased (0.242714 --> 0.235630).  Saving model ...
	 Train_Loss: 0.2562 Train_Acc: 92.580 Val_Loss: 0.2356  BEST VAL Loss: 0.2356  Val_Acc: 92.311

Epoch 7: Validation loss decreased (0.235630 --> 0.227809).  Saving model ...
	 Train_Loss: 0.2454 Train_Acc: 92.614 Val_Loss: 0.2278  BEST VAL Loss: 0.2278  Val_Acc: 92.266

Epoch 8: Validation loss decreased (0.227809 --> 0.222915).  Saving model ...
	 Train_Loss: 0.2364 Train_Acc: 92.885 Val_Loss: 0.2229  BEST VAL Loss: 0.2229  Val_Acc: 92.085

Epoch 9: Validation loss decreased (0.222915 --> 0.221073).  Saving model ...
	 Train_Loss: 0.2283 Train_Acc: 93.066 Val_Loss: 0.2211  BEST VAL Loss: 0.2211  Val_Acc: 91.542

Epoch 10: Validation loss decreased (0.221073 --> 0.218483).  Saving model ...
	 Train_Loss: 0.2211 Train_Acc: 93.423 Val_Loss: 0.2185  BEST VAL Loss: 0.2185  Val_Acc: 92.899

Epoch 11: Validation loss decreased (0.218483 --> 0.215626).  Saving model ...
	 Train_Loss: 0.2145 Train_Acc: 93.575 Val_Loss: 0.2156  BEST VAL Loss: 0.2156  Val_Acc: 93.306

Epoch 12: Validation loss decreased (0.215626 --> 0.212481).  Saving model ...
	 Train_Loss: 0.2084 Train_Acc: 94.017 Val_Loss: 0.2125  BEST VAL Loss: 0.2125  Val_Acc: 92.718

Epoch 13: Validation loss decreased (0.212481 --> 0.210327).  Saving model ...
	 Train_Loss: 0.2025 Train_Acc: 94.378 Val_Loss: 0.2103  BEST VAL Loss: 0.2103  Val_Acc: 93.035

Epoch 14: Validation loss did not decrease
	 Train_Loss: 0.1973 Train_Acc: 94.503 Val_Loss: 0.2105  BEST VAL Loss: 0.2103  Val_Acc: 92.899

Epoch 15: Validation loss did not decrease
	 Train_Loss: 0.1929 Train_Acc: 94.209 Val_Loss: 0.2104  BEST VAL Loss: 0.2103  Val_Acc: 92.628

Epoch 16: Validation loss decreased (0.210327 --> 0.208664).  Saving model ...
	 Train_Loss: 0.1887 Train_Acc: 94.441 Val_Loss: 0.2087  BEST VAL Loss: 0.2087  Val_Acc: 92.854

Epoch 17: Validation loss decreased (0.208664 --> 0.207841).  Saving model ...
	 Train_Loss: 0.1848 Train_Acc: 94.774 Val_Loss: 0.2078  BEST VAL Loss: 0.2078  Val_Acc: 92.718

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.1810 Train_Acc: 94.746 Val_Loss: 0.2086  BEST VAL Loss: 0.2078  Val_Acc: 92.537

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1778 Train_Acc: 94.831 Val_Loss: 0.2078  BEST VAL Loss: 0.2078  Val_Acc: 93.035

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1747 Train_Acc: 94.944 Val_Loss: 0.2091  BEST VAL Loss: 0.2078  Val_Acc: 92.583

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1717 Train_Acc: 94.955 Val_Loss: 0.2090  BEST VAL Loss: 0.2078  Val_Acc: 92.899

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1691 Train_Acc: 94.729 Val_Loss: 0.2091  BEST VAL Loss: 0.2078  Val_Acc: 93.216

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1665 Train_Acc: 95.136 Val_Loss: 0.2098  BEST VAL Loss: 0.2078  Val_Acc: 92.854

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1638 Train_Acc: 95.334 Val_Loss: 0.2103  BEST VAL Loss: 0.2078  Val_Acc: 92.944

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1614 Train_Acc: 95.442 Val_Loss: 0.2103  BEST VAL Loss: 0.2078  Val_Acc: 93.171

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1592 Train_Acc: 95.379 Val_Loss: 0.2102  BEST VAL Loss: 0.2078  Val_Acc: 93.080

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1571 Train_Acc: 95.329 Val_Loss: 0.2102  BEST VAL Loss: 0.2078  Val_Acc: 93.442

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1550 Train_Acc: 95.453 Val_Loss: 0.2128  BEST VAL Loss: 0.2078  Val_Acc: 93.351

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1531 Train_Acc: 95.510 Val_Loss: 0.2132  BEST VAL Loss: 0.2078  Val_Acc: 93.532

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1512 Train_Acc: 95.487 Val_Loss: 0.2132  BEST VAL Loss: 0.2078  Val_Acc: 93.397

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1496 Train_Acc: 95.385 Val_Loss: 0.2137  BEST VAL Loss: 0.2078  Val_Acc: 92.990

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1480 Train_Acc: 95.493 Val_Loss: 0.2139  BEST VAL Loss: 0.2078  Val_Acc: 92.718

Epoch 33: Validation loss did not decrease
Early stopped at epoch : 33
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      9832
           1       0.44      0.44      0.44      7850

    accuracy                           0.50     17682
   macro avg       0.50      0.50      0.50     17682
weighted avg       0.50      0.50      0.50     17682

Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.53      0.54      1229
           1       0.42      0.43      0.43       982

    accuracy                           0.49      2211
   macro avg       0.48      0.48      0.48      2211
weighted avg       0.49      0.49      0.49      2211

Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.55      0.55      1229
           1       0.42      0.41      0.42       982

    accuracy                           0.49      2211
   macro avg       0.48      0.48      0.48      2211
weighted avg       0.49      0.49      0.49      2211

              precision    recall  f1-score   support

           0       0.54      0.55      0.55      1229
           1       0.42      0.41      0.42       982

    accuracy                           0.49      2211
   macro avg       0.48      0.48      0.48      2211
weighted avg       0.49      0.49      0.49      2211

Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.54      0.53      0.54      4168
           1       0.44      0.45      0.44      3398

    accuracy                           0.49      7566
   macro avg       0.49      0.49      0.49      7566
weighted avg       0.49      0.49      0.49      7566

              precision    recall  f1-score   support

           0       0.54      0.53      0.54      4168
           1       0.44      0.45      0.44      3398

    accuracy                           0.49      7566
   macro avg       0.49      0.49      0.49      7566
weighted avg       0.49      0.49      0.49      7566

completed
