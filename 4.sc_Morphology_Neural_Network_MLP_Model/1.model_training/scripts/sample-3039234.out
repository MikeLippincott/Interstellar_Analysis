[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'e393739d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'daaec975'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '69dfc7d2'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '80b724e9'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_1.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (29984, 1276)
Number of total missing values across all columns: 27532
Data Subset Is Off
Wells held out for testing: ['D14' 'K20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'K16' 'K17' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.194417).  Saving model ...
	 Train_Loss: 0.3043 Train_Acc: 86.852 Val_Loss: 0.1944  BEST VAL Loss: 0.1944  Val_Acc: 92.552

Epoch 1: Validation loss decreased (0.194417 --> 0.168171).  Saving model ...
	 Train_Loss: 0.2326 Train_Acc: 93.834 Val_Loss: 0.1682  BEST VAL Loss: 0.1682  Val_Acc: 94.535

Epoch 2: Validation loss decreased (0.168171 --> 0.150099).  Saving model ...
	 Train_Loss: 0.1942 Train_Acc: 95.382 Val_Loss: 0.1501  BEST VAL Loss: 0.1501  Val_Acc: 95.505

Epoch 3: Validation loss decreased (0.150099 --> 0.136684).  Saving model ...
	 Train_Loss: 0.1686 Train_Acc: 96.440 Val_Loss: 0.1367  BEST VAL Loss: 0.1367  Val_Acc: 96.078

Epoch 4: Validation loss decreased (0.136684 --> 0.130008).  Saving model ...
	 Train_Loss: 0.1500 Train_Acc: 96.898 Val_Loss: 0.1300  BEST VAL Loss: 0.1300  Val_Acc: 96.783

Epoch 5: Validation loss decreased (0.130008 --> 0.122193).  Saving model ...
	 Train_Loss: 0.1364 Train_Acc: 97.267 Val_Loss: 0.1222  BEST VAL Loss: 0.1222  Val_Acc: 96.386

Epoch 6: Validation loss decreased (0.122193 --> 0.117116).  Saving model ...
	 Train_Loss: 0.1254 Train_Acc: 97.575 Val_Loss: 0.1171  BEST VAL Loss: 0.1171  Val_Acc: 96.562

Epoch 7: Validation loss decreased (0.117116 --> 0.116689).  Saving model ...
	 Train_Loss: 0.1169 Train_Acc: 97.641 Val_Loss: 0.1167  BEST VAL Loss: 0.1167  Val_Acc: 96.871

Epoch 8: Validation loss decreased (0.116689 --> 0.113115).  Saving model ...
	 Train_Loss: 0.1101 Train_Acc: 97.812 Val_Loss: 0.1131  BEST VAL Loss: 0.1131  Val_Acc: 97.179

Epoch 9: Validation loss decreased (0.113115 --> 0.110062).  Saving model ...
	 Train_Loss: 0.1037 Train_Acc: 98.204 Val_Loss: 0.1101  BEST VAL Loss: 0.1101  Val_Acc: 96.606

Epoch 10: Validation loss decreased (0.110062 --> 0.108514).  Saving model ...
	 Train_Loss: 0.0984 Train_Acc: 98.143 Val_Loss: 0.1085  BEST VAL Loss: 0.1085  Val_Acc: 96.959

Epoch 11: Validation loss decreased (0.108514 --> 0.107580).  Saving model ...
	 Train_Loss: 0.0941 Train_Acc: 98.154 Val_Loss: 0.1076  BEST VAL Loss: 0.1076  Val_Acc: 96.783

Epoch 12: Validation loss decreased (0.107580 --> 0.106831).  Saving model ...
	 Train_Loss: 0.0900 Train_Acc: 98.435 Val_Loss: 0.1068  BEST VAL Loss: 0.1068  Val_Acc: 96.783

Epoch 13: Validation loss decreased (0.106831 --> 0.106234).  Saving model ...
	 Train_Loss: 0.0864 Train_Acc: 98.407 Val_Loss: 0.1062  BEST VAL Loss: 0.1062  Val_Acc: 96.783

Epoch 14: Validation loss did not decrease
	 Train_Loss: 0.0829 Train_Acc: 98.556 Val_Loss: 0.1065  BEST VAL Loss: 0.1062  Val_Acc: 96.915

Epoch 15: Validation loss decreased (0.106234 --> 0.105764).  Saving model ...
	 Train_Loss: 0.0799 Train_Acc: 98.507 Val_Loss: 0.1058  BEST VAL Loss: 0.1058  Val_Acc: 96.871

Epoch 16: Validation loss decreased (0.105764 --> 0.105117).  Saving model ...
	 Train_Loss: 0.0771 Train_Acc: 98.677 Val_Loss: 0.1051  BEST VAL Loss: 0.1051  Val_Acc: 96.739

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.0748 Train_Acc: 98.655 Val_Loss: 0.1055  BEST VAL Loss: 0.1051  Val_Acc: 97.003

Epoch 18: Validation loss decreased (0.105117 --> 0.104620).  Saving model ...
	 Train_Loss: 0.0725 Train_Acc: 98.766 Val_Loss: 0.1046  BEST VAL Loss: 0.1046  Val_Acc: 96.474

Epoch 19: Validation loss decreased (0.104620 --> 0.103910).  Saving model ...
	 Train_Loss: 0.0703 Train_Acc: 98.859 Val_Loss: 0.1039  BEST VAL Loss: 0.1039  Val_Acc: 96.959

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.0685 Train_Acc: 98.749 Val_Loss: 0.1041  BEST VAL Loss: 0.1039  Val_Acc: 97.091

Epoch 21: Validation loss decreased (0.103910 --> 0.103652).  Saving model ...
	 Train_Loss: 0.0668 Train_Acc: 98.755 Val_Loss: 0.1037  BEST VAL Loss: 0.1037  Val_Acc: 96.959

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.0652 Train_Acc: 98.661 Val_Loss: 0.1038  BEST VAL Loss: 0.1037  Val_Acc: 97.091

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.0637 Train_Acc: 98.688 Val_Loss: 0.1042  BEST VAL Loss: 0.1037  Val_Acc: 96.827

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.0622 Train_Acc: 98.959 Val_Loss: 0.1040  BEST VAL Loss: 0.1037  Val_Acc: 97.179

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.0609 Train_Acc: 98.925 Val_Loss: 0.1044  BEST VAL Loss: 0.1037  Val_Acc: 96.562

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.0597 Train_Acc: 99.003 Val_Loss: 0.1038  BEST VAL Loss: 0.1037  Val_Acc: 97.312

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.0586 Train_Acc: 98.881 Val_Loss: 0.1039  BEST VAL Loss: 0.1037  Val_Acc: 97.356

Epoch 28: Validation loss decreased (0.103652 --> 0.103350).  Saving model ...
	 Train_Loss: 0.0575 Train_Acc: 98.936 Val_Loss: 0.1033  BEST VAL Loss: 0.1033  Val_Acc: 97.091

Epoch 29: Validation loss decreased (0.103350 --> 0.102944).  Saving model ...
	 Train_Loss: 0.0563 Train_Acc: 99.118 Val_Loss: 0.1029  BEST VAL Loss: 0.1029  Val_Acc: 97.268

Epoch 30: Validation loss decreased (0.102944 --> 0.102521).  Saving model ...
	 Train_Loss: 0.0553 Train_Acc: 98.881 Val_Loss: 0.1025  BEST VAL Loss: 0.1025  Val_Acc: 97.047

Epoch 31: Validation loss decreased (0.102521 --> 0.101920).  Saving model ...
	 Train_Loss: 0.0544 Train_Acc: 98.959 Val_Loss: 0.1019  BEST VAL Loss: 0.1019  Val_Acc: 97.664

Epoch 32: Validation loss decreased (0.101920 --> 0.101138).  Saving model ...
	 Train_Loss: 0.0533 Train_Acc: 99.168 Val_Loss: 0.1011  BEST VAL Loss: 0.1011  Val_Acc: 97.312

Epoch 33: Validation loss decreased (0.101138 --> 0.100556).  Saving model ...
	 Train_Loss: 0.0524 Train_Acc: 99.140 Val_Loss: 0.1006  BEST VAL Loss: 0.1006  Val_Acc: 97.356

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0515 Train_Acc: 99.146 Val_Loss: 0.1006  BEST VAL Loss: 0.1006  Val_Acc: 97.400

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.0506 Train_Acc: 99.262 Val_Loss: 0.1009  BEST VAL Loss: 0.1006  Val_Acc: 97.532

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.0497 Train_Acc: 99.289 Val_Loss: 0.1009  BEST VAL Loss: 0.1006  Val_Acc: 97.664

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.0491 Train_Acc: 99.080 Val_Loss: 0.1015  BEST VAL Loss: 0.1006  Val_Acc: 97.268

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.0484 Train_Acc: 98.997 Val_Loss: 0.1016  BEST VAL Loss: 0.1006  Val_Acc: 97.179

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.0478 Train_Acc: 99.096 Val_Loss: 0.1019  BEST VAL Loss: 0.1006  Val_Acc: 97.356

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.0472 Train_Acc: 99.151 Val_Loss: 0.1025  BEST VAL Loss: 0.1006  Val_Acc: 97.003

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.0466 Train_Acc: 99.124 Val_Loss: 0.1023  BEST VAL Loss: 0.1006  Val_Acc: 97.223

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.0461 Train_Acc: 99.162 Val_Loss: 0.1024  BEST VAL Loss: 0.1006  Val_Acc: 97.091

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.0455 Train_Acc: 99.008 Val_Loss: 0.1028  BEST VAL Loss: 0.1006  Val_Acc: 97.268

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.0450 Train_Acc: 99.146 Val_Loss: 0.1031  BEST VAL Loss: 0.1006  Val_Acc: 97.047

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.0444 Train_Acc: 99.328 Val_Loss: 0.1035  BEST VAL Loss: 0.1006  Val_Acc: 97.268

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.0438 Train_Acc: 99.350 Val_Loss: 0.1032  BEST VAL Loss: 0.1006  Val_Acc: 97.179

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.0433 Train_Acc: 99.377 Val_Loss: 0.1035  BEST VAL Loss: 0.1006  Val_Acc: 97.091

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.0427 Train_Acc: 99.339 Val_Loss: 0.1038  BEST VAL Loss: 0.1006  Val_Acc: 97.312

Epoch 49: Validation loss did not decrease
Early stopped at epoch : 49
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      9777
           1       0.47      0.47      0.47      8370

    accuracy                           0.51     18147
   macro avg       0.51      0.51      0.51     18147
weighted avg       0.51      0.51      0.51     18147

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1222
           1       0.47      0.47      0.47      1047

    accuracy                           0.52      2269
   macro avg       0.51      0.51      0.51      2269
weighted avg       0.51      0.52      0.52      2269

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1223
           1       0.47      0.47      0.47      1046

    accuracy                           0.52      2269
   macro avg       0.51      0.51      0.51      2269
weighted avg       0.52      0.52      0.52      2269

              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1223
           1       0.47      0.47      0.47      1046

    accuracy                           0.52      2269
   macro avg       0.51      0.51      0.51      2269
weighted avg       0.52      0.52      0.52      2269

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_1.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      3996
           1       0.46      0.46      0.46      3303

    accuracy                           0.51      7299
   macro avg       0.50      0.50      0.50      7299
weighted avg       0.51      0.51      0.51      7299

              precision    recall  f1-score   support

           0       0.55      0.55      0.55      3996
           1       0.46      0.46      0.46      3303

    accuracy                           0.51      7299
   macro avg       0.50      0.50      0.50      7299
weighted avg       0.51      0.51      0.51      7299

completed
