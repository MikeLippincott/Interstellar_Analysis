[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a7b27520'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'e38bf2f3'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'f5c88949'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '00071702'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_10.000_DMSO_0.025']
The dimensions of the data are: (30022, 1276)
Number of total missing values across all columns: 60044
Data Subset Is Off
Wells held out for testing: ['E14' 'E20']
Wells to use for training, validation, and testing ['E15' 'E16' 'E17' 'E21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.443129).  Saving model ...
	 Train_Loss: 0.5629 Train_Acc: 66.800 Val_Loss: 0.4431  BEST VAL Loss: 0.4431  Val_Acc: 81.567

Epoch 1: Validation loss decreased (0.443129 --> 0.416521).  Saving model ...
	 Train_Loss: 0.5062 Train_Acc: 80.956 Val_Loss: 0.4165  BEST VAL Loss: 0.4165  Val_Acc: 85.040

Epoch 2: Validation loss decreased (0.416521 --> 0.393599).  Saving model ...
	 Train_Loss: 0.4720 Train_Acc: 83.077 Val_Loss: 0.3936  BEST VAL Loss: 0.3936  Val_Acc: 85.574

Epoch 3: Validation loss decreased (0.393599 --> 0.375193).  Saving model ...
	 Train_Loss: 0.4450 Train_Acc: 85.232 Val_Loss: 0.3752  BEST VAL Loss: 0.3752  Val_Acc: 88.424

Epoch 4: Validation loss decreased (0.375193 --> 0.358263).  Saving model ...
	 Train_Loss: 0.4206 Train_Acc: 87.397 Val_Loss: 0.3583  BEST VAL Loss: 0.3583  Val_Acc: 88.825

Epoch 5: Validation loss decreased (0.358263 --> 0.346346).  Saving model ...
	 Train_Loss: 0.4028 Train_Acc: 87.792 Val_Loss: 0.3463  BEST VAL Loss: 0.3463  Val_Acc: 89.270

Epoch 6: Validation loss decreased (0.346346 --> 0.336054).  Saving model ...
	 Train_Loss: 0.3860 Train_Acc: 89.139 Val_Loss: 0.3361  BEST VAL Loss: 0.3361  Val_Acc: 90.917

Epoch 7: Validation loss decreased (0.336054 --> 0.329242).  Saving model ...
	 Train_Loss: 0.3721 Train_Acc: 89.741 Val_Loss: 0.3292  BEST VAL Loss: 0.3292  Val_Acc: 90.516

Epoch 8: Validation loss decreased (0.329242 --> 0.324718).  Saving model ...
	 Train_Loss: 0.3606 Train_Acc: 90.019 Val_Loss: 0.3247  BEST VAL Loss: 0.3247  Val_Acc: 89.804

Epoch 9: Validation loss decreased (0.324718 --> 0.320813).  Saving model ...
	 Train_Loss: 0.3514 Train_Acc: 90.108 Val_Loss: 0.3208  BEST VAL Loss: 0.3208  Val_Acc: 90.739

Epoch 10: Validation loss decreased (0.320813 --> 0.316153).  Saving model ...
	 Train_Loss: 0.3419 Train_Acc: 90.926 Val_Loss: 0.3162  BEST VAL Loss: 0.3162  Val_Acc: 91.229

Epoch 11: Validation loss decreased (0.316153 --> 0.312146).  Saving model ...
	 Train_Loss: 0.3346 Train_Acc: 90.720 Val_Loss: 0.3121  BEST VAL Loss: 0.3121  Val_Acc: 90.338

Epoch 12: Validation loss decreased (0.312146 --> 0.309341).  Saving model ...
	 Train_Loss: 0.3273 Train_Acc: 91.372 Val_Loss: 0.3093  BEST VAL Loss: 0.3093  Val_Acc: 90.695

Epoch 13: Validation loss decreased (0.309341 --> 0.307888).  Saving model ...
	 Train_Loss: 0.3208 Train_Acc: 91.772 Val_Loss: 0.3079  BEST VAL Loss: 0.3079  Val_Acc: 90.650

Epoch 14: Validation loss decreased (0.307888 --> 0.304851).  Saving model ...
	 Train_Loss: 0.3147 Train_Acc: 91.873 Val_Loss: 0.3049  BEST VAL Loss: 0.3049  Val_Acc: 91.006

Epoch 15: Validation loss decreased (0.304851 --> 0.302085).  Saving model ...
	 Train_Loss: 0.3091 Train_Acc: 91.939 Val_Loss: 0.3021  BEST VAL Loss: 0.3021  Val_Acc: 91.986

Epoch 16: Validation loss decreased (0.302085 --> 0.301252).  Saving model ...
	 Train_Loss: 0.3042 Train_Acc: 91.778 Val_Loss: 0.3013  BEST VAL Loss: 0.3013  Val_Acc: 90.516

Epoch 17: Validation loss decreased (0.301252 --> 0.299435).  Saving model ...
	 Train_Loss: 0.2997 Train_Acc: 92.084 Val_Loss: 0.2994  BEST VAL Loss: 0.2994  Val_Acc: 91.407

Epoch 18: Validation loss decreased (0.299435 --> 0.299272).  Saving model ...
	 Train_Loss: 0.2956 Train_Acc: 92.084 Val_Loss: 0.2993  BEST VAL Loss: 0.2993  Val_Acc: 91.318

Epoch 19: Validation loss decreased (0.299272 --> 0.297803).  Saving model ...
	 Train_Loss: 0.2914 Train_Acc: 92.713 Val_Loss: 0.2978  BEST VAL Loss: 0.2978  Val_Acc: 92.075

Epoch 20: Validation loss decreased (0.297803 --> 0.297025).  Saving model ...
	 Train_Loss: 0.2876 Train_Acc: 92.719 Val_Loss: 0.2970  BEST VAL Loss: 0.2970  Val_Acc: 91.630

Epoch 21: Validation loss decreased (0.297025 --> 0.296935).  Saving model ...
	 Train_Loss: 0.2838 Train_Acc: 93.164 Val_Loss: 0.2969  BEST VAL Loss: 0.2969  Val_Acc: 91.674

Epoch 22: Validation loss decreased (0.296935 --> 0.296580).  Saving model ...
	 Train_Loss: 0.2801 Train_Acc: 93.192 Val_Loss: 0.2966  BEST VAL Loss: 0.2966  Val_Acc: 91.852

Epoch 23: Validation loss decreased (0.296580 --> 0.294914).  Saving model ...
	 Train_Loss: 0.2767 Train_Acc: 93.225 Val_Loss: 0.2949  BEST VAL Loss: 0.2949  Val_Acc: 92.253

Epoch 24: Validation loss decreased (0.294914 --> 0.294425).  Saving model ...
	 Train_Loss: 0.2735 Train_Acc: 93.403 Val_Loss: 0.2944  BEST VAL Loss: 0.2944  Val_Acc: 91.763

Epoch 25: Validation loss decreased (0.294425 --> 0.293951).  Saving model ...
	 Train_Loss: 0.2701 Train_Acc: 93.821 Val_Loss: 0.2940  BEST VAL Loss: 0.2940  Val_Acc: 91.808

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.2671 Train_Acc: 93.532 Val_Loss: 0.2948  BEST VAL Loss: 0.2940  Val_Acc: 91.229

Epoch 27: Validation loss decreased (0.293951 --> 0.293318).  Saving model ...
	 Train_Loss: 0.2651 Train_Acc: 92.735 Val_Loss: 0.2933  BEST VAL Loss: 0.2933  Val_Acc: 91.451

Epoch 28: Validation loss decreased (0.293318 --> 0.292873).  Saving model ...
	 Train_Loss: 0.2627 Train_Acc: 93.353 Val_Loss: 0.2929  BEST VAL Loss: 0.2929  Val_Acc: 92.164

Epoch 29: Validation loss decreased (0.292873 --> 0.292516).  Saving model ...
	 Train_Loss: 0.2603 Train_Acc: 93.860 Val_Loss: 0.2925  BEST VAL Loss: 0.2925  Val_Acc: 91.763

Epoch 30: Validation loss decreased (0.292516 --> 0.292052).  Saving model ...
	 Train_Loss: 0.2580 Train_Acc: 93.609 Val_Loss: 0.2921  BEST VAL Loss: 0.2921  Val_Acc: 91.451

Epoch 31: Validation loss decreased (0.292052 --> 0.291446).  Saving model ...
	 Train_Loss: 0.2557 Train_Acc: 93.893 Val_Loss: 0.2914  BEST VAL Loss: 0.2914  Val_Acc: 91.941

Epoch 32: Validation loss decreased (0.291446 --> 0.291211).  Saving model ...
	 Train_Loss: 0.2534 Train_Acc: 94.016 Val_Loss: 0.2912  BEST VAL Loss: 0.2912  Val_Acc: 91.897

Epoch 33: Validation loss decreased (0.291211 --> 0.290929).  Saving model ...
	 Train_Loss: 0.2511 Train_Acc: 94.105 Val_Loss: 0.2909  BEST VAL Loss: 0.2909  Val_Acc: 91.451

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.2489 Train_Acc: 94.322 Val_Loss: 0.2911  BEST VAL Loss: 0.2909  Val_Acc: 91.852

Epoch 35: Validation loss decreased (0.290929 --> 0.290469).  Saving model ...
	 Train_Loss: 0.2472 Train_Acc: 93.693 Val_Loss: 0.2905  BEST VAL Loss: 0.2905  Val_Acc: 91.897

Epoch 36: Validation loss decreased (0.290469 --> 0.290063).  Saving model ...
	 Train_Loss: 0.2453 Train_Acc: 94.305 Val_Loss: 0.2901  BEST VAL Loss: 0.2901  Val_Acc: 92.119

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.2436 Train_Acc: 94.049 Val_Loss: 0.2901  BEST VAL Loss: 0.2901  Val_Acc: 91.095

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2418 Train_Acc: 94.300 Val_Loss: 0.2904  BEST VAL Loss: 0.2901  Val_Acc: 91.808

Epoch 39: Validation loss decreased (0.290063 --> 0.289968).  Saving model ...
	 Train_Loss: 0.2403 Train_Acc: 94.155 Val_Loss: 0.2900  BEST VAL Loss: 0.2900  Val_Acc: 92.164

Epoch 40: Validation loss decreased (0.289968 --> 0.289946).  Saving model ...
	 Train_Loss: 0.2386 Train_Acc: 94.344 Val_Loss: 0.2899  BEST VAL Loss: 0.2899  Val_Acc: 92.119

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2370 Train_Acc: 94.461 Val_Loss: 0.2902  BEST VAL Loss: 0.2899  Val_Acc: 92.164

Epoch 42: Validation loss decreased (0.289946 --> 0.289855).  Saving model ...
	 Train_Loss: 0.2355 Train_Acc: 94.350 Val_Loss: 0.2899  BEST VAL Loss: 0.2899  Val_Acc: 91.719

Epoch 43: Validation loss decreased (0.289855 --> 0.289210).  Saving model ...
	 Train_Loss: 0.2340 Train_Acc: 94.400 Val_Loss: 0.2892  BEST VAL Loss: 0.2892  Val_Acc: 92.520

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.2324 Train_Acc: 94.923 Val_Loss: 0.2893  BEST VAL Loss: 0.2892  Val_Acc: 92.743

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2308 Train_Acc: 95.007 Val_Loss: 0.2909  BEST VAL Loss: 0.2892  Val_Acc: 92.297

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.2295 Train_Acc: 94.394 Val_Loss: 0.2930  BEST VAL Loss: 0.2892  Val_Acc: 92.119

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.2283 Train_Acc: 94.433 Val_Loss: 0.2928  BEST VAL Loss: 0.2892  Val_Acc: 92.164

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.2271 Train_Acc: 94.728 Val_Loss: 0.2923  BEST VAL Loss: 0.2892  Val_Acc: 92.075

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2257 Train_Acc: 94.923 Val_Loss: 0.2926  BEST VAL Loss: 0.2892  Val_Acc: 92.119

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.2244 Train_Acc: 94.840 Val_Loss: 0.2943  BEST VAL Loss: 0.2892  Val_Acc: 91.719

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.2233 Train_Acc: 94.851 Val_Loss: 0.2949  BEST VAL Loss: 0.2892  Val_Acc: 91.585

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.2220 Train_Acc: 95.090 Val_Loss: 0.2964  BEST VAL Loss: 0.2892  Val_Acc: 91.674

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.2208 Train_Acc: 94.973 Val_Loss: 0.2979  BEST VAL Loss: 0.2892  Val_Acc: 91.763

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.2196 Train_Acc: 94.934 Val_Loss: 0.2988  BEST VAL Loss: 0.2892  Val_Acc: 92.386

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.2187 Train_Acc: 94.611 Val_Loss: 0.2994  BEST VAL Loss: 0.2892  Val_Acc: 91.897

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.2176 Train_Acc: 95.168 Val_Loss: 0.3004  BEST VAL Loss: 0.2892  Val_Acc: 91.897

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.2165 Train_Acc: 95.268 Val_Loss: 0.3012  BEST VAL Loss: 0.2892  Val_Acc: 92.119

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.2156 Train_Acc: 94.868 Val_Loss: 0.3022  BEST VAL Loss: 0.2892  Val_Acc: 91.407

Epoch 59: Validation loss did not decrease
Early stopped at epoch : 59
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.97      0.98     10114
           1       0.96      0.99      0.97      7850

    accuracy                           0.97     17964
   macro avg       0.97      0.98      0.97     17964
weighted avg       0.98      0.97      0.97     17964

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.92      0.93      1264
           1       0.90      0.93      0.92       982

    accuracy                           0.93      2246
   macro avg       0.92      0.93      0.92      2246
weighted avg       0.93      0.93      0.93      2246

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.94      0.93      0.94      1264
           1       0.91      0.93      0.92       982

    accuracy                           0.93      2246
   macro avg       0.93      0.93      0.93      2246
weighted avg       0.93      0.93      0.93      2246

              precision    recall  f1-score   support

           0       0.94      0.93      0.94      1264
           1       0.91      0.93      0.92       982

    accuracy                           0.93      2246
   macro avg       0.93      0.93      0.93      2246
weighted avg       0.93      0.93      0.93      2246

Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.95      0.95      4168
           1       0.94      0.93      0.94      3398

    accuracy                           0.94      7566
   macro avg       0.94      0.94      0.94      7566
weighted avg       0.94      0.94      0.94      7566

              precision    recall  f1-score   support

           0       0.95      0.95      0.95      4168
           1       0.94      0.93      0.94      3398

    accuracy                           0.94      7566
   macro avg       0.94      0.94      0.94      7566
weighted avg       0.94      0.94      0.94      7566

completed
