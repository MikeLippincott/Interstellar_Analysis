[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'af67914c'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '6d6eb1bb'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'c7c78306'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'b268501e'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: DMSO_0.100_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_1.000_DMSO_0.025
TREATMENT_NAME: DMSO_0.100_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['DMSO_0.100_DMSO_0.025' 'Thapsigargin_1.000_DMSO_0.025']
The dimensions of the data are: (49409, 1276)
Number of total missing values across all columns: 98818
Data Subset Is Off
Wells held out for testing: ['I14' 'K14']
Wells to use for training, validation, and testing ['B14' 'C14' 'D14' 'B15' 'C15' 'D15' 'J14' 'I15' 'J15' 'K15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.363042).  Saving model ...
	 Train_Loss: 0.5410 Train_Acc: 63.542 Val_Loss: 0.3630  BEST VAL Loss: 0.3630  Val_Acc: 85.711

Epoch 1: Validation loss decreased (0.363042 --> 0.353094).  Saving model ...
	 Train_Loss: 0.4984 Train_Acc: 69.159 Val_Loss: 0.3531  BEST VAL Loss: 0.3531  Val_Acc: 88.068

Epoch 2: Validation loss decreased (0.353094 --> 0.346698).  Saving model ...
	 Train_Loss: 0.4764 Train_Acc: 73.786 Val_Loss: 0.3467  BEST VAL Loss: 0.3467  Val_Acc: 74.872

Epoch 3: Validation loss decreased (0.346698 --> 0.340362).  Saving model ...
	 Train_Loss: 0.4589 Train_Acc: 74.867 Val_Loss: 0.3404  BEST VAL Loss: 0.3404  Val_Acc: 74.872

Epoch 4: Validation loss decreased (0.340362 --> 0.334494).  Saving model ...
	 Train_Loss: 0.4376 Train_Acc: 77.951 Val_Loss: 0.3345  BEST VAL Loss: 0.3345  Val_Acc: 88.554

Epoch 5: Validation loss decreased (0.334494 --> 0.326895).  Saving model ...
	 Train_Loss: 0.4214 Train_Acc: 84.045 Val_Loss: 0.3269  BEST VAL Loss: 0.3269  Val_Acc: 89.016

Epoch 6: Validation loss decreased (0.326895 --> 0.320366).  Saving model ...
	 Train_Loss: 0.4078 Train_Acc: 84.655 Val_Loss: 0.3204  BEST VAL Loss: 0.3204  Val_Acc: 89.550

Epoch 7: Validation loss decreased (0.320366 --> 0.315321).  Saving model ...
	 Train_Loss: 0.3965 Train_Acc: 85.163 Val_Loss: 0.3153  BEST VAL Loss: 0.3153  Val_Acc: 89.405

Epoch 8: Validation loss decreased (0.315321 --> 0.309585).  Saving model ...
	 Train_Loss: 0.3866 Train_Acc: 85.585 Val_Loss: 0.3096  BEST VAL Loss: 0.3096  Val_Acc: 89.939

Epoch 9: Validation loss decreased (0.309585 --> 0.304683).  Saving model ...
	 Train_Loss: 0.3784 Train_Acc: 85.567 Val_Loss: 0.3047  BEST VAL Loss: 0.3047  Val_Acc: 90.377

Epoch 10: Validation loss decreased (0.304683 --> 0.300423).  Saving model ...
	 Train_Loss: 0.3712 Train_Acc: 85.576 Val_Loss: 0.3004  BEST VAL Loss: 0.3004  Val_Acc: 90.668

Epoch 11: Validation loss decreased (0.300423 --> 0.296824).  Saving model ...
	 Train_Loss: 0.3648 Train_Acc: 85.956 Val_Loss: 0.2968  BEST VAL Loss: 0.2968  Val_Acc: 90.328

Epoch 12: Validation loss decreased (0.296824 --> 0.293888).  Saving model ...
	 Train_Loss: 0.3593 Train_Acc: 85.950 Val_Loss: 0.2939  BEST VAL Loss: 0.2939  Val_Acc: 90.425

Epoch 13: Validation loss decreased (0.293888 --> 0.291125).  Saving model ...
	 Train_Loss: 0.3539 Train_Acc: 86.688 Val_Loss: 0.2911  BEST VAL Loss: 0.2911  Val_Acc: 90.984

Epoch 14: Validation loss decreased (0.291125 --> 0.288848).  Saving model ...
	 Train_Loss: 0.3493 Train_Acc: 86.654 Val_Loss: 0.2888  BEST VAL Loss: 0.2888  Val_Acc: 91.179

Epoch 15: Validation loss decreased (0.288848 --> 0.286723).  Saving model ...
	 Train_Loss: 0.3451 Train_Acc: 86.642 Val_Loss: 0.2867  BEST VAL Loss: 0.2867  Val_Acc: 91.106

Epoch 16: Validation loss decreased (0.286723 --> 0.284892).  Saving model ...
	 Train_Loss: 0.3413 Train_Acc: 86.597 Val_Loss: 0.2849  BEST VAL Loss: 0.2849  Val_Acc: 91.179

Epoch 17: Validation loss decreased (0.284892 --> 0.282935).  Saving model ...
	 Train_Loss: 0.3373 Train_Acc: 87.298 Val_Loss: 0.2829  BEST VAL Loss: 0.2829  Val_Acc: 91.179

Epoch 18: Validation loss decreased (0.282935 --> 0.281355).  Saving model ...
	 Train_Loss: 0.3340 Train_Acc: 86.995 Val_Loss: 0.2814  BEST VAL Loss: 0.2814  Val_Acc: 91.203

Epoch 19: Validation loss decreased (0.281355 --> 0.280769).  Saving model ...
	 Train_Loss: 0.3306 Train_Acc: 87.557 Val_Loss: 0.2808  BEST VAL Loss: 0.2808  Val_Acc: 91.276

Epoch 20: Validation loss decreased (0.280769 --> 0.279558).  Saving model ...
	 Train_Loss: 0.3277 Train_Acc: 87.298 Val_Loss: 0.2796  BEST VAL Loss: 0.2796  Val_Acc: 90.863

Epoch 21: Validation loss decreased (0.279558 --> 0.278504).  Saving model ...
	 Train_Loss: 0.3250 Train_Acc: 87.660 Val_Loss: 0.2785  BEST VAL Loss: 0.2785  Val_Acc: 90.887

Epoch 22: Validation loss decreased (0.278504 --> 0.277215).  Saving model ...
	 Train_Loss: 0.3225 Train_Acc: 87.402 Val_Loss: 0.2772  BEST VAL Loss: 0.2772  Val_Acc: 91.689

Epoch 23: Validation loss decreased (0.277215 --> 0.276756).  Saving model ...
	 Train_Loss: 0.3201 Train_Acc: 87.304 Val_Loss: 0.2768  BEST VAL Loss: 0.2768  Val_Acc: 91.640

Epoch 24: Validation loss decreased (0.276756 --> 0.276105).  Saving model ...
	 Train_Loss: 0.3178 Train_Acc: 87.499 Val_Loss: 0.2761  BEST VAL Loss: 0.2761  Val_Acc: 91.689

Epoch 25: Validation loss decreased (0.276105 --> 0.275607).  Saving model ...
	 Train_Loss: 0.3155 Train_Acc: 88.003 Val_Loss: 0.2756  BEST VAL Loss: 0.2756  Val_Acc: 91.543

Epoch 26: Validation loss decreased (0.275607 --> 0.275447).  Saving model ...
	 Train_Loss: 0.3134 Train_Acc: 87.818 Val_Loss: 0.2754  BEST VAL Loss: 0.2754  Val_Acc: 91.543

Epoch 27: Validation loss decreased (0.275447 --> 0.274893).  Saving model ...
	 Train_Loss: 0.3115 Train_Acc: 87.891 Val_Loss: 0.2749  BEST VAL Loss: 0.2749  Val_Acc: 91.495

Epoch 28: Validation loss decreased (0.274893 --> 0.274759).  Saving model ...
	 Train_Loss: 0.3095 Train_Acc: 87.912 Val_Loss: 0.2748  BEST VAL Loss: 0.2748  Val_Acc: 91.179

Epoch 29: Validation loss decreased (0.274759 --> 0.274683).  Saving model ...
	 Train_Loss: 0.3077 Train_Acc: 88.046 Val_Loss: 0.2747  BEST VAL Loss: 0.2747  Val_Acc: 91.130

Epoch 30: Validation loss decreased (0.274683 --> 0.274596).  Saving model ...
	 Train_Loss: 0.3060 Train_Acc: 88.082 Val_Loss: 0.2746  BEST VAL Loss: 0.2746  Val_Acc: 91.543

Epoch 31: Validation loss decreased (0.274596 --> 0.274538).  Saving model ...
	 Train_Loss: 0.3044 Train_Acc: 87.885 Val_Loss: 0.2745  BEST VAL Loss: 0.2745  Val_Acc: 91.567

Epoch 32: Validation loss decreased (0.274538 --> 0.274079).  Saving model ...
	 Train_Loss: 0.3027 Train_Acc: 88.179 Val_Loss: 0.2741  BEST VAL Loss: 0.2741  Val_Acc: 91.689

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.3011 Train_Acc: 88.504 Val_Loss: 0.2746  BEST VAL Loss: 0.2741  Val_Acc: 91.616

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.2996 Train_Acc: 88.471 Val_Loss: 0.2743  BEST VAL Loss: 0.2741  Val_Acc: 91.422

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.2981 Train_Acc: 88.416 Val_Loss: 0.2742  BEST VAL Loss: 0.2741  Val_Acc: 91.738

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.2967 Train_Acc: 88.398 Val_Loss: 0.2744  BEST VAL Loss: 0.2741  Val_Acc: 91.665

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.2955 Train_Acc: 88.213 Val_Loss: 0.2743  BEST VAL Loss: 0.2741  Val_Acc: 91.713

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2942 Train_Acc: 88.383 Val_Loss: 0.2747  BEST VAL Loss: 0.2741  Val_Acc: 91.835

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.2930 Train_Acc: 88.255 Val_Loss: 0.2746  BEST VAL Loss: 0.2741  Val_Acc: 91.810

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2917 Train_Acc: 88.833 Val_Loss: 0.2748  BEST VAL Loss: 0.2741  Val_Acc: 91.592

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2905 Train_Acc: 88.747 Val_Loss: 0.2749  BEST VAL Loss: 0.2741  Val_Acc: 91.640

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2894 Train_Acc: 88.729 Val_Loss: 0.2756  BEST VAL Loss: 0.2741  Val_Acc: 90.863

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.2882 Train_Acc: 88.854 Val_Loss: 0.2759  BEST VAL Loss: 0.2741  Val_Acc: 91.835

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.2872 Train_Acc: 88.477 Val_Loss: 0.2759  BEST VAL Loss: 0.2741  Val_Acc: 92.078

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2862 Train_Acc: 88.617 Val_Loss: 0.2766  BEST VAL Loss: 0.2741  Val_Acc: 91.713

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.2853 Train_Acc: 88.617 Val_Loss: 0.2772  BEST VAL Loss: 0.2741  Val_Acc: 91.446

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.2844 Train_Acc: 88.526 Val_Loss: 0.2781  BEST VAL Loss: 0.2741  Val_Acc: 91.932

Epoch 48: Validation loss did not decrease
Early stopped at epoch : 48
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.96      0.97     24644
           1       0.89      0.96      0.93      8273

    accuracy                           0.96     32917
   macro avg       0.94      0.96      0.95     32917
weighted avg       0.96      0.96      0.96     32917

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.94      0.94      3081
           1       0.82      0.86      0.84      1034

    accuracy                           0.92      4115
   macro avg       0.89      0.90      0.89      4115
weighted avg       0.92      0.92      0.92      4115

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.95      0.94      0.94      3081
           1       0.82      0.86      0.84      1034

    accuracy                           0.92      4115
   macro avg       0.89      0.90      0.89      4115
weighted avg       0.92      0.92      0.92      4115

              precision    recall  f1-score   support

           0       0.95      0.94      0.94      3081
           1       0.82      0.86      0.84      1034

    accuracy                           0.92      4115
   macro avg       0.89      0.90      0.89      4115
weighted avg       0.92      0.92      0.92      4115

Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_1.000_DMSO_0.025_vs_DMSO_0.100_DMSO_0.025
              precision    recall  f1-score   support

           0       0.90      0.96      0.93      4837
           1       0.94      0.85      0.89      3425

    accuracy                           0.91      8262
   macro avg       0.92      0.90      0.91      8262
weighted avg       0.91      0.91      0.91      8262

              precision    recall  f1-score   support

           0       0.90      0.96      0.93      4837
           1       0.94      0.85      0.89      3425

    accuracy                           0.91      8262
   macro avg       0.92      0.90      0.91      8262
weighted avg       0.91      0.91      0.91      8262

completed
