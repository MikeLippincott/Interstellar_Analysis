[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '10a31e63'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '4357e758'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '887702e9'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '4dfce1eb'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_10.000_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_10.000_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_10.000_DMSO_0.025']
The dimensions of the data are: (30022, 1276)
Number of total missing values across all columns: 60044
Data Subset Is Off
Wells held out for testing: ['E14' 'E20']
Wells to use for training, validation, and testing ['E15' 'E16' 'E17' 'E21' 'L14' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.310689).  Saving model ...
	 Train_Loss: 0.4688 Train_Acc: 77.093 Val_Loss: 0.3107  BEST VAL Loss: 0.3107  Val_Acc: 87.311

Epoch 1: Validation loss decreased (0.310689 --> 0.279594).  Saving model ...
	 Train_Loss: 0.3969 Train_Acc: 84.992 Val_Loss: 0.2796  BEST VAL Loss: 0.2796  Val_Acc: 89.225

Epoch 2: Validation loss decreased (0.279594 --> 0.262249).  Saving model ...
	 Train_Loss: 0.3555 Train_Acc: 87.698 Val_Loss: 0.2622  BEST VAL Loss: 0.2622  Val_Acc: 91.318

Epoch 3: Validation loss decreased (0.262249 --> 0.248355).  Saving model ...
	 Train_Loss: 0.3258 Train_Acc: 88.828 Val_Loss: 0.2484  BEST VAL Loss: 0.2484  Val_Acc: 91.095

Epoch 4: Validation loss decreased (0.248355 --> 0.240534).  Saving model ...
	 Train_Loss: 0.3033 Train_Acc: 89.763 Val_Loss: 0.2405  BEST VAL Loss: 0.2405  Val_Acc: 92.164

Epoch 5: Validation loss decreased (0.240534 --> 0.231793).  Saving model ...
	 Train_Loss: 0.2852 Train_Acc: 90.759 Val_Loss: 0.2318  BEST VAL Loss: 0.2318  Val_Acc: 92.075

Epoch 6: Validation loss decreased (0.231793 --> 0.227796).  Saving model ...
	 Train_Loss: 0.2699 Train_Acc: 91.383 Val_Loss: 0.2278  BEST VAL Loss: 0.2278  Val_Acc: 92.787

Epoch 7: Validation loss decreased (0.227796 --> 0.221695).  Saving model ...
	 Train_Loss: 0.2571 Train_Acc: 91.689 Val_Loss: 0.2217  BEST VAL Loss: 0.2217  Val_Acc: 93.455

Epoch 8: Validation loss decreased (0.221695 --> 0.218705).  Saving model ...
	 Train_Loss: 0.2470 Train_Acc: 91.433 Val_Loss: 0.2187  BEST VAL Loss: 0.2187  Val_Acc: 92.743

Epoch 9: Validation loss decreased (0.218705 --> 0.215239).  Saving model ...
	 Train_Loss: 0.2373 Train_Acc: 92.145 Val_Loss: 0.2152  BEST VAL Loss: 0.2152  Val_Acc: 92.609

Epoch 10: Validation loss decreased (0.215239 --> 0.212227).  Saving model ...
	 Train_Loss: 0.2292 Train_Acc: 92.735 Val_Loss: 0.2122  BEST VAL Loss: 0.2122  Val_Acc: 92.743

Epoch 11: Validation loss decreased (0.212227 --> 0.210361).  Saving model ...
	 Train_Loss: 0.2219 Train_Acc: 93.643 Val_Loss: 0.2104  BEST VAL Loss: 0.2104  Val_Acc: 92.698

Epoch 12: Validation loss decreased (0.210361 --> 0.208848).  Saving model ...
	 Train_Loss: 0.2156 Train_Acc: 93.877 Val_Loss: 0.2088  BEST VAL Loss: 0.2088  Val_Acc: 92.609

Epoch 13: Validation loss did not decrease
	 Train_Loss: 0.2099 Train_Acc: 93.999 Val_Loss: 0.2089  BEST VAL Loss: 0.2088  Val_Acc: 92.030

Epoch 14: Validation loss decreased (0.208848 --> 0.205674).  Saving model ...
	 Train_Loss: 0.2044 Train_Acc: 94.333 Val_Loss: 0.2057  BEST VAL Loss: 0.2057  Val_Acc: 92.743

Epoch 15: Validation loss decreased (0.205674 --> 0.203931).  Saving model ...
	 Train_Loss: 0.1994 Train_Acc: 94.367 Val_Loss: 0.2039  BEST VAL Loss: 0.2039  Val_Acc: 92.876

Epoch 16: Validation loss did not decrease
	 Train_Loss: 0.1949 Train_Acc: 94.444 Val_Loss: 0.2045  BEST VAL Loss: 0.2039  Val_Acc: 92.565

Epoch 17: Validation loss did not decrease
	 Train_Loss: 0.1907 Train_Acc: 94.650 Val_Loss: 0.2040  BEST VAL Loss: 0.2039  Val_Acc: 93.232

Epoch 18: Validation loss did not decrease
	 Train_Loss: 0.1870 Train_Acc: 94.712 Val_Loss: 0.2050  BEST VAL Loss: 0.2039  Val_Acc: 92.743

Epoch 19: Validation loss decreased (0.203931 --> 0.203486).  Saving model ...
	 Train_Loss: 0.1833 Train_Acc: 94.790 Val_Loss: 0.2035  BEST VAL Loss: 0.2035  Val_Acc: 93.099

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1799 Train_Acc: 95.085 Val_Loss: 0.2052  BEST VAL Loss: 0.2035  Val_Acc: 93.010

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1768 Train_Acc: 94.973 Val_Loss: 0.2056  BEST VAL Loss: 0.2035  Val_Acc: 93.143

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1739 Train_Acc: 95.023 Val_Loss: 0.2048  BEST VAL Loss: 0.2035  Val_Acc: 93.321

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1710 Train_Acc: 95.380 Val_Loss: 0.2065  BEST VAL Loss: 0.2035  Val_Acc: 93.455

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1684 Train_Acc: 95.352 Val_Loss: 0.2060  BEST VAL Loss: 0.2035  Val_Acc: 93.099

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1657 Train_Acc: 95.714 Val_Loss: 0.2056  BEST VAL Loss: 0.2035  Val_Acc: 93.099

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1631 Train_Acc: 95.831 Val_Loss: 0.2065  BEST VAL Loss: 0.2035  Val_Acc: 93.277

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1609 Train_Acc: 95.380 Val_Loss: 0.2059  BEST VAL Loss: 0.2035  Val_Acc: 93.277

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1587 Train_Acc: 95.519 Val_Loss: 0.2058  BEST VAL Loss: 0.2035  Val_Acc: 93.589

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1568 Train_Acc: 95.580 Val_Loss: 0.2074  BEST VAL Loss: 0.2035  Val_Acc: 93.054

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1548 Train_Acc: 95.691 Val_Loss: 0.2079  BEST VAL Loss: 0.2035  Val_Acc: 93.010

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.1531 Train_Acc: 95.686 Val_Loss: 0.2084  BEST VAL Loss: 0.2035  Val_Acc: 93.455

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.1516 Train_Acc: 95.480 Val_Loss: 0.2094  BEST VAL Loss: 0.2035  Val_Acc: 92.698

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1500 Train_Acc: 95.758 Val_Loss: 0.2110  BEST VAL Loss: 0.2035  Val_Acc: 92.609

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.1484 Train_Acc: 95.708 Val_Loss: 0.2124  BEST VAL Loss: 0.2035  Val_Acc: 93.321

Epoch 35: Validation loss did not decrease
Early stopped at epoch : 35
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.55      0.56     10114
           1       0.44      0.45      0.44      7850

    accuracy                           0.51     17964
   macro avg       0.50      0.50      0.50     17964
weighted avg       0.51      0.51      0.51     17964

LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.54      0.55      1264
           1       0.43      0.44      0.43       982

    accuracy                           0.50      2246
   macro avg       0.49      0.49      0.49      2246
weighted avg       0.50      0.50      0.50      2246

LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.56      0.55      0.55      1264
           1       0.43      0.44      0.44       982

    accuracy                           0.50      2246
   macro avg       0.49      0.49      0.49      2246
weighted avg       0.50      0.50      0.50      2246

              precision    recall  f1-score   support

           0       0.56      0.55      0.55      1264
           1       0.43      0.44      0.44       982

    accuracy                           0.50      2246
   macro avg       0.49      0.49      0.49      2246
weighted avg       0.50      0.50      0.50      2246

LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_10.000_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.56      0.55      4168
           1       0.45      0.44      0.45      3398

    accuracy                           0.51      7566
   macro avg       0.50      0.50      0.50      7566
weighted avg       0.51      0.51      0.51      7566

              precision    recall  f1-score   support

           0       0.55      0.56      0.55      4168
           1       0.45      0.44      0.45      3398

    accuracy                           0.51      7566
   macro avg       0.50      0.50      0.50      7566
weighted avg       0.51      0.51      0.51      7566

completed
