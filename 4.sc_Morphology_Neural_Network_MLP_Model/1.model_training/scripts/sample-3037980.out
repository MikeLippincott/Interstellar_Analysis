[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '87a4c1f9'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '22063be5'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '063f0d9c'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'f96bc3bf'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: True
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: True
True
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (29430, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'K20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'K16' 'K17' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.297657).  Saving model ...
	 Train_Loss: 0.4957 Train_Acc: 77.411 Val_Loss: 0.2977  BEST VAL Loss: 0.2977  Val_Acc: 90.789

Epoch 1: Validation loss decreased (0.297657 --> 0.248218).  Saving model ...
	 Train_Loss: 0.4094 Train_Acc: 88.354 Val_Loss: 0.2482  BEST VAL Loss: 0.2482  Val_Acc: 93.784

Epoch 2: Validation loss decreased (0.248218 --> 0.219501).  Saving model ...
	 Train_Loss: 0.3592 Train_Acc: 91.150 Val_Loss: 0.2195  BEST VAL Loss: 0.2195  Val_Acc: 95.236

Epoch 3: Validation loss decreased (0.219501 --> 0.199720).  Saving model ...
	 Train_Loss: 0.3226 Train_Acc: 92.710 Val_Loss: 0.1997  BEST VAL Loss: 0.1997  Val_Acc: 95.281

Epoch 4: Validation loss decreased (0.199720 --> 0.186862).  Saving model ...
	 Train_Loss: 0.3000 Train_Acc: 93.306 Val_Loss: 0.1869  BEST VAL Loss: 0.1869  Val_Acc: 95.780

Epoch 5: Validation loss decreased (0.186862 --> 0.178356).  Saving model ...
	 Train_Loss: 0.2830 Train_Acc: 93.516 Val_Loss: 0.1784  BEST VAL Loss: 0.1784  Val_Acc: 96.234

Epoch 6: Validation loss decreased (0.178356 --> 0.171208).  Saving model ...
	 Train_Loss: 0.2681 Train_Acc: 94.305 Val_Loss: 0.1712  BEST VAL Loss: 0.1712  Val_Acc: 96.960

Epoch 7: Validation loss decreased (0.171208 --> 0.166319).  Saving model ...
	 Train_Loss: 0.2559 Train_Acc: 94.860 Val_Loss: 0.1663  BEST VAL Loss: 0.1663  Val_Acc: 96.733

Epoch 8: Validation loss decreased (0.166319 --> 0.159772).  Saving model ...
	 Train_Loss: 0.2459 Train_Acc: 94.940 Val_Loss: 0.1598  BEST VAL Loss: 0.1598  Val_Acc: 96.869

Epoch 9: Validation loss decreased (0.159772 --> 0.156572).  Saving model ...
	 Train_Loss: 0.2372 Train_Acc: 95.059 Val_Loss: 0.1566  BEST VAL Loss: 0.1566  Val_Acc: 96.915

Epoch 10: Validation loss decreased (0.156572 --> 0.152850).  Saving model ...
	 Train_Loss: 0.2298 Train_Acc: 95.382 Val_Loss: 0.1529  BEST VAL Loss: 0.1529  Val_Acc: 97.005

Epoch 11: Validation loss decreased (0.152850 --> 0.147987).  Saving model ...
	 Train_Loss: 0.2230 Train_Acc: 95.524 Val_Loss: 0.1480  BEST VAL Loss: 0.1480  Val_Acc: 97.323

Epoch 12: Validation loss decreased (0.147987 --> 0.143947).  Saving model ...
	 Train_Loss: 0.2170 Train_Acc: 95.632 Val_Loss: 0.1439  BEST VAL Loss: 0.1439  Val_Acc: 97.323

Epoch 13: Validation loss decreased (0.143947 --> 0.141331).  Saving model ...
	 Train_Loss: 0.2117 Train_Acc: 95.660 Val_Loss: 0.1413  BEST VAL Loss: 0.1413  Val_Acc: 97.686

Epoch 14: Validation loss decreased (0.141331 --> 0.138679).  Saving model ...
	 Train_Loss: 0.2078 Train_Acc: 95.263 Val_Loss: 0.1387  BEST VAL Loss: 0.1387  Val_Acc: 97.278

Epoch 15: Validation loss decreased (0.138679 --> 0.136792).  Saving model ...
	 Train_Loss: 0.2038 Train_Acc: 95.360 Val_Loss: 0.1368  BEST VAL Loss: 0.1368  Val_Acc: 97.550

Epoch 16: Validation loss decreased (0.136792 --> 0.134348).  Saving model ...
	 Train_Loss: 0.1995 Train_Acc: 95.927 Val_Loss: 0.1343  BEST VAL Loss: 0.1343  Val_Acc: 97.686

Epoch 17: Validation loss decreased (0.134348 --> 0.131951).  Saving model ...
	 Train_Loss: 0.1958 Train_Acc: 95.921 Val_Loss: 0.1320  BEST VAL Loss: 0.1320  Val_Acc: 97.459

Epoch 18: Validation loss decreased (0.131951 --> 0.130386).  Saving model ...
	 Train_Loss: 0.1923 Train_Acc: 96.063 Val_Loss: 0.1304  BEST VAL Loss: 0.1304  Val_Acc: 97.096

Epoch 19: Validation loss did not decrease
	 Train_Loss: 0.1890 Train_Acc: 96.125 Val_Loss: 0.1308  BEST VAL Loss: 0.1304  Val_Acc: 97.278

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1859 Train_Acc: 96.052 Val_Loss: 0.1318  BEST VAL Loss: 0.1304  Val_Acc: 97.278

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1833 Train_Acc: 95.870 Val_Loss: 0.1323  BEST VAL Loss: 0.1304  Val_Acc: 97.142

Epoch 22: Validation loss decreased (0.130386 --> 0.129883).  Saving model ...
	 Train_Loss: 0.1807 Train_Acc: 96.160 Val_Loss: 0.1299  BEST VAL Loss: 0.1299  Val_Acc: 97.686

Epoch 23: Validation loss decreased (0.129883 --> 0.128044).  Saving model ...
	 Train_Loss: 0.1784 Train_Acc: 96.245 Val_Loss: 0.1280  BEST VAL Loss: 0.1280  Val_Acc: 97.323

Epoch 24: Validation loss decreased (0.128044 --> 0.127822).  Saving model ...
	 Train_Loss: 0.1762 Train_Acc: 96.222 Val_Loss: 0.1278  BEST VAL Loss: 0.1278  Val_Acc: 97.913

Epoch 25: Validation loss decreased (0.127822 --> 0.127027).  Saving model ...
	 Train_Loss: 0.1740 Train_Acc: 96.233 Val_Loss: 0.1270  BEST VAL Loss: 0.1270  Val_Acc: 97.368

Epoch 26: Validation loss decreased (0.127027 --> 0.125908).  Saving model ...
	 Train_Loss: 0.1721 Train_Acc: 96.137 Val_Loss: 0.1259  BEST VAL Loss: 0.1259  Val_Acc: 97.641

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1702 Train_Acc: 96.131 Val_Loss: 0.1263  BEST VAL Loss: 0.1259  Val_Acc: 98.094

Epoch 28: Validation loss decreased (0.125908 --> 0.125044).  Saving model ...
	 Train_Loss: 0.1683 Train_Acc: 96.392 Val_Loss: 0.1250  BEST VAL Loss: 0.1250  Val_Acc: 97.641

Epoch 29: Validation loss decreased (0.125044 --> 0.124480).  Saving model ...
	 Train_Loss: 0.1665 Train_Acc: 96.403 Val_Loss: 0.1245  BEST VAL Loss: 0.1245  Val_Acc: 97.777

Epoch 30: Validation loss decreased (0.124480 --> 0.122999).  Saving model ...
	 Train_Loss: 0.1648 Train_Acc: 96.415 Val_Loss: 0.1230  BEST VAL Loss: 0.1230  Val_Acc: 97.913

Epoch 31: Validation loss decreased (0.122999 --> 0.122502).  Saving model ...
	 Train_Loss: 0.1631 Train_Acc: 96.574 Val_Loss: 0.1225  BEST VAL Loss: 0.1225  Val_Acc: 97.641

Epoch 32: Validation loss decreased (0.122502 --> 0.122088).  Saving model ...
	 Train_Loss: 0.1614 Train_Acc: 96.557 Val_Loss: 0.1221  BEST VAL Loss: 0.1221  Val_Acc: 97.232

Epoch 33: Validation loss decreased (0.122088 --> 0.121953).  Saving model ...
	 Train_Loss: 0.1598 Train_Acc: 96.540 Val_Loss: 0.1220  BEST VAL Loss: 0.1220  Val_Acc: 97.595

Epoch 34: Validation loss decreased (0.121953 --> 0.121243).  Saving model ...
	 Train_Loss: 0.1586 Train_Acc: 96.273 Val_Loss: 0.1212  BEST VAL Loss: 0.1212  Val_Acc: 97.777

Epoch 35: Validation loss decreased (0.121243 --> 0.120808).  Saving model ...
	 Train_Loss: 0.1574 Train_Acc: 96.352 Val_Loss: 0.1208  BEST VAL Loss: 0.1208  Val_Acc: 97.550

Epoch 36: Validation loss decreased (0.120808 --> 0.120431).  Saving model ...
	 Train_Loss: 0.1560 Train_Acc: 96.715 Val_Loss: 0.1204  BEST VAL Loss: 0.1204  Val_Acc: 98.049

Epoch 37: Validation loss decreased (0.120431 --> 0.119947).  Saving model ...
	 Train_Loss: 0.1547 Train_Acc: 96.574 Val_Loss: 0.1199  BEST VAL Loss: 0.1199  Val_Acc: 97.822

Epoch 38: Validation loss decreased (0.119947 --> 0.119307).  Saving model ...
	 Train_Loss: 0.1536 Train_Acc: 96.420 Val_Loss: 0.1193  BEST VAL Loss: 0.1193  Val_Acc: 97.958

Epoch 39: Validation loss decreased (0.119307 --> 0.118981).  Saving model ...
	 Train_Loss: 0.1525 Train_Acc: 96.494 Val_Loss: 0.1190  BEST VAL Loss: 0.1190  Val_Acc: 97.822

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.1513 Train_Acc: 96.653 Val_Loss: 0.1191  BEST VAL Loss: 0.1190  Val_Acc: 97.777

Epoch 41: Validation loss decreased (0.118981 --> 0.118813).  Saving model ...
	 Train_Loss: 0.1501 Train_Acc: 96.857 Val_Loss: 0.1188  BEST VAL Loss: 0.1188  Val_Acc: 97.913

Epoch 42: Validation loss decreased (0.118813 --> 0.118404).  Saving model ...
	 Train_Loss: 0.1491 Train_Acc: 96.806 Val_Loss: 0.1184  BEST VAL Loss: 0.1184  Val_Acc: 97.822

Epoch 43: Validation loss decreased (0.118404 --> 0.118341).  Saving model ...
	 Train_Loss: 0.1479 Train_Acc: 96.920 Val_Loss: 0.1183  BEST VAL Loss: 0.1183  Val_Acc: 97.777

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.1468 Train_Acc: 96.869 Val_Loss: 0.1186  BEST VAL Loss: 0.1183  Val_Acc: 97.822

Epoch 45: Validation loss decreased (0.118341 --> 0.118240).  Saving model ...
	 Train_Loss: 0.1458 Train_Acc: 96.920 Val_Loss: 0.1182  BEST VAL Loss: 0.1182  Val_Acc: 98.049

Epoch 46: Validation loss decreased (0.118240 --> 0.117849).  Saving model ...
	 Train_Loss: 0.1448 Train_Acc: 96.835 Val_Loss: 0.1178  BEST VAL Loss: 0.1178  Val_Acc: 98.185

Epoch 47: Validation loss decreased (0.117849 --> 0.117318).  Saving model ...
	 Train_Loss: 0.1440 Train_Acc: 96.676 Val_Loss: 0.1173  BEST VAL Loss: 0.1173  Val_Acc: 97.686

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1432 Train_Acc: 96.721 Val_Loss: 0.1192  BEST VAL Loss: 0.1173  Val_Acc: 97.913

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1425 Train_Acc: 96.636 Val_Loss: 0.1188  BEST VAL Loss: 0.1173  Val_Acc: 98.276

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1416 Train_Acc: 96.897 Val_Loss: 0.1184  BEST VAL Loss: 0.1173  Val_Acc: 97.777

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1409 Train_Acc: 96.613 Val_Loss: 0.1187  BEST VAL Loss: 0.1173  Val_Acc: 97.686

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1403 Train_Acc: 96.540 Val_Loss: 0.1186  BEST VAL Loss: 0.1173  Val_Acc: 97.822

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1394 Train_Acc: 97.027 Val_Loss: 0.1189  BEST VAL Loss: 0.1173  Val_Acc: 98.049

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1387 Train_Acc: 96.908 Val_Loss: 0.1185  BEST VAL Loss: 0.1173  Val_Acc: 97.822

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.1380 Train_Acc: 96.840 Val_Loss: 0.1183  BEST VAL Loss: 0.1173  Val_Acc: 98.049

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.1374 Train_Acc: 96.755 Val_Loss: 0.1184  BEST VAL Loss: 0.1173  Val_Acc: 97.777

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.1369 Train_Acc: 96.693 Val_Loss: 0.1188  BEST VAL Loss: 0.1173  Val_Acc: 98.004

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.1362 Train_Acc: 96.852 Val_Loss: 0.1188  BEST VAL Loss: 0.1173  Val_Acc: 97.913

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.1356 Train_Acc: 96.999 Val_Loss: 0.1185  BEST VAL Loss: 0.1173  Val_Acc: 97.913

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.1350 Train_Acc: 96.869 Val_Loss: 0.1186  BEST VAL Loss: 0.1173  Val_Acc: 97.958

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.1344 Train_Acc: 97.005 Val_Loss: 0.1194  BEST VAL Loss: 0.1173  Val_Acc: 97.868

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.1338 Train_Acc: 96.965 Val_Loss: 0.1200  BEST VAL Loss: 0.1173  Val_Acc: 98.321

Epoch 63: Validation loss did not decrease
Early stopped at epoch : 63
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      9778
           1       0.45      0.45      0.45      7850

    accuracy                           0.51     17628
   macro avg       0.50      0.50      0.50     17628
weighted avg       0.51      0.51      0.51     17628

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.54      0.54      1222
           1       0.44      0.45      0.45       982

    accuracy                           0.50      2204
   macro avg       0.49      0.49      0.49      2204
weighted avg       0.50      0.50      0.50      2204

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1222
           1       0.44      0.45      0.45       982

    accuracy                           0.50      2204
   macro avg       0.50      0.50      0.50      2204
weighted avg       0.50      0.50      0.50      2204

              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1222
           1       0.44      0.45      0.45       982

    accuracy                           0.50      2204
   macro avg       0.50      0.50      0.50      2204
weighted avg       0.50      0.50      0.50      2204

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025_shuffle
              precision    recall  f1-score   support

           0       0.55      0.54      0.54      3996
           1       0.47      0.47      0.47      3398

    accuracy                           0.51      7394
   macro avg       0.51      0.51      0.51      7394
weighted avg       0.51      0.51      0.51      7394

              precision    recall  f1-score   support

           0       0.55      0.54      0.54      3996
           1       0.47      0.47      0.47      3398

    accuracy                           0.51      7394
   macro avg       0.51      0.51      0.51      7394
weighted avg       0.51      0.51      0.51      7394

completed
