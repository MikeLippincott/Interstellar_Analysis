[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a47c758e'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'c90d4089'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'a45abb2f'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '2052c15f'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: Thapsigargin_10.000_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: Thapsigargin_10.000_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (29430, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'K20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'K16' 'K17' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.384701).  Saving model ...
	 Train_Loss: 0.5334 Train_Acc: 66.928 Val_Loss: 0.3847  BEST VAL Loss: 0.3847  Val_Acc: 93.376

Epoch 1: Validation loss decreased (0.384701 --> 0.301085).  Saving model ...
	 Train_Loss: 0.4226 Train_Acc: 92.886 Val_Loss: 0.3011  BEST VAL Loss: 0.3011  Val_Acc: 95.009

Epoch 2: Validation loss decreased (0.301085 --> 0.249448).  Saving model ...
	 Train_Loss: 0.3419 Train_Acc: 94.322 Val_Loss: 0.2494  BEST VAL Loss: 0.2494  Val_Acc: 95.599

Epoch 3: Validation loss decreased (0.249448 --> 0.215455).  Saving model ...
	 Train_Loss: 0.2914 Train_Acc: 95.121 Val_Loss: 0.2155  BEST VAL Loss: 0.2155  Val_Acc: 96.234

Epoch 4: Validation loss decreased (0.215455 --> 0.194949).  Saving model ...
	 Train_Loss: 0.2566 Train_Acc: 95.598 Val_Loss: 0.1949  BEST VAL Loss: 0.1949  Val_Acc: 96.642

Epoch 5: Validation loss decreased (0.194949 --> 0.178754).  Saving model ...
	 Train_Loss: 0.2307 Train_Acc: 96.182 Val_Loss: 0.1788  BEST VAL Loss: 0.1788  Val_Acc: 96.869

Epoch 6: Validation loss decreased (0.178754 --> 0.166581).  Saving model ...
	 Train_Loss: 0.2112 Train_Acc: 96.523 Val_Loss: 0.1666  BEST VAL Loss: 0.1666  Val_Acc: 97.368

Epoch 7: Validation loss decreased (0.166581 --> 0.159396).  Saving model ...
	 Train_Loss: 0.1959 Train_Acc: 96.670 Val_Loss: 0.1594  BEST VAL Loss: 0.1594  Val_Acc: 96.688

Epoch 8: Validation loss decreased (0.159396 --> 0.152303).  Saving model ...
	 Train_Loss: 0.1840 Train_Acc: 96.710 Val_Loss: 0.1523  BEST VAL Loss: 0.1523  Val_Acc: 96.960

Epoch 9: Validation loss decreased (0.152303 --> 0.148421).  Saving model ...
	 Train_Loss: 0.1737 Train_Acc: 97.010 Val_Loss: 0.1484  BEST VAL Loss: 0.1484  Val_Acc: 96.779

Epoch 10: Validation loss decreased (0.148421 --> 0.144962).  Saving model ...
	 Train_Loss: 0.1651 Train_Acc: 97.033 Val_Loss: 0.1450  BEST VAL Loss: 0.1450  Val_Acc: 97.323

Epoch 11: Validation loss decreased (0.144962 --> 0.141348).  Saving model ...
	 Train_Loss: 0.1575 Train_Acc: 97.198 Val_Loss: 0.1413  BEST VAL Loss: 0.1413  Val_Acc: 97.595

Epoch 12: Validation loss decreased (0.141348 --> 0.139904).  Saving model ...
	 Train_Loss: 0.1509 Train_Acc: 97.362 Val_Loss: 0.1399  BEST VAL Loss: 0.1399  Val_Acc: 97.459

Epoch 13: Validation loss decreased (0.139904 --> 0.138716).  Saving model ...
	 Train_Loss: 0.1458 Train_Acc: 97.186 Val_Loss: 0.1387  BEST VAL Loss: 0.1387  Val_Acc: 96.824

Epoch 14: Validation loss decreased (0.138716 --> 0.136414).  Saving model ...
	 Train_Loss: 0.1416 Train_Acc: 96.993 Val_Loss: 0.1364  BEST VAL Loss: 0.1364  Val_Acc: 97.505

Epoch 15: Validation loss decreased (0.136414 --> 0.135301).  Saving model ...
	 Train_Loss: 0.1371 Train_Acc: 97.345 Val_Loss: 0.1353  BEST VAL Loss: 0.1353  Val_Acc: 97.232

Epoch 16: Validation loss decreased (0.135301 --> 0.133487).  Saving model ...
	 Train_Loss: 0.1331 Train_Acc: 97.442 Val_Loss: 0.1335  BEST VAL Loss: 0.1335  Val_Acc: 97.686

Epoch 17: Validation loss decreased (0.133487 --> 0.132217).  Saving model ...
	 Train_Loss: 0.1293 Train_Acc: 97.703 Val_Loss: 0.1322  BEST VAL Loss: 0.1322  Val_Acc: 97.278

Epoch 18: Validation loss decreased (0.132217 --> 0.131390).  Saving model ...
	 Train_Loss: 0.1259 Train_Acc: 97.578 Val_Loss: 0.1314  BEST VAL Loss: 0.1314  Val_Acc: 97.550

Epoch 19: Validation loss decreased (0.131390 --> 0.129826).  Saving model ...
	 Train_Loss: 0.1227 Train_Acc: 97.737 Val_Loss: 0.1298  BEST VAL Loss: 0.1298  Val_Acc: 97.641

Epoch 20: Validation loss did not decrease
	 Train_Loss: 0.1197 Train_Acc: 97.782 Val_Loss: 0.1301  BEST VAL Loss: 0.1298  Val_Acc: 97.096

Epoch 21: Validation loss did not decrease
	 Train_Loss: 0.1166 Train_Acc: 98.185 Val_Loss: 0.1304  BEST VAL Loss: 0.1298  Val_Acc: 97.414

Epoch 22: Validation loss did not decrease
	 Train_Loss: 0.1139 Train_Acc: 98.117 Val_Loss: 0.1304  BEST VAL Loss: 0.1298  Val_Acc: 97.323

Epoch 23: Validation loss did not decrease
	 Train_Loss: 0.1115 Train_Acc: 98.032 Val_Loss: 0.1306  BEST VAL Loss: 0.1298  Val_Acc: 97.550

Epoch 24: Validation loss did not decrease
	 Train_Loss: 0.1093 Train_Acc: 97.975 Val_Loss: 0.1302  BEST VAL Loss: 0.1298  Val_Acc: 97.278

Epoch 25: Validation loss did not decrease
	 Train_Loss: 0.1073 Train_Acc: 98.128 Val_Loss: 0.1304  BEST VAL Loss: 0.1298  Val_Acc: 97.005

Epoch 26: Validation loss did not decrease
	 Train_Loss: 0.1055 Train_Acc: 97.941 Val_Loss: 0.1302  BEST VAL Loss: 0.1298  Val_Acc: 97.505

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.1036 Train_Acc: 98.122 Val_Loss: 0.1302  BEST VAL Loss: 0.1298  Val_Acc: 97.595

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.1019 Train_Acc: 98.094 Val_Loss: 0.1303  BEST VAL Loss: 0.1298  Val_Acc: 97.505

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.1002 Train_Acc: 98.196 Val_Loss: 0.1310  BEST VAL Loss: 0.1298  Val_Acc: 97.414

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.0988 Train_Acc: 98.049 Val_Loss: 0.1307  BEST VAL Loss: 0.1298  Val_Acc: 97.459

Epoch 31: Validation loss did not decrease
	 Train_Loss: 0.0973 Train_Acc: 98.139 Val_Loss: 0.1318  BEST VAL Loss: 0.1298  Val_Acc: 97.368

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.0959 Train_Acc: 98.241 Val_Loss: 0.1334  BEST VAL Loss: 0.1298  Val_Acc: 97.414

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.0947 Train_Acc: 98.077 Val_Loss: 0.1336  BEST VAL Loss: 0.1298  Val_Acc: 97.550

Epoch 34: Validation loss did not decrease
	 Train_Loss: 0.0935 Train_Acc: 98.219 Val_Loss: 0.1340  BEST VAL Loss: 0.1298  Val_Acc: 97.595

Epoch 35: Validation loss did not decrease
Early stopped at epoch : 35
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      9778
           1       1.00      0.99      0.99      7850

    accuracy                           0.99     17628
   macro avg       0.99      0.99      0.99     17628
weighted avg       0.99      0.99      0.99     17628

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1222
           1       0.97      0.97      0.97       982

    accuracy                           0.98      2204
   macro avg       0.98      0.98      0.98      2204
weighted avg       0.98      0.98      0.98      2204

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1222
           1       0.97      0.96      0.97       982

    accuracy                           0.97      2204
   macro avg       0.97      0.97      0.97      2204
weighted avg       0.97      0.97      0.97      2204

              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1222
           1       0.97      0.96      0.97       982

    accuracy                           0.97      2204
   macro avg       0.97      0.97      0.97      2204
weighted avg       0.97      0.97      0.97      2204

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_Thapsigargin_10.000_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      3996
           1       0.98      0.98      0.98      3398

    accuracy                           0.98      7394
   macro avg       0.98      0.98      0.98      7394
weighted avg       0.98      0.98      0.98      7394

              precision    recall  f1-score   support

           0       0.98      0.98      0.98      3996
           1       0.98      0.98      0.98      3398

    accuracy                           0.98      7394
   macro avg       0.98      0.98      0.98      7394
weighted avg       0.98      0.98      0.98      7394

completed
