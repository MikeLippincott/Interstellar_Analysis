[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '4b52ab05'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '5b14cc1e'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to 'edeaad1d'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to '218eb712'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_10.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_10.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['LPS_Nigericin_1.000_1.0_DMSO_0.025'
 'LPS_Nigericin_1.000_10.0_DMSO_0.025']
The dimensions of the data are: (30620, 1276)
Number of total missing values across all columns: 0
Data Subset Is Off
Wells held out for testing: ['M16' 'K20']
Wells to use for training, validation, and testing ['K16' 'K17' 'M17' 'M20' 'K21' 'M21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.437027).  Saving model ...
	 Train_Loss: 0.5860 Train_Acc: 68.101 Val_Loss: 0.4370  BEST VAL Loss: 0.4370  Val_Acc: 80.764

Epoch 1: Validation loss decreased (0.437027 --> 0.398735).  Saving model ...
	 Train_Loss: 0.5314 Train_Acc: 78.269 Val_Loss: 0.3987  BEST VAL Loss: 0.3987  Val_Acc: 84.980

Epoch 2: Validation loss decreased (0.398735 --> 0.364850).  Saving model ...
	 Train_Loss: 0.4934 Train_Acc: 80.438 Val_Loss: 0.3649  BEST VAL Loss: 0.3649  Val_Acc: 87.044

Epoch 3: Validation loss decreased (0.364850 --> 0.352658).  Saving model ...
	 Train_Loss: 0.4602 Train_Acc: 84.726 Val_Loss: 0.3527  BEST VAL Loss: 0.3527  Val_Acc: 88.625

Epoch 4: Validation loss decreased (0.352658 --> 0.335288).  Saving model ...
	 Train_Loss: 0.4353 Train_Acc: 85.802 Val_Loss: 0.3353  BEST VAL Loss: 0.3353  Val_Acc: 89.592

Epoch 5: Validation loss decreased (0.335288 --> 0.324167).  Saving model ...
	 Train_Loss: 0.4118 Train_Acc: 88.042 Val_Loss: 0.3242  BEST VAL Loss: 0.3242  Val_Acc: 90.646

Epoch 6: Validation loss decreased (0.324167 --> 0.313227).  Saving model ...
	 Train_Loss: 0.3924 Train_Acc: 88.558 Val_Loss: 0.3132  BEST VAL Loss: 0.3132  Val_Acc: 90.953

Epoch 7: Validation loss decreased (0.313227 --> 0.304049).  Saving model ...
	 Train_Loss: 0.3757 Train_Acc: 89.552 Val_Loss: 0.3040  BEST VAL Loss: 0.3040  Val_Acc: 90.997

Epoch 8: Validation loss decreased (0.304049 --> 0.299606).  Saving model ...
	 Train_Loss: 0.3624 Train_Acc: 89.497 Val_Loss: 0.2996  BEST VAL Loss: 0.2996  Val_Acc: 91.392

Epoch 9: Validation loss decreased (0.299606 --> 0.293042).  Saving model ...
	 Train_Loss: 0.3513 Train_Acc: 89.881 Val_Loss: 0.2930  BEST VAL Loss: 0.2930  Val_Acc: 91.656

Epoch 10: Validation loss decreased (0.293042 --> 0.287193).  Saving model ...
	 Train_Loss: 0.3410 Train_Acc: 90.601 Val_Loss: 0.2872  BEST VAL Loss: 0.2872  Val_Acc: 91.963

Epoch 11: Validation loss decreased (0.287193 --> 0.283684).  Saving model ...
	 Train_Loss: 0.3319 Train_Acc: 90.985 Val_Loss: 0.2837  BEST VAL Loss: 0.2837  Val_Acc: 91.919

Epoch 12: Validation loss decreased (0.283684 --> 0.281588).  Saving model ...
	 Train_Loss: 0.3243 Train_Acc: 90.732 Val_Loss: 0.2816  BEST VAL Loss: 0.2816  Val_Acc: 91.612

Epoch 13: Validation loss decreased (0.281588 --> 0.279735).  Saving model ...
	 Train_Loss: 0.3172 Train_Acc: 90.903 Val_Loss: 0.2797  BEST VAL Loss: 0.2797  Val_Acc: 91.919

Epoch 14: Validation loss decreased (0.279735 --> 0.276965).  Saving model ...
	 Train_Loss: 0.3111 Train_Acc: 90.875 Val_Loss: 0.2770  BEST VAL Loss: 0.2770  Val_Acc: 92.314

Epoch 15: Validation loss decreased (0.276965 --> 0.274264).  Saving model ...
	 Train_Loss: 0.3053 Train_Acc: 91.369 Val_Loss: 0.2743  BEST VAL Loss: 0.2743  Val_Acc: 93.412

Epoch 16: Validation loss decreased (0.274264 --> 0.271577).  Saving model ...
	 Train_Loss: 0.2997 Train_Acc: 91.858 Val_Loss: 0.2716  BEST VAL Loss: 0.2716  Val_Acc: 91.524

Epoch 17: Validation loss decreased (0.271577 --> 0.269890).  Saving model ...
	 Train_Loss: 0.2946 Train_Acc: 91.682 Val_Loss: 0.2699  BEST VAL Loss: 0.2699  Val_Acc: 92.710

Epoch 18: Validation loss decreased (0.269890 --> 0.268632).  Saving model ...
	 Train_Loss: 0.2894 Train_Acc: 92.533 Val_Loss: 0.2686  BEST VAL Loss: 0.2686  Val_Acc: 92.929

Epoch 19: Validation loss decreased (0.268632 --> 0.267343).  Saving model ...
	 Train_Loss: 0.2851 Train_Acc: 91.935 Val_Loss: 0.2673  BEST VAL Loss: 0.2673  Val_Acc: 92.227

Epoch 20: Validation loss decreased (0.267343 --> 0.264586).  Saving model ...
	 Train_Loss: 0.2815 Train_Acc: 91.858 Val_Loss: 0.2646  BEST VAL Loss: 0.2646  Val_Acc: 93.544

Epoch 21: Validation loss decreased (0.264586 --> 0.262856).  Saving model ...
	 Train_Loss: 0.2776 Train_Acc: 92.149 Val_Loss: 0.2629  BEST VAL Loss: 0.2629  Val_Acc: 91.787

Epoch 22: Validation loss decreased (0.262856 --> 0.261396).  Saving model ...
	 Train_Loss: 0.2742 Train_Acc: 92.171 Val_Loss: 0.2614  BEST VAL Loss: 0.2614  Val_Acc: 92.973

Epoch 23: Validation loss decreased (0.261396 --> 0.260561).  Saving model ...
	 Train_Loss: 0.2708 Train_Acc: 92.275 Val_Loss: 0.2606  BEST VAL Loss: 0.2606  Val_Acc: 92.710

Epoch 24: Validation loss decreased (0.260561 --> 0.260125).  Saving model ...
	 Train_Loss: 0.2673 Train_Acc: 92.934 Val_Loss: 0.2601  BEST VAL Loss: 0.2601  Val_Acc: 92.929

Epoch 25: Validation loss decreased (0.260125 --> 0.258587).  Saving model ...
	 Train_Loss: 0.2640 Train_Acc: 92.934 Val_Loss: 0.2586  BEST VAL Loss: 0.2586  Val_Acc: 92.929

Epoch 26: Validation loss decreased (0.258587 --> 0.257146).  Saving model ...
	 Train_Loss: 0.2607 Train_Acc: 93.428 Val_Loss: 0.2571  BEST VAL Loss: 0.2571  Val_Acc: 92.402

Epoch 27: Validation loss did not decrease
	 Train_Loss: 0.2575 Train_Acc: 92.983 Val_Loss: 0.2592  BEST VAL Loss: 0.2571  Val_Acc: 92.885

Epoch 28: Validation loss did not decrease
	 Train_Loss: 0.2548 Train_Acc: 93.071 Val_Loss: 0.2581  BEST VAL Loss: 0.2571  Val_Acc: 92.622

Epoch 29: Validation loss did not decrease
	 Train_Loss: 0.2519 Train_Acc: 93.516 Val_Loss: 0.2582  BEST VAL Loss: 0.2571  Val_Acc: 92.754

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.2493 Train_Acc: 93.483 Val_Loss: 0.2580  BEST VAL Loss: 0.2571  Val_Acc: 92.973

Epoch 31: Validation loss decreased (0.257146 --> 0.256648).  Saving model ...
	 Train_Loss: 0.2466 Train_Acc: 93.785 Val_Loss: 0.2566  BEST VAL Loss: 0.2566  Val_Acc: 92.710

Epoch 32: Validation loss did not decrease
	 Train_Loss: 0.2439 Train_Acc: 94.010 Val_Loss: 0.2576  BEST VAL Loss: 0.2566  Val_Acc: 93.808

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.2413 Train_Acc: 93.928 Val_Loss: 0.2573  BEST VAL Loss: 0.2566  Val_Acc: 92.841

Epoch 34: Validation loss decreased (0.256648 --> 0.255905).  Saving model ...
	 Train_Loss: 0.2392 Train_Acc: 93.379 Val_Loss: 0.2559  BEST VAL Loss: 0.2559  Val_Acc: 93.017

Epoch 35: Validation loss did not decrease
	 Train_Loss: 0.2372 Train_Acc: 93.346 Val_Loss: 0.2564  BEST VAL Loss: 0.2559  Val_Acc: 93.105

Epoch 36: Validation loss did not decrease
	 Train_Loss: 0.2350 Train_Acc: 93.950 Val_Loss: 0.2566  BEST VAL Loss: 0.2559  Val_Acc: 93.193

Epoch 37: Validation loss did not decrease
	 Train_Loss: 0.2326 Train_Acc: 94.636 Val_Loss: 0.2578  BEST VAL Loss: 0.2559  Val_Acc: 92.798

Epoch 38: Validation loss did not decrease
	 Train_Loss: 0.2303 Train_Acc: 94.367 Val_Loss: 0.2588  BEST VAL Loss: 0.2559  Val_Acc: 92.754

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.2283 Train_Acc: 94.125 Val_Loss: 0.2597  BEST VAL Loss: 0.2559  Val_Acc: 92.929

Epoch 40: Validation loss did not decrease
	 Train_Loss: 0.2262 Train_Acc: 94.707 Val_Loss: 0.2614  BEST VAL Loss: 0.2559  Val_Acc: 93.544

Epoch 41: Validation loss did not decrease
	 Train_Loss: 0.2244 Train_Acc: 94.049 Val_Loss: 0.2613  BEST VAL Loss: 0.2559  Val_Acc: 93.500

Epoch 42: Validation loss did not decrease
	 Train_Loss: 0.2226 Train_Acc: 94.252 Val_Loss: 0.2630  BEST VAL Loss: 0.2559  Val_Acc: 92.754

Epoch 43: Validation loss did not decrease
	 Train_Loss: 0.2209 Train_Acc: 94.092 Val_Loss: 0.2633  BEST VAL Loss: 0.2559  Val_Acc: 93.720

Epoch 44: Validation loss did not decrease
	 Train_Loss: 0.2193 Train_Acc: 94.208 Val_Loss: 0.2635  BEST VAL Loss: 0.2559  Val_Acc: 93.632

Epoch 45: Validation loss did not decrease
	 Train_Loss: 0.2175 Train_Acc: 94.598 Val_Loss: 0.2651  BEST VAL Loss: 0.2559  Val_Acc: 93.852

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.2158 Train_Acc: 94.631 Val_Loss: 0.2677  BEST VAL Loss: 0.2559  Val_Acc: 93.983

Epoch 47: Validation loss did not decrease
	 Train_Loss: 0.2144 Train_Acc: 94.268 Val_Loss: 0.2684  BEST VAL Loss: 0.2559  Val_Acc: 92.314

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.2131 Train_Acc: 93.494 Val_Loss: 0.2691  BEST VAL Loss: 0.2559  Val_Acc: 93.895

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.2119 Train_Acc: 94.301 Val_Loss: 0.2689  BEST VAL Loss: 0.2559  Val_Acc: 93.193

Epoch 50: Validation loss did not decrease
Early stopped at epoch : 50
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       1.00      0.94      0.97      9778
           1       0.94      1.00      0.97      8436

    accuracy                           0.97     18214
   macro avg       0.97      0.97      0.97     18214
weighted avg       0.97      0.97      0.97     18214

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.91      0.93      1222
           1       0.90      0.96      0.93      1055

    accuracy                           0.93      2277
   macro avg       0.93      0.93      0.93      2277
weighted avg       0.93      0.93      0.93      2277

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.96      0.90      0.93      1222
           1       0.89      0.96      0.92      1055

    accuracy                           0.92      2277
   macro avg       0.92      0.93      0.92      2277
weighted avg       0.93      0.92      0.92      2277

              precision    recall  f1-score   support

           0       0.96      0.90      0.93      1222
           1       0.89      0.96      0.92      1055

    accuracy                           0.92      2277
   macro avg       0.92      0.93      0.92      2277
weighted avg       0.93      0.92      0.92      2277

LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
LPS_Nigericin_1.000_1.0_DMSO_0.025_vs_LPS_Nigericin_1.000_10.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.88      0.88      0.88      3996
           1       0.87      0.87      0.87      3856

    accuracy                           0.87      7852
   macro avg       0.87      0.87      0.87      7852
weighted avg       0.87      0.87      0.87      7852

              precision    recall  f1-score   support

           0       0.88      0.88      0.88      3996
           1       0.87      0.87      0.87      3856

    accuracy                           0.87      7852
   macro avg       0.87      0.87      0.87      7852
weighted avg       0.87      0.87      0.87      7852

completed
