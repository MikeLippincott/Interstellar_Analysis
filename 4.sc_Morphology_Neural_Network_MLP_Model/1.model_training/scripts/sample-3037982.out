[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multi-class.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '47de42e3'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'be04aebf'.
  validate(nb)
[NbConvertApp] Writing 14456 bytes to Hyperparameter_Optimization_model_multi-class.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_regression.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3e2c848c' detected. Corrected to '7abe87eb'.
  validate(nb)
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar/lib/python3.10/site-packages/nbformat/__init__.py:93: DuplicateCellId: Non-unique cell id '3c53f7e4' detected. Corrected to 'ffcc8933'.
  validate(nb)
[NbConvertApp] Writing 12852 bytes to Hyperparameter_Optimization_model_regression.py
[NbConvertApp] Converting notebook ../notebooks/train_binary_model.ipynb to script
[NbConvertApp] Writing 19547 bytes to train_binary_model.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025 shuffle: False
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:997: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y1], palette="blue", label="Train")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:998: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.lineplot(x=df[x], y=df[y2], palette="orange", label="Validation")
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1018: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
/gpfs/alpine1/scratch/mlippincott@xsede.org/Interstellar_Analysis/4.sc_Morphology_Neural_Network_MLP_Model/1.model_training/scripts/../../MLP_utils/utils.py:1324: UserWarning: The figure layout has changed to tight
  plt.tight_layout()
CELL_TYPE: SHSY5Y
CONTROL_NAME: Thapsigargin_10.000_DMSO_0.025
TREATMENT_NAME: LPS_Nigericin_1.000_1.0_DMSO_0.025
SHUFFLE: False
False
Selected Catagories are:
['Thapsigargin_10.000_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (29430, 1276)
Number of total missing values across all columns: 26424
Data Subset Is Off
Wells held out for testing: ['E14' 'K20']
Wells to use for training, validation, and testing ['E15' 'L14' 'L15' 'K16' 'K17' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
Adam
Epoch 0: Validation loss decreased (inf --> 0.214919).  Saving model ...
	 Train_Loss: 0.4252 Train_Acc: 77.439 Val_Loss: 0.2149  BEST VAL Loss: 0.2149  Val_Acc: 91.289

Epoch 1: Validation loss decreased (0.214919 --> 0.181788).  Saving model ...
	 Train_Loss: 0.3608 Train_Acc: 84.519 Val_Loss: 0.1818  BEST VAL Loss: 0.1818  Val_Acc: 94.328

Epoch 2: Validation loss decreased (0.181788 --> 0.161003).  Saving model ...
	 Train_Loss: 0.3258 Train_Acc: 86.187 Val_Loss: 0.1610  BEST VAL Loss: 0.1610  Val_Acc: 94.828

Epoch 3: Validation loss decreased (0.161003 --> 0.146054).  Saving model ...
	 Train_Loss: 0.3019 Train_Acc: 87.582 Val_Loss: 0.1461  BEST VAL Loss: 0.1461  Val_Acc: 95.780

Epoch 4: Validation loss decreased (0.146054 --> 0.137622).  Saving model ...
	 Train_Loss: 0.2847 Train_Acc: 88.410 Val_Loss: 0.1376  BEST VAL Loss: 0.1376  Val_Acc: 95.826

Epoch 5: Validation loss decreased (0.137622 --> 0.131141).  Saving model ...
	 Train_Loss: 0.2715 Train_Acc: 88.428 Val_Loss: 0.1311  BEST VAL Loss: 0.1311  Val_Acc: 96.189

Epoch 6: Validation loss decreased (0.131141 --> 0.127312).  Saving model ...
	 Train_Loss: 0.2607 Train_Acc: 89.891 Val_Loss: 0.1273  BEST VAL Loss: 0.1273  Val_Acc: 95.826

Epoch 7: Validation loss decreased (0.127312 --> 0.123255).  Saving model ...
	 Train_Loss: 0.2531 Train_Acc: 90.050 Val_Loss: 0.1233  BEST VAL Loss: 0.1233  Val_Acc: 96.279

Epoch 8: Validation loss decreased (0.123255 --> 0.119892).  Saving model ...
	 Train_Loss: 0.2460 Train_Acc: 89.948 Val_Loss: 0.1199  BEST VAL Loss: 0.1199  Val_Acc: 96.688

Epoch 9: Validation loss decreased (0.119892 --> 0.118875).  Saving model ...
	 Train_Loss: 0.2400 Train_Acc: 89.295 Val_Loss: 0.1189  BEST VAL Loss: 0.1189  Val_Acc: 96.325

Epoch 10: Validation loss decreased (0.118875 --> 0.115571).  Saving model ...
	 Train_Loss: 0.2359 Train_Acc: 90.197 Val_Loss: 0.1156  BEST VAL Loss: 0.1156  Val_Acc: 96.733

Epoch 11: Validation loss decreased (0.115571 --> 0.113960).  Saving model ...
	 Train_Loss: 0.2313 Train_Acc: 90.294 Val_Loss: 0.1140  BEST VAL Loss: 0.1140  Val_Acc: 96.506

Epoch 12: Validation loss decreased (0.113960 --> 0.111087).  Saving model ...
	 Train_Loss: 0.2273 Train_Acc: 90.793 Val_Loss: 0.1111  BEST VAL Loss: 0.1111  Val_Acc: 95.962

Epoch 13: Validation loss decreased (0.111087 --> 0.110192).  Saving model ...
	 Train_Loss: 0.2243 Train_Acc: 90.265 Val_Loss: 0.1102  BEST VAL Loss: 0.1102  Val_Acc: 95.735

Epoch 14: Validation loss decreased (0.110192 --> 0.108025).  Saving model ...
	 Train_Loss: 0.2213 Train_Acc: 90.691 Val_Loss: 0.1080  BEST VAL Loss: 0.1080  Val_Acc: 96.642

Epoch 15: Validation loss decreased (0.108025 --> 0.107657).  Saving model ...
	 Train_Loss: 0.2181 Train_Acc: 90.907 Val_Loss: 0.1077  BEST VAL Loss: 0.1077  Val_Acc: 96.597

Epoch 16: Validation loss decreased (0.107657 --> 0.106904).  Saving model ...
	 Train_Loss: 0.2150 Train_Acc: 91.292 Val_Loss: 0.1069  BEST VAL Loss: 0.1069  Val_Acc: 96.915

Epoch 17: Validation loss decreased (0.106904 --> 0.106043).  Saving model ...
	 Train_Loss: 0.2122 Train_Acc: 91.355 Val_Loss: 0.1060  BEST VAL Loss: 0.1060  Val_Acc: 97.096

Epoch 18: Validation loss decreased (0.106043 --> 0.105858).  Saving model ...
	 Train_Loss: 0.2092 Train_Acc: 91.865 Val_Loss: 0.1059  BEST VAL Loss: 0.1059  Val_Acc: 96.869

Epoch 19: Validation loss decreased (0.105858 --> 0.105422).  Saving model ...
	 Train_Loss: 0.2070 Train_Acc: 91.479 Val_Loss: 0.1054  BEST VAL Loss: 0.1054  Val_Acc: 96.733

Epoch 20: Validation loss decreased (0.105422 --> 0.105157).  Saving model ...
	 Train_Loss: 0.2049 Train_Acc: 91.162 Val_Loss: 0.1052  BEST VAL Loss: 0.1052  Val_Acc: 97.051

Epoch 21: Validation loss decreased (0.105157 --> 0.104730).  Saving model ...
	 Train_Loss: 0.2029 Train_Acc: 91.292 Val_Loss: 0.1047  BEST VAL Loss: 0.1047  Val_Acc: 96.779

Epoch 22: Validation loss decreased (0.104730 --> 0.104302).  Saving model ...
	 Train_Loss: 0.2009 Train_Acc: 91.763 Val_Loss: 0.1043  BEST VAL Loss: 0.1043  Val_Acc: 96.869

Epoch 23: Validation loss decreased (0.104302 --> 0.103983).  Saving model ...
	 Train_Loss: 0.1991 Train_Acc: 91.621 Val_Loss: 0.1040  BEST VAL Loss: 0.1040  Val_Acc: 97.142

Epoch 24: Validation loss decreased (0.103983 --> 0.103441).  Saving model ...
	 Train_Loss: 0.1974 Train_Acc: 91.848 Val_Loss: 0.1034  BEST VAL Loss: 0.1034  Val_Acc: 97.368

Epoch 25: Validation loss decreased (0.103441 --> 0.102756).  Saving model ...
	 Train_Loss: 0.1957 Train_Acc: 91.729 Val_Loss: 0.1028  BEST VAL Loss: 0.1028  Val_Acc: 97.641

Epoch 26: Validation loss decreased (0.102756 --> 0.102094).  Saving model ...
	 Train_Loss: 0.1941 Train_Acc: 91.962 Val_Loss: 0.1021  BEST VAL Loss: 0.1021  Val_Acc: 96.915

Epoch 27: Validation loss decreased (0.102094 --> 0.101610).  Saving model ...
	 Train_Loss: 0.1925 Train_Acc: 92.007 Val_Loss: 0.1016  BEST VAL Loss: 0.1016  Val_Acc: 97.142

Epoch 28: Validation loss decreased (0.101610 --> 0.101315).  Saving model ...
	 Train_Loss: 0.1910 Train_Acc: 92.172 Val_Loss: 0.1013  BEST VAL Loss: 0.1013  Val_Acc: 97.323

Epoch 29: Validation loss decreased (0.101315 --> 0.100947).  Saving model ...
	 Train_Loss: 0.1895 Train_Acc: 92.035 Val_Loss: 0.1009  BEST VAL Loss: 0.1009  Val_Acc: 97.323

Epoch 30: Validation loss did not decrease
	 Train_Loss: 0.1884 Train_Acc: 91.729 Val_Loss: 0.1012  BEST VAL Loss: 0.1009  Val_Acc: 97.368

Epoch 31: Validation loss decreased (0.100947 --> 0.100427).  Saving model ...
	 Train_Loss: 0.1873 Train_Acc: 91.888 Val_Loss: 0.1004  BEST VAL Loss: 0.1004  Val_Acc: 97.005

Epoch 32: Validation loss decreased (0.100427 --> 0.100096).  Saving model ...
	 Train_Loss: 0.1863 Train_Acc: 91.627 Val_Loss: 0.1001  BEST VAL Loss: 0.1001  Val_Acc: 96.960

Epoch 33: Validation loss did not decrease
	 Train_Loss: 0.1854 Train_Acc: 92.206 Val_Loss: 0.1007  BEST VAL Loss: 0.1001  Val_Acc: 97.096

Epoch 34: Validation loss decreased (0.100096 --> 0.100084).  Saving model ...
	 Train_Loss: 0.1842 Train_Acc: 92.347 Val_Loss: 0.1001  BEST VAL Loss: 0.1001  Val_Acc: 97.550

Epoch 35: Validation loss decreased (0.100084 --> 0.099578).  Saving model ...
	 Train_Loss: 0.1831 Train_Acc: 92.489 Val_Loss: 0.0996  BEST VAL Loss: 0.0996  Val_Acc: 97.505

Epoch 36: Validation loss decreased (0.099578 --> 0.099196).  Saving model ...
	 Train_Loss: 0.1820 Train_Acc: 92.189 Val_Loss: 0.0992  BEST VAL Loss: 0.0992  Val_Acc: 97.187

Epoch 37: Validation loss decreased (0.099196 --> 0.098857).  Saving model ...
	 Train_Loss: 0.1812 Train_Acc: 92.058 Val_Loss: 0.0989  BEST VAL Loss: 0.0989  Val_Acc: 97.096

Epoch 38: Validation loss decreased (0.098857 --> 0.098808).  Saving model ...
	 Train_Loss: 0.1803 Train_Acc: 92.126 Val_Loss: 0.0988  BEST VAL Loss: 0.0988  Val_Acc: 96.688

Epoch 39: Validation loss did not decrease
	 Train_Loss: 0.1792 Train_Acc: 92.937 Val_Loss: 0.0988  BEST VAL Loss: 0.0988  Val_Acc: 97.368

Epoch 40: Validation loss decreased (0.098808 --> 0.098501).  Saving model ...
	 Train_Loss: 0.1785 Train_Acc: 91.826 Val_Loss: 0.0985  BEST VAL Loss: 0.0985  Val_Acc: 97.595

Epoch 41: Validation loss decreased (0.098501 --> 0.098433).  Saving model ...
	 Train_Loss: 0.1777 Train_Acc: 92.279 Val_Loss: 0.0984  BEST VAL Loss: 0.0984  Val_Acc: 97.096

Epoch 42: Validation loss decreased (0.098433 --> 0.098079).  Saving model ...
	 Train_Loss: 0.1770 Train_Acc: 92.047 Val_Loss: 0.0981  BEST VAL Loss: 0.0981  Val_Acc: 97.368

Epoch 43: Validation loss decreased (0.098079 --> 0.097569).  Saving model ...
	 Train_Loss: 0.1763 Train_Acc: 92.155 Val_Loss: 0.0976  BEST VAL Loss: 0.0976  Val_Acc: 97.187

Epoch 44: Validation loss decreased (0.097569 --> 0.097194).  Saving model ...
	 Train_Loss: 0.1755 Train_Acc: 92.614 Val_Loss: 0.0972  BEST VAL Loss: 0.0972  Val_Acc: 97.505

Epoch 45: Validation loss decreased (0.097194 --> 0.097011).  Saving model ...
	 Train_Loss: 0.1747 Train_Acc: 92.501 Val_Loss: 0.0970  BEST VAL Loss: 0.0970  Val_Acc: 97.232

Epoch 46: Validation loss did not decrease
	 Train_Loss: 0.1739 Train_Acc: 92.688 Val_Loss: 0.0970  BEST VAL Loss: 0.0970  Val_Acc: 97.278

Epoch 47: Validation loss decreased (0.097011 --> 0.096769).  Saving model ...
	 Train_Loss: 0.1731 Train_Acc: 92.710 Val_Loss: 0.0968  BEST VAL Loss: 0.0968  Val_Acc: 97.641

Epoch 48: Validation loss did not decrease
	 Train_Loss: 0.1725 Train_Acc: 92.438 Val_Loss: 0.0968  BEST VAL Loss: 0.0968  Val_Acc: 97.187

Epoch 49: Validation loss did not decrease
	 Train_Loss: 0.1716 Train_Acc: 93.130 Val_Loss: 0.0969  BEST VAL Loss: 0.0968  Val_Acc: 97.459

Epoch 50: Validation loss did not decrease
	 Train_Loss: 0.1709 Train_Acc: 92.648 Val_Loss: 0.0973  BEST VAL Loss: 0.0968  Val_Acc: 97.187

Epoch 51: Validation loss did not decrease
	 Train_Loss: 0.1737 Train_Acc: 89.517 Val_Loss: 0.0985  BEST VAL Loss: 0.0968  Val_Acc: 97.232

Epoch 52: Validation loss did not decrease
	 Train_Loss: 0.1761 Train_Acc: 90.555 Val_Loss: 0.1029  BEST VAL Loss: 0.0968  Val_Acc: 96.234

Epoch 53: Validation loss did not decrease
	 Train_Loss: 0.1801 Train_Acc: 86.113 Val_Loss: 0.1054  BEST VAL Loss: 0.0968  Val_Acc: 93.784

Epoch 54: Validation loss did not decrease
	 Train_Loss: 0.1829 Train_Acc: 88.263 Val_Loss: 0.1080  BEST VAL Loss: 0.0968  Val_Acc: 93.557

Epoch 55: Validation loss did not decrease
	 Train_Loss: 0.1854 Train_Acc: 88.603 Val_Loss: 0.1107  BEST VAL Loss: 0.0968  Val_Acc: 93.920

Epoch 56: Validation loss did not decrease
	 Train_Loss: 0.1876 Train_Acc: 89.131 Val_Loss: 0.1128  BEST VAL Loss: 0.0968  Val_Acc: 95.191

Epoch 57: Validation loss did not decrease
	 Train_Loss: 0.1897 Train_Acc: 89.364 Val_Loss: 0.1144  BEST VAL Loss: 0.0968  Val_Acc: 94.601

Epoch 58: Validation loss did not decrease
	 Train_Loss: 0.1918 Train_Acc: 88.853 Val_Loss: 0.1157  BEST VAL Loss: 0.0968  Val_Acc: 95.508

Epoch 59: Validation loss did not decrease
	 Train_Loss: 0.1936 Train_Acc: 89.619 Val_Loss: 0.1170  BEST VAL Loss: 0.0968  Val_Acc: 95.735

Epoch 60: Validation loss did not decrease
	 Train_Loss: 0.1951 Train_Acc: 90.112 Val_Loss: 0.1183  BEST VAL Loss: 0.0968  Val_Acc: 96.416

Epoch 61: Validation loss did not decrease
	 Train_Loss: 0.1965 Train_Acc: 90.504 Val_Loss: 0.1199  BEST VAL Loss: 0.0968  Val_Acc: 96.370

Epoch 62: Validation loss did not decrease
	 Train_Loss: 0.1978 Train_Acc: 90.731 Val_Loss: 0.1210  BEST VAL Loss: 0.0968  Val_Acc: 96.733

Epoch 63: Validation loss did not decrease
Early stopped at epoch : 63
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.99      0.99      0.99      9778
           1       0.99      0.99      0.99      7850

    accuracy                           0.99     17628
   macro avg       0.99      0.99      0.99     17628
weighted avg       0.99      0.99      0.99     17628

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1222
           1       0.98      0.97      0.97       982

    accuracy                           0.98      2204
   macro avg       0.98      0.98      0.98      2204
weighted avg       0.98      0.98      0.98      2204

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1222
           1       0.98      0.96      0.97       982

    accuracy                           0.97      2204
   macro avg       0.97      0.97      0.97      2204
weighted avg       0.97      0.97      0.97      2204

              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1222
           1       0.98      0.96      0.97       982

    accuracy                           0.97      2204
   macro avg       0.97      0.97      0.97      2204
weighted avg       0.97      0.97      0.97      2204

Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
../../trained_models/model_save_states/Binary_Classification/SHSY5Y
Thapsigargin_10.000_DMSO_0.025_vs_LPS_Nigericin_1.000_1.0_DMSO_0.025
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      3996
           1       0.98      0.97      0.98      3398

    accuracy                           0.98      7394
   macro avg       0.98      0.98      0.98      7394
weighted avg       0.98      0.98      0.98      7394

              precision    recall  f1-score   support

           0       0.98      0.98      0.98      3996
           1       0.98      0.97      0.98      3398

    accuracy                           0.98      7394
   macro avg       0.98      0.98      0.98      7394
weighted avg       0.98      0.98      0.98      7394

completed
