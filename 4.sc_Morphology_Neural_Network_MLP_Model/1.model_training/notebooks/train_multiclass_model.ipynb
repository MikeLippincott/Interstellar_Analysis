{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db0f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:06.332907Z",
     "iopub.status.busy": "2023-11-13T03:46:06.332549Z",
     "iopub.status.idle": "2023-11-13T03:46:09.390164Z",
     "shell.execute_reply": "2023-11-13T03:46:09.389559Z"
    },
    "papermill": {
     "duration": 3.068881,
     "end_time": "2023-11-13T03:46:09.391537",
     "exception": false,
     "start_time": "2023-11-13T03:46:06.322656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from MLP_utils.parameters import Parameters\n",
    "from MLP_utils.utils import (\n",
    "    Dataset_formatter,\n",
    "    output_stats,\n",
    "    parameter_set,\n",
    "    plot_metric_vs_epoch,\n",
    "    results_output,\n",
    "    test_optimized_model,\n",
    "    train_optimized_model,\n",
    "    un_nest,\n",
    ")\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the parser\n",
    "parser = argparse.ArgumentParser(description=\"Train MLP model\")\n",
    "\n",
    "# add arguments to parser\n",
    "parser.add_argument(\n",
    "    \"--CELL_TYPE\",\n",
    "    type=str,\n",
    "    default=\"all\",\n",
    "    help=\"Cell type to train model on. Default is all\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--MODEL_NAME\",\n",
    "    type=str,\n",
    "    default=\"MLP\",\n",
    "    help=\"Model name to train. Default is MLP\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--SHUFFLE\",\n",
    "    type=str,\n",
    "    default=\"False\",\n",
    "    help=\"Shuffle data before training. Default is False\",\n",
    ")\n",
    "\n",
    "# parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "CELL_TYPE = args.CELL_TYPE\n",
    "MODEL_NAME = args.MODEL_NAME\n",
    "SHUFFLE = args.SHUFFLE\n",
    "SHUFFLE = ast.literal_eval(SHUFFLE)\n",
    "print(CELL_TYPE, MODEL_NAME, SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe861fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:09.446006Z",
     "iopub.status.busy": "2023-11-13T03:46:09.445617Z",
     "iopub.status.idle": "2023-11-13T03:46:09.451397Z",
     "shell.execute_reply": "2023-11-13T03:46:09.450978Z"
    },
    "papermill": {
     "duration": 0.015233,
     "end_time": "2023-11-13T03:46:09.452351",
     "exception": false,
     "start_time": "2023-11-13T03:46:09.437118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml_configs_file = pathlib.Path(\"../../MLP_utils/multi_class_config.toml\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "ml_configs = toml.load(ml_configs_file)\n",
    "params = Parameters()\n",
    "mlp_params = parameter_set(params, ml_configs)\n",
    "\n",
    "# overwrite params via command line arguments from papermill\n",
    "mlp_params.CELL_TYPE = CELL_TYPE\n",
    "mlp_params.MODEL_NAME = MODEL_NAME\n",
    "\n",
    "mlp_params.MODEL_NAME = MODEL_NAME\n",
    "mlp_params.SHUFFLE = SHUFFLE\n",
    "\n",
    "# load in the class weights\n",
    "class_weights_file_path = pathlib.Path(\n",
    "    f\"../../0.hyperparameter_optimization/class_weights/{CELL_TYPE}/multi_class/class_weights.txt\"\n",
    ").resolve(strict=True)\n",
    "# read the class weights into a list for use in the loss function as a list\n",
    "class_weights = []\n",
    "with open(class_weights_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        class_weights.append(float(line.strip()))\n",
    "# check the class weights are correct\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05b3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:09.469050Z",
     "iopub.status.busy": "2023-11-13T03:46:09.468681Z",
     "iopub.status.idle": "2023-11-13T03:46:38.488828Z",
     "shell.execute_reply": "2023-11-13T03:46:38.488256Z"
    },
    "papermill": {
     "duration": 29.029959,
     "end_time": "2023-11-13T03:46:38.490348",
     "exception": false,
     "start_time": "2023-11-13T03:46:09.460389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# set data file path under pathlib path for multi-system use\n",
    "\n",
    "file_path = pathlib.Path(\n",
    "    f\"../../../data/{mlp_params.CELL_TYPE}_preprocessed_sc_norm.parquet\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "df1 = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c372a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:38.510926Z",
     "iopub.status.busy": "2023-11-13T03:46:38.510590Z",
     "iopub.status.idle": "2023-11-13T03:46:38.515770Z",
     "shell.execute_reply": "2023-11-13T03:46:38.515225Z"
    },
    "papermill": {
     "duration": 0.014971,
     "end_time": "2023-11-13T03:46:38.517006",
     "exception": false,
     "start_time": "2023-11-13T03:46:38.502035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get paths for toml files\n",
    "ground_truth_file_path = pathlib.Path(f\"../../MLP_utils/ground_truth.toml\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "treatment_splits_file_path = pathlib.Path(f\"../../MLP_utils/splits.toml\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "# read toml files\n",
    "ground_truth = toml.load(ground_truth_file_path)\n",
    "treatment_splits = toml.load(treatment_splits_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63523a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:38.533853Z",
     "iopub.status.busy": "2023-11-13T03:46:38.533375Z",
     "iopub.status.idle": "2023-11-13T03:46:38.536081Z",
     "shell.execute_reply": "2023-11-13T03:46:38.535698Z"
    },
    "papermill": {
     "duration": 0.012175,
     "end_time": "2023-11-13T03:46:38.536982",
     "exception": false,
     "start_time": "2023-11-13T03:46:38.524807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get information from toml files\n",
    "apoptosis_groups_list = ground_truth[\"Apoptosis\"][\"apoptosis_groups_list\"]\n",
    "pyroptosis_groups_list = ground_truth[\"Pyroptosis\"][\"pyroptosis_groups_list\"]\n",
    "healthy_groups_list = ground_truth[\"Healthy\"][\"healthy_groups_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e28b5",
   "metadata": {
    "papermill": {
     "duration": 0.007627,
     "end_time": "2023-11-13T03:46:38.552274",
     "exception": false,
     "start_time": "2023-11-13T03:46:38.544647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Set up Data to be compatible with model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd38f4",
   "metadata": {
    "papermill": {
     "duration": 0.00759,
     "end_time": "2023-11-13T03:46:38.567493",
     "exception": false,
     "start_time": "2023-11-13T03:46:38.559903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Classification Models:\n",
    "Comment out code if using regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c99cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:38.583852Z",
     "iopub.status.busy": "2023-11-13T03:46:38.583443Z",
     "iopub.status.idle": "2023-11-13T03:46:38.586992Z",
     "shell.execute_reply": "2023-11-13T03:46:38.586604Z"
    },
    "papermill": {
     "duration": 0.012778,
     "end_time": "2023-11-13T03:46:38.587892",
     "exception": false,
     "start_time": "2023-11-13T03:46:38.575114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "if ast.literal_eval(mlp_params.DATA_SUBSET_OPTION):\n",
    "    df1 = df1.groupby(\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\").apply(\n",
    "        lambda x: x.sample(n=mlp_params.DATA_SUBSET_NUMBER, random_state=0)\n",
    "    )\n",
    "    print(\"Data Subset Is On\")\n",
    "    print(f\"Data is subset to {mlp_params.DATA_SUBSET_NUMBER} per treatment group\")\n",
    "    print(df1.shape)\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    print(\"Data Subset Is Off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b83d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:22:33.025827Z",
     "iopub.status.busy": "2023-11-13T03:22:33.025490Z",
     "iopub.status.idle": "2023-11-13T03:36:58.170038Z",
     "shell.execute_reply": "2023-11-13T03:36:58.169419Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-11-13T03:46:38.595635",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add apoptosis, pyroptosis and healthy columns to dataframe\n",
    "df1[\"apoptosis\"] = df1.apply(\n",
    "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
    "    in apoptosis_groups_list,\n",
    "    axis=1,\n",
    ")\n",
    "df1[\"pyroptosis\"] = df1.apply(\n",
    "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
    "    in pyroptosis_groups_list,\n",
    "    axis=1,\n",
    ")\n",
    "df1[\"healthy\"] = df1.apply(\n",
    "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
    "    in healthy_groups_list,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# merge apoptosis, pyroptosis, and healthy columns into one column\n",
    "df1[\"labels\"] = df1.apply(\n",
    "    lambda row: \"apoptosis\"\n",
    "    if row[\"apoptosis\"]\n",
    "    else \"pyroptosis\"\n",
    "    if row[\"pyroptosis\"]\n",
    "    else \"healthy\",\n",
    "    axis=1,\n",
    ")\n",
    "# # drop apoptosis, pyroptosis, and healthy columns\n",
    "df1.drop(columns=[\"apoptosis\", \"pyroptosis\", \"healthy\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a24e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:36:58.231714Z",
     "iopub.status.busy": "2023-11-13T03:36:58.231431Z",
     "iopub.status.idle": "2023-11-13T03:36:58.299402Z",
     "shell.execute_reply": "2023-11-13T03:36:58.298842Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set path for index file\n",
    "index_file_path = pathlib.Path(\n",
    "    f\"../../0.hyperparameter_optimization/indexes/{params.CELL_TYPE}/multi_class/{params.MODEL_NAME}_data_split_indexes.tsv\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# read index file\n",
    "index_df = pd.read_csv(index_file_path, sep=\"\\t\")\n",
    "index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb7489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:36:58.317279Z",
     "iopub.status.busy": "2023-11-13T03:36:58.316855Z",
     "iopub.status.idle": "2023-11-13T03:36:58.439495Z",
     "shell.execute_reply": "2023-11-13T03:36:58.438949Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get train, validation, test, and holdout indexes\n",
    "train_indexes = index_df.loc[index_df[\"label\"] == \"train\"][\"labeled_data_index\"]\n",
    "val_indexes = index_df.loc[index_df[\"label\"] == \"val\"][\"labeled_data_index\"]\n",
    "test_indexes = index_df.loc[index_df[\"label\"] == \"test\"][\"labeled_data_index\"]\n",
    "treatment_holdout = index_df.loc[index_df[\"label\"] == \"treatment_holdout\"][\n",
    "    \"labeled_data_index\"\n",
    "]\n",
    "holdout_indexes = index_df.loc[index_df[\"label\"] == \"holdout\"][\"labeled_data_index\"]\n",
    "print(\n",
    "    train_indexes.shape,\n",
    "    val_indexes.shape,\n",
    "    test_indexes.shape,\n",
    "    treatment_holdout,\n",
    "    holdout_indexes.shape,\n",
    ")\n",
    "\n",
    "assert (\n",
    "    train_indexes.shape[0]\n",
    "    + val_indexes.shape[0]\n",
    "    + test_indexes.shape[0]\n",
    "    + treatment_holdout.shape[0]\n",
    "    + holdout_indexes.shape[0]\n",
    ") == index_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9db49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:36:58.457224Z",
     "iopub.status.busy": "2023-11-13T03:36:58.456928Z",
     "iopub.status.idle": "2023-11-13T03:37:05.029867Z",
     "shell.execute_reply": "2023-11-13T03:37:05.029317Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code snippet for metadata extraction by Jenna Tomkinson\n",
    "df_metadata = list(df1.columns[df1.columns.str.startswith(\"Metadata\")])\n",
    "\n",
    "# define which columns are data and which are descriptive\n",
    "df_descriptive = df1[df_metadata]\n",
    "df_descriptive[\"labels\"] = df1[\"labels\"]\n",
    "df_values = df1.drop(columns=df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924c31e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:05.052514Z",
     "iopub.status.busy": "2023-11-13T03:37:05.052246Z",
     "iopub.status.idle": "2023-11-13T03:37:12.172056Z",
     "shell.execute_reply": "2023-11-13T03:37:12.171584Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting strings into numbers\n",
    "df_values[\"new_labels\"] = le.fit_transform(df_values[\"labels\"])\n",
    "# get mini dataframe that contains the decoder\n",
    "df_labels = df_values[[\"labels\", \"new_labels\"]]\n",
    "# split into X and Y where Y are the predictive column and x are the observable data\n",
    "df_values_X = df_values.drop(\n",
    "    [\n",
    "        \"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\",\n",
    "        \"twob_Metadata_Treatment_Dose_Inhibitor_Dose\",\n",
    "        \"threeb_Metadata_Treatment_Dose_Inhibitor_Dose\",\n",
    "        \"fourb_Metadata_Treatment_Dose_Inhibitor_Dose\",\n",
    "        \"labels\",\n",
    "        \"new_labels\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_values_Y = df_values[\"new_labels\"]\n",
    "df_values_Y.unique()\n",
    "df_labels.drop_duplicates(inplace=True)\n",
    "# pandas chaining to reset index and drop old index\n",
    "df_labels.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18afc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:12.193904Z",
     "iopub.status.busy": "2023-11-13T03:37:12.193605Z",
     "iopub.status.idle": "2023-11-13T03:37:12.196539Z",
     "shell.execute_reply": "2023-11-13T03:37:12.196160Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    train_indexes.shape,\n",
    "    val_indexes.shape,\n",
    "    test_indexes.shape,\n",
    "    treatment_holdout,\n",
    "    holdout_indexes.shape,\n",
    ")\n",
    "print(\n",
    "    train_indexes.shape[0]\n",
    "    + val_indexes.shape[0]\n",
    "    + test_indexes.shape[0]\n",
    "    + treatment_holdout.shape[0]\n",
    "    + holdout_indexes.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c24e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:12.213904Z",
     "iopub.status.busy": "2023-11-13T03:37:12.213628Z",
     "iopub.status.idle": "2023-11-13T03:37:14.457835Z",
     "shell.execute_reply": "2023-11-13T03:37:14.457329Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the train, validation, test, and holdout dataframes from the indexes\n",
    "X_train = df_values_X.iloc[train_indexes.values]\n",
    "X_val = df_values_X.iloc[val_indexes.values]\n",
    "X_test = df_values_X.iloc[test_indexes.values]\n",
    "X_treatment_holdout = df_values_X.iloc[treatment_holdout.values]\n",
    "X_holdout = df_values_X.iloc[holdout_indexes.values]\n",
    "\n",
    "Y_train = df_values_Y.iloc[train_indexes.values]\n",
    "Y_val = df_values_Y.iloc[val_indexes.values]\n",
    "Y_test = df_values_Y.iloc[test_indexes.values]\n",
    "Y_treatment_holdout = df_values_Y.iloc[treatment_holdout.values]\n",
    "Y_holdout = df_values_Y.iloc[holdout_indexes.values]\n",
    "\n",
    "metadata_train = df_descriptive.iloc[train_indexes.values]\n",
    "metadata_val = df_descriptive.iloc[val_indexes.values]\n",
    "metadata_test = df_descriptive.iloc[test_indexes.values]\n",
    "metadata_treatment_holdout = df_descriptive.iloc[treatment_holdout.values]\n",
    "metadata_holdout = df_descriptive.iloc[holdout_indexes.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea706992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:14.478143Z",
     "iopub.status.busy": "2023-11-13T03:37:14.477912Z",
     "iopub.status.idle": "2023-11-13T03:37:14.487042Z",
     "shell.execute_reply": "2023-11-13T03:37:14.486667Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    Y_train[Y_train == 0].shape,\n",
    "    Y_train[Y_train == 1].shape,\n",
    "    Y_train[Y_train == 2].shape,\n",
    ")\n",
    "print(Y_val[Y_val == 0].shape, Y_val[Y_val == 1].shape, Y_val[Y_val == 2].shape)\n",
    "print(Y_test[Y_test == 0].shape, Y_test[Y_test == 1].shape, Y_test[Y_test == 2].shape)\n",
    "print(\n",
    "    Y_treatment_holdout[Y_treatment_holdout == 0].shape,\n",
    "    Y_treatment_holdout[Y_treatment_holdout == 1].shape,\n",
    "    Y_treatment_holdout[Y_treatment_holdout == 2].shape,\n",
    ")\n",
    "print(\n",
    "    Y_holdout[Y_holdout == 0].shape,\n",
    "    Y_holdout[Y_holdout == 1].shape,\n",
    "    Y_holdout[Y_holdout == 2].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc42ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:14.504746Z",
     "iopub.status.busy": "2023-11-13T03:37:14.504457Z",
     "iopub.status.idle": "2023-11-13T03:37:14.507803Z",
     "shell.execute_reply": "2023-11-13T03:37:14.507423Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reset indexes for all dataframes\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_treatment_holdout.reset_index(drop=True, inplace=True)\n",
    "X_holdout.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "Y_val.reset_index(drop=True, inplace=True)\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "Y_treatment_holdout.reset_index(drop=True, inplace=True)\n",
    "Y_holdout.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0f8c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:14.525385Z",
     "iopub.status.busy": "2023-11-13T03:37:14.525116Z",
     "iopub.status.idle": "2023-11-13T03:37:14.527731Z",
     "shell.execute_reply": "2023-11-13T03:37:14.527355Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    X_train.shape, X_val.shape, X_test.shape, X_treatment_holdout.shape, X_holdout.shape\n",
    ")\n",
    "print(\n",
    "    Y_train.shape, Y_val.shape, Y_test.shape, Y_treatment_holdout.shape, Y_holdout.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc7c36",
   "metadata": {},
   "source": [
    "#### Shuffle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "if SHUFFLE:\n",
    "    print(\"Shuffling Data\")\n",
    "    for column in X_train.columns:\n",
    "        X_train[column] = np.random.permutation(X_train[column].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa9d10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Split Data - All Models can proceed through this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_params.OUT_FEATURES = Y_train.unique().shape[0]\n",
    "print(mlp_params.OUT_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = torch.tensor(Y_train.values)\n",
    "Y_train = torch.nn.functional.one_hot(\n",
    "    Y_train, num_classes=mlp_params.OUT_FEATURES\n",
    ").float()\n",
    "\n",
    "Y_val = torch.tensor(Y_val.values)\n",
    "Y_val = torch.nn.functional.one_hot(Y_val, num_classes=mlp_params.OUT_FEATURES).float()\n",
    "\n",
    "Y_test = torch.tensor(Y_test.values)\n",
    "Y_test = torch.nn.functional.one_hot(\n",
    "    Y_test, num_classes=mlp_params.OUT_FEATURES\n",
    ").float()\n",
    "\n",
    "Y_holdout = torch.tensor(Y_holdout.values)\n",
    "Y_holdout = torch.nn.functional.one_hot(\n",
    "    Y_holdout, num_classes=mlp_params.OUT_FEATURES\n",
    ").float()\n",
    "\n",
    "Y_treatment_holdout = torch.tensor(Y_treatment_holdout.values)\n",
    "Y_treatment_holdout = torch.nn.functional.one_hot(\n",
    "    Y_treatment_holdout, num_classes=mlp_params.OUT_FEATURES\n",
    ").float()\n",
    "\n",
    "# convert the X dataframes to tensors\n",
    "X_train = torch.tensor(X_train.values)\n",
    "X_val = torch.tensor(X_val.values)\n",
    "X_test = torch.tensor(X_test.values)\n",
    "X_holdout = torch.tensor(X_holdout.values)\n",
    "X_treatment_holdout = torch.tensor(X_treatment_holdout.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce data objects for train, val and test datasets\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "val_data = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "test_data = torch.utils.data.TensorDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c628ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:14.715138Z",
     "iopub.status.busy": "2023-11-13T03:37:14.714939Z",
     "iopub.status.idle": "2023-11-13T03:37:14.905206Z",
     "shell.execute_reply": "2023-11-13T03:37:14.904707Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_params.IN_FEATURES = X_train.shape[1]\n",
    "print(\"Number of in features: \", mlp_params.IN_FEATURES)\n",
    "if mlp_params.MODEL_TYPE == \"Regression\":\n",
    "    mlp_params.OUT_FEATURES = 1\n",
    "else:\n",
    "    mlp_params.OUT_FEATURES = len(df_values[\"labels\"].unique())\n",
    "\n",
    "print(\"Number of out features: \", mlp_params.OUT_FEATURES)\n",
    "\n",
    "if mlp_params.OUT_FEATURES > 2:\n",
    "    mlp_params.MODEL_TYPE = \"Multi_Class\"\n",
    "elif mlp_params.OUT_FEATURES == 2:\n",
    "    mlp_params.OUT_FEATURES = mlp_params.OUT_FEATURES - 1\n",
    "    mlp_params.MODEL_TYPE = \"Binary_Classification\"\n",
    "elif mlp_params.OUT_FEATURES == 1:\n",
    "    mlp_params.MODEL_TYPE = \"Regression\"\n",
    "else:\n",
    "    pass\n",
    "print(mlp_params.MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3c505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:14.923652Z",
     "iopub.status.busy": "2023-11-13T03:37:14.923418Z",
     "iopub.status.idle": "2023-11-13T03:37:14.926694Z",
     "shell.execute_reply": "2023-11-13T03:37:14.926322Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert data class into a dataloader to be compatible with pytorch\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, batch_size=mlp_params.BATCH_SIZE, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_data, batch_size=mlp_params.BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab763c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:37:14.945223Z",
     "iopub.status.busy": "2023-11-13T03:37:14.944639Z",
     "iopub.status.idle": "2023-11-13T03:45:26.118958Z",
     "shell.execute_reply": "2023-11-13T03:45:26.118478Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call the optimized training model\n",
    "(\n",
    "    train_loss,\n",
    "    train_acc,\n",
    "    valid_loss,\n",
    "    valid_acc,\n",
    "    epochs_ran,\n",
    "    model,\n",
    ") = train_optimized_model(\n",
    "    mlp_params.TRAIN_EPOCHS,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    params,\n",
    "    mlp_params.MODEL_NAME,\n",
    "    mlp_params.SHUFFLE,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "# get training_metrics\n",
    "if mlp_params.MODEL_TYPE == \"Regression\":\n",
    "    training_stats = pd.DataFrame(\n",
    "        zip(train_loss, valid_loss, epochs_ran),\n",
    "        columns=[\"train_loss\", \"valid_loss\", \"epochs_ran\"],\n",
    "    )\n",
    "else:\n",
    "    training_stats = pd.DataFrame(\n",
    "        zip(train_loss, train_acc, valid_loss, valid_acc, epochs_ran),\n",
    "        columns=[\"train_loss\", \"train_acc\", \"valid_loss\", \"valid_acc\", \"epochs_ran\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5115e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:26.159482Z",
     "iopub.status.busy": "2023-11-13T03:45:26.159314Z",
     "iopub.status.idle": "2023-11-13T03:45:26.166918Z",
     "shell.execute_reply": "2023-11-13T03:45:26.166470Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe to store the model stats\n",
    "model_stats_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"metric\",\n",
    "        \"value\",\n",
    "        \"group\",\n",
    "        \"shuffled_data\",\n",
    "    ]\n",
    ")\n",
    "# check empty dataframe\n",
    "model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f283d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:26.191425Z",
     "iopub.status.busy": "2023-11-13T03:45:26.191262Z",
     "iopub.status.idle": "2023-11-13T03:45:26.833630Z",
     "shell.execute_reply": "2023-11-13T03:45:26.833186Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mlp_params.MODEL_TYPE == \"Regression\":\n",
    "    pass\n",
    "else:\n",
    "    plot_metric_vs_epoch(\n",
    "        training_stats,\n",
    "        x=\"epochs_ran\",\n",
    "        y1=\"train_acc\",\n",
    "        y2=\"valid_acc\",\n",
    "        title=\"Accuracy vs. Epochs\",\n",
    "        x_axis_label=\"Epochs\",\n",
    "        y_axis_label=\"Accuracy\",\n",
    "        params=params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e491e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:26.861418Z",
     "iopub.status.busy": "2023-11-13T03:45:26.860827Z",
     "iopub.status.idle": "2023-11-13T03:45:27.176560Z",
     "shell.execute_reply": "2023-11-13T03:45:27.176174Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_metric_vs_epoch(\n",
    "    training_stats,\n",
    "    x=\"epochs_ran\",\n",
    "    y1=\"train_loss\",\n",
    "    y2=\"valid_loss\",\n",
    "    title=\"Loss vs. Epochs\",\n",
    "    x_axis_label=\"Epochs\",\n",
    "    y_axis_label=\"Loss\",\n",
    "    params=params,\n",
    "    model_name=mlp_params.MODEL_NAME,\n",
    "    shuffle=mlp_params.SHUFFLE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6993ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Test Models on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c595792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:27.226417Z",
     "iopub.status.busy": "2023-11-13T03:45:27.226011Z",
     "iopub.status.idle": "2023-11-13T03:45:29.777801Z",
     "shell.execute_reply": "2023-11-13T03:45:29.777257Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the model on training data\n",
    "# calling the testing function and outputting list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    (y_pred_list, y_pred_prob_list, Y_test_list,) = test_optimized_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        mlp_params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    y_pred_list, y_pred_prob_list = test_optimized_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        mlp_params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")\n",
    "\n",
    "# un-nest list if nested i.e. length of input data does not match length of output data\n",
    "if len(y_pred_list) != len(Y_test):\n",
    "    y_pred_list = un_nest(y_pred_list)\n",
    "    if mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "        y_pred_prob_list = un_nest(y_pred_prob_list)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05448f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9113015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output tensors into arrays\n",
    "\n",
    "# list of tensors to list of lists\n",
    "y_pred_prob_list = [tensor.tolist() for tensor in y_pred_prob_list]\n",
    "Y_test_list = [tensor.tolist() for tensor in Y_test_list]\n",
    "# completly flatten list of lists\n",
    "new_prob_list = []\n",
    "for i in y_pred_prob_list:\n",
    "    for j in i:\n",
    "        new_prob_list.append(j)\n",
    "\n",
    "new_y_test_list = []\n",
    "for i in Y_test_list:\n",
    "    for j in i:\n",
    "        new_y_test_list.append(j)\n",
    "\n",
    "# list of lists to array\n",
    "y_pred_prob_list = np.array(new_prob_list)\n",
    "Y_test = np.array(new_y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PR curve for each class\n",
    "pr_curve_0 = precision_recall_curve(Y_test[:, 0], y_pred_prob_list[:, 0])\n",
    "pr_curve_1 = precision_recall_curve(Y_test[:, 1], y_pred_prob_list[:, 1])\n",
    "pr_curve_2 = precision_recall_curve(Y_test[:, 2], y_pred_prob_list[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the precision-recall curves for each class\n",
    "pr_curve_0 = pd.DataFrame(\n",
    "    zip(pr_curve_0[0], pr_curve_0[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_0[\"label\"] = 0\n",
    "\n",
    "pr_curve_1 = pd.DataFrame(\n",
    "    zip(pr_curve_1[0], pr_curve_1[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_1[\"label\"] = 1\n",
    "\n",
    "pr_curve_2 = pd.DataFrame(\n",
    "    zip(pr_curve_2[0], pr_curve_2[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_2[\"label\"] = 2\n",
    "\n",
    "# make the precision-recall curve dataframe\n",
    "pr_curve_df = pd.DataFrame(columns=[\"precision\", \"recall\", \"label\"])\n",
    "# concatenate the dataframes together\n",
    "pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])\n",
    "\n",
    "# get the decoded labels\n",
    "tmp_df = df_values[[\"new_labels\", \"labels\"]]\n",
    "# get the unique rows\n",
    "tmp_df.drop_duplicates(inplace=True)\n",
    "# make a dict of the labels and new labels\n",
    "label_dict = dict(zip(tmp_df[\"new_labels\"], tmp_df[\"labels\"]))\n",
    "\n",
    "# change the label column to the actual labels from the label dict\n",
    "pr_curve_df[\"label\"] = pr_curve_df[\"label\"].map(label_dict)\n",
    "pr_curve_df[\"data_split\"] = \"train\"\n",
    "pr_curve_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d126ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prob_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"apoptosis_prob\",\n",
    "        \"control_prob\",\n",
    "        \"pyroptosis_prob\",\n",
    "        \"label_true\",\n",
    "        \"label_pred\",\n",
    "        \"data_split\",\n",
    "        \"shuffle\",\n",
    "        \"class_name\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_prob = []\n",
    "class_1_prob = []\n",
    "class_2_prob = []\n",
    "\n",
    "for i in y_pred_prob_list:\n",
    "    class_0_prob.append(i[0])\n",
    "    class_1_prob.append(i[1])\n",
    "    class_2_prob.append(i[2])\n",
    "\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    zip(class_0_prob, class_1_prob, class_2_prob),\n",
    "    columns=[\"apoptosis_prob\", \"control_prob\", \"pyroptosis_prob\"],\n",
    ")\n",
    "label_true = [np.argmax(i) for i in Y_test]\n",
    "\n",
    "prob_df[\"label_true\"] = label_true\n",
    "prob_df[\"label_pred\"] = y_pred_list\n",
    "prob_df[\"data_split\"] = \"train\"\n",
    "prob_df[\"shuffle\"] = mlp_params.SHUFFLE\n",
    "prob_df[\"class_name\"] = prob_df[\"label_true\"].map(label_dict)\n",
    "\n",
    "main_prob_df = pd.concat([main_prob_df, prob_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a master dataframe to store all the model stats\n",
    "# across all data splits and shuffled data\n",
    "pr_curve_df_all = pd.DataFrame(\n",
    "    columns=[\"precision\", \"recall\", \"label\", \"data_split\", \"shuffle\"]\n",
    ")\n",
    "pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4468bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve_df_all[\"data_split\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior to using the output_stats function, the y_pred_list and Y_test_list must be converted to a single list each\n",
    "# Y_test_list de code the one hot encoding\n",
    "Y_test_list = [np.argmax(i) for i in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_prob = []\n",
    "class_1_prob = []\n",
    "class_2_prob = []\n",
    "\n",
    "for i in y_pred_prob_list:\n",
    "    class_0_prob.append(i[0])\n",
    "    class_1_prob.append(i[1])\n",
    "    class_2_prob.append(i[2])\n",
    "\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    zip(class_0_prob, class_1_prob, class_2_prob),\n",
    "    columns=[\"apoptosis_prob\", \"control_prob\", \"pyroptosis_prob\"],\n",
    ")\n",
    "label_true = [np.argmax(i) for i in Y_test]\n",
    "\n",
    "prob_df[\"label_true\"] = label_true\n",
    "prob_df[\"label_pred\"] = y_pred_list\n",
    "prob_df[\"data_split\"] = \"train\"\n",
    "prob_df[\"shuffle\"] = mlp_params.SHUFFLE\n",
    "prob_df[\"class_name\"] = prob_df[\"label_true\"].map(label_dict)\n",
    "\n",
    "main_prob_df = pd.concat([main_prob_df, prob_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b269c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:29.806395Z",
     "iopub.status.busy": "2023-11-13T03:45:29.806222Z",
     "iopub.status.idle": "2023-11-13T03:45:30.151598Z",
     "shell.execute_reply": "2023-11-13T03:45:30.151179Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df = output_stats(\n",
    "    y_pred_list,\n",
    "    Y_test_list,\n",
    "    mlp_params,\n",
    "    test_name=f\"{mlp_params.MODEL_NAME}_train\",\n",
    "    model_name=mlp_params.MODEL_NAME,\n",
    "    title=mlp_params.MODEL_NAME,\n",
    "    shuffle=mlp_params.SHUFFLE,\n",
    ")\n",
    "# make into a dataframe\n",
    "stats_df = pd.DataFrame.from_dict(stats_df).transpose()\n",
    "stats_df.reset_index(inplace=True)\n",
    "stats_df.rename(columns={\"index\": \"label\"}, inplace=True)\n",
    "stats_df = stats_df.melt(id_vars=[\"label\"], var_name=\"metric\", value_name=\"value\")\n",
    "# if accuracy in row of column 1 then change value of column 2 to accuracy\n",
    "stats_df.loc[stats_df[\"label\"] == \"accuracy\", \"metric\"] = \"accuracy\"\n",
    "# create a decoder dictionary from df_labels\n",
    "df_labels[\"new_labels\"] = df_labels[\"new_labels\"].astype(str)\n",
    "decoder = df_labels.set_index(\"new_labels\").to_dict()[\"labels\"]\n",
    "decoder[\"accuracy\"] = \"accuracy\"\n",
    "decoder[\"macro avg\"] = \"macro avg\"\n",
    "decoder[\"weighted avg\"] = \"weighted avg\"\n",
    "stats_df[\"label\"] = stats_df[\"label\"].map(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde3d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:30.178667Z",
     "iopub.status.busy": "2023-11-13T03:45:30.178280Z",
     "iopub.status.idle": "2023-11-13T03:45:30.181886Z",
     "shell.execute_reply": "2023-11-13T03:45:30.181494Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df[\"group\"] = \"train\"\n",
    "stats_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "stats_df\n",
    "model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd00a92e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Test models on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5399f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:30.229982Z",
     "iopub.status.busy": "2023-11-13T03:45:30.229718Z",
     "iopub.status.idle": "2023-11-13T03:45:30.837000Z",
     "shell.execute_reply": "2023-11-13T03:45:30.836480Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the model on training data\n",
    "# calling the testing function and outputting list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    (y_pred_list, y_pred_prob_list, Y_test_list) = test_optimized_model(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        mlp_params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    y_pred_list, y_pred_prob_list = test_optimized_model(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        mlp_params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")\n",
    "\n",
    "# un-nest list if nested i.e. length of input data does not match length of output data\n",
    "if len(y_pred_list) != len(Y_test):\n",
    "    y_pred_list = un_nest(y_pred_list)\n",
    "    if mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "        y_pred_prob_list = un_nest(y_pred_prob_list)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output tensors into arrays\n",
    "\n",
    "# list of tensors to list of lists\n",
    "y_pred_prob_list = [tensor.tolist() for tensor in y_pred_prob_list]\n",
    "Y_test_list = [tensor.tolist() for tensor in Y_test_list]\n",
    "# completly flatten list of lists\n",
    "new_prob_list = []\n",
    "for i in y_pred_prob_list:\n",
    "    for j in i:\n",
    "        new_prob_list.append(j)\n",
    "\n",
    "new_y_test_list = []\n",
    "for i in Y_test_list:\n",
    "    for j in i:\n",
    "        new_y_test_list.append(j)\n",
    "\n",
    "# list of lists to array\n",
    "y_pred_prob_list = np.array(new_prob_list)\n",
    "Y_test = np.array(new_y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PR curve for each class\n",
    "pr_curve_0 = precision_recall_curve(Y_test[:, 0], y_pred_prob_list[:, 0])\n",
    "pr_curve_1 = precision_recall_curve(Y_test[:, 1], y_pred_prob_list[:, 1])\n",
    "pr_curve_2 = precision_recall_curve(Y_test[:, 2], y_pred_prob_list[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the precision-recall curves for each class\n",
    "pr_curve_0 = pd.DataFrame(\n",
    "    zip(pr_curve_0[0], pr_curve_0[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_0[\"label\"] = 0\n",
    "\n",
    "pr_curve_1 = pd.DataFrame(\n",
    "    zip(pr_curve_1[0], pr_curve_1[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_1[\"label\"] = 1\n",
    "\n",
    "pr_curve_2 = pd.DataFrame(\n",
    "    zip(pr_curve_2[0], pr_curve_2[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_2[\"label\"] = 2\n",
    "\n",
    "# make the precision-recall curve dataframe\n",
    "pr_curve_df = pd.DataFrame(columns=[\"precision\", \"recall\", \"label\"])\n",
    "pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])\n",
    "\n",
    "\n",
    "# get the decoded labels\n",
    "tmp_df = df_values[[\"new_labels\", \"labels\"]]\n",
    "# get the unique rows\n",
    "tmp_df.drop_duplicates(inplace=True)\n",
    "# make a dict of the labels and new labels\n",
    "label_dict = dict(zip(tmp_df[\"new_labels\"], tmp_df[\"labels\"]))\n",
    "\n",
    "# change the label column to the actual labels from the label dict\n",
    "pr_curve_df[\"label\"] = pr_curve_df[\"label\"].map(label_dict)\n",
    "pr_curve_df[\"data_split\"] = \"validation\"\n",
    "pr_curve_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a master dataframe to store all the model stats\n",
    "# across all data splits and shuffled data\n",
    "pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve_df_all[\"data_split\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_prob = []\n",
    "class_1_prob = []\n",
    "class_2_prob = []\n",
    "\n",
    "for i in y_pred_prob_list:\n",
    "    class_0_prob.append(i[0])\n",
    "    class_1_prob.append(i[1])\n",
    "    class_2_prob.append(i[2])\n",
    "\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    zip(class_0_prob, class_1_prob, class_2_prob),\n",
    "    columns=[\"apoptosis_prob\", \"control_prob\", \"pyroptosis_prob\"],\n",
    ")\n",
    "label_true = [np.argmax(i) for i in Y_test]\n",
    "\n",
    "prob_df[\"label_true\"] = label_true\n",
    "prob_df[\"label_pred\"] = y_pred_list\n",
    "prob_df[\"data_split\"] = \"validation\"\n",
    "prob_df[\"shuffle\"] = mlp_params.SHUFFLE\n",
    "prob_df[\"class_name\"] = prob_df[\"label_true\"].map(label_dict)\n",
    "\n",
    "main_prob_df = pd.concat([main_prob_df, prob_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f18f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_list = [np.argmax(i) for i in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b8e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:30.862100Z",
     "iopub.status.busy": "2023-11-13T03:45:30.861930Z",
     "iopub.status.idle": "2023-11-13T03:45:30.962171Z",
     "shell.execute_reply": "2023-11-13T03:45:30.961719Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df = output_stats(\n",
    "    y_pred_list,\n",
    "    Y_test_list,\n",
    "    mlp_params,\n",
    "    test_name=f\"{mlp_params.MODEL_NAME}_train\",\n",
    "    model_name=mlp_params.MODEL_NAME,\n",
    "    title=mlp_params.MODEL_NAME,\n",
    "    shuffle=mlp_params.SHUFFLE,\n",
    ")\n",
    "# make into a dataframe\n",
    "stats_df = pd.DataFrame.from_dict(stats_df).transpose()\n",
    "stats_df.reset_index(inplace=True)\n",
    "stats_df.rename(columns={\"index\": \"label\"}, inplace=True)\n",
    "stats_df = stats_df.melt(id_vars=[\"label\"], var_name=\"metric\", value_name=\"value\")\n",
    "# if accuracy in row of column 1 then change value of column 2 to accuracy\n",
    "stats_df.loc[stats_df[\"label\"] == \"accuracy\", \"metric\"] = \"accuracy\"\n",
    "# create a decoder dictionary from df_labels\n",
    "df_labels[\"new_labels\"] = df_labels[\"new_labels\"].astype(str)\n",
    "decoder = df_labels.set_index(\"new_labels\").to_dict()[\"labels\"]\n",
    "decoder[\"accuracy\"] = \"accuracy\"\n",
    "decoder[\"macro avg\"] = \"macro avg\"\n",
    "decoder[\"weighted avg\"] = \"weighted avg\"\n",
    "stats_df[\"label\"] = stats_df[\"label\"].map(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573e7a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:30.989234Z",
     "iopub.status.busy": "2023-11-13T03:45:30.989069Z",
     "iopub.status.idle": "2023-11-13T03:45:30.992766Z",
     "shell.execute_reply": "2023-11-13T03:45:30.992326Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df[\"group\"] = \"validation\"\n",
    "stats_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "\n",
    "model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a18529",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Testing on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e76f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:31.073811Z",
     "iopub.status.busy": "2023-11-13T03:45:31.073498Z",
     "iopub.status.idle": "2023-11-13T03:45:31.077443Z",
     "shell.execute_reply": "2023-11-13T03:45:31.077009Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe to store the model confusion matrix\n",
    "data_split_conf_mat_df_all = pd.DataFrame(\n",
    "    columns=[\"True_Label\", \"Predicted_Label\", \"Count\", \"data_split\", \"Recall\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cef39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:31.103198Z",
     "iopub.status.busy": "2023-11-13T03:45:31.102764Z",
     "iopub.status.idle": "2023-11-13T03:45:58.660822Z",
     "shell.execute_reply": "2023-11-13T03:45:58.660292Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calling the testing function and outputting list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    (y_pred_list, y_pred_prob_list, Y_test_list,) = test_optimized_model(\n",
    "        model,\n",
    "        test_loader,\n",
    "        params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    y_pred_list, y_pred_prob_list = test_optimized_model(\n",
    "        model,\n",
    "        test_loader,\n",
    "        params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fe0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output tensors into arrays\n",
    "\n",
    "# list of tensors to list of lists\n",
    "y_pred_prob_list = [tensor.tolist() for tensor in y_pred_prob_list]\n",
    "Y_test_list = [tensor.tolist() for tensor in Y_test_list]\n",
    "# completly flatten list of lists\n",
    "new_prob_list = []\n",
    "for i in y_pred_prob_list:\n",
    "    for j in i:\n",
    "        new_prob_list.append(j)\n",
    "\n",
    "new_y_test_list = []\n",
    "for i in Y_test_list:\n",
    "    for j in i:\n",
    "        new_y_test_list.append(j)\n",
    "\n",
    "# list of lists to array\n",
    "y_pred_prob_list = np.array(new_prob_list)\n",
    "Y_test = np.array(new_y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8027c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PR curve for each class\n",
    "pr_curve_0 = precision_recall_curve(Y_test[:, 0], y_pred_prob_list[:, 0])\n",
    "pr_curve_1 = precision_recall_curve(Y_test[:, 1], y_pred_prob_list[:, 1])\n",
    "pr_curve_2 = precision_recall_curve(Y_test[:, 2], y_pred_prob_list[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca960ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the precision-recall curves for each class\n",
    "pr_curve_0 = pd.DataFrame(\n",
    "    zip(pr_curve_0[0], pr_curve_0[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_0[\"label\"] = 0\n",
    "\n",
    "pr_curve_1 = pd.DataFrame(\n",
    "    zip(pr_curve_1[0], pr_curve_1[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_1[\"label\"] = 1\n",
    "\n",
    "pr_curve_2 = pd.DataFrame(\n",
    "    zip(pr_curve_2[0], pr_curve_2[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_2[\"label\"] = 2\n",
    "\n",
    "# make the precision-recall curve dataframe\n",
    "pr_curve_df = pd.DataFrame(columns=[\"precision\", \"recall\", \"label\"])\n",
    "pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])\n",
    "\n",
    "\n",
    "# get the decoded labels\n",
    "tmp_df = df_values[[\"new_labels\", \"labels\"]]\n",
    "# get the unique rows\n",
    "tmp_df.drop_duplicates(inplace=True)\n",
    "# make a dict of the labels and new labels\n",
    "label_dict = dict(zip(tmp_df[\"new_labels\"], tmp_df[\"labels\"]))\n",
    "\n",
    "# change the label column to the actual labels from the label dict\n",
    "pr_curve_df[\"label\"] = pr_curve_df[\"label\"].map(label_dict)\n",
    "pr_curve_df[\"data_split\"] = \"testing\"\n",
    "pr_curve_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a master dataframe to store all the model stats\n",
    "# across all data splits and shuffled data\n",
    "\n",
    "pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve_df_all[\"data_split\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a540aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_prob = []\n",
    "class_1_prob = []\n",
    "class_2_prob = []\n",
    "\n",
    "for i in y_pred_prob_list:\n",
    "    class_0_prob.append(i[0])\n",
    "    class_1_prob.append(i[1])\n",
    "    class_2_prob.append(i[2])\n",
    "\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    zip(class_0_prob, class_1_prob, class_2_prob),\n",
    "    columns=[\"apoptosis_prob\", \"control_prob\", \"pyroptosis_prob\"],\n",
    ")\n",
    "label_true = [np.argmax(i) for i in Y_test]\n",
    "\n",
    "prob_df[\"label_true\"] = label_true\n",
    "prob_df[\"label_pred\"] = y_pred_list\n",
    "prob_df[\"data_split\"] = \"testing\"\n",
    "prob_df[\"shuffle\"] = mlp_params.SHUFFLE\n",
    "prob_df[\"class_name\"] = prob_df[\"label_true\"].map(label_dict)\n",
    "\n",
    "main_prob_df = pd.concat([main_prob_df, prob_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_list = [np.argmax(i) for i in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c00bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:45:58.690648Z",
     "iopub.status.busy": "2023-11-13T03:45:58.690243Z",
     "iopub.status.idle": "2023-11-13T03:46:00.411883Z",
     "shell.execute_reply": "2023-11-13T03:46:00.411387Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call visualization function\n",
    "# calling the testing function and outputing list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    confusion_matrix_df = results_output(\n",
    "        y_pred_list,\n",
    "        Y_test_list,\n",
    "        params,\n",
    "        test_name=f\"{mlp_params.MODEL_NAME}_testing\",\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        title=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    results_output(\n",
    "        y_pred_list,\n",
    "        Y_test,\n",
    "        params,\n",
    "        y_pred_prob_list,\n",
    "        test_name=f\"{mlp_params.MODEL_NAME}_testing\",\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        title=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a final dataframe to store the predictions\n",
    "final_predictions_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of the predictions and the true labels\n",
    "y_pred_df = pd.DataFrame(y_pred_list, columns=[\"predicted_label\"])\n",
    "y_true_df = pd.DataFrame(Y_test_list, columns=[\"true_label\"])\n",
    "# concat the two dataframes\n",
    "# final_predictions_df = pd.concat([y_true_df, y_pred_df], axis=1)\n",
    "y_pred_df = pd.concat([y_true_df, y_pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f348692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge y_pred_df with metadata_holdout whiile keeping the index of metadata_holdout\n",
    "metadata_test.reset_index(inplace=True)\n",
    "y_pred_df = pd.concat([y_pred_df, metadata_test], axis=1)\n",
    "# set the index to the index column\n",
    "y_pred_df.set_index(\"index\", inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df[\"data_split\"] = \"test\"\n",
    "y_pred_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for the model confusion matrices\n",
    "y_pred_df_path = pathlib.Path(\n",
    "    f\"../../results/Multi_Class/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}/testing_single_cell_predictions.parquet\"\n",
    ")\n",
    "y_pred_df_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if y_pred_df_path.exists():\n",
    "    predictions_df_tmp = pd.read_parquet(y_pred_df_path)\n",
    "    if len(predictions_df_tmp[\"data_split\"].unique()) > 1:\n",
    "        pass\n",
    "    elif predictions_df_tmp[\"data_split\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        metrics_df = pd.concat([predictions_df_tmp, y_pred_df], axis=0)\n",
    "        metrics_df.to_parquet(y_pred_df_path, index=False)\n",
    "else:\n",
    "    y_pred_df.to_parquet(y_pred_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de56eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.concat([final_predictions_df, y_pred_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74124b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:00.441418Z",
     "iopub.status.busy": "2023-11-13T03:46:00.441055Z",
     "iopub.status.idle": "2023-11-13T03:46:00.444604Z",
     "shell.execute_reply": "2023-11-13T03:46:00.444216Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rename columns from the decoder dictionary\n",
    "confusion_matrix_df.rename(\n",
    "    columns={0: \"apoptosis\", 1: \"healthy\", 2: \"pyroptosis\"}, inplace=True\n",
    ")\n",
    "confusion_matrix_df.rename(\n",
    "    index={0: \"apoptosis\", 1: \"healthy\", 2: \"pyroptosis\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570c014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:46:00.504227Z",
     "iopub.status.busy": "2023-11-13T03:46:00.503933Z",
     "iopub.status.idle": "2023-11-13T03:46:02.086994Z",
     "shell.execute_reply": "2023-11-13T03:46:02.086381Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrices = confusion_matrix_df.reset_index()\n",
    "# melt the DataFrame to a long format\n",
    "confusion_matrices = pd.melt(\n",
    "    confusion_matrices,\n",
    "    id_vars=[\"index\"],\n",
    "    value_vars=[\"healthy\", \"apoptosis\", \"pyroptosis\"],\n",
    ")\n",
    "\n",
    "# rename the columns\n",
    "confusion_matrices.columns = [\"True_Label\", \"Predicted_Label\", \"Count\"]\n",
    "confusion_matrices[\"data_split\"] = \"testing\"\n",
    "# sum of the columns of the confusion matrix gives the total number of samples per class\n",
    "sum_of_columns = confusion_matrix_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c57815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:29.587463Z",
     "iopub.status.busy": "2023-11-13T03:21:29.587136Z",
     "iopub.status.idle": "2023-11-13T03:21:29.590692Z",
     "shell.execute_reply": "2023-11-13T03:21:29.590308Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize confusion matrix\n",
    "# get the sum of each column to normalize the confusion matrix by the total number of samples per class\n",
    "\n",
    "# divide the apotosis column by the sum of the apotosis column\n",
    "confusion_matrix_df[\"apoptosis\"] = confusion_matrix_df[\"apoptosis\"] / sum_of_columns[0]\n",
    "# divide the healthy column by the sum of the healthy column\n",
    "confusion_matrix_df[\"healthy\"] = confusion_matrix_df[\"healthy\"] / sum_of_columns[1]\n",
    "# divide the pyroptosis column by the sum of the pyroptosis column\n",
    "confusion_matrix_df[\"pyroptosis\"] = (\n",
    "    confusion_matrix_df[\"pyroptosis\"] / sum_of_columns[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394f6ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:29.644888Z",
     "iopub.status.busy": "2023-11-13T03:21:29.644610Z",
     "iopub.status.idle": "2023-11-13T03:21:29.653537Z",
     "shell.execute_reply": "2023-11-13T03:21:29.653154Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrices_recall = confusion_matrix_df.reset_index()\n",
    "# melt the DataFrame to a long format\n",
    "confusion_matrices_recall = pd.melt(\n",
    "    confusion_matrices_recall,\n",
    "    id_vars=[\"index\"],\n",
    "    value_vars=[\"healthy\", \"apoptosis\", \"pyroptosis\"],\n",
    ")\n",
    "\n",
    "# rename the columns\n",
    "confusion_matrices_recall.columns = [\"True_Label\", \"Predicted_Label\", \"Count\"]\n",
    "confusion_matrices_recall[\"data_split\"] = \"testing\"\n",
    "confusion_matrices_recall.rename(columns={\"Count\": \"Recall\"}, inplace=True)\n",
    "data_split_conf_mat_df = pd.merge(\n",
    "    confusion_matrices,\n",
    "    confusion_matrices_recall,\n",
    "    on=[\"True_Label\", \"Predicted_Label\", \"data_split\"],\n",
    ")\n",
    "data_split_conf_mat_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "data_split_conf_mat_df_all = pd.concat(\n",
    "    [data_split_conf_mat_df_all, data_split_conf_mat_df], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8a8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:29.709364Z",
     "iopub.status.busy": "2023-11-13T03:21:29.709087Z",
     "iopub.status.idle": "2023-11-13T03:21:29.875053Z",
     "shell.execute_reply": "2023-11-13T03:21:29.874655Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(confusion_matrix_df, annot=True)\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(\"Actual Values\", size=15)\n",
    "plt.ylabel(\"Predicted Values\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b9cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:29.900899Z",
     "iopub.status.busy": "2023-11-13T03:21:29.900540Z",
     "iopub.status.idle": "2023-11-13T03:21:30.312654Z",
     "shell.execute_reply": "2023-11-13T03:21:30.312226Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df = output_stats(\n",
    "    y_pred_list,\n",
    "    Y_test_list,\n",
    "    mlp_params,\n",
    "    test_name=f\"{mlp_params.MODEL_NAME}_train\",\n",
    "    model_name=mlp_params.MODEL_NAME,\n",
    "    title=mlp_params.MODEL_NAME,\n",
    "    shuffle=mlp_params.SHUFFLE,\n",
    ")\n",
    "# make into a dataframe\n",
    "stats_df = pd.DataFrame.from_dict(stats_df).transpose()\n",
    "stats_df.reset_index(inplace=True)\n",
    "stats_df.rename(columns={\"index\": \"label\"}, inplace=True)\n",
    "stats_df = stats_df.melt(id_vars=[\"label\"], var_name=\"metric\", value_name=\"value\")\n",
    "# if accuracy in row of column 1 then change value of column 2 to accuracy\n",
    "stats_df.loc[stats_df[\"label\"] == \"accuracy\", \"metric\"] = \"accuracy\"\n",
    "# create a decoder dictionary from df_labels\n",
    "df_labels[\"new_labels\"] = df_labels[\"new_labels\"].astype(str)\n",
    "decoder = df_labels.set_index(\"new_labels\").to_dict()[\"labels\"]\n",
    "decoder[\"accuracy\"] = \"accuracy\"\n",
    "decoder[\"macro avg\"] = \"macro avg\"\n",
    "decoder[\"weighted avg\"] = \"weighted avg\"\n",
    "stats_df[\"label\"] = stats_df[\"label\"].map(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49a4d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:30.340165Z",
     "iopub.status.busy": "2023-11-13T03:21:30.339794Z",
     "iopub.status.idle": "2023-11-13T03:21:30.343391Z",
     "shell.execute_reply": "2023-11-13T03:21:30.342990Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df[\"group\"] = \"test\"\n",
    "stats_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "\n",
    "model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92180a40",
   "metadata": {},
   "source": [
    "## Test the treatment holdout data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_holdout_data = Dataset_formatter(\n",
    "    X_treatment_holdout,\n",
    "    Y_treatment_holdout,\n",
    ")\n",
    "\n",
    "# convert data class into a dataloader to be compatible with pytorch\n",
    "treatment_holdout_loader = torch.utils.data.DataLoader(\n",
    "    dataset=treatment_holdout_data, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "# calling the testing function and outputting list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    (y_pred_list, y_pred_prob_list, Y_test_list,) = test_optimized_model(\n",
    "        model,\n",
    "        treatment_holdout_loader,\n",
    "        params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    y_pred_list, y_pred_prob_list = test_optimized_model(\n",
    "        model,\n",
    "        treatment_holdout_loader,\n",
    "        params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")\n",
    "\n",
    "# un-nest list if nested i.e. length of input data does not match length of output data\n",
    "if len(y_pred_list) != len(Y_treatment_holdout):\n",
    "    y_pred_list = un_nest(y_pred_list)\n",
    "    if mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "        y_pred_prob_list = un_nest(y_pred_prob_list)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output tensors into arrays\n",
    "\n",
    "# list of tensors to list of lists\n",
    "y_pred_prob_list = [tensor.tolist() for tensor in y_pred_prob_list]\n",
    "Y_test_list = [tensor.tolist() for tensor in Y_test_list]\n",
    "# completly flatten list of lists\n",
    "new_prob_list = []\n",
    "for i in y_pred_prob_list:\n",
    "    for j in i:\n",
    "        new_prob_list.append(j)\n",
    "\n",
    "new_y_test_list = []\n",
    "for i in Y_test_list:\n",
    "    for j in i:\n",
    "        new_y_test_list.append(j)\n",
    "\n",
    "# list of lists to array\n",
    "y_pred_prob_list = np.array(new_prob_list)\n",
    "Y_test = np.array(new_y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PR curve for each class\n",
    "pr_curve_0 = precision_recall_curve(Y_test[:, 0], y_pred_prob_list[:, 0])\n",
    "pr_curve_1 = precision_recall_curve(Y_test[:, 1], y_pred_prob_list[:, 1])\n",
    "pr_curve_2 = precision_recall_curve(Y_test[:, 2], y_pred_prob_list[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the precision-recall curves for each class\n",
    "pr_curve_0 = pd.DataFrame(\n",
    "    zip(pr_curve_0[0], pr_curve_0[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_0[\"label\"] = 0\n",
    "\n",
    "pr_curve_1 = pd.DataFrame(\n",
    "    zip(pr_curve_1[0], pr_curve_1[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_1[\"label\"] = 1\n",
    "\n",
    "pr_curve_2 = pd.DataFrame(\n",
    "    zip(pr_curve_2[0], pr_curve_2[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_2[\"label\"] = 2\n",
    "\n",
    "# make the precision-recall curve dataframe\n",
    "pr_curve_df = pd.DataFrame(columns=[\"precision\", \"recall\", \"label\"])\n",
    "pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])\n",
    "\n",
    "\n",
    "# get the decoded labels\n",
    "tmp_df = df_values[[\"new_labels\", \"labels\"]]\n",
    "# get the unique rows\n",
    "tmp_df.drop_duplicates(inplace=True)\n",
    "# make a dict of the labels and new labels\n",
    "label_dict = dict(zip(tmp_df[\"new_labels\"], tmp_df[\"labels\"]))\n",
    "\n",
    "# change the label column to the actual labels from the label dict\n",
    "pr_curve_df[\"label\"] = pr_curve_df[\"label\"].map(label_dict)\n",
    "pr_curve_df[\"data_split\"] = \"treatment_holdout\"\n",
    "pr_curve_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa22275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a master dataframe to store all the model stats\n",
    "# across all data splits and shuffled data\n",
    "pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the precision-recall curves\n",
    "sns.lineplot(\n",
    "    data=pr_curve_df,\n",
    "    x=\"recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"label\",\n",
    "    palette=\"bright\",\n",
    ")\n",
    "plt.title(\"Precision-Recall Curve Treatment Holdout Data\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve_df_all[\"data_split\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_prob = []\n",
    "class_1_prob = []\n",
    "class_2_prob = []\n",
    "\n",
    "for i in y_pred_prob_list:\n",
    "    class_0_prob.append(i[0])\n",
    "    class_1_prob.append(i[1])\n",
    "    class_2_prob.append(i[2])\n",
    "\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    zip(class_0_prob, class_1_prob, class_2_prob),\n",
    "    columns=[\"apoptosis_prob\", \"control_prob\", \"pyroptosis_prob\"],\n",
    ")\n",
    "label_true = [np.argmax(i) for i in Y_test]\n",
    "\n",
    "prob_df[\"label_true\"] = label_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[\"label_pred\"] = y_pred_list\n",
    "prob_df[\"data_split\"] = \"treatment_holdout\"\n",
    "prob_df[\"shuffle\"] = mlp_params.SHUFFLE\n",
    "prob_df[\"class_name\"] = prob_df[\"label_true\"].map(label_dict)\n",
    "\n",
    "main_prob_df = pd.concat([main_prob_df, prob_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_list = [np.argmax(i) for i in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call visualization function\n",
    "# calling the testing function and outputing list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    confusion_matrix_df = results_output(\n",
    "        y_pred_list,\n",
    "        Y_test_list,\n",
    "        params,\n",
    "        test_name=f\"{mlp_params.MODEL_NAME}_treatment_hold_out\",\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        title=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    results_output(\n",
    "        y_pred_list,\n",
    "        Y_treatment_holdout,\n",
    "        params,\n",
    "        y_pred_prob_list,\n",
    "        test_name=f\"{mlp_params.MODEL_NAME}_treatment_hold_out\",\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        title=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(confusion_matrix_df, annot=True, fmt=\"d\")\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "#\n",
    "# plt.title(f\"Confusion Matrix for Binary Classifier \\n {title}\", fontsize=20)\n",
    "plt.xlabel(\"Actual Values\", size=15)\n",
    "plt.ylabel(\"Predicted Values\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of the predictions and the true labels\n",
    "y_pred_df = pd.DataFrame(y_pred_list, columns=[\"predicted_label\"])\n",
    "y_true_df = pd.DataFrame(Y_test_list, columns=[\"true_label\"])\n",
    "# concat the two dataframes\n",
    "y_pred_df = pd.concat([y_true_df, y_pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge y_pred_df with metadata_holdout whiile keeping the index of metadata_holdout\n",
    "metadata_treatment_holdout.reset_index(inplace=True)\n",
    "y_pred_df = pd.concat([y_pred_df, metadata_treatment_holdout], axis=1)\n",
    "# set the index to the index column\n",
    "y_pred_df.set_index(\"index\", inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df[\"data_split\"] = \"treatment_holdout\"\n",
    "y_pred_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for the model training metrics\n",
    "y_pred_df_path = pathlib.Path(\n",
    "    f\"../../results/Multi_Class/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "y_pred_df_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "y_pred_file = pathlib.Path(\n",
    "    f\"{y_pred_df_path}/treatment_holdout_single_cell_predictions.parquet\"\n",
    ")\n",
    "if y_pred_file.exists():\n",
    "    y_pred_old = pd.read_parquet(y_pred_file)\n",
    "    if len(y_pred_old[\"shuffle\"].unique()) > 1:\n",
    "        pass\n",
    "    elif y_pred_old[\"shuffle\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        pr_curve_df_old = pd.concat([y_pred_old, y_pred_df], axis=0)\n",
    "        pr_curve_df_old.to_parquet(y_pred_file, index=False)\n",
    "else:\n",
    "    y_pred_df.to_parquet(y_pred_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.concat([final_predictions_df, y_pred_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecba069",
   "metadata": {},
   "source": [
    "Do not do confusion matrix for this data as it is one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba765b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = output_stats(\n",
    "    y_pred_list,\n",
    "    Y_test_list,\n",
    "    mlp_params,\n",
    "    test_name=f\"{mlp_params.MODEL_NAME}_train\",\n",
    "    model_name=mlp_params.MODEL_NAME,\n",
    "    title=mlp_params.MODEL_NAME,\n",
    "    shuffle=mlp_params.SHUFFLE,\n",
    ")\n",
    "# make into a dataframe\n",
    "stats_df = pd.DataFrame.from_dict(stats_df).transpose()\n",
    "stats_df.reset_index(inplace=True)\n",
    "stats_df.rename(columns={\"index\": \"label\"}, inplace=True)\n",
    "stats_df = stats_df.melt(id_vars=[\"label\"], var_name=\"metric\", value_name=\"value\")\n",
    "# if accuracy in row of column 1 then change value of column 2 to accuracy\n",
    "stats_df.loc[stats_df[\"label\"] == \"accuracy\", \"metric\"] = \"accuracy\"\n",
    "# create a decoder dictionary from df_labels\n",
    "df_labels[\"new_labels\"] = df_labels[\"new_labels\"].astype(str)\n",
    "decoder = df_labels.set_index(\"new_labels\").to_dict()[\"labels\"]\n",
    "decoder[\"accuracy\"] = \"accuracy\"\n",
    "decoder[\"macro avg\"] = \"macro avg\"\n",
    "decoder[\"weighted avg\"] = \"weighted avg\"\n",
    "stats_df[\"label\"] = stats_df[\"label\"].map(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126024d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[\"group\"] = \"treatment_holdout\"\n",
    "stats_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "\n",
    "model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07fbb7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Test the hold out wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45de2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:30.393006Z",
     "iopub.status.busy": "2023-11-13T03:21:30.392740Z",
     "iopub.status.idle": "2023-11-13T03:21:53.066480Z",
     "shell.execute_reply": "2023-11-13T03:21:53.066002Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "holdout_data = Dataset_formatter(X_holdout, Y_holdout)\n",
    "\n",
    "# convert data class into a dataloader to be compatible with pytorch\n",
    "holdout_loader = torch.utils.data.DataLoader(\n",
    "    dataset=holdout_data, batch_size=1, shuffle=mlp_params.SHUFFLE\n",
    ")\n",
    "\n",
    "# calling the testing function and outputting list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    (y_pred_list, y_pred_prob_list, Y_test_list,) = test_optimized_model(\n",
    "        model,\n",
    "        holdout_loader,\n",
    "        params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    y_pred_list, y_pred_prob_list = test_optimized_model(\n",
    "        model,\n",
    "        holdout_loader,\n",
    "        params,\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")\n",
    "\n",
    "# un-nest list if nested i.e. length of input data does not match length of output data\n",
    "if len(y_pred_list) != len(Y_holdout):\n",
    "    y_pred_list = un_nest(y_pred_list)\n",
    "    if mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "        y_pred_prob_list = un_nest(y_pred_prob_list)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ed5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output tensors into arrays\n",
    "\n",
    "# list of tensors to list of lists\n",
    "y_pred_prob_list = [tensor.tolist() for tensor in y_pred_prob_list]\n",
    "Y_test_list = [tensor.tolist() for tensor in Y_test_list]\n",
    "# completly flatten list of lists\n",
    "new_prob_list = []\n",
    "for i in y_pred_prob_list:\n",
    "    for j in i:\n",
    "        new_prob_list.append(j)\n",
    "\n",
    "new_y_test_list = []\n",
    "for i in Y_test_list:\n",
    "    for j in i:\n",
    "        new_y_test_list.append(j)\n",
    "\n",
    "# list of lists to array\n",
    "y_pred_prob_list = np.array(new_prob_list)\n",
    "Y_test = np.array(new_y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a777d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PR curve for each class\n",
    "pr_curve_0 = precision_recall_curve(Y_test[:, 0], y_pred_prob_list[:, 0])\n",
    "pr_curve_1 = precision_recall_curve(Y_test[:, 1], y_pred_prob_list[:, 1])\n",
    "pr_curve_2 = precision_recall_curve(Y_test[:, 2], y_pred_prob_list[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the precision-recall curves for each class\n",
    "pr_curve_0 = pd.DataFrame(\n",
    "    zip(pr_curve_0[0], pr_curve_0[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_0[\"label\"] = 0\n",
    "\n",
    "pr_curve_1 = pd.DataFrame(\n",
    "    zip(pr_curve_1[0], pr_curve_1[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_1[\"label\"] = 1\n",
    "\n",
    "pr_curve_2 = pd.DataFrame(\n",
    "    zip(pr_curve_2[0], pr_curve_2[1]),\n",
    "    columns=[\"precision\", \"recall\"],\n",
    ")\n",
    "pr_curve_2[\"label\"] = 2\n",
    "\n",
    "# make the precision-recall curve dataframe\n",
    "pr_curve_df = pd.DataFrame(columns=[\"precision\", \"recall\", \"label\"])\n",
    "pr_curve_df = pd.concat([pr_curve_df, pr_curve_0, pr_curve_1, pr_curve_2])\n",
    "\n",
    "\n",
    "# get the decoded labels\n",
    "tmp_df = df_values[[\"new_labels\", \"labels\"]]\n",
    "# get the unique rows\n",
    "tmp_df.drop_duplicates(inplace=True)\n",
    "# make a dict of the labels and new labels\n",
    "label_dict = dict(zip(tmp_df[\"new_labels\"], tmp_df[\"labels\"]))\n",
    "\n",
    "# change the label column to the actual labels from the label dict\n",
    "pr_curve_df[\"label\"] = pr_curve_df[\"label\"].map(label_dict)\n",
    "pr_curve_df[\"data_split\"] = \"holdout\"\n",
    "pr_curve_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab74aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a master dataframe to store all the model stats\n",
    "# across all data splits and shuffled data\n",
    "pr_curve_df_all = pd.concat([pr_curve_df_all, pr_curve_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a272386",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curve_df_all[\"data_split\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_prob = []\n",
    "class_1_prob = []\n",
    "class_2_prob = []\n",
    "\n",
    "for i in y_pred_prob_list:\n",
    "    class_0_prob.append(i[0])\n",
    "    class_1_prob.append(i[1])\n",
    "    class_2_prob.append(i[2])\n",
    "\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    zip(class_0_prob, class_1_prob, class_2_prob),\n",
    "    columns=[\"apoptosis_prob\", \"control_prob\", \"pyroptosis_prob\"],\n",
    ")\n",
    "label_true = [np.argmax(i) for i in Y_test]\n",
    "\n",
    "prob_df[\"label_true\"] = label_true\n",
    "prob_df[\"label_pred\"] = y_pred_list\n",
    "prob_df[\"data_split\"] = \"holdout\"\n",
    "prob_df[\"shuffle\"] = mlp_params.SHUFFLE\n",
    "prob_df[\"class_name\"] = prob_df[\"label_true\"].map(label_dict)\n",
    "\n",
    "main_prob_df = pd.concat([main_prob_df, prob_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d16015",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_list = [np.argmax(i) for i in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call visualization function\n",
    "# calling the testing function and outputing list values of tested model\n",
    "if any(\n",
    "    model_type == mlp_params.MODEL_TYPE for model_type in [\"Multi_Class\", \"Regression\"]\n",
    "):\n",
    "    confusion_matrix_df = results_output(\n",
    "        y_pred_list,\n",
    "        Y_test_list,\n",
    "        params,\n",
    "        test_name=f\"{mlp_params.MODEL_NAME}_hold_out\",\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        title=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "elif mlp_params.MODEL_TYPE == \"Binary_Classification\":\n",
    "    results_output(\n",
    "        y_pred_list,\n",
    "        Y_holdout,\n",
    "        params,\n",
    "        y_pred_prob_list,\n",
    "        test_name=f\"{mlp_params.MODEL_NAME}_hold_out\",\n",
    "        model_name=mlp_params.MODEL_NAME,\n",
    "        title=mlp_params.MODEL_NAME,\n",
    "        shuffle=mlp_params.SHUFFLE,\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Model type must be specified for proper model testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62acfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of the predictions and the true labels\n",
    "y_pred_df = pd.DataFrame(y_pred_list, columns=[\"predicted_label\"])\n",
    "y_true_df = pd.DataFrame(Y_test_list, columns=[\"true_label\"])\n",
    "# concat the two dataframes\n",
    "y_pred_df = pd.concat([y_true_df, y_pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge y_pred_df with metadata_holdout whiile keeping the index of metadata_holdout\n",
    "metadata_holdout.reset_index(inplace=True)\n",
    "y_pred_df = pd.concat([y_pred_df, metadata_holdout], axis=1)\n",
    "# set the index to the index column\n",
    "y_pred_df.set_index(\"index\", inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58899c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df[\"data_split\"] = \"holdout\"\n",
    "y_pred_df[\"shuffle\"] = mlp_params.SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for the model training metrics\n",
    "y_pred_df_path = pathlib.Path(\n",
    "    f\"../../results/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "y_pred_df_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "y_pred_df_file = pathlib.Path(\n",
    "    f\"{y_pred_df_path}/holdout_single_cell_predictions.parquet\"\n",
    ")\n",
    "if y_pred_df_file.exists():\n",
    "    prediction_old = pd.read_parquet(y_pred_df_file)\n",
    "    if len(prediction_old[\"shuffle\"].unique()) > 1:\n",
    "        pass\n",
    "    elif prediction_old[\"shuffle\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        probabilties_df_old = pd.concat([prediction_old, y_pred_df], axis=0)\n",
    "        probabilties_df_old.to_parquet(y_pred_df_file, index=False)\n",
    "else:\n",
    "    y_pred_df.to_parquet(y_pred_df_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = pd.concat([final_predictions_df, y_pred_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f1ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.099747Z",
     "iopub.status.busy": "2023-11-13T03:21:53.099370Z",
     "iopub.status.idle": "2023-11-13T03:21:53.102914Z",
     "shell.execute_reply": "2023-11-13T03:21:53.102508Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rename columns from the decoder dictionary\n",
    "confusion_matrix_df.rename(\n",
    "    columns={0: \"apoptosis\", 1: \"healthy\", 2: \"pyroptosis\"}, inplace=True\n",
    ")\n",
    "# rename index from the decoder dictionary\n",
    "confusion_matrix_df.rename(\n",
    "    index={0: \"apoptosis\", 1: \"healthy\", 2: \"pyroptosis\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3690d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.130959Z",
     "iopub.status.busy": "2023-11-13T03:21:53.130694Z",
     "iopub.status.idle": "2023-11-13T03:21:53.137032Z",
     "shell.execute_reply": "2023-11-13T03:21:53.136624Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrices = confusion_matrix_df.reset_index()\n",
    "# melt the DataFrame to a long format\n",
    "confusion_matrices = pd.melt(\n",
    "    confusion_matrices,\n",
    "    id_vars=[\"index\"],\n",
    "    value_vars=[\"healthy\", \"apoptosis\", \"pyroptosis\"],\n",
    ")\n",
    "\n",
    "# rename the columns\n",
    "confusion_matrices.columns = [\"True_Label\", \"Predicted_Label\", \"Count\"]\n",
    "confusion_matrices[\"data_split\"] = \"holdout\"\n",
    "# sum of the columns of the confusion matrix gives the total number of samples per class\n",
    "sum_of_columns = confusion_matrix_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1f6af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.198562Z",
     "iopub.status.busy": "2023-11-13T03:21:53.198279Z",
     "iopub.status.idle": "2023-11-13T03:21:53.201909Z",
     "shell.execute_reply": "2023-11-13T03:21:53.201527Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize confusion matrix\n",
    "# get the sum of each column to normalize the confusion matrix by the total number of samples per class\n",
    "\n",
    "# divide the apotosis column by the sum of the apotosis column\n",
    "confusion_matrix_df[\"apoptosis\"] = confusion_matrix_df[\"apoptosis\"] / sum_of_columns[0]\n",
    "# divide the healthy column by the sum of the healthy column\n",
    "confusion_matrix_df[\"healthy\"] = confusion_matrix_df[\"healthy\"] / sum_of_columns[1]\n",
    "# divide the pyroptosis column by the sum of the pyroptosis column\n",
    "confusion_matrix_df[\"pyroptosis\"] = (\n",
    "    confusion_matrix_df[\"pyroptosis\"] / sum_of_columns[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe647ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.230116Z",
     "iopub.status.busy": "2023-11-13T03:21:53.229842Z",
     "iopub.status.idle": "2023-11-13T03:21:53.233015Z",
     "shell.execute_reply": "2023-11-13T03:21:53.232631Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change the order of the columns\n",
    "confusion_matrix_df = confusion_matrix_df[[\"healthy\", \"apoptosis\", \"pyroptosis\"]]\n",
    "# change the order of the index\n",
    "confusion_matrix_df = confusion_matrix_df.reindex(\n",
    "    index=[\"healthy\", \"apoptosis\", \"pyroptosis\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cd8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.261598Z",
     "iopub.status.busy": "2023-11-13T03:21:53.261247Z",
     "iopub.status.idle": "2023-11-13T03:21:53.270004Z",
     "shell.execute_reply": "2023-11-13T03:21:53.269614Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrices_recall = confusion_matrix_df.reset_index()\n",
    "# melt the DataFrame to a long format\n",
    "confusion_matrices_recall = pd.melt(\n",
    "    confusion_matrices_recall,\n",
    "    id_vars=[\"index\"],\n",
    "    value_vars=[\"healthy\", \"apoptosis\", \"pyroptosis\"],\n",
    ")\n",
    "\n",
    "# rename the columns\n",
    "confusion_matrices_recall.columns = [\"True_Label\", \"Predicted_Label\", \"Count\"]\n",
    "confusion_matrices_recall[\"data_split\"] = \"holdout\"\n",
    "confusion_matrices_recall.rename(columns={\"Count\": \"Recall\"}, inplace=True)\n",
    "data_split_conf_mat_df = pd.merge(\n",
    "    confusion_matrices,\n",
    "    confusion_matrices_recall,\n",
    "    on=[\"True_Label\", \"Predicted_Label\", \"data_split\"],\n",
    ")\n",
    "data_split_conf_mat_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "data_split_conf_mat_df_all = pd.concat(\n",
    "    [data_split_conf_mat_df_all, data_split_conf_mat_df], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c326b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.332808Z",
     "iopub.status.busy": "2023-11-13T03:21:53.332528Z",
     "iopub.status.idle": "2023-11-13T03:21:53.496525Z",
     "shell.execute_reply": "2023-11-13T03:21:53.496138Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(confusion_matrix_df, annot=True)\n",
    "\n",
    "plt.xlabel(\"Actual Values\", size=15)\n",
    "plt.ylabel(\"Predicted Values\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c38144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.525955Z",
     "iopub.status.busy": "2023-11-13T03:21:53.525663Z",
     "iopub.status.idle": "2023-11-13T03:21:53.783498Z",
     "shell.execute_reply": "2023-11-13T03:21:53.783087Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df = output_stats(\n",
    "    y_pred_list,\n",
    "    Y_test_list,\n",
    "    mlp_params,\n",
    "    test_name=f\"{mlp_params.MODEL_NAME}_train\",\n",
    "    model_name=mlp_params.MODEL_NAME,\n",
    "    title=mlp_params.MODEL_NAME,\n",
    "    shuffle=mlp_params.SHUFFLE,\n",
    ")\n",
    "# make into a dataframe\n",
    "stats_df = pd.DataFrame.from_dict(stats_df).transpose()\n",
    "stats_df.reset_index(inplace=True)\n",
    "stats_df.rename(columns={\"index\": \"label\"}, inplace=True)\n",
    "stats_df = stats_df.melt(id_vars=[\"label\"], var_name=\"metric\", value_name=\"value\")\n",
    "# if accuracy in row of column 1 then change value of column 2 to accuracy\n",
    "stats_df.loc[stats_df[\"label\"] == \"accuracy\", \"metric\"] = \"accuracy\"\n",
    "# create a decoder dictionary from df_labels\n",
    "df_labels[\"new_labels\"] = df_labels[\"new_labels\"].astype(str)\n",
    "decoder = df_labels.set_index(\"new_labels\").to_dict()[\"labels\"]\n",
    "decoder[\"accuracy\"] = \"accuracy\"\n",
    "decoder[\"macro avg\"] = \"macro avg\"\n",
    "decoder[\"weighted avg\"] = \"weighted avg\"\n",
    "stats_df[\"label\"] = stats_df[\"label\"].map(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61319294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.812945Z",
     "iopub.status.busy": "2023-11-13T03:21:53.812603Z",
     "iopub.status.idle": "2023-11-13T03:21:53.821134Z",
     "shell.execute_reply": "2023-11-13T03:21:53.820751Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df[\"group\"] = \"holdout\"\n",
    "stats_df[\"shuffled_data\"] = mlp_params.SHUFFLE\n",
    "\n",
    "model_stats_df = pd.concat([model_stats_df, stats_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806d13c",
   "metadata": {},
   "source": [
    "### Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for the model confusion matrices\n",
    "predictions_path = pathlib.Path(\n",
    "    f\"../../results/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "predictions_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "predictions_file = pathlib.Path(f\"{predictions_path}/single_cell_predictions.parquet\")\n",
    "if predictions_file.exists():\n",
    "    predictions_df_tmp = pd.read_parquet(predictions_file)\n",
    "    if len(predictions_df_tmp[\"data_split\"].unique()) > 1:\n",
    "        pass\n",
    "    elif predictions_df_tmp[\"data_split\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        metrics_df = pd.concat([predictions_df_tmp, final_predictions_df], axis=0)\n",
    "        metrics_df.to_parquet(predictions_file, index=False)\n",
    "else:\n",
    "    final_predictions_df.to_parquet(predictions_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320cb8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.887891Z",
     "iopub.status.busy": "2023-11-13T03:21:53.887611Z",
     "iopub.status.idle": "2023-11-13T03:21:53.892841Z",
     "shell.execute_reply": "2023-11-13T03:21:53.892460Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set path for the model confusion matrices\n",
    "matrix_path = pathlib.Path(\n",
    "    f\"../../results/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "matrix_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "matrix_file = pathlib.Path(f\"{matrix_path}/confusion_matrices.parquet\")\n",
    "if matrix_file.exists():\n",
    "    metrics_df = pd.read_parquet(matrix_file)\n",
    "    if len(metrics_df[\"shuffled_data\"].unique()) > 1:\n",
    "        pass\n",
    "    elif metrics_df[\"shuffled_data\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        metrics_df = pd.concat([metrics_df, data_split_conf_mat_df_all], axis=0)\n",
    "        metrics_df.to_parquet(matrix_file, index=False)\n",
    "else:\n",
    "    data_split_conf_mat_df_all.to_parquet(matrix_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4b224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T03:21:53.922948Z",
     "iopub.status.busy": "2023-11-13T03:21:53.922663Z",
     "iopub.status.idle": "2023-11-13T03:21:53.927617Z",
     "shell.execute_reply": "2023-11-13T03:21:53.927236Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set path for the model training metrics\n",
    "metrics_path = pathlib.Path(\n",
    "    f\"../../results/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "metrics_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "metrics_file = pathlib.Path(f\"{metrics_path}/training_metrics.parquet\")\n",
    "if metrics_file.exists():\n",
    "    metrics_df = pd.read_parquet(metrics_file)\n",
    "    if len(metrics_df[\"shuffled_data\"].unique()) > 1:\n",
    "        pass\n",
    "    elif metrics_df[\"shuffled_data\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        metrics_df = pd.concat([metrics_df, model_stats_df], axis=0)\n",
    "        metrics_df.to_parquet(metrics_file, index=False)\n",
    "else:\n",
    "    model_stats_df.to_parquet(metrics_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b342f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for the model training metrics\n",
    "pr_curve_file_path = pathlib.Path(\n",
    "    f\"../../results/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "pr_curve_file_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "pr_curve_file = pathlib.Path(f\"{pr_curve_file_path}/PR_curves.parquet\")\n",
    "if pr_curve_file.exists():\n",
    "    pr_curve_df_old = pd.read_parquet(pr_curve_file)\n",
    "    if len(pr_curve_df_old[\"shuffle\"].unique()) > 1:\n",
    "        pass\n",
    "    elif pr_curve_df_old[\"shuffle\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        pr_curve_df_old = pd.concat([pr_curve_df_old, pr_curve_df_all], axis=0)\n",
    "        pr_curve_df_old.to_parquet(pr_curve_file, index=False)\n",
    "else:\n",
    "    pr_curve_df_all.to_parquet(pr_curve_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81929db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for the model training metrics\n",
    "probabilties_df_path = pathlib.Path(\n",
    "    f\"../../results/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}\"\n",
    ")\n",
    "probabilties_df_path.mkdir(parents=True, exist_ok=True)\n",
    "# check if the model training metrics file exists\n",
    "probabilties_df_file = pathlib.Path(f\"{probabilties_df_path}/probabilities.parquet\")\n",
    "if probabilties_df_file.exists():\n",
    "    probabilties_df_old = pd.read_parquet(probabilties_df_file)\n",
    "    if len(probabilties_df_old[\"shuffle\"].unique()) > 1:\n",
    "        pass\n",
    "    elif probabilties_df_old[\"shuffle\"].unique() == mlp_params.SHUFFLE:\n",
    "        pass\n",
    "    else:\n",
    "        probabilties_df_old = pd.concat([probabilties_df_old, main_prob_df], axis=0)\n",
    "        probabilties_df_old.to_parquet(probabilties_df_file, index=False)\n",
    "else:\n",
    "    main_prob_df.to_parquet(probabilties_df_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interstellar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "train_multiclass_model.ipynb",
   "output_path": "train_multiclass_model.ipynb",
   "parameters": {
    "CELL_TYPE": "PBMC",
    "MODEL_NAME": "MultiClass_MLP",
    "SHUFFLE": true
   },
   "start_time": "2023-11-13T03:46:04.352506",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
