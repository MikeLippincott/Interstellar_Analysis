{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "8e60628c",
            "metadata": {
                "papermill": {
                    "duration": 0.002385,
                    "end_time": "2023-10-18T18:20:04.868387",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:04.866002",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "## Hyperparameter tuning via Optuna"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fac71a5",
            "metadata": {
                "papermill": {
                    "duration": 0.001919,
                    "end_time": "2023-10-18T18:20:04.873048",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:04.871129",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "### Being a binary model this notebook will be limited to predicting one class 1 or 0, yes or no.\n",
                "### Here I will be predicting if a cell received a treatment or not"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "860a61dd",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:20:04.878009Z",
                    "iopub.status.busy": "2023-10-18T18:20:04.877582Z",
                    "iopub.status.idle": "2023-10-18T18:20:08.479452Z",
                    "shell.execute_reply": "2023-10-18T18:20:08.478953Z"
                },
                "papermill": {
                    "duration": 3.605361,
                    "end_time": "2023-10-18T18:20:08.480497",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:04.875136",
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "import pathlib\n",
                "import sys\n",
                "\n",
                "import numpy as np\n",
                "import optuna\n",
                "import pandas as pd\n",
                "import pyarrow.parquet as pq\n",
                "import toml\n",
                "import torch\n",
                "from sklearn import preprocessing\n",
                "\n",
                "sys.path.append(\"../..\")\n",
                "\n",
                "import argparse\n",
                "\n",
                "from MLP_utils.parameters import Parameters\n",
                "from MLP_utils.utils import (\n",
                "    Dataset_formatter,\n",
                "    data_split,\n",
                "    extract_best_trial_params,\n",
                "    objective_model_optimizer,\n",
                "    parameter_set,\n",
                "    plot_metric_vs_epoch,\n",
                "    results_output,\n",
                "    test_optimized_model,\n",
                "    train_optimized_model,\n",
                "    un_nest,\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "sys.path.append(\"../../..\")\n",
                "from utils.utils import df_stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15fbe814",
            "metadata": {},
            "outputs": [],
            "source": [
                "# set up the parser\n",
                "parser = argparse.ArgumentParser(description=\"Run hyperparameter optimization\")\n",
                "parser.add_argument(\n",
                "    \"--cell_type\",\n",
                "    type=str,\n",
                "    default=\"all\",\n",
                "    help=\"Cell type to run hyperparameter optimization for\",\n",
                ")\n",
                "parser.add_argument(\n",
                "    \"--model_name\",\n",
                "    type=str,\n",
                "    default=\"all\",\n",
                "    help=\"Model name to run hyperparameter optimization for\",\n",
                ")\n",
                "\n",
                "# parse arguments\n",
                "args = parser.parse_args()\n",
                "\n",
                "CELL_TYPE = args.cell_type\n",
                "MODEL_NAME = args.model_name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "394294ce",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:20:08.498713Z",
                    "iopub.status.busy": "2023-10-18T18:20:08.498501Z",
                    "iopub.status.idle": "2023-10-18T18:20:08.502638Z",
                    "shell.execute_reply": "2023-10-18T18:20:08.502299Z"
                },
                "papermill": {
                    "duration": 0.007175,
                    "end_time": "2023-10-18T18:20:08.503377",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:08.496202",
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "ml_configs_file = pathlib.Path(\"../../MLP_utils/multi_class_config.toml\").resolve(\n",
                "    strict=True\n",
                ")\n",
                "ml_configs = toml.load(ml_configs_file)\n",
                "params = Parameters()\n",
                "mlp_params = parameter_set(params, ml_configs)\n",
                "\n",
                "# overwrite params via command line arguments from papermill\n",
                "mlp_params.CELL_TYPE = CELL_TYPE\n",
                "mlp_params.MODEL_NAME = MODEL_NAME\n",
                "MODEL_TYPE = mlp_params.MODEL_TYPE\n",
                "HYPERPARAMETER_BATCH_SIZE = mlp_params.HYPERPARAMETER_BATCH_SIZE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "44baa945",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:20:08.507805Z",
                    "iopub.status.busy": "2023-10-18T18:20:08.507608Z"
                },
                "papermill": {
                    "duration": 54.581971,
                    "end_time": "2023-10-18T18:21:03.087265",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:08.505294",
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Import Data\n",
                "# set data file path under pathlib path for multi-system use\n",
                "\n",
                "file_path = pathlib.Path(\n",
                "    f\"../../../data/{mlp_params.CELL_TYPE}_preprocessed_sc_norm.parquet\"\n",
                ").resolve(strict=True)\n",
                "\n",
                "df1 = pd.read_parquet(file_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8ef96f3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get paths for toml files\n",
                "ground_truth_file_path = pathlib.Path(f\"../../MLP_utils/ground_truth.toml\").resolve(\n",
                "    strict=True\n",
                ")\n",
                "treatment_splits_file_path = pathlib.Path(f\"../../MLP_utils/splits.toml\").resolve(\n",
                "    strict=True\n",
                ")\n",
                "# read toml files\n",
                "ground_truth = toml.load(ground_truth_file_path)\n",
                "treatment_splits = toml.load(treatment_splits_file_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29c1f3cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get information from toml files\n",
                "apoptosis_groups_list = ground_truth[\"Apoptosis\"][\"apoptosis_groups_list\"]\n",
                "pyroptosis_groups_list = ground_truth[\"Pyroptosis\"][\"pyroptosis_groups_list\"]\n",
                "healthy_groups_list = ground_truth[\"Healthy\"][\"healthy_groups_list\"]\n",
                "test_split_100 = treatment_splits[\"splits\"][\"data_splits_100\"]\n",
                "test_split_75 = treatment_splits[\"splits\"][\"data_splits_75\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "159e2281",
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(0)\n",
                "if mlp_params.DATA_SUBSET_OPTION == \"True\":\n",
                "    df1 = df1.groupby(\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\").apply(\n",
                "        lambda x: x.sample(n=mlp_params.DATA_SUBSET_NUMBER, random_state=0)\n",
                "    )\n",
                "    print(\"Data Subset Is On\")\n",
                "    print(f\"Data is subset to {mlp_params.DATA_SUBSET_NUMBER} per treatment group\")\n",
                "    print(df1.shape)\n",
                "    df1.reset_index(drop=True, inplace=True)\n",
                "else:\n",
                "    print(\"Data Subset Is Off\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a6ec3fa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# add apoptosis, pyroptosis and healthy columns to dataframe\n",
                "df1[\"apoptosis\"] = df1.apply(\n",
                "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "    in apoptosis_groups_list,\n",
                "    axis=1,\n",
                ")\n",
                "df1[\"pyroptosis\"] = df1.apply(\n",
                "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "    in pyroptosis_groups_list,\n",
                "    axis=1,\n",
                ")\n",
                "df1[\"healthy\"] = df1.apply(\n",
                "    lambda row: row[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"]\n",
                "    in healthy_groups_list,\n",
                "    axis=1,\n",
                ")\n",
                "\n",
                "# merge apoptosis, pyroptosis, and healthy columns into one column\n",
                "df1[\"labels\"] = df1.apply(\n",
                "    lambda row: \"apoptosis\"\n",
                "    if row[\"apoptosis\"]\n",
                "    else \"pyroptosis\"\n",
                "    if row[\"pyroptosis\"]\n",
                "    else \"healthy\",\n",
                "    axis=1,\n",
                ")\n",
                "# drop apoptosis, pyroptosis, and healthy columns\n",
                "df1.drop(columns=[\"apoptosis\", \"pyroptosis\", \"healthy\"], inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3cb179dd",
            "metadata": {},
            "source": [
                "### Split said data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "20568fd5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# randomly select wells to hold out for testing one per treatment group\n",
                "# stratified by treatment group\n",
                "np.random.seed(seed=0)\n",
                "wells_to_hold = (\n",
                "    df1.groupby(\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\")\n",
                "    .agg(np.random.choice)[\"Metadata_Well\"]\n",
                "    .to_list()\n",
                ")\n",
                "df_holdout = df1[df1[\"Metadata_Well\"].isin(wells_to_hold)]\n",
                "df = df1[~df1[\"Metadata_Well\"].isin(wells_to_hold)]\n",
                "\n",
                "\n",
                "print(\"Wells held out for testing:\", df_holdout[\"Metadata_Well\"].unique())\n",
                "print(\n",
                "    \"Wells to use for training, validation, and testing\", df1[\"Metadata_Well\"].unique()\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a46d3a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# variable test and train set splits\n",
                "# 100% test set\n",
                "# subset the following treatments for test set\n",
                "treatment_holdout = df[\n",
                "    df[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(test_split_100)\n",
                "]\n",
                "# 75% test set and 25% train set\n",
                "test_set_75 = df[df[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(test_split_75)]\n",
                "\n",
                "test_100_and_75 = test_split_100 + test_split_75\n",
                "\n",
                "# 50% test set and 50% train set\n",
                "# get all treatments that are not in the_test_set_all and the test_set_75\n",
                "test_set_50 = df[\n",
                "    ~df[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(test_100_and_75)\n",
                "]\n",
                "\n",
                "print(treatment_holdout.shape, test_set_75.shape, test_set_50.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26b32963",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the train test splits from each group\n",
                "# 100% test set\n",
                "treatment_holdout\n",
                "\n",
                "# 75% test set and 25% train set\n",
                "test_ratio = 0.75\n",
                "training_data_set_75, testing_data_set_75 = train_test_split(\n",
                "    test_set_75,\n",
                "    test_size=test_ratio,\n",
                "    stratify=test_set_75[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
                "    random_state=0,\n",
                ")\n",
                "\n",
                "# 50% test set and 50% train set\n",
                "test_ratio = 0.5\n",
                "training_data_set_50, testing_data_set_50 = train_test_split(\n",
                "    test_set_50,\n",
                "    test_size=test_ratio,\n",
                "    stratify=test_set_50[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
                "    random_state=0,\n",
                ")\n",
                "\n",
                "# verify that the correct splits have been made\n",
                "# 100% test set\n",
                "print(f\"Shape for the 100% test set: {treatment_holdout.shape}\\n\")\n",
                "\n",
                "# 75% test set and 25% train set\n",
                "print(\n",
                "    f\"Shape for the 75% test set: {training_data_set_75.shape};\\nShape for the 75% train set: {testing_data_set_75.shape}\\n\"\n",
                ")\n",
                "\n",
                "# 50% test set and 50% train set\n",
                "print(f\"Shape for the holdout set: {df_holdout.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9bf407c4",
            "metadata": {},
            "outputs": [],
            "source": [
                "treatment_holdout\n",
                "# combine all testing sets together while preserving the index\n",
                "testing_data_set = pd.concat([testing_data_set_75, testing_data_set_50], axis=0)\n",
                "testing_data_set = testing_data_set.sort_index()\n",
                "testing_data_set\n",
                "\n",
                "# combine all training sets together while preserving the index\n",
                "training_data_set = pd.concat([training_data_set_75, training_data_set_50], axis=0)\n",
                "training_data_set = training_data_set.sort_index()\n",
                "training_data_set\n",
                "\n",
                "training_data_set, val_data_set = train_test_split(\n",
                "    training_data_set,\n",
                "    test_size=0.20,\n",
                "    stratify=training_data_set[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"],\n",
                ")\n",
                "print(\n",
                "    f\"\"\"\n",
                "    Testing set length: {len(testing_data_set)}\\n\n",
                "    Training set length: {len(training_data_set)}\\n\n",
                "    Validation set length: {len(val_data_set)}\\n\n",
                "    Treatment Holdout set length: {len(treatment_holdout)}\\n\n",
                "    Holdout set length: {len(df_holdout)}\"\"\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5da1578c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the indexes for the training and testing sets\n",
                "\n",
                "training_data_set_index = training_data_set.index\n",
                "val_data_set_index = val_data_set.index\n",
                "testing_data_set_index = testing_data_set.index\n",
                "treatment_holdout_index = treatment_holdout.index\n",
                "df_holdout_index = df_holdout.index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bce398ed",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\n",
                "    training_data_set_index.shape,\n",
                "    val_data_set_index.shape,\n",
                "    testing_data_set_index.shape,\n",
                "    treatment_holdout_index.shape,\n",
                "    df_holdout_index.shape,\n",
                ")\n",
                "print(\n",
                "    training_data_set_index.shape[0]\n",
                "    + val_data_set_index.shape[0]\n",
                "    + testing_data_set_index.shape[0]\n",
                "    + treatment_holdout_index.shape[0]\n",
                "    + df_holdout_index.shape[0]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad856f53",
            "metadata": {},
            "outputs": [],
            "source": [
                "# create pandas dataframe with all indexes and their respective labels, stratified by phenotypic class\n",
                "index_data = []\n",
                "for index in training_data_set_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"train\"})\n",
                "for index in val_data_set_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"val\"})\n",
                "for index in testing_data_set_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"test\"})\n",
                "for index in treatment_holdout_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"treatment_holdout\"})\n",
                "for index in df_holdout_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"holdout\"})\n",
                "\n",
                "# make index data a dataframe and sort it by labeled data index\n",
                "index_data = pd.DataFrame(index_data)\n",
                "index_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70798091",
            "metadata": {},
            "outputs": [],
            "source": [
                "index_data[\"label\"].unique()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "931ee871",
            "metadata": {},
            "outputs": [],
            "source": [
                "save_path = pathlib.Path(f\"../indexes/{CELL_TYPE}/multi_class/\")\n",
                "\n",
                "print(save_path)\n",
                "# create save path if it doesn't exist\n",
                "save_path.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "686d1a5d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# save indexes as tsv file\n",
                "index_data.to_csv(\n",
                "    f\"{save_path}/{params.MODEL_NAME}_data_split_indexes.tsv\", sep=\"\\t\", index=False\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "5c486e00",
            "metadata": {
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "#### Set up Data to be compatible with model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "95754baa",
            "metadata": {
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "##### Classification Models:\n",
                "Comment out code if using regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "786e3a76",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:19:56.707754Z",
                    "iopub.status.busy": "2023-10-18T18:19:56.707475Z",
                    "iopub.status.idle": "2023-10-18T18:19:56.711213Z",
                    "shell.execute_reply": "2023-10-18T18:19:56.710882Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Code snippet for metadata extraction by Jenna Tomkinson\n",
                "df_metadata = list(df.columns[df.columns.str.contains(\"Metadata\")])\n",
                "\n",
                "# define which columns are data and which are descriptive\n",
                "df_descriptive = df1[df_metadata]\n",
                "df_values = df1.drop(columns=df_metadata)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a354640",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the class weights for the loss function to account for class imbalance\n",
                "# get the number of samples in each class\n",
                "targets, counts = np.unique(df1[\"labels\"], return_counts=True)\n",
                "print(targets, counts)\n",
                "total_counts = np.sum(counts)\n",
                "# get the class weights\n",
                "class_weights = []\n",
                "class_targets = []\n",
                "for class_name in enumerate(targets):\n",
                "    class_targets.append(class_name[0])\n",
                "for count in enumerate(counts):\n",
                "    class_weights.append(1 - (count[1] / total_counts))\n",
                "print(class_targets, class_weights)\n",
                "# write the class weights to a file for use in the model\n",
                "class_weights_file = pathlib.Path(f\"../class_weights/{CELL_TYPE}/multi_class/\")\n",
                "class_weights_file.mkdir(parents=True, exist_ok=True)\n",
                "with open(f\"{class_weights_file}/class_weights.txt\", \"w\") as filehandle:\n",
                "    for listitem in class_weights:\n",
                "        filehandle.write(\"%s\\n\" % listitem)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "595afb65",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:19:56.717727Z",
                    "iopub.status.busy": "2023-10-18T18:19:56.717549Z",
                    "iopub.status.idle": "2023-10-18T18:19:56.720945Z",
                    "shell.execute_reply": "2023-10-18T18:19:56.720676Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Creating label encoder\n",
                "le = preprocessing.LabelEncoder()\n",
                "df_values[\"new_labels\"] = le.fit_transform(df_values[\"labels\"])\n",
                "# get mini dataframe that contains the decoder\n",
                "decoder = df_values[[\"labels\", \"new_labels\"]].drop_duplicates()\n",
                "# split into X and Y where Y are the predictive column and x are the observable data\n",
                "df_values_X = df_values.drop(\n",
                "    [\"new_labels\", \"labels\"],\n",
                "    axis=1,\n",
                ")\n",
                "df_values_Y = df_values[\"new_labels\"]\n",
                "df_values_Y.head()\n",
                "df_values_Y.unique()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "260a048e",
            "metadata": {
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "#### Split Data - All Models can proceed through this point"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5b6d4fa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# split into train and test sets from indexes previously defined\n",
                "\n",
                "X_train = df_values_X.loc[training_data_set_index]\n",
                "X_val = df_values_X.loc[val_data_set_index]\n",
                "X_test = df_values_X.loc[testing_data_set_index]\n",
                "X_holdout = df_values_X.loc[df_holdout_index]\n",
                "\n",
                "Y_train = df_values_Y.loc[training_data_set_index]\n",
                "Y_val = df_values_Y.loc[val_data_set_index]\n",
                "Y_test = df_values_Y.loc[testing_data_set_index]\n",
                "Y_holdout = df_values_Y.loc[df_holdout_index]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "36894826",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:16:16.591057Z",
                    "iopub.status.busy": "2023-10-18T18:16:16.590719Z",
                    "iopub.status.idle": "2023-10-18T18:16:16.621857Z",
                    "shell.execute_reply": "2023-10-18T18:16:16.621432Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# produce data objects for train, val and test datasets\n",
                "train_data = Dataset_formatter(\n",
                "    torch.FloatTensor(X_train.values), torch.FloatTensor(Y_train.values)\n",
                ")\n",
                "val_data = Dataset_formatter(\n",
                "    torch.FloatTensor(X_val.values), torch.FloatTensor(Y_val.values)\n",
                ")\n",
                "test_data = Dataset_formatter(\n",
                "    torch.FloatTensor(X_test.values), torch.FloatTensor(Y_test.values)\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4382d712",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:16:16.631025Z",
                    "iopub.status.busy": "2023-10-18T18:16:16.630857Z",
                    "iopub.status.idle": "2023-10-18T18:16:16.634504Z",
                    "shell.execute_reply": "2023-10-18T18:16:16.634156Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "mlp_params.IN_FEATURES = X_train.shape[1]\n",
                "print(\"Number of in features: \", mlp_params.IN_FEATURES)\n",
                "if mlp_params.MODEL_TYPE == \"Regression\":\n",
                "    mlp_params.OUT_FEATURES = 1\n",
                "else:\n",
                "    mlp_params.OUT_FEATURES = len(df_values[\"labels\"].unique())\n",
                "\n",
                "print(\"Number of out features: \", mlp_params.OUT_FEATURES)\n",
                "\n",
                "if mlp_params.OUT_FEATURES > 2:\n",
                "    mlp_params.MODEL_TYPE = \"Multi_Class\"\n",
                "elif mlp_params.OUT_FEATURES == 2:\n",
                "    mlp_params.OUT_FEATURES = mlp_params.OUT_FEATURES - 1\n",
                "    mlp_params.MODEL_TYPE = \"Binary_Classification\"\n",
                "elif mlp_params.OUT_FEATURES == 1:\n",
                "    mlp_params.MODEL_TYPE = \"Regression\"\n",
                "else:\n",
                "    pass\n",
                "print(mlp_params.MODEL_TYPE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "77effa5c",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:16:16.642779Z",
                    "iopub.status.busy": "2023-10-18T18:16:16.642487Z",
                    "iopub.status.idle": "2023-10-18T18:16:16.645220Z",
                    "shell.execute_reply": "2023-10-18T18:16:16.644897Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# convert data class into a dataloader to be compatible with pytorch\n",
                "train_loader = torch.utils.data.DataLoader(\n",
                "    dataset=train_data, batch_size=mlp_params.HYPERPARAMETER_BATCH_SIZE, shuffle=True\n",
                ")\n",
                "valid_loader = torch.utils.data.DataLoader(\n",
                "    dataset=val_data, batch_size=mlp_params.HYPERPARAMETER_BATCH_SIZE, shuffle=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb7635c8",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:16:16.653709Z",
                    "iopub.status.busy": "2023-10-18T18:16:16.653361Z",
                    "iopub.status.idle": "2023-10-18T18:16:16.655989Z",
                    "shell.execute_reply": "2023-10-18T18:16:16.655583Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# check device\n",
                "print(mlp_params.DEVICE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a969042f",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:16:16.663986Z",
                    "iopub.status.busy": "2023-10-18T18:16:16.663699Z",
                    "iopub.status.idle": "2023-10-18T18:17:37.283791Z",
                    "shell.execute_reply": "2023-10-18T18:17:37.283496Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# no accuracy function must be loss for regression\n",
                "if mlp_params.MODEL_TYPE == \"Regression\":\n",
                "    mlp_params.METRIC = \"loss\"\n",
                "    pass\n",
                "\n",
                "\n",
                "# wrap the objective function inside of a lambda function to pass args...\n",
                "objective_lambda_func = lambda trial: objective_model_optimizer(\n",
                "    train_loader,\n",
                "    valid_loader,\n",
                "    trial=trial,\n",
                "    params=params,\n",
                "    metric=mlp_params.METRIC,\n",
                "    return_info=False,\n",
                "    class_weights=class_weights,\n",
                ")\n",
                "\n",
                "\n",
                "# Study is the object for model optimization\n",
                "study = optuna.create_study(direction=f\"{mlp_params.DIRECTION}\")\n",
                "# Here I apply the optimize function of the study to the objective function\n",
                "# This optimizes each parameter specified to be optimized from the defined search space\n",
                "study.optimize(objective_lambda_func, n_trials=mlp_params.N_TRIALS)\n",
                "# Prints out the best trial's optimized parameters\n",
                "objective_model_optimizer(\n",
                "    train_loader,\n",
                "    valid_loader,\n",
                "    trial=study.best_trial,\n",
                "    params=params,\n",
                "    metric=mlp_params.METRIC,\n",
                "    return_info=True,\n",
                "    class_weights=class_weights,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2620589",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:17:37.298015Z",
                    "iopub.status.busy": "2023-10-18T18:17:37.297873Z",
                    "iopub.status.idle": "2023-10-18T18:17:37.750921Z",
                    "shell.execute_reply": "2023-10-18T18:17:37.750533Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# create graph directory for this model\n",
                "graph_path = pathlib.Path(\n",
                "    f\"../../figures/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}/hyperparameter_optimization\"\n",
                ")\n",
                "\n",
                "pathlib.Path(graph_path).mkdir(parents=True, exist_ok=True)\n",
                "fig = optuna.visualization.plot_optimization_history(study)\n",
                "\n",
                "\n",
                "graph_path = f\"{graph_path}/plot_optimization_history_graph\"\n",
                "\n",
                "fig.write_image(pathlib.Path(f\"{graph_path}.png\"))\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "92103de1",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:17:37.839841Z",
                    "iopub.status.busy": "2023-10-18T18:17:37.839637Z",
                    "iopub.status.idle": "2023-10-18T18:17:37.910906Z",
                    "shell.execute_reply": "2023-10-18T18:17:37.910591Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# create graph directory for this model\n",
                "graph_path = pathlib.Path(\n",
                "    f\"../../figures/{mlp_params.MODEL_TYPE}/{mlp_params.MODEL_NAME}/{mlp_params.CELL_TYPE}/hyperparameter_optimization\"\n",
                ").resolve(strict=True)\n",
                "\n",
                "pathlib.Path(graph_path).mkdir(parents=True, exist_ok=True)\n",
                "fig = optuna.visualization.plot_intermediate_values(study)\n",
                "\n",
                "graph_path = f\"{graph_path}/plot_intermediate_values_graph\"\n",
                "\n",
                "fig.write_image(pathlib.Path(f\"{graph_path}.png\"))\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "325a1ec3",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:17:37.971533Z",
                    "iopub.status.busy": "2023-10-18T18:17:37.971249Z",
                    "iopub.status.idle": "2023-10-18T18:17:37.974028Z",
                    "shell.execute_reply": "2023-10-18T18:17:37.973638Z"
                },
                "papermill": {
                    "duration": null,
                    "end_time": null,
                    "exception": null,
                    "start_time": null,
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "param_dict = extract_best_trial_params(\n",
                "    study.best_params, params, model_name=mlp_params.MODEL_NAME\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "cell_metadata_filter": "-all",
            "encoding": "# coding: utf-8",
            "executable": "/usr/bin/env python",
            "formats": "ipynb,py",
            "main_language": "python"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "papermill": {
            "default_parameters": {},
            "duration": 59.524284,
            "end_time": "2023-10-18T18:21:03.252265",
            "environment_variables": {},
            "exception": null,
            "input_path": "Hyperparameter_Optimization_model_binary.ipynb",
            "output_path": "Hyperparameter_Optimization_model_binary.ipynb",
            "parameters": {
                "CELL_TYPE": "PBMC",
                "CONTROL_NAME": "DMSO_0.100_%_DMSO_0.025_%",
                "MODEL_NAME": "DMSO_0.025_vs_Thapsigargin_10",
                "TREATMENT_NAME": "Thapsigargin_10.000_uM_DMSO_0.025_%"
            },
            "start_time": "2023-10-18T18:20:03.727981",
            "version": "2.4.0"
        },
        "vscode": {
            "interpreter": {
                "hash": "72ae02083a9ca7d143c492d1aec380c7bf553ec51bd66e90e72bba65228121b6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
