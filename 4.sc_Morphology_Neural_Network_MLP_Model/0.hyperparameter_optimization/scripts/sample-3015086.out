[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025
[I 2023-09-17 20:35:47,329] A new study created in memory with name: no-name-54457b1b-d6e6-4aaf-abf1-4ed5f7a5a6f8
[I 2023-09-17 20:35:51,258] Trial 0 finished with value: 0.00022154484380735085 and parameters: {'n_layers': 2, 'n_units_l0': 5, 'dropout_0': 0.3718874970432122, 'n_units_l1': 9, 'dropout_1': 0.3120584327073891, 'learning_rate': 0.07648974175498198, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.00022154484380735085.
[I 2023-09-17 20:35:54,879] Trial 1 finished with value: 0.0020752548053860663 and parameters: {'n_layers': 1, 'n_units_l0': 3, 'dropout_0': 0.17841740890696162, 'learning_rate': 0.02990725013216775, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.00022154484380735085.
[I 2023-09-17 20:35:58,628] Trial 2 finished with value: 0.0024188546901302743 and parameters: {'n_layers': 2, 'n_units_l0': 7, 'dropout_0': 0.3130410910893092, 'n_units_l1': 7, 'dropout_1': 0.17991171438167525, 'learning_rate': 0.05437733856730985, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.00022154484380735085.
[I 2023-09-17 20:36:02,512] Trial 3 finished with value: 0.000358645007681569 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_0': 0.24229437530718725, 'n_units_l1': 6, 'dropout_1': 0.25223839164770956, 'n_units_l2': 6, 'dropout_2': 0.3482221076740242, 'learning_rate': 0.021791575199230737, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.00022154484380735085.
[I 2023-09-17 20:36:05,958] Trial 4 finished with value: 0.3798167992196977 and parameters: {'n_layers': 1, 'n_units_l0': 3, 'dropout_0': 0.24756036768449016, 'learning_rate': 0.03162848503058429, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.00022154484380735085.
[I 2023-09-17 20:36:09,648] Trial 5 finished with value: 0.0001315851544677571 and parameters: {'n_layers': 3, 'n_units_l0': 6, 'dropout_0': 0.32863245451750156, 'n_units_l1': 4, 'dropout_1': 0.20505267570374158, 'n_units_l2': 7, 'dropout_2': 0.27792777442906597, 'learning_rate': 0.08228627389925107, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.0001315851544677571.
[I 2023-09-17 20:36:09,718] Trial 6 pruned.
[I 2023-09-17 20:36:09,783] Trial 7 pruned.
[I 2023-09-17 20:36:13,629] Trial 8 finished with value: 3.507676773325308e-05 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_0': 0.217046522304174, 'n_units_l1': 3, 'dropout_1': 0.21015098403410895, 'n_units_l2': 7, 'dropout_2': 0.27019206643627625, 'learning_rate': 0.07433738819436624, 'optimizer': 'Adam'}. Best is trial 8 with value: 3.507676773325308e-05.
[I 2023-09-17 20:36:13,693] Trial 9 pruned.
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025']
The dimensions of the data are: (13766, 1276)
Number of total missing values across all columns: 27532
Data Subset Is Off
Wells held out for testing: ['K15']
Wells to use for training, validation, and testing ['D14' 'D15' 'K14']
Number of in features:  1251
Number of out features:  1
Regression
cpu
Validation Loss: 1.5225171939869107e-07
Training Loss: 0.00019727493401728511
completed
