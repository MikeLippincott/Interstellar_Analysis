[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: DMSO_0.100_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025
[I 2023-09-17 20:32:17,472] A new study created in memory with name: no-name-9ede66ba-8b1c-4b20-9db6-58e5c4213a57
[I 2023-09-17 20:32:33,683] Trial 0 finished with value: 0.3080589198470116 and parameters: {'n_layers': 1, 'n_units_l0': 5, 'dropout_0': 0.31997161589849704, 'learning_rate': 0.04017018511450476, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.3080589198470116.
[I 2023-09-17 20:32:50,208] Trial 1 finished with value: 0.5046802639961243 and parameters: {'n_layers': 2, 'n_units_l0': 8, 'dropout_0': 0.10291651901140418, 'n_units_l1': 3, 'dropout_1': 0.3777562583812175, 'learning_rate': 0.07024202475880546, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.3080589198470116.
[I 2023-09-17 20:33:07,369] Trial 2 finished with value: 0.2992443203926087 and parameters: {'n_layers': 3, 'n_units_l0': 2, 'dropout_0': 0.16638372670827714, 'n_units_l1': 8, 'dropout_1': 0.18106418650791384, 'n_units_l2': 10, 'dropout_2': 0.26306131828387613, 'learning_rate': 0.07565340631874498, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.2992443203926087.
[I 2023-09-17 20:33:23,948] Trial 3 finished with value: 0.2843918502926826 and parameters: {'n_layers': 1, 'n_units_l0': 8, 'dropout_0': 0.29474611915077564, 'learning_rate': 0.061658810083242914, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.2843918502926826.
[I 2023-09-17 20:33:40,504] Trial 4 finished with value: 0.38251603305339815 and parameters: {'n_layers': 2, 'n_units_l0': 7, 'dropout_0': 0.21576273251251177, 'n_units_l1': 10, 'dropout_1': 0.21926749842255916, 'learning_rate': 0.0536285762287766, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.2843918502926826.
[I 2023-09-17 20:33:40,893] Trial 5 pruned.
[I 2023-09-17 20:33:57,606] Trial 6 finished with value: 0.26973056465387346 and parameters: {'n_layers': 2, 'n_units_l0': 3, 'dropout_0': 0.27854279447751995, 'n_units_l1': 8, 'dropout_1': 0.14713309834462987, 'learning_rate': 0.06163851889629616, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.26973056465387346.
[I 2023-09-17 20:33:57,914] Trial 7 pruned.
[I 2023-09-17 20:34:14,946] Trial 8 finished with value: 0.24956439673900607 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_0': 0.17789752919975382, 'n_units_l1': 9, 'dropout_1': 0.13094135897664524, 'learning_rate': 0.015908891139130882, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.24956439673900607.
[I 2023-09-17 20:34:15,238] Trial 9 pruned.
Selected Catagories are:
['DMSO_0.100_DMSO_0.025' 'Thapsigargin_1.000_DMSO_0.025']
The dimensions of the data are: (49409, 1276)
Number of total missing values across all columns: 98818
Data Subset Is Off
Wells held out for testing: ['I14' 'K14']
Wells to use for training, validation, and testing ['B14' 'C14' 'D14' 'B15' 'C15' 'D15' 'J14' 'I15' 'J15' 'K15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 90.39076549210205
Validation Loss: 0.29820264589786527
Training Accuracy: 93.8919707142206
Training Loss: 0.1559459897920941
completed
