[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: LPS_10.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025
[I 2023-09-19 01:31:07,313] A new study created in memory with name: no-name-01f1f378-58dc-46ee-b812-8cd6da6c34ce
[I 2023-09-19 01:31:17,363] Trial 0 finished with value: 0.3463501438498497 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_0': 0.24111293857247437, 'n_units_l1': 3, 'dropout_1': 0.10865294907992415, 'learning_rate': 0.043899459737055474, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.3463501438498497.
[I 2023-09-19 01:31:26,997] Trial 1 finished with value: 0.6113700153430304 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_0': 0.1885203569261642, 'n_units_l1': 2, 'dropout_1': 0.3652278577122656, 'n_units_l2': 4, 'dropout_2': 0.215250102050439, 'learning_rate': 0.04801760719150297, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.3463501438498497.
[I 2023-09-19 01:31:36,270] Trial 2 finished with value: 0.3060824502507845 and parameters: {'n_layers': 2, 'n_units_l0': 6, 'dropout_0': 0.17809269620447896, 'n_units_l1': 4, 'dropout_1': 0.20272460338190185, 'learning_rate': 0.09345828062879086, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.3060824502507845.
[I 2023-09-19 01:31:45,790] Trial 3 finished with value: 0.30864799668391546 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_0': 0.20321180533300964, 'n_units_l1': 3, 'dropout_1': 0.24073793035320717, 'learning_rate': 0.02295346177973779, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.3060824502507845.
[I 2023-09-19 01:31:54,851] Trial 4 finished with value: 0.2649319158991178 and parameters: {'n_layers': 1, 'n_units_l0': 9, 'dropout_0': 0.22532334389807868, 'learning_rate': 0.06981264769628893, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.2649319158991178.
[I 2023-09-19 01:31:55,023] Trial 5 pruned.
[I 2023-09-19 01:31:55,272] Trial 6 pruned.
[I 2023-09-19 01:32:04,950] Trial 7 finished with value: 0.2646451089779536 and parameters: {'n_layers': 3, 'n_units_l0': 4, 'dropout_0': 0.332998670486529, 'n_units_l1': 2, 'dropout_1': 0.21859622674736137, 'n_units_l2': 4, 'dropout_2': 0.22472154938978184, 'learning_rate': 0.05259461807016302, 'optimizer': 'Adam'}. Best is trial 7 with value: 0.2646451089779536.
[I 2023-09-19 01:32:14,264] Trial 8 finished with value: 0.30634007424116133 and parameters: {'n_layers': 1, 'n_units_l0': 8, 'dropout_0': 0.2886413350732648, 'learning_rate': 0.06497513055391842, 'optimizer': 'Adam'}. Best is trial 7 with value: 0.2646451089779536.
[I 2023-09-19 01:32:14,438] Trial 9 pruned.
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_10.000_DMSO_0.025']
The dimensions of the data are: (30576, 1276)
Number of total missing values across all columns: 61152
Data Subset Is Off
Wells held out for testing: ['D14' 'E20']
Wells to use for training, validation, and testing ['D15' 'E16' 'E17' 'E21' 'K14' 'K15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 88.22241453916054
Validation Loss: 0.34915609041849777
Training Accuracy: 84.86793269490883
Training Loss: 0.35453151707586494
completed
