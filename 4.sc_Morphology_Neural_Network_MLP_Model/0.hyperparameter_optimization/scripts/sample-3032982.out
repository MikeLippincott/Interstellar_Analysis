[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: DMSO_0.100_DMSO_0.025
[I 2023-09-19 00:09:12,135] A new study created in memory with name: no-name-c56ec553-369f-4fcc-b551-3520ecfd23ca
[I 2023-09-19 00:09:28,802] Trial 0 finished with value: 0.31866962361335754 and parameters: {'n_layers': 1, 'n_units_l0': 2, 'dropout_0': 0.13488896288004487, 'learning_rate': 0.05770920524418738, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.31866962361335754.
[I 2023-09-19 00:09:45,871] Trial 1 finished with value: 0.3089541425704956 and parameters: {'n_layers': 3, 'n_units_l0': 2, 'dropout_0': 0.1929003595452547, 'n_units_l1': 5, 'dropout_1': 0.265332404168641, 'n_units_l2': 9, 'dropout_2': 0.22751670581303401, 'learning_rate': 0.06413846153232655, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.3089541425704956.
[I 2023-09-19 00:10:02,419] Trial 2 finished with value: 0.3102554948925972 and parameters: {'n_layers': 1, 'n_units_l0': 5, 'dropout_0': 0.3971134543744409, 'learning_rate': 0.025425005255022348, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.3089541425704956.
[I 2023-09-19 00:10:18,899] Trial 3 finished with value: 0.24094286274909973 and parameters: {'n_layers': 1, 'n_units_l0': 3, 'dropout_0': 0.30314908175556643, 'learning_rate': 0.042450930075985896, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.24094286274909973.
[I 2023-09-19 00:10:35,652] Trial 4 finished with value: 0.273661449611187 and parameters: {'n_layers': 1, 'n_units_l0': 10, 'dropout_0': 0.26106233525768563, 'learning_rate': 0.005649414336261632, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.24094286274909973.
[I 2023-09-19 00:10:36,059] Trial 5 pruned.
[I 2023-09-19 00:10:53,600] Trial 6 finished with value: 0.374274248957634 and parameters: {'n_layers': 3, 'n_units_l0': 9, 'dropout_0': 0.21264678227405784, 'n_units_l1': 9, 'dropout_1': 0.21623409408041713, 'n_units_l2': 9, 'dropout_2': 0.29413268365321105, 'learning_rate': 0.054710548630802144, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.24094286274909973.
[I 2023-09-19 00:10:55,061] Trial 7 pruned.
[I 2023-09-19 00:11:12,270] Trial 8 finished with value: 0.21275619697570802 and parameters: {'n_layers': 2, 'n_units_l0': 10, 'dropout_0': 0.2673681254441027, 'n_units_l1': 2, 'dropout_1': 0.2749059041107469, 'learning_rate': 0.010544985742923188, 'optimizer': 'Adam'}. Best is trial 8 with value: 0.21275619697570802.
[I 2023-09-19 00:11:12,593] Trial 9 pruned.
Selected Catagories are:
['DMSO_0.100_DMSO_0.025' 'Thapsigargin_1.000_DMSO_0.025']
The dimensions of the data are: (49409, 1276)
Number of total missing values across all columns: 98818
Data Subset Is Off
Wells held out for testing: ['I14' 'K14']
Wells to use for training, validation, and testing ['B14' 'C14' 'D14' 'B15' 'C15' 'D15' 'J14' 'I15' 'J15' 'K15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 91.85710814094777
Validation Loss: 0.21210833680629734
Training Accuracy: 87.50116960840906
Training Loss: 0.2847616693377495
completed
