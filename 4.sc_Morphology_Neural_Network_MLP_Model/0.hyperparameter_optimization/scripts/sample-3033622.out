[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: Thapsigargin_10.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025
[I 2023-09-19 01:31:41,521] A new study created in memory with name: no-name-f789eb81-337e-4d2e-9cfc-f81ae3a1c1b4
[I 2023-09-19 01:31:50,079] Trial 0 finished with value: 0.5751062049468358 and parameters: {'n_layers': 2, 'n_units_l0': 8, 'dropout_0': 0.22136106134654238, 'n_units_l1': 2, 'dropout_1': 0.2901507846642994, 'learning_rate': 0.035593151401921395, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.5751062049468358.
[I 2023-09-19 01:31:58,948] Trial 1 finished with value: 0.6861985099315642 and parameters: {'n_layers': 3, 'n_units_l0': 4, 'dropout_0': 0.12762675782226687, 'n_units_l1': 4, 'dropout_1': 0.20487886709455796, 'n_units_l2': 9, 'dropout_2': 0.19914585303185867, 'learning_rate': 0.013617533182324264, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.5751062049468358.
[I 2023-09-19 01:32:07,124] Trial 2 finished with value: 0.5387728506326674 and parameters: {'n_layers': 1, 'n_units_l0': 9, 'dropout_0': 0.23565354617601042, 'learning_rate': 0.03033962763056401, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.5387728506326674.
[I 2023-09-19 01:32:16,047] Trial 3 finished with value: 0.6494973679383597 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_0': 0.31911408151629816, 'n_units_l1': 6, 'dropout_1': 0.20451423898434687, 'n_units_l2': 10, 'dropout_2': 0.15801539456436617, 'learning_rate': 0.020902695229233215, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.5387728506326674.
[I 2023-09-19 01:32:24,625] Trial 4 finished with value: 0.5526734469334285 and parameters: {'n_layers': 1, 'n_units_l0': 6, 'dropout_0': 0.3048433963075991, 'learning_rate': 0.08190164410674039, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.5387728506326674.
[I 2023-09-19 01:32:33,380] Trial 5 finished with value: 0.5383250639835994 and parameters: {'n_layers': 3, 'n_units_l0': 2, 'dropout_0': 0.21684099840226062, 'n_units_l1': 4, 'dropout_1': 0.14469928062847404, 'n_units_l2': 6, 'dropout_2': 0.3257250763368258, 'learning_rate': 0.022630199515134446, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.5383250639835994.
[I 2023-09-19 01:32:41,890] Trial 6 finished with value: 0.5035464950402577 and parameters: {'n_layers': 1, 'n_units_l0': 2, 'dropout_0': 0.1439749652810996, 'learning_rate': 0.006437872481749375, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.5035464950402577.
[I 2023-09-19 01:32:42,047] Trial 7 pruned.
[I 2023-09-19 01:32:42,288] Trial 8 pruned.
[I 2023-09-19 01:32:42,437] Trial 9 pruned.
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'Thapsigargin_10.000_DMSO_0.025']
The dimensions of the data are: (26978, 1276)
Number of total missing values across all columns: 53956
Data Subset Is Off
Wells held out for testing: ['E14' 'K14']
Wells to use for training, validation, and testing ['D14' 'D15' 'E15' 'L14' 'K15' 'L15']
Number of in features:  1251
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 73.37400793650794
Validation Loss: 0.5360941503445307
Training Accuracy: 76.66340011164175
Training Loss: 0.4830443338962162
completed
