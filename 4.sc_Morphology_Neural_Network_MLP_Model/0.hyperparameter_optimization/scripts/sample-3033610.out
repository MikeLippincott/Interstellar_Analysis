[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: LPS_100.000_DMSO_0.025 treatment_name: Thapsigargin_1.000_DMSO_0.025
[I 2023-09-19 01:30:56,311] A new study created in memory with name: no-name-c112134f-4c1f-4f02-8f65-353406eddaa2
[I 2023-09-19 01:31:05,065] Trial 0 finished with value: 0.2441910023490588 and parameters: {'n_layers': 2, 'n_units_l0': 7, 'dropout_0': 0.34341040782664567, 'n_units_l1': 8, 'dropout_1': 0.3732047838785608, 'learning_rate': 0.05939845704145914, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.2441910023490588.
[I 2023-09-19 01:31:13,678] Trial 1 finished with value: 0.18539695223172503 and parameters: {'n_layers': 2, 'n_units_l0': 4, 'dropout_0': 0.16730504900204332, 'n_units_l1': 7, 'dropout_1': 0.3484196820903409, 'learning_rate': 0.0711399597080339, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.18539695223172503.
[I 2023-09-19 01:31:22,479] Trial 2 finished with value: 0.22615501771370575 and parameters: {'n_layers': 3, 'n_units_l0': 9, 'dropout_0': 0.23281628943165347, 'n_units_l1': 3, 'dropout_1': 0.13442145520000726, 'n_units_l2': 2, 'dropout_2': 0.1427627219537342, 'learning_rate': 0.07364435645681836, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.18539695223172503.
[I 2023-09-19 01:31:30,967] Trial 3 finished with value: 0.18220013598601026 and parameters: {'n_layers': 2, 'n_units_l0': 4, 'dropout_0': 0.2743222890673215, 'n_units_l1': 10, 'dropout_1': 0.37896659433344415, 'learning_rate': 0.012789786317173679, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.18220013598601026.
[I 2023-09-19 01:31:39,570] Trial 4 finished with value: 0.16432278811931614 and parameters: {'n_layers': 2, 'n_units_l0': 7, 'dropout_0': 0.15675805130726464, 'n_units_l1': 6, 'dropout_1': 0.17497182864070535, 'learning_rate': 0.005842960489266215, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.16432278811931614.
[I 2023-09-19 01:31:39,807] Trial 5 pruned.
[I 2023-09-19 01:31:39,969] Trial 6 pruned.
[I 2023-09-19 01:31:48,902] Trial 7 finished with value: 0.1635267009337743 and parameters: {'n_layers': 3, 'n_units_l0': 3, 'dropout_0': 0.17166716525485481, 'n_units_l1': 10, 'dropout_1': 0.3812636657280336, 'n_units_l2': 9, 'dropout_2': 0.17554505337147683, 'learning_rate': 0.007452651334888088, 'optimizer': 'Adam'}. Best is trial 7 with value: 0.1635267009337743.
[I 2023-09-19 01:31:49,366] Trial 8 pruned.
[I 2023-09-19 01:31:49,604] Trial 9 pruned.
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_100.000_DMSO_0.025']
The dimensions of the data are: (29753, 1276)
Number of total missing values across all columns: 59506
Data Subset Is Off
Wells held out for testing: ['D14' 'J20']
Wells to use for training, validation, and testing ['D15' 'K14' 'K15' 'J16' 'J17' 'J21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 96.31011826544021
Validation Loss: 0.17383976722757022
Training Accuracy: 96.89135910634104
Training Loss: 0.13492240173252004
completed
