[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: SHSY5Y control_name: LPS_0.010_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_1.0_DMSO_0.025
[I 2023-09-19 00:52:59,388] A new study created in memory with name: no-name-28422fd9-24a9-47b9-929a-d6afd6041a5f
[I 2023-09-19 00:53:08,298] Trial 0 finished with value: 0.22596307620406153 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_0': 0.14740821152279168, 'n_units_l1': 6, 'dropout_1': 0.33509351127773945, 'n_units_l2': 5, 'dropout_2': 0.2744251133360873, 'learning_rate': 0.07005055659055694, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:53:16,885] Trial 1 finished with value: 0.6410453520218531 and parameters: {'n_layers': 3, 'n_units_l0': 5, 'dropout_0': 0.38708174923073824, 'n_units_l1': 7, 'dropout_1': 0.3544354809706296, 'n_units_l2': 7, 'dropout_2': 0.18438609830643352, 'learning_rate': 0.09086281469998447, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:53:25,792] Trial 2 finished with value: 0.6922643514474233 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_0': 0.26404249390144297, 'n_units_l1': 7, 'dropout_1': 0.39211388998812524, 'n_units_l2': 8, 'dropout_2': 0.35685504906750853, 'learning_rate': 0.008607181369810025, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:53:34,409] Trial 3 finished with value: 0.5519826330741247 and parameters: {'n_layers': 2, 'n_units_l0': 7, 'dropout_0': 0.35192792857185107, 'n_units_l1': 4, 'dropout_1': 0.2717787872754951, 'learning_rate': 0.0847758665998186, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:53:42,885] Trial 4 finished with value: 0.5406086483597756 and parameters: {'n_layers': 1, 'n_units_l0': 7, 'dropout_0': 0.2802741602602649, 'learning_rate': 0.09273374523321427, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:53:51,364] Trial 5 finished with value: 0.459364965558052 and parameters: {'n_layers': 2, 'n_units_l0': 3, 'dropout_0': 0.21161339945073523, 'n_units_l1': 10, 'dropout_1': 0.26569790029798923, 'learning_rate': 0.09929781928005747, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:53:51,604] Trial 6 pruned.
[I 2023-09-19 00:54:00,361] Trial 7 finished with value: 0.3328850777943929 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_0': 0.21707843779752783, 'n_units_l1': 9, 'dropout_1': 0.3455652109422395, 'n_units_l2': 6, 'dropout_2': 0.314888118023561, 'learning_rate': 0.0250905361730549, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:54:08,824] Trial 8 finished with value: 0.3536848979194959 and parameters: {'n_layers': 3, 'n_units_l0': 6, 'dropout_0': 0.2866098436085433, 'n_units_l1': 3, 'dropout_1': 0.1341058573449678, 'n_units_l2': 3, 'dropout_2': 0.26748447108936735, 'learning_rate': 0.0502091992040603, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.22596307620406153.
[I 2023-09-19 00:54:17,106] Trial 9 finished with value: 0.17624416614572205 and parameters: {'n_layers': 1, 'n_units_l0': 6, 'dropout_0': 0.25110040722282256, 'learning_rate': 0.08836892744443739, 'optimizer': 'SGD'}. Best is trial 9 with value: 0.17624416614572205.
Selected Catagories are:
['LPS_0.010_DMSO_0.025' 'LPS_Nigericin_1.000_1.0_DMSO_0.025']
The dimensions of the data are: (32077, 1276)
Number of total missing values across all columns: 31718
Data Subset Is Off
Wells held out for testing: ['B20' 'K16']
Wells to use for training, validation, and testing ['B16' 'B17' 'B21' 'K17' 'K20' 'K21']
Number of in features:  1251
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 93.2472049689441
Validation Loss: 0.17460610826810202
Training Accuracy: 93.92843827671913
Training Loss: 0.16718099728226657
completed
