[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: PBMC control_name: LPS_Nigericin_1.000_3.0_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025
[I 2023-09-19 00:58:19,451] A new study created in memory with name: no-name-4c7c2726-3d6a-494b-9789-3ad422734454
[I 2023-09-19 00:59:04,267] Trial 0 finished with value: 0.00047887373106831124 and parameters: {'n_layers': 1, 'n_units_l0': 5, 'dropout_0': 0.392479111212503, 'learning_rate': 0.00627316893227212, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.00047887373106831124.
[I 2023-09-19 00:59:50,662] Trial 1 finished with value: 0.0024794901728252276 and parameters: {'n_layers': 2, 'n_units_l0': 2, 'dropout_0': 0.2824142199027794, 'n_units_l1': 10, 'dropout_1': 0.21165735322279544, 'learning_rate': 0.03002501062324657, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.00047887373106831124.
[I 2023-09-19 01:00:40,449] Trial 2 finished with value: 2.8055483470783125e-07 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_0': 0.17507345160901328, 'n_units_l1': 3, 'dropout_1': 0.176855168398162, 'n_units_l2': 9, 'dropout_2': 0.276270914233018, 'learning_rate': 0.05688584041724647, 'optimizer': 'Adam'}. Best is trial 2 with value: 2.8055483470783125e-07.
[I 2023-09-19 01:01:27,125] Trial 3 finished with value: 6.294833252999836e-05 and parameters: {'n_layers': 1, 'n_units_l0': 6, 'dropout_0': 0.16502725073800933, 'learning_rate': 0.0396566536311375, 'optimizer': 'Adam'}. Best is trial 2 with value: 2.8055483470783125e-07.
[I 2023-09-19 01:02:14,005] Trial 4 finished with value: 0.6468184963409217 and parameters: {'n_layers': 2, 'n_units_l0': 7, 'dropout_0': 0.2754440802798173, 'n_units_l1': 2, 'dropout_1': 0.3056098379960785, 'learning_rate': 0.08455545417255023, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 2.8055483470783125e-07.
[I 2023-09-19 01:02:59,731] Trial 5 finished with value: 0.000391155495329128 and parameters: {'n_layers': 1, 'n_units_l0': 10, 'dropout_0': 0.3402241169295035, 'learning_rate': 0.014817784707782824, 'optimizer': 'SGD'}. Best is trial 2 with value: 2.8055483470783125e-07.
[I 2023-09-19 01:03:44,280] Trial 6 finished with value: 0.00010875875464664983 and parameters: {'n_layers': 1, 'n_units_l0': 2, 'dropout_0': 0.3153343697138399, 'learning_rate': 0.09615001405518607, 'optimizer': 'SGD'}. Best is trial 2 with value: 2.8055483470783125e-07.
[I 2023-09-19 01:04:31,731] Trial 7 finished with value: 3.259662955054832e-07 and parameters: {'n_layers': 2, 'n_units_l0': 4, 'dropout_0': 0.2337861515651614, 'n_units_l1': 10, 'dropout_1': 0.18280609561857764, 'learning_rate': 0.007983117623323734, 'optimizer': 'Adam'}. Best is trial 2 with value: 2.8055483470783125e-07.
[I 2023-09-19 01:05:21,087] Trial 8 finished with value: 3.424863971528773e-08 and parameters: {'n_layers': 3, 'n_units_l0': 8, 'dropout_0': 0.11689072667096251, 'n_units_l1': 8, 'dropout_1': 0.2391905434950656, 'n_units_l2': 10, 'dropout_2': 0.3001963592322605, 'learning_rate': 0.02573335213008303, 'optimizer': 'Adam'}. Best is trial 8 with value: 3.424863971528773e-08.
[I 2023-09-19 01:06:11,172] Trial 9 finished with value: 3.877698336433037e-08 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_0': 0.3586716647196483, 'n_units_l1': 5, 'dropout_1': 0.1907454405861883, 'n_units_l2': 7, 'dropout_2': 0.2800320708707198, 'learning_rate': 0.05688062887830564, 'optimizer': 'Adam'}. Best is trial 8 with value: 3.424863971528773e-08.
Selected Catagories are:
['LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (174261, 1270)
Number of total missing values across all columns: 0
Data Subset Is Off
Wells held out for testing: ['L09']
Wells to use for training, validation, and testing ['L02' 'L03' 'L08']
Number of in features:  1245
Number of out features:  1
Regression
cpu
Validation Loss: 2.0504785843378263e-08
Training Loss: 0.00023638588315729768
completed
