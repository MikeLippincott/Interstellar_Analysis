[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8867 bytes to Hyperparameter_Optimization_model_binary.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multiclass.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Writing 15027 bytes to Hyperparameter_Optimization_model_multiclass.py
[I 2023-12-01 11:31:02,341] A new study created in memory with name: no-name-57772a0f-1fe6-47fa-863a-716dd3fec8ef
[I 2023-12-01 12:10:36,544] Trial 0 finished with value: 0.6551316542429558 and parameters: {'n_layers': 5, 'n_units_l0': 24, 'dropout_0': 0.3570045074336897, 'n_units_l1': 38, 'dropout_1': 0.36398184969391933, 'n_units_l2': 44, 'dropout_2': 0.18194869091216828, 'n_units_l3': 18, 'dropout_3': 0.32239854135796486, 'n_units_l4': 20, 'dropout_4': 0.1044121470968986, 'learning_rate': 0.006109809255521686, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.6551316542429558.
[I 2023-12-01 12:52:33,177] Trial 1 finished with value: 0.9649391974909786 and parameters: {'n_layers': 7, 'n_units_l0': 45, 'dropout_0': 0.17287772302846338, 'n_units_l1': 44, 'dropout_1': 0.3899906645140224, 'n_units_l2': 13, 'dropout_2': 0.37486514207984234, 'n_units_l3': 47, 'dropout_3': 0.10732130159452126, 'n_units_l4': 10, 'dropout_4': 0.2845354764933669, 'n_units_l5': 13, 'dropout_5': 0.18156078234241962, 'n_units_l6': 12, 'dropout_6': 0.22299868981217957, 'learning_rate': 0.09268996652483523, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6551316542429558.
[I 2023-12-01 13:31:24,613] Trial 2 finished with value: 0.46896275387860564 and parameters: {'n_layers': 5, 'n_units_l0': 43, 'dropout_0': 0.10294025658055844, 'n_units_l1': 21, 'dropout_1': 0.3439691163975902, 'n_units_l2': 24, 'dropout_2': 0.3950363060756935, 'n_units_l3': 8, 'dropout_3': 0.13352596489357502, 'n_units_l4': 24, 'dropout_4': 0.31217896479641505, 'learning_rate': 0.09607281870636898, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.46896275387860564.
[I 2023-12-01 14:14:51,977] Trial 3 finished with value: 0.9633562804940066 and parameters: {'n_layers': 13, 'n_units_l0': 13, 'dropout_0': 0.333206166534157, 'n_units_l1': 15, 'dropout_1': 0.2685668550089416, 'n_units_l2': 44, 'dropout_2': 0.36705600030303487, 'n_units_l3': 5, 'dropout_3': 0.14164822726615497, 'n_units_l4': 22, 'dropout_4': 0.151257714647956, 'n_units_l5': 45, 'dropout_5': 0.2292538498322751, 'n_units_l6': 15, 'dropout_6': 0.38710154355416204, 'n_units_l7': 11, 'dropout_7': 0.13034103591083773, 'n_units_l8': 29, 'dropout_8': 0.21030828748270208, 'n_units_l9': 45, 'dropout_9': 0.3668329407542672, 'n_units_l10': 21, 'dropout_10': 0.35653591796663775, 'n_units_l11': 18, 'dropout_11': 0.3127157534113751, 'n_units_l12': 5, 'dropout_12': 0.2590033783260343, 'learning_rate': 0.07040836394999048, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.46896275387860564.
[I 2023-12-01 14:51:20,897] Trial 4 finished with value: 0.9776704681510628 and parameters: {'n_layers': 1, 'n_units_l0': 17, 'dropout_0': 0.31717233589787985, 'learning_rate': 0.09997970017196914, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.46896275387860564.
[I 2023-12-01 14:51:45,876] Trial 5 pruned. 
[I 2023-12-01 14:52:11,904] Trial 6 pruned. 
[I 2023-12-01 14:52:39,847] Trial 7 pruned. 
[I 2023-12-01 14:53:04,357] Trial 8 pruned. 
[I 2023-12-01 14:53:27,637] Trial 9 pruned. 
[I 2023-12-01 15:30:23,296] Trial 10 finished with value: 0.45323649339989963 and parameters: {'n_layers': 1, 'n_units_l0': 50, 'dropout_0': 0.23963699739450184, 'learning_rate': 0.04789104464878147, 'optimizer': 'SGD'}. Best is trial 10 with value: 0.45323649339989963.
[I 2023-12-01 16:07:14,563] Trial 11 finished with value: 0.45542105169539404 and parameters: {'n_layers': 1, 'n_units_l0': 49, 'dropout_0': 0.2514181454854828, 'learning_rate': 0.04003548749176618, 'optimizer': 'SGD'}. Best is trial 10 with value: 0.45323649339989963.
[I 2023-12-01 16:44:22,268] Trial 12 finished with value: 0.4569187712129047 and parameters: {'n_layers': 1, 'n_units_l0': 49, 'dropout_0': 0.24467301154184423, 'learning_rate': 0.04170231534830649, 'optimizer': 'SGD'}. Best is trial 10 with value: 0.45323649339989963.
[I 2023-12-01 16:44:44,997] Trial 13 pruned. 
[I 2023-12-01 17:22:10,036] Trial 14 finished with value: 0.5142586557588227 and parameters: {'n_layers': 2, 'n_units_l0': 38, 'dropout_0': 0.2635389746563476, 'n_units_l1': 2, 'dropout_1': 0.10197539664452049, 'learning_rate': 0.05547450012112759, 'optimizer': 'SGD'}. Best is trial 10 with value: 0.45323649339989963.
[I 2023-12-01 17:22:35,860] Trial 15 pruned. 
[I 2023-12-01 17:22:58,822] Trial 16 pruned. 
[I 2023-12-01 18:00:29,207] Trial 17 finished with value: 0.46567173121924443 and parameters: {'n_layers': 1, 'n_units_l0': 50, 'dropout_0': 0.2843605022483361, 'learning_rate': 0.029705558242872576, 'optimizer': 'SGD'}. Best is trial 10 with value: 0.45323649339989963.
[I 2023-12-01 18:00:53,104] Trial 18 pruned. 
[I 2023-12-01 18:01:19,750] Trial 19 pruned. 
[I 2023-12-01 18:01:42,024] Trial 20 pruned. 
[I 2023-12-01 18:39:09,110] Trial 21 finished with value: 0.45108362344945796 and parameters: {'n_layers': 1, 'n_units_l0': 49, 'dropout_0': 0.23709126524784732, 'learning_rate': 0.0487031595606109, 'optimizer': 'SGD'}. Best is trial 21 with value: 0.45108362344945796.
[I 2023-12-01 19:16:49,868] Trial 22 finished with value: 0.468786329822905 and parameters: {'n_layers': 2, 'n_units_l0': 45, 'dropout_0': 0.23486606568486845, 'n_units_l1': 9, 'dropout_1': 0.34146380787600894, 'learning_rate': 0.0491368527107173, 'optimizer': 'SGD'}. Best is trial 21 with value: 0.45108362344945796.
[I 2023-12-01 19:54:13,644] Trial 23 finished with value: 0.4502283377451532 and parameters: {'n_layers': 1, 'n_units_l0': 50, 'dropout_0': 0.2214649957141535, 'learning_rate': 0.062307615630293176, 'optimizer': 'SGD'}. Best is trial 23 with value: 0.4502283377451532.
[I 2023-12-01 19:54:37,140] Trial 24 pruned. 
[I 2023-12-01 20:32:40,826] Trial 25 finished with value: 0.44520926332591954 and parameters: {'n_layers': 2, 'n_units_l0': 45, 'dropout_0': 0.2006385296678047, 'n_units_l1': 28, 'dropout_1': 0.28117425438563115, 'learning_rate': 0.04936405686348102, 'optimizer': 'SGD'}. Best is trial 25 with value: 0.44520926332591954.
[I 2023-12-01 20:33:04,786] Trial 26 pruned. 
[I 2023-12-01 21:10:24,997] Trial 27 finished with value: 0.43927620386097993 and parameters: {'n_layers': 2, 'n_units_l0': 40, 'dropout_0': 0.15368175358046124, 'n_units_l1': 25, 'dropout_1': 0.27444511649773634, 'learning_rate': 0.0675200559551761, 'optimizer': 'SGD'}. Best is trial 27 with value: 0.43927620386097993.
[I 2023-12-01 21:10:48,117] Trial 28 pruned. 
[I 2023-12-01 21:11:12,553] Trial 29 pruned. 
[I 2023-12-01 21:11:35,994] Trial 30 pruned. 
[I 2023-12-01 21:49:21,236] Trial 31 finished with value: 0.4492704610480127 and parameters: {'n_layers': 2, 'n_units_l0': 46, 'dropout_0': 0.2225931132556789, 'n_units_l1': 19, 'dropout_1': 0.2908874772633285, 'learning_rate': 0.051272570356981156, 'optimizer': 'SGD'}. Best is trial 27 with value: 0.43927620386097993.
[I 2023-12-01 22:27:06,167] Trial 32 finished with value: 0.4463745339168368 and parameters: {'n_layers': 2, 'n_units_l0': 45, 'dropout_0': 0.17644044679531654, 'n_units_l1': 18, 'dropout_1': 0.2907709988093083, 'learning_rate': 0.05733250232896396, 'optimizer': 'SGD'}. Best is trial 27 with value: 0.43927620386097993.
[I 2023-12-01 23:04:21,975] Trial 33 finished with value: 0.4471346904518923 and parameters: {'n_layers': 2, 'n_units_l0': 43, 'dropout_0': 0.17359992543331493, 'n_units_l1': 19, 'dropout_1': 0.30356857459427045, 'learning_rate': 0.053998654738026314, 'optimizer': 'SGD'}. Best is trial 27 with value: 0.43927620386097993.
[I 2023-12-01 23:04:46,180] Trial 34 pruned. 
[I 2023-12-01 23:42:11,491] Trial 35 finished with value: 0.43679091745834514 and parameters: {'n_layers': 2, 'n_units_l0': 42, 'dropout_0': 0.15675565684762852, 'n_units_l1': 25, 'dropout_1': 0.27641451674002715, 'learning_rate': 0.06931090079700916, 'optimizer': 'SGD'}. Best is trial 35 with value: 0.43679091745834514.
[I 2023-12-01 23:42:35,154] Trial 36 pruned. 
[I 2023-12-01 23:42:59,016] Trial 37 pruned. 
[I 2023-12-02 00:03:02,276] Trial 38 pruned. 
[I 2023-12-02 00:03:24,804] Trial 39 pruned. 
[I 2023-12-02 00:03:49,813] Trial 40 pruned. 
[I 2023-12-02 00:41:01,174] Trial 41 finished with value: 0.44921770972587566 and parameters: {'n_layers': 2, 'n_units_l0': 42, 'dropout_0': 0.17571493608951547, 'n_units_l1': 19, 'dropout_1': 0.2942713251541287, 'learning_rate': 0.058252672932173316, 'optimizer': 'SGD'}. Best is trial 35 with value: 0.43679091745834514.
[I 2023-12-02 01:18:52,859] Trial 42 finished with value: 0.43184459619160764 and parameters: {'n_layers': 2, 'n_units_l0': 46, 'dropout_0': 0.1603687300169109, 'n_units_l1': 27, 'dropout_1': 0.2742865637905837, 'learning_rate': 0.06563803980083825, 'optimizer': 'SGD'}. Best is trial 42 with value: 0.43184459619160764.
[I 2023-12-02 01:19:17,024] Trial 43 pruned. 
[I 2023-12-02 01:19:39,542] Trial 44 pruned. 
[I 2023-12-02 01:20:07,367] Trial 45 pruned. 
[I 2023-12-02 01:20:31,297] Trial 46 pruned. 
[I 2023-12-02 01:20:57,238] Trial 47 pruned. 
[I 2023-12-02 01:58:28,993] Trial 48 finished with value: 0.4437526357705802 and parameters: {'n_layers': 2, 'n_units_l0': 45, 'dropout_0': 0.13464366300015618, 'n_units_l1': 15, 'dropout_1': 0.27023896210992226, 'learning_rate': 0.06275034201635765, 'optimizer': 'SGD'}. Best is trial 42 with value: 0.43184459619160764.
[I 2023-12-02 01:58:56,016] Trial 49 pruned. 
Data Subset Is Off
Wells held out for testing: ['C04' 'B05' 'B08' 'C08' 'D02' 'E03' 'D06' 'E06' 'E10' 'D11' 'D12' 'F03'
 'G03' 'F06' 'G07' 'G10' 'F11' 'H02' 'H06' 'I06' 'I09' 'I10' 'H11' 'J02'
 'J04' 'K09' 'K11' 'M02' 'L03' 'L04' 'M04' 'L05' 'M05' 'M11' 'N05' 'O05'
 'O08' 'N09' 'N10' 'O11' 'N12' 'O12']
Wells to use for training, validation, and testing ['B02' 'C02' 'B03' 'C03' 'B04' 'C04' 'B05' 'C05' 'B06' 'C06' 'B07' 'C07'
 'B08' 'C08' 'B09' 'C09' 'B10' 'C10' 'B11' 'C11' 'B12' 'C12' 'D02' 'E02'
 'D03' 'E03' 'D04' 'E04' 'D05' 'E05' 'D06' 'E06' 'D07' 'E07' 'D08' 'E08'
 'D09' 'E09' 'D10' 'E10' 'D11' 'E11' 'D12' 'E12' 'F02' 'G02' 'F03' 'G03'
 'F04' 'G04' 'F05' 'G05' 'F06' 'G06' 'F07' 'G07' 'F08' 'G08' 'F09' 'G09'
 'F10' 'G10' 'F11' 'G11' 'F12' 'G12' 'H02' 'I02' 'H03' 'I03' 'H04' 'I04'
 'H05' 'I05' 'H06' 'I06' 'H07' 'I07' 'H08' 'I08' 'H09' 'I09' 'H10' 'I10'
 'H11' 'I11' 'H12' 'I12' 'J02' 'K02' 'J03' 'K03' 'J04' 'K04' 'J05' 'K05'
 'J06' 'K06' 'J07' 'K07' 'J08' 'K08' 'J09' 'K09' 'J10' 'K10' 'J11' 'K11'
 'J12' 'K12' 'L02' 'M02' 'L03' 'M03' 'L04' 'M04' 'L05' 'M05' 'L06' 'M06'
 'L07' 'M07' 'L08' 'M08' 'L09' 'M09' 'L10' 'M10' 'L11' 'M11' 'L12' 'M12'
 'N02' 'O02' 'N03' 'O03' 'N04' 'O04' 'N05' 'O05' 'N06' 'O06' 'N07' 'O07'
 'N08' 'O08' 'N09' 'O09' 'N10' 'O10' 'N11' 'O11' 'N12' 'O12']
(419412, 1271) (224861, 1271) (3417566, 1271)
Shape for the 100% test set: (419412, 1271)

Shape for the 75% test set: (56215, 1271);
Shape for the 75% train set: (168646, 1271)

Shape for the 50% test set: (1708783, 1271);
Shape for the 50% train set: (1708783, 1271)
Shape for the holdout set: (1536843, 1271)

    Testing set length: 1877429

    Training set length: 1411998

    Validation set length: 353000

    Treatment Holdout set length: 419412

    Holdout set length: 1536843
(1411998,) (353000,) (1877429,) (419412,) (1536843,)
5598682
../indexes/PBMC/multi_class
['apoptosis' 'healthy' 'pyroptosis'] [ 315440 2566195 2717047]
[0, 1, 2] [0.9436581681188537, 0.5416430152668075, 0.5146988166143389]
Number of in features:  1245
Number of out features:  3
Multi_Class
cpu
Validation Accuracy: 83.12973654390935
Validation Loss: 0.43197680045795167
Training Accuracy: 81.65865886495591
Training Loss: 0.46807854194964454
Done
