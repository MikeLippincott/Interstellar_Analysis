[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8867 bytes to Hyperparameter_Optimization_model_binary.py
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_multiclass.ipynb to script
/projects/mlippincott@xsede.org/software/anaconda/envs/Interstellar_python/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Writing 14974 bytes to Hyperparameter_Optimization_model_multiclass.py
[I 2023-11-22 13:07:40,342] A new study created in memory with name: no-name-1c83e17c-8435-4d03-b65b-0beb19f05525
[I 2023-11-22 13:48:53,731] Trial 0 finished with value: 0.9616216653231987 and parameters: {'n_layers': 2, 'n_units_l0': 42, 'dropout_0': 0.28471794997145905, 'n_units_l1': 33, 'dropout_1': 0.11735222234195503, 'learning_rate': 0.0880723689988763, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.9616216653231987.
[I 2023-11-22 14:35:07,321] Trial 1 finished with value: 0.9595793279643969 and parameters: {'n_layers': 9, 'n_units_l0': 3, 'dropout_0': 0.3734518779751973, 'n_units_l1': 12, 'dropout_1': 0.2262109914152445, 'n_units_l2': 32, 'dropout_2': 0.31880753137254547, 'n_units_l3': 7, 'dropout_3': 0.357369897733683, 'n_units_l4': 2, 'dropout_4': 0.16305680165122372, 'n_units_l5': 21, 'dropout_5': 0.25032065640452156, 'n_units_l6': 48, 'dropout_6': 0.34218749751159017, 'n_units_l7': 44, 'dropout_7': 0.39491133368163556, 'n_units_l8': 34, 'dropout_8': 0.21861164463865573, 'learning_rate': 0.015645552881548435, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.9595793279643969.
[I 2023-11-22 15:23:21,744] Trial 2 finished with value: 0.9617491950773485 and parameters: {'n_layers': 8, 'n_units_l0': 16, 'dropout_0': 0.291407263671144, 'n_units_l1': 36, 'dropout_1': 0.3501249044582029, 'n_units_l2': 24, 'dropout_2': 0.30897037397137767, 'n_units_l3': 3, 'dropout_3': 0.1643223514689655, 'n_units_l4': 35, 'dropout_4': 0.2785279933969762, 'n_units_l5': 39, 'dropout_5': 0.3995740120556067, 'n_units_l6': 37, 'dropout_6': 0.2692317913264044, 'n_units_l7': 35, 'dropout_7': 0.1803769063826186, 'learning_rate': 0.094016379839076, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.9595793279643969.
[I 2023-11-22 16:07:07,476] Trial 3 finished with value: 0.7929626210825141 and parameters: {'n_layers': 8, 'n_units_l0': 16, 'dropout_0': 0.24375790267897654, 'n_units_l1': 17, 'dropout_1': 0.18754953447551298, 'n_units_l2': 41, 'dropout_2': 0.2669608478593044, 'n_units_l3': 5, 'dropout_3': 0.3846626027719504, 'n_units_l4': 32, 'dropout_4': 0.2925616481970422, 'n_units_l5': 18, 'dropout_5': 0.38582481444515315, 'n_units_l6': 21, 'dropout_6': 0.2834300482743479, 'n_units_l7': 20, 'dropout_7': 0.13983849373280138, 'learning_rate': 0.06936025108685799, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.7929626210825141.
[I 2023-11-22 16:46:53,766] Trial 4 finished with value: 0.9618797845274932 and parameters: {'n_layers': 8, 'n_units_l0': 7, 'dropout_0': 0.12176618089010538, 'n_units_l1': 18, 'dropout_1': 0.15606926850784555, 'n_units_l2': 33, 'dropout_2': 0.3619502900994852, 'n_units_l3': 16, 'dropout_3': 0.14969421450290377, 'n_units_l4': 2, 'dropout_4': 0.33070877165059526, 'n_units_l5': 18, 'dropout_5': 0.12560810462896288, 'n_units_l6': 8, 'dropout_6': 0.2758321721194716, 'n_units_l7': 14, 'dropout_7': 0.23279838068964928, 'learning_rate': 0.09899722791442542, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.7929626210825141.
[I 2023-11-22 17:32:52,200] Trial 5 finished with value: 0.6989767051803456 and parameters: {'n_layers': 9, 'n_units_l0': 23, 'dropout_0': 0.2816440793960594, 'n_units_l1': 38, 'dropout_1': 0.28551516520369896, 'n_units_l2': 23, 'dropout_2': 0.3417130170573698, 'n_units_l3': 12, 'dropout_3': 0.3281609713444853, 'n_units_l4': 47, 'dropout_4': 0.19240391351430147, 'n_units_l5': 27, 'dropout_5': 0.17580057084583683, 'n_units_l6': 8, 'dropout_6': 0.30282262513072056, 'n_units_l7': 24, 'dropout_7': 0.1028978581587478, 'n_units_l8': 15, 'dropout_8': 0.2363734299649093, 'learning_rate': 0.042878316582651774, 'optimizer': 'SGD'}. Best is trial 5 with value: 0.6989767051803456.
[I 2023-11-22 17:36:33,103] Trial 6 pruned. 
[I 2023-11-22 18:18:28,681] Trial 7 finished with value: 0.4880044647882568 and parameters: {'n_layers': 3, 'n_units_l0': 40, 'dropout_0': 0.24339488651586386, 'n_units_l1': 40, 'dropout_1': 0.3668011413415909, 'n_units_l2': 45, 'dropout_2': 0.2995211060168543, 'learning_rate': 0.0019387574506093785, 'optimizer': 'Adam'}. Best is trial 7 with value: 0.4880044647882568.
[I 2023-11-22 18:18:56,682] Trial 8 pruned. 
[I 2023-11-22 18:19:18,325] Trial 9 pruned. 
[I 2023-11-22 19:03:47,218] Trial 10 finished with value: 0.4497666071704776 and parameters: {'n_layers': 4, 'n_units_l0': 48, 'dropout_0': 0.18476212682866783, 'n_units_l1': 50, 'dropout_1': 0.2991048776941432, 'n_units_l2': 50, 'dropout_2': 0.16622998757408347, 'n_units_l3': 31, 'dropout_3': 0.11501246720943004, 'learning_rate': 0.0008869334872927007, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 19:47:15,605] Trial 11 finished with value: 0.45749471672220693 and parameters: {'n_layers': 4, 'n_units_l0': 50, 'dropout_0': 0.18509202115402765, 'n_units_l1': 48, 'dropout_1': 0.3084137145425656, 'n_units_l2': 49, 'dropout_2': 0.18256026290230618, 'n_units_l3': 30, 'dropout_3': 0.1014694027213598, 'learning_rate': 8.253688476181006e-05, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 20:35:36,653] Trial 12 finished with value: 0.47344218232641316 and parameters: {'n_layers': 5, 'n_units_l0': 50, 'dropout_0': 0.1573051405658173, 'n_units_l1': 50, 'dropout_1': 0.2986919822107498, 'n_units_l2': 50, 'dropout_2': 0.16795653888014483, 'n_units_l3': 31, 'dropout_3': 0.11383798709947443, 'n_units_l4': 47, 'dropout_4': 0.10114911638990462, 'learning_rate': 0.00232529620326936, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 20:46:31,399] Trial 13 pruned. 
[I 2023-11-22 20:47:24,235] Trial 14 pruned. 
[I 2023-11-22 21:32:29,479] Trial 15 finished with value: 0.48646851753529835 and parameters: {'n_layers': 4, 'n_units_l0': 43, 'dropout_0': 0.1965831019435627, 'n_units_l1': 27, 'dropout_1': 0.2518826510006723, 'n_units_l2': 16, 'dropout_2': 0.21777218757958772, 'n_units_l3': 38, 'dropout_3': 0.10921906966980528, 'learning_rate': 0.001070429949848886, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 21:33:00,914] Trial 16 pruned. 
[I 2023-11-22 21:33:26,962] Trial 17 pruned. 
[I 2023-11-22 22:09:46,762] Trial 18 finished with value: 0.5960302733820725 and parameters: {'n_layers': 1, 'n_units_l0': 38, 'dropout_0': 0.1354532250531436, 'learning_rate': 0.03254504683403747, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 22:10:18,720] Trial 19 pruned. 
[I 2023-11-22 22:10:41,613] Trial 20 pruned. 
[I 2023-11-22 22:59:54,043] Trial 21 finished with value: 0.504654044000447 and parameters: {'n_layers': 6, 'n_units_l0': 50, 'dropout_0': 0.16393143894957976, 'n_units_l1': 50, 'dropout_1': 0.3048512585626323, 'n_units_l2': 49, 'dropout_2': 0.17754960936957187, 'n_units_l3': 35, 'dropout_3': 0.10243108601732169, 'n_units_l4': 49, 'dropout_4': 0.1259938755379255, 'n_units_l5': 11, 'dropout_5': 0.33545354454665194, 'learning_rate': 0.003019712160768196, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 23:51:55,551] Trial 22 finished with value: 0.4736213426906464 and parameters: {'n_layers': 6, 'n_units_l0': 46, 'dropout_0': 0.21675232415899978, 'n_units_l1': 50, 'dropout_1': 0.2993741291650409, 'n_units_l2': 50, 'dropout_2': 0.14579345460894144, 'n_units_l3': 31, 'dropout_3': 0.12474928661617833, 'n_units_l4': 42, 'dropout_4': 0.1055507238398647, 'n_units_l5': 48, 'dropout_5': 0.21098527064980913, 'learning_rate': 0.0009038937837751877, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-22 23:56:42,560] Trial 23 pruned. 
[I 2023-11-22 23:57:58,377] Trial 24 pruned. 
[I 2023-11-22 23:58:27,120] Trial 25 pruned. 
[I 2023-11-23 00:43:46,771] Trial 26 finished with value: 0.4661430550465044 and parameters: {'n_layers': 4, 'n_units_l0': 45, 'dropout_0': 0.22455775709677467, 'n_units_l1': 35, 'dropout_1': 0.2779226022111224, 'n_units_l2': 30, 'dropout_2': 0.1556405702764461, 'n_units_l3': 50, 'dropout_3': 0.10101771762037366, 'learning_rate': 0.0002484514129424052, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.4497666071704776.
[I 2023-11-23 00:47:50,506] Trial 27 pruned. 
[I 2023-11-23 00:48:22,224] Trial 28 pruned. 
[I 2023-11-23 00:51:37,236] Trial 29 pruned. 
[I 2023-11-23 00:52:03,125] Trial 30 pruned. 
[I 2023-11-23 01:40:06,215] Trial 31 finished with value: 0.44376139192407665 and parameters: {'n_layers': 5, 'n_units_l0': 50, 'dropout_0': 0.14685140414783332, 'n_units_l1': 47, 'dropout_1': 0.2963881472980839, 'n_units_l2': 41, 'dropout_2': 0.16870633933287194, 'n_units_l3': 30, 'dropout_3': 0.10127871340606484, 'n_units_l4': 44, 'dropout_4': 0.1740009582121554, 'learning_rate': 0.00047797837753439166, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.44376139192407665.
[I 2023-11-23 01:48:20,364] Trial 32 pruned. 
[I 2023-11-23 01:49:19,154] Trial 33 pruned. 
[I 2023-11-23 01:50:29,559] Trial 34 pruned. 
[I 2023-11-23 02:38:47,388] Trial 35 finished with value: 0.44454331507901296 and parameters: {'n_layers': 5, 'n_units_l0': 48, 'dropout_0': 0.1835540959981868, 'n_units_l1': 47, 'dropout_1': 0.10027117619786924, 'n_units_l2': 37, 'dropout_2': 0.1657450987043529, 'n_units_l3': 29, 'dropout_3': 0.13426413092309786, 'n_units_l4': 44, 'dropout_4': 0.25628598077997505, 'learning_rate': 0.0012933703158689185, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.44376139192407665.
[I 2023-11-23 02:41:19,143] Trial 36 pruned. 
[I 2023-11-23 02:41:50,044] Trial 37 pruned. 
[I 2023-11-23 02:42:16,156] Trial 38 pruned. 
[I 2023-11-23 02:42:50,893] Trial 39 pruned. 
[I 2023-11-23 02:43:20,108] Trial 40 pruned. 
[I 2023-11-23 03:24:21,850] Trial 41 finished with value: 0.4561237711019593 and parameters: {'n_layers': 3, 'n_units_l0': 45, 'dropout_0': 0.18177527307430405, 'n_units_l1': 47, 'dropout_1': 0.26549339622795326, 'n_units_l2': 31, 'dropout_2': 0.16393426576530934, 'learning_rate': 0.0019517160186087778, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.44376139192407665.
[I 2023-11-23 04:05:42,318] Trial 42 finished with value: 0.45613528693863004 and parameters: {'n_layers': 3, 'n_units_l0': 50, 'dropout_0': 0.15731090395830008, 'n_units_l1': 47, 'dropout_1': 0.20065969610889248, 'n_units_l2': 42, 'dropout_2': 0.16973568558587449, 'learning_rate': 0.0042601373629244715, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.44376139192407665.
[I 2023-11-23 04:47:52,977] Trial 43 finished with value: 0.44632584205695575 and parameters: {'n_layers': 3, 'n_units_l0': 47, 'dropout_0': 0.14482393814513642, 'n_units_l1': 44, 'dropout_1': 0.10011371622653331, 'n_units_l2': 33, 'dropout_2': 0.16536409851607523, 'learning_rate': 0.004970773084018924, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.44376139192407665.
[I 2023-11-23 05:23:33,965] Trial 44 finished with value: 0.477233327575771 and parameters: {'n_layers': 1, 'n_units_l0': 40, 'dropout_0': 0.1368816016822853, 'learning_rate': 0.005456033414823702, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.44376139192407665.
[I 2023-11-23 05:24:25,417] Trial 45 pruned. 
[I 2023-11-23 05:24:48,565] Trial 46 pruned. 
[I 2023-11-23 05:25:10,026] Trial 47 pruned. 
[I 2023-11-23 05:31:03,305] Trial 48 pruned. 
[I 2023-11-23 05:31:26,009] Trial 49 pruned. 
Data Subset Is Off
Wells held out for testing: ['C04' 'B05' 'B08' 'C08' 'D02' 'E03' 'D06' 'E06' 'E10' 'D11' 'D12' 'F03'
 'G03' 'F06' 'G07' 'G10' 'F11' 'H02' 'H06' 'I06' 'I09' 'I10' 'H11' 'J02'
 'J04' 'K09' 'K11' 'M02' 'L03' 'L04' 'M04' 'L05' 'M05' 'M11' 'N05' 'O05'
 'O08' 'N09' 'N10' 'O11' 'N12' 'O12']
Wells to use for training, validation, and testing ['B02' 'C02' 'B03' 'C03' 'B04' 'C04' 'B05' 'C05' 'B06' 'C06' 'B07' 'C07'
 'B08' 'C08' 'B09' 'C09' 'B10' 'C10' 'B11' 'C11' 'B12' 'C12' 'D02' 'E02'
 'D03' 'E03' 'D04' 'E04' 'D05' 'E05' 'D06' 'E06' 'D07' 'E07' 'D08' 'E08'
 'D09' 'E09' 'D10' 'E10' 'D11' 'E11' 'D12' 'E12' 'F02' 'G02' 'F03' 'G03'
 'F04' 'G04' 'F05' 'G05' 'F06' 'G06' 'F07' 'G07' 'F08' 'G08' 'F09' 'G09'
 'F10' 'G10' 'F11' 'G11' 'F12' 'G12' 'H02' 'I02' 'H03' 'I03' 'H04' 'I04'
 'H05' 'I05' 'H06' 'I06' 'H07' 'I07' 'H08' 'I08' 'H09' 'I09' 'H10' 'I10'
 'H11' 'I11' 'H12' 'I12' 'J02' 'K02' 'J03' 'K03' 'J04' 'K04' 'J05' 'K05'
 'J06' 'K06' 'J07' 'K07' 'J08' 'K08' 'J09' 'K09' 'J10' 'K10' 'J11' 'K11'
 'J12' 'K12' 'L02' 'M02' 'L03' 'M03' 'L04' 'M04' 'L05' 'M05' 'L06' 'M06'
 'L07' 'M07' 'L08' 'M08' 'L09' 'M09' 'L10' 'M10' 'L11' 'M11' 'L12' 'M12'
 'N02' 'O02' 'N03' 'O03' 'N04' 'O04' 'N05' 'O05' 'N06' 'O06' 'N07' 'O07'
 'N08' 'O08' 'N09' 'O09' 'N10' 'O10' 'N11' 'O11' 'N12' 'O12']
(240721, 1271) (224861, 1271) (3596257, 1271)
Shape for the 100% test set: (240721, 1271)

Shape for the 75% test set: (56215, 1271);
Shape for the 75% train set: (168646, 1271)

Shape for the 50% test set: (1798128, 1271);
Shape for the 50% train set: (1798129, 1271)
Shape for the holdout set: (1536843, 1271)

    Testing set length: 1966775

    Training set length: 1483474

    Validation set length: 370869

    Treatment Holdout set length: 240721

    Holdout set length: 1536843
(1483474,) (370869,) (1966775,) (240721,) (1536843,)
5598682
../indexes/PBMC/multi_class
['apoptosis' 'healthy' 'pyroptosis'] [ 315440 2662110 2621132]
[0, 1, 2] [0.9436581681188537, 0.5245113046249099, 0.5318305272562364]
Number of in features:  1245
Number of out features:  3
Multi_Class
cpu
Validation Accuracy: 82.60060560467441
Validation Loss: 0.4477702382672828
Training Accuracy: 80.91891061117352
Training Loss: 0.47595551741271813
Done
