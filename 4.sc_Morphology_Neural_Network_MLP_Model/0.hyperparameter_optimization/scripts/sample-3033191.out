[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook ../notebooks/Hyperparameter_Optimization_model_binary.ipynb to script
[NbConvertApp] Writing 8579 bytes to Hyperparameter_Optimization_model_binary.py
cell_type: PBMC control_name: Thapsigargin_1.000_DMSO_0.025 treatment_name: LPS_Nigericin_1.000_3.0_DMSO_0.025
[I 2023-09-19 01:00:34,933] A new study created in memory with name: no-name-77eb5958-4849-4f76-acac-32a1b33b0c1f
[I 2023-09-19 01:02:05,019] Trial 0 finished with value: 0.3701970278978348 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_0': 0.3977029045500321, 'n_units_l1': 5, 'dropout_1': 0.3520367568775291, 'n_units_l2': 8, 'dropout_2': 0.3181548684631542, 'learning_rate': 0.037407199106737454, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.3701970278978348.
[I 2023-09-19 01:03:30,956] Trial 1 finished with value: 0.690665252161026 and parameters: {'n_layers': 2, 'n_units_l0': 6, 'dropout_0': 0.25597216896581115, 'n_units_l1': 2, 'dropout_1': 0.23810068552456326, 'learning_rate': 0.09275514159739132, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.3701970278978348.
[I 2023-09-19 01:04:55,277] Trial 2 finished with value: 0.19699770569205285 and parameters: {'n_layers': 2, 'n_units_l0': 4, 'dropout_0': 0.17098747872700967, 'n_units_l1': 9, 'dropout_1': 0.2612452196279603, 'learning_rate': 0.011499783117482894, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.19699770569205285.
[I 2023-09-19 01:06:24,507] Trial 3 finished with value: 0.14961246028542521 and parameters: {'n_layers': 3, 'n_units_l0': 8, 'dropout_0': 0.22154474057425974, 'n_units_l1': 8, 'dropout_1': 0.29594052717166086, 'n_units_l2': 2, 'dropout_2': 0.2891428113466629, 'learning_rate': 0.03305937953686763, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.14961246028542521.
[I 2023-09-19 01:07:55,257] Trial 4 finished with value: 0.12173500993251801 and parameters: {'n_layers': 3, 'n_units_l0': 10, 'dropout_0': 0.34405279688711266, 'n_units_l1': 10, 'dropout_1': 0.11184943690658007, 'n_units_l2': 5, 'dropout_2': 0.22353778552866765, 'learning_rate': 0.004100201010460868, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.12173500993251801.
[I 2023-09-19 01:09:20,348] Trial 5 finished with value: 0.1410075411379337 and parameters: {'n_layers': 1, 'n_units_l0': 9, 'dropout_0': 0.15214641277440077, 'learning_rate': 0.07328958383818979, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.12173500993251801.
[I 2023-09-19 01:09:22,089] Trial 6 pruned.
[I 2023-09-19 01:09:55,763] Trial 7 pruned.
[I 2023-09-19 01:10:51,411] Trial 8 pruned.
[I 2023-09-19 01:10:52,983] Trial 9 pruned.
Selected Catagories are:
['Thapsigargin_1.000_DMSO_0.025' 'LPS_Nigericin_1.000_3.0_DMSO_0.025']
The dimensions of the data are: (322717, 1270)
Number of total missing values across all columns: 296912
Data Subset Is Off
Wells held out for testing: ['K06' 'L09']
Wells to use for training, validation, and testing ['D06' 'D07' 'K07' 'L02' 'L03' 'L08']
Number of in features:  1245
Number of out features:  2
Binary_Classification
cpu
Validation Accuracy: 95.1623287115868
Validation Loss: 0.12341675854921343
Training Accuracy: 93.63219614043622
Training Loss: 0.16816570345876794
completed
